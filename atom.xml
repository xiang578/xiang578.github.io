<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>算法花园</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xiang578.com/"/>
  <updated>2020-09-06T09:59:06.824Z</updated>
  <id>https://xiang578.com/</id>
  
  <author>
    <name>RyenX</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>李宏毅强化学习课程笔记 Imitation Learning</title>
    <link href="https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html"/>
    <id>https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html</id>
    <published>2020-09-06T15:14:47.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<p>apprenticeship learning</p><ol type="1"><li>无法从环境中获得 reward。</li><li>某些任务中很难定义 reward。</li><li>人为设计的奖励可能导致意外的行为。</li></ol><p>学习专家的行为。</p><h2 id="behavior-cloning">Behavior Cloning</h2><p>监督学习，但是样本有限。</p><p>Dataset Aggregation</p><ol type="1"><li>通过行为克隆得到 actor <span class="math inline">\(\pi_1\)</span></li><li>利用 <span class="math inline">\(\pi_1\)</span> 和环境交互得到一些新的样本</li><li>由专家对上一步采样得到的样本进行标注</li><li>利用新得到的样本训练 <span class="math inline">\(\pi_2\)</span></li></ol><p>如果机器的学习能力有限，可能复制专家多余无用的动作。监督学习无法区分哪些是需要学习、哪些是需要忽视的行为。</p><h3 id="miss-match">Miss match</h3><p>监督学习中，我们假设训练数据和测试数据有相同的分布。Behavior Cloning 中可能分布不同。</p><p><img src="https://media.xiang578.com/15733541795048.jpg"></p><h2 id="inverse-reinfofcement-learning">Inverse Reinfofcement Learning</h2><p>反向强化学习 没有 reward 函数，通过专家和环境互动学到一个 reward function，然后再训练 actor。 <img src="https://media.xiang578.com/15733545279563.jpg"></p><p>类似于 GAN 的训练方法（actor 换成 generator，reward function 换成 discriminator）。 学到 actor 的 pi 后，调整 reward function，保证专家的行为得分大于学到的行为。</p><p><img src="https://media.xiang578.com/15733546273431.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;apprenticeship learning&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;无法从环境中获得 reward。&lt;/li&gt;
&lt;li&gt;某些任务中很难定义 reward。&lt;/li&gt;
&lt;li&gt;人为设计的奖励可能导致意外的行为。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;学习专家的行
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="algorithm" scheme="https://xiang578.com/tags/algorithm/"/>
    
      <category term="reinforcement-learning" scheme="https://xiang578.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 Sparse Reward</title>
    <link href="https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html"/>
    <id>https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html</id>
    <published>2020-09-06T14:14:47.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<h2 id="reward-shaping">Reward Shaping</h2><p>如果 reward 分布非常稀疏的时候，actor 会很难学习，所以刻意设计 reward 引导模型学习。</p><h3 id="curiosity-intrinsic-curiosity-module-icm">Curiosity Intrinsic Curiosity module (ICM)</h3><p>在原来 Reward 函数的基础上，引入 ICM 函数。ICM 鼓励模型去探索新的动作。最后 ICM 和 Reward 和越大越好。</p><p><img src="https://media.xiang578.com/15732008339393.jpg"></p><p>鼓励探索新动作之后，会导致系统风险变大。对比预测的下一个状态和真正的状态的差异程度进行抑制。</p><p><img src="https://media.xiang578.com/15732012078318.jpg"></p><p>Feature Ext 对状态进行抽取，过滤没有意义的内容。 Network 1 预测下一个状态，然后再和真实状态计算 diff 程度。 Network 2 预测 action，和真实的 action 进行对比。如果两个 action 接近，说明 f 可以进行特征提取。重要程度计算。</p><p><img src="https://media.xiang578.com/15732013249053.jpg"></p><h3 id="curriculum-learning">Curriculum Learning</h3><p>规划学习路线，从简单任务学习。</p><p>Reverse Curriculum Generation</p><p><img src="https://media.xiang578.com/15732019855111.jpg"></p><p><img src="https://media.xiang578.com/15732020592680.jpg"></p><h3 id="hierarchical-reinforcement-learning">Hierarchical Reinforcement Learning</h3><p>对 agent 分层，高层负责定目标，分配给底层 agent 执行。如果低一层的agent没法达到目标，那么高一层的agent会受到惩罚（高层agent将自己的愿景传达给底层agent）。</p><p>如果一个agent到了一个错误的目标，那就假设最初的目标本来就是一个错误的目标（保证已经实现的成果不被浪费）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;reward-shaping&quot;&gt;Reward Shaping&lt;/h2&gt;
&lt;p&gt;如果 reward 分布非常稀疏的时候，actor 会很难学习，所以刻意设计 reward 引导模型学习。&lt;/p&gt;
&lt;h3 id=&quot;curiosity-intrinsic-curiosi
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="algorithm" scheme="https://xiang578.com/tags/algorithm/"/>
    
      <category term="reinforcement-learning" scheme="https://xiang578.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 Actor Critic</title>
    <link href="https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html"/>
    <id>https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html</id>
    <published>2020-09-06T13:14:47.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<h2 id="info">Info</h2><p>课件下载：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">Hung-yi Lee - Deep Reinforcement Learning</a></p><p>课程视频：<a href="https://www.youtube.com/watch?v=z95ZYgPgXOY&amp;list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_" target="_blank" rel="noopener">DRL Lecture 1: Policy Gradient (Review) - YouTube</a></p><ul><li>Change Log<ul><li>上半部分笔记见：<a href="https://xiang578.com/post/reinforce-learnning-basic.html">李宏毅强化学习课程笔记 上| 算法花园</a></li><li>20200906： 整理 Actor Critic 相关内容</li></ul></li></ul><h2 id="actor-critic">Actor Critic</h2><h3 id="policy-gradient">policy gradient</h3><ul><li>给定在某个 state 采取某个 action 的概率。</li><li>baseline b 的作用是保证 reward 大的样本有更大的概率被采样到。</li><li>从当前时间点累加 reward，并且当前 action 对后面的 reward 影响很小，添加折扣系数。</li><li>PG 效果受到采样数量和质量影响。</li></ul><p><img src="https://media.xiang578.com/15731332476541.jpg"></p><h3 id="q-learning">Q-learning</h3><p>状态价值函数 <span class="math inline">\(V^{\pi}(s)\)</span> 状态行动价值函数 <span class="math inline">\(Q^{\pi}(s,a)\)</span></p><p><img src="https://media.xiang578.com/15731335120798.jpg"></p><h3 id="actor-critic-1">Actor-Critic</h3><p>用 V 和 Q 替换 PG 中的累积 reward 和 baseline。新的模型需要训练两个网络，比较困难。</p><p><img src="https://media.xiang578.com/15731338724158.jpg"></p><h3 id="advantage-actor-critic">Advantage Actor-Critic</h3><p>用 V 去替代 Q，能降低模型整体方差（MC 到 TD)。最下面两个公式转化是由实验得到。</p><p><img src="https://media.xiang578.com/15731340046945.jpg"></p><p>训练过程：</p><p><img src="https://media.xiang578.com/15731341766172.jpg"></p><p>tip:</p><ol type="1"><li>actor 和 critic 具有相同的输入 s，可以共享部分网络结构。</li><li>output entropy 作为 pi 的正则项，entropy 越大采样效果越好。</li></ol><p><img src="https://media.xiang578.com/15731342994690.jpg"></p><h3 id="asynchronous-advantage-acotr-critic-a3c">Asynchronous Advantage Acotr-Critic A3C</h3><ol type="1"><li>利用多个 worker 去训练。</li><li>每个 worker 复制主模型的参数。</li><li>每个模型单独采样，并且计算梯度。</li><li>更新全局参数。</li></ol><h3 id="pathwise-derivative-policy-gradient">Pathwise derivative policy gradient</h3><p>该网络不仅仅告诉 actor 某一个 action 的好坏，还告诉 actor 应该返回哪一个 action。</p><p><img src="https://media.xiang578.com/15731346701840.jpg"></p><p>将这个 actor 返回的 action 和 state 一起输入到一个固定的 Q，利用梯度上升更新 actor。</p><p><img src="https://media.xiang578.com/15731348191409.jpg"></p><p>完整的训练过程和 conditional GAN 类似， actor 是 generator，Q 是 discriminator。</p><p><img src="https://media.xiang578.com/15731350424130.jpg"></p><p>算法：</p><ol type="1"><li>action 由训练的 actor 决定</li><li>利用 s 和 a 更新 Q <img src="https://media.xiang578.com/15731351315860.jpg"></li></ol><h3 id="gan-和-ac-方法对比">GAN 和 AC 方法对比</h3><p><img src="https://media.xiang578.com/15731353632450.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;info&quot;&gt;Info&lt;/h2&gt;
&lt;p&gt;课件下载：&lt;a href=&quot;http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hung-yi Lee
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="algorithm" scheme="https://xiang578.com/tags/algorithm/"/>
    
      <category term="reinforcement-learning" scheme="https://xiang578.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Never-Reading 202007 互联网商业模式</title>
    <link href="https://xiang578.com/post/Never-Reading-202007.html"/>
    <id>https://xiang578.com/post/Never-Reading-202007.html</id>
    <published>2020-08-08T15:55:11.000Z</published>
    <updated>2020-09-06T09:59:06.820Z</updated>
    
    <content type="html"><![CDATA[<p>不知不觉中每月分享已经进行半年，不过前 6 期都没有想到取什么名字。上期的标题「Never Reading」来自稍后读列表名称，仔细一想不正好成为每月分享的名字吗？而且还有致敬「Λ-Reading」的成分。</p><h2 id="互联网商业模式">互联网商业模式</h2><p>在 <a href="http://xiang578.com/post/monthly-issue-202006.html">202006 Never Reading</a> 中摘录过「即刻半月刊」的一段内容：</p><ul><li>所谓“商业模式”其实指的是这家公司的“价值创造模式”，即用什么样的模型创造了更多价值。<ul><li>世界上现存所有的商业模式无非三种，一是[[边际效应]]（规模效应/协同效应），二是[[双边效应]]，三是梅特卡夫[[网络效应]]。不同的价值创造模型，带来不同的增长动力，继而带来不同的货币化方法。</li><li>滴滴是什么模型？</li></ul></li></ul><p>当时没有找到上面这一段内容的解释，只是觉得有点神奇就记录下来。这个月收听「三五环」中刘飞和少楠关于交易平台两期内容（14、17），其中提到许小年教授的一本书「商业的本质和互联网」。在书中详细的介绍商业平台的效应，感兴趣的可以找来详细的阅读。</p><ul><li>[[规模效应]]：[[边际成本]]越低和边际收益越高</li><li>[[协同效应]]：依赖于品种增加带来的 1+1 &gt; 2<ul><li>百货公司拥有协同效应</li><li>#problem 协同效应失败的例子？</li></ul></li><li>[[双边市场效应]]<ul><li>双边供需的进入，都会有正外部性。公式：V=k<em>m</em>n</li><li>电商平台的效应弱：需要平台来管控质量，即变成了单边的；滴滴的效应强：司机和乘客的增加，都会带来正向效应（但边际收益未必持续提升）。</li></ul></li><li>[[梅特卡夫效应]]<ul><li>一个网络的价值与用户量的平方成正比。与常见网络效应的概念基本相同。任何用户的进入，都会有正外部性。</li><li>社交网络</li><li>曾李青定律：V=k*n²/r²（r 受 T、S、I、C 影响）。</li></ul></li></ul><p>读完之后，滴滴是什么模型这个问题就迎刃而解。这个月 有一个比较火视频 <a href="https://www.bilibili.com/video/BV1UT4y1j7Ht" target="_blank" rel="noopener">复盘出行大战：被BAT选中的滴滴，如何完成垄断霸业_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a>，介绍滴滴创业的故事。做为一名内部人士，也被里面的内容给震撼到。之前沈南鹏在吴海波的采访中说过一句话「未来十年看滴滴」。记录自己看到的三个细节：</p><ul><li>滴滴接受腾讯投资后，程维和王刚彻底关上和阿里的联系。</li><li>和快的补贴大战中：马化腾建议每次补贴金额在 12-20 中间的一个随机数</li><li>和 uber 大战中：腾讯封禁 uber 在微信上的微信号</li></ul><p>有一个梦想是能看到程维的传记，之前特意查过他的花名 —— 常遇春。知乎热榜这个月出现过一个奇怪的问题，里面有一段引用：</p><blockquote><p>王保保这个“奇男子”的称号，是朱元璋给的，评价较帝国双壁之一常遇春更高，含金量十足：朱元璋曾大会诸将，问道：“天下奇男子谁也？”诸将都说：“[[常遇春]]将不过万人，横行无敌，真奇男子。”明太祖笑曰：“遇春虽人杰，吾得而臣之。吾不能臣王保保，其人奇男子也。”（《明史》-《扩廓帖木儿传》）</p></blockquote><h2 id="商业">商业</h2><ul><li>阿里很强调价值观，但[[阿里巴巴]]首先是一家商业组织。</li><li>上学时，[[黄峥]]就意识到了机会可贵：“我在上学时就意识到几个事：一是寒门出贵子是小概率事件，大部分富二代，尤其是官二代非常优秀。二是田忌赛马，在整体资源劣势的情况下可以创造出局部优势，进而有机会获得整个战役的胜利。基于此，平凡人可以成就不凡事。第三是钱是工具，不是目的。”</li><li>趣头条本质上是一款游戏产品，只不过披着内容信息流的外衣。<ul><li>不过这一些信息流产品只有学习用户兴趣，真正的影响用户行为还是要靠基于强化学习开发的游戏。</li></ul></li><li>字节系广告变现涉及4个角色：内容消费者，内容生产者，平台，广告主；阿里系广告变现涉及3个角色：消费者，商家，平台；所以阿里的广告变现链条更短，一定程度上也是效率更高的原因。多一个角色，多一份成本，最终效果要多乘一个参数。</li><li><a href="https://medium.learningbyshipping.com/apples-relentless-strategy-and-execution-7544a76aa26" target="_blank" rel="noopener">Apple’s Relentless Strategy, Execution, and Point of View | by Steven Sinofsky | Jun, 2020 | Learning By Shipping</a> Steven Sinofsky 是前微软 Windows 业务部门的总裁评价苹果架构迁移。<ul><li>其实没有仔细看这一篇文章。震惊到我的是，这位兄弟管理过 20000 人的工程团队？不知道现在国内有哪一些公司的 CTO 能管理这么多的人。</li></ul></li><li><a href="https://sspai.com/post/61174" target="_blank" rel="noopener">从苹果的系统更新，理解设计中的「控制」与「自由」 - 少数派</a><ul><li>雪城大学建筑学教授理查德·洛萨（Rhichard Rosa）认为<strong>“设计的本质即是在于控制与自由。”</strong>这句话非常简洁但直接地介入了设计的核心——设计行为为混乱与无序赋予秩序，使之得以承载可控的人类行为；但设计也一定不是绝对的控制，它一定要为使用者预留一些自由。</li><li>设计师对美学的自信会落脚在对细节的强制：贝聿铭设计京都东郊的美秀美术馆，下雨天需要领取相应颜色的雨伞才能进入。</li><li><strong>有些人说：“消费者想要什么就给他们什么。”</strong> <strong>人们不知道想要什么，直到你把它摆在他们面前</strong></li><li><strong>设计一定需要控制，但一定不是绝对的控制。平台的搭建者需要预留一些自由给开发者以及用户，对秩序的全盘接手最终会导致自下而上民意的反弹。</strong></li><li>[[设计的哲学]]，设计的背后是不是需要有哲学？一味满足用户需求的软件是不是会变得十分的复杂？比如 Emacs 之类自由度高的软件？</li></ul></li><li><a href="https://www.jianshu.com/p/03080891e4ba" target="_blank" rel="noopener">顶级PM的产品观：王慧文看行业-合集 - 简书</a><ul><li>互联网 AB 面：A类是供给和履约在线上，B类是供给和履约在线下。</li><li>B类又可以分为：以SKU为中心的供给B1和以Location为中心的服务B2。</li><li>A 类能力体现在产品设计领域，体现在用户理解上，体现在对于通讯、社交以及内容把握上。</li><li>B1里面，主要体现在对于品类的理解，对于供应链的理解，对于定价的理解。</li><li>B2里面，如果你们去盘点一下B2的公司，他们总体来说有一个比较共有的特征，大规模的线下团队。</li><li>是否有大规模的线下团队是B1和B2一个很大的差别。</li><li>在LBS的方向上，中国和美国的企业这样的差距是怎么样发生的？大概有四个因素决定：人力成本、人口密度、人口规模、代际竞争。</li></ul></li></ul><figure><img src="https://media.xiang578.com/a-b1-b2.png" alt="互联网 AB 面"><figcaption>互联网 AB 面</figcaption></figure><h2 id="阅读">阅读</h2><ul><li>理解世界的一个有效方法是，在人生的某个阶段，把任何之前视为理所当然的事情全都重新研究与思考一遍，并弄清楚它们运行的真正起源与机理。在这个过程中，自问的问题越基础、越显得不需要去质疑，收获往往就会越多——人为什么要每天吃三顿饭、买东西为什么要花钱、书籍和文章为什么会存在——真理通常就藏在这些大多数人想都不会想的事情里。[[张潇雨]]<ul><li>「银河系漫游指」里面有一句：任何在我出生时已经有的科技都是稀松平常的世界本来秩序的一部分。</li></ul></li><li>什么是第一原理？<ul><li>[[亚里士多德]] 在[[形而上学]]中提出这个哲学概念，指「公理：无法再分、无法证明且不证自明的命题。」</li><li>“第一原理”本身并不是什么原理，它只是个简称。准确地说，应该叫做“从第一原理推理”（reasoning by first principle），是<strong>分析问题，找出其不可继续拆分的根本原因，即第一原理，再从第一原理反推出解决方案的思考方式。</strong></li><li>第一原理是先验</li></ul></li></ul><h2 id="算法">算法</h2><ul><li><a href="https://www.tanglei.name/blog/shuffle-algorithm.html" target="_blank" rel="noopener">面试官：会玩牌吧？给我讲讲洗牌算法和应用场景吧！ | 唐磊的个人博客</a> [[洗牌算法]]<ul><li>之前的分享中写房租分配 <a href="https://xiang578.com/post/week-issue-9.html">每周分享第 9 期：拼多多 | 算法花园</a></li><li>保证每次的概率是相同</li><li>#[[problem]]实现以下算法：一组数，每次不放回抽样，得到一个随机序列。白板编程。分析时间空间复杂度。<ul><li>follow up：能否时间O(n)完成，能否空间O(1)完成。</li></ul></li></ul></li><li><a href="https://www.youtube.com/watch?v=733m6qBH-jI" target="_blank" rel="noopener">Stanford CS230: Deep Learning | Autumn 2018 | Lecture 8 - Career Advice / Reading Research Papers - YouTube</a>：Ag 在课程中介绍如何阅读论文，又一次感受到大佬的真诚。<ul><li>主题阅读<ul><li>收集资料</li><li>列出一个 list ，标注阅读进度。挑选有价值的论文阅读。</li><li>5-20 初步了解</li><li>50-100 很好理解前沿工作</li></ul></li><li>如何阅读论文<ul><li>多遍阅读</li><li>第一遍：标题，摘要，图片</li><li>第二遍：简介、结论、图片相关材料</li><li>第三遍：进入论文主体部分，但是可以跳过数学，明白每个参数的含义。</li><li>第四遍：阅读整篇文章，跳过没意义的部分(内容过时，没有火起来过)。</li></ul></li><li>阅读时思考的问题<ul><li>作者试图解决什么问题？</li><li>研究方法的关键是什么？（最具有开创性）</li><li>哪些东西可以为你所用？</li><li>有哪些参考文献可以继续跟进？</li></ul></li><li>最后发现一篇相关实践文章： <a href="https://towardsdatascience.com/how-you-should-read-research-papers-according-to-andrew-ng-stanford-deep-learning-lectures-98ecbd3ccfb3" target="_blank" rel="noopener">How You Should Read Research Papers According To Andrew Ng (Stanford Deep Learning Lectures) | by Richmond Alake | Jul, 2020 | Towards Data Science</a><br></li></ul></li><li>另外一篇和论文阅读有关的文章：<a href="https://zhuanlan.zhihu.com/p/163227375" target="_blank" rel="noopener">沈向洋、华刚：读科研论文的三个层次、四个阶段与十个问题 - 知乎</a><ul><li>三个层次：速度、精读与研读</li><li>四个阶段：Passive Reading、Active Reading、Critical Reading、Creative Reading</li></ul></li></ul><p>这就是本期的 「Never-Reading」，我们下个月再见。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;不知不觉中每月分享已经进行半年，不过前 6 期都没有想到取什么名字。上期的标题「Never Reading」来自稍后读列表名称，仔细一想不正好成为每月分享的名字吗？而且还有致敬「Λ-Reading」的成分。&lt;/p&gt;
&lt;h2 id=&quot;互联网商业模式&quot;&gt;互联网商业模式&lt;/h2&gt;
      
    
    </summary>
    
      <category term="Never-Reading" scheme="https://xiang578.com/categories/Never-Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>202006 Never Reading</title>
    <link href="https://xiang578.com/post/monthly-issue-202006.html"/>
    <id>https://xiang578.com/post/monthly-issue-202006.html</id>
    <published>2020-07-14T15:55:11.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<p>这一份 6 月的阅读总结来的有一点晚。前几年一直断断续续在实践 GTD，人的拖延症超乎想象，学到的一个经验是 「Now or Never」。所以，这个月将自己的阅读列表取名为「Never Reading」。</p><h2 id="笔记方法精进">笔记方法精进</h2><p>Roam Research 引起现在这一波 Backlink 笔记软件浪潮，本月依然阅读一些和笔记方法相关的文章。</p><h3 id="我的-zettelkasten-卡片盒笔记法实践-吕立青的博客">我的 Zettelkasten 卡片盒笔记法实践 | 吕立青的博客</h3><p><a href="https://blog.jimmylv.info/2020-06-03-zettelkasten-in-action/" target="_blank" rel="noopener">文章链接</a></p><p>上个月介绍过，自己已经从 Roam Research 迁移到 Obsidian，然后就看到吕立青这一篇基于 Obsidian 的 Zettelkasten 实践。文章中将卡片盒笔记分拆分成四步：</p><blockquote><p>1️⃣ 第一步：必须用自己的话写笔记卡片，以确保你将来能够理解。</p><p>2️⃣ 第二步：无论何时添加新笔记，主动查找可链接到的已有笔记。</p><p>3️⃣ 第三步：通过添加新记录并联系起来，延续这一系列的连续思考。</p><p>4️⃣ 第四步：使用 Anki 间隔重复加深记忆，主动由大脑触发远程联想。</p></blockquote><p>搞笑地是，在他写出这篇文章没有多久之后，已经开始尝试往 Roam Research 上迁移……另外，为了更好在国内推广，他参与发起 <a href="roam-cn.github.io">roam/cn</a> 组织，从英文世界翻译一些推特、视频以及分享一些个人的案例。</p><h3 id="zettelkasten-note-taking-in-10-minutes">Zettelkasten note-taking in 10 minutes</h3><p>两条原则：</p><ul><li>Don’t try to get this method perfect from the get go.</li><li>The advanced practices are useful only when you’ve got close to 1000 notes</li></ul><h3 id="my-productivity-app-for-the-past-12-years-has-been-a-single-.txt-file">My productivity app for the past 12 years has been a single .txt file</h3><p>12 年间使用一个 txt 进行任务管理方法分享。作者提到 to do list 变成 what done list 的过程，每天晚上将日历中第二天的代办事项整理到 txt 中，第二天顺手记录任务相关的信息（比如讨论出的结论，或者得到的信息）。结合自己使用经历，OmniFocus 是一个 to do list，基于纯文本的任务管理方式（org-mode 或 taskpaper）更容易成为 what done list。</p><h2 id="商业">商业</h2><ul><li><a href="https://36kr.com/p/736767189897093" target="_blank" rel="noopener">滴滴重踩油门_详细解读_最新资讯_热点事件_36氪</a> 不开玩笑地说，看完这里面的分析，我才理解公司很多的战略。</li><li><a href="https://www.notion.so/c074ab234e2b4dbb9143a48faceec031" target="_blank" rel="noopener">淘宝宣战拼多多的前夜：吕晋杰、陈琪、徐易容和葛永昌的至暗时刻</a> 和很多同学聊天，17 年找工作的时候低估了拼多多的潜力。<ul><li>淘宝 PC 转移动互联网时，流量入口从网页短 300 个减少到手淘 app 48 个。是不是能解释淘宝这几年涌现出那么多的推荐系统相关的问题。</li><li>电视广告投放策略以及如何理解流量暴涨，拼多多上一直很火爆的「百亿补贴」</li><li>任何人都可以成为英雄，哪怕是做了一件不起眼的事情。</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/146363597" target="_blank" rel="noopener">快手的普通，抖音的美好，算法的价值观 - 知乎</a><ul><li>算法没有价值观，算法只会实现设计者的意图。</li><li>主动降低算法效率，是为了实现某些短期无法衡量的业务目标。</li><li>抖音将流量集中在头部。</li><li>快手整体的点击率被牺牲，普通人的流量被保证。</li><li>抖音说记录美好生活，快手说记录世界记录你。</li></ul></li></ul><h2 id="阅读">阅读</h2><h3 id="人生总有一刻我们会开始思考死亡">人生总有一刻，我们会开始思考死亡</h3><p>茨威格在《人类的群星闪耀时》所说的那种幸运是什么 —— 「最大的幸运，莫过于在年富力强的时候，发现了自己的使命」</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/24640592" target="_blank" rel="noopener">人生总有一刻，我们会开始思考死亡 - 知乎</a> [[张潇雨]] 于是我发现，面对死亡最终可能只有两种方法。 - 一种是将自己与一些更宏大的东西联系起来：一个数学定理、一本文学著作、一件艺术作品或一种恒久的信仰。马尔克斯与康德靠《百年孤独》与《纯粹理性批判》遗世独立，米开朗基罗把《创世纪》和《最后的审判》印刻在西斯廷大教堂里，供千万后朝拜——他们肉身虽灭，但精神不朽——反正建筑是永远戳在那儿的。 - 还有一种就是，生活在当下的每个瞬间里，不烦扰过去、不担忧将来。</p></blockquote><h3 id="即刻半月刊">即刻半月刊</h3><p>6.18？即刻重新开放，之前对这个社区没有太多印象，尝试关注一些人之后，信息流的质量也不错。「即刻半月刊」是某些爱好者挑选的一些即友发言集合。摘录一些我觉得有意思的内容。</p><ul><li>老罗带货首卖小米的中性笔：数量太少。之前看到过一个分析，抖音自己做电商最大的困难点在于没有用户的物流地址。简单想一下，用户可能在填写地址这一步流失。</li><li><span class="citation" data-cites="Rey_L">@Rey_L</span> 我在判断哪家公司的产品会换灰色皮肤的时候，基本都猜对了hhh，滴滴头条一定会做，京东会做，淘宝也许会做，拼多多肯定不做。</li><li>微信小程序减少 App Store 在国内的下载量。不过大部分巨头的小程序还是想要从微信中引流的。</li><li>底下有回复cite了一个working paper，大意说的是名校（elite education，这里用的是211高校与非211高校的断点回归）能够给学生带来的起薪上 30-45% 的提升，其原因在于名校当中的social network，elite的peer普遍家庭条件更好，成绩也更好。上网课带来不了这种social connection...所以说不单只是知识改变命运，环境也很重要</li><li>所谓“商业模式”其实指的是这家公司的“价值创造模式”，即用什么样的模型创造了更多价值。<ul><li>世界上现存所有的商业模式无非三种，一是[[边际效应]]（规模效应/协同效应），二是[[双边效应]]，三是梅特卡夫[[网络效应]]。不同的价值创造模型，带来不同的增长动力，继而带来不同的货币化方法。</li><li>滴滴是什么模型？</li></ul></li><li>A/B test只能做模型的小优化，但找不到大的绝对的增长点，所以除了推荐系统里的参数，即刻的很多产品决策希望尽量远离A/B test</li><li>张小龙的饭否，张一鸣的微博，黄峥的微信公众号，很可惜这几个都成了过去形态。现在只剩下王兴的饭否了。</li><li>一个反向小思考：兴趣社交刚需归刚需，社交app的本质还是信息分发。RSS订阅20周年，分发的有效性也不是什么相似推荐，^<sup>而是真正提供专注力的阅读</sup>^。因为个性推荐的本质就是专注力。信息廉价甩卖的今天，打开微博广场/抖音主页/朋友圈（广告），用户最需要的不是新鲜事而是减少噪音的阅读器，人永远看到自己想看到的，产品顺水推舟不也是更讨巧，所以需要设计者克制。沟通永远是双向的，当用户感到一个舒适被接纳的舆论场所，也会更加有表达欲。</li><li>芒果系这么多年都在做一件事，就是把握这个时代的情绪。[[乘风破浪的姐姐们]]</li></ul><h2 id="机器学习">机器学习</h2><p>本月 KDD 2020 的文章应该已经放出，推荐阅读 Airbnb 深度模型实践相关的文章 Managing Diversity in Airbnb Search 以及 Improving Deep Learning For Airbnb Search。 7 月份希望能写一篇博客分享自己的阅读笔记。</p><ul><li><a href="https://www.zhihu.com/question/389912594" target="_blank" rel="noopener">神经网络中对需要concat的特征进行线性变换然后相加是否好于直接concat? - 知乎</a> [<a href="#机器学习">机器学习</a>]<ul><li>concat 没有信息融合，也没有信息缺失</li><li>add 不同向量之间的权重相同，会导致信息缺失。待融合的特征具有相同分布，或特征属于同一类，直接相加才有可能提升模型性能。</li><li>concat + MLP 能够自动学习不同 channel 的权重，MLP 能引入非线性。利于通道间的信息融合，计算量大。</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/24851814" target="_blank" rel="noopener">【机器学习】Bootstrap详解 - 知乎</a>：随机森林里面用到的数据重采样方法，老板要求训练两个有差异模型时给我推荐的方法。</li><li><a href="https://zhuanlan.zhihu.com/p/148729018" target="_blank" rel="noopener">BERT 可解释性-从"头"说起 - 知乎</a>：蛮有意思的对 BERT 组件效果分析，这位作者举的例子有些蹭热点。</li><li><a href="https://zhuanlan.zhihu.com/p/63982470" target="_blank" rel="noopener">都9102年了，别再用Adam + L2 regularization了 - 知乎</a>：说明为什么要用 AdamW。另外推荐一下，之前一位同事写的 AdamW 实现：<a href="https://zhuanlan.zhihu.com/p/40814046" target="_blank" rel="noopener">L2正则=Weight Decay？并不是这样 - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这一份 6 月的阅读总结来的有一点晚。前几年一直断断续续在实践 GTD，人的拖延症超乎想象，学到的一个经验是 「Now or Never」。所以，这个月将自己的阅读列表取名为「Never Reading」。&lt;/p&gt;
&lt;h2 id=&quot;笔记方法精进&quot;&gt;笔记方法精进&lt;/h2&gt;
&lt;
      
    
    </summary>
    
      <category term="每月分享" scheme="https://xiang578.com/categories/%E6%AF%8F%E6%9C%88%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>每月分享 202005 Newsletter</title>
    <link href="https://xiang578.com/post/monthly-issue-202005.html"/>
    <id>https://xiang578.com/post/monthly-issue-202005.html</id>
    <published>2020-06-07T15:55:11.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<h2 id="newsletter">Newsletter</h2><p>从去年开始给我一种 RSS 复兴的感觉，这个月尝试使用 Newsletter。对于创作者来说，RSS 不仅无法统计数据，也很难开展会员模式。Newsletter 通过邮箱订阅的的手段，完美解决这两个问题，国外开始有一站式的解决方案，可能几个月之后也会在国内火起来。推荐自己订阅的一些邮件组给大家。</p><ul><li><a href="https://www.notion.so/PRODUCT-THINKING-a601a12335044f349a22caf57f274c27" target="_blank" rel="noopener">PRODUCT THINKING · 产品沉思录精选</a>：第一个付费订阅的邮件周刊，目前的价格是 199 元/年。根据少楠自己写的介绍，内容包括但不限于产品设计，服务设计，数据分析，互联网技术，经济学，心理学，社会学，决策学，自然科学，城市规划，零售，团队管理等内容。每周会推荐几篇网上比较好的文章，偶尔也翻译一些英语文章。挑选几篇我觉得不错的公开内容：<ul><li><a href="https://www.notion.so/Zettelkasten-25627d7ce99344c487f4e42d861f9e0a" target="_blank" rel="noopener">原子笔记法：Zettelkasten</a></li><li><a href="https://www.notion.so/P-A-R-A-Notion-19909e5aac3049d887197dcfb1e97fd5" target="_blank" rel="noopener">如何管理信息：P.A.R.A. 是什么及在 Notion中的应用</a></li></ul></li><li><a href="https://rizime.substack.com/" target="_blank" rel="noopener">Λ-Reading</a> 阅读相关分享，作者的读书笔记网站也值得一看 <a href="https://rizi.me/" target="_blank" rel="noopener">— Read the Word,Read the World.</a>。推荐内容：<ul><li><a href="https://rizime.substack.com/p/f08" target="_blank" rel="noopener">路径依赖和困扰计算机的简单问题 - Λ-Reading</a></li><li><a href="https://rizime.substack.com/p/d28" target="_blank" rel="noopener">号外：知识管理工具 - Λ-Reading</a> 中文为数不多关于 TiddlyWiki 的介绍。</li></ul></li><li><a href="https://clearbox.substack.com/" target="_blank" rel="noopener">透明盒子计划</a> 深度阅读分享，盒子对应 Zettelkasten。<ul><li><a href="https://clearbox.substack.com/p/coming-soon" target="_blank" rel="noopener">透明盒子计划 - 透明盒子计划</a></li></ul></li><li><a href="https://superorganizers.substack.com/" target="_blank" rel="noopener">Superorganizers</a> 对国外人士的采访，有关于效率、数字生活等。目前只看他的免费内容……<ul><li><a href="https://superorganizers.substack.com/p/how-to-build-a-learning-machine" target="_blank" rel="noopener">How to Make Yourself Into a Learning Machine - Superorganizers</a>：对一名高中辍学的小哥的采访，介绍来一些自我教育的方法。</li></ul></li></ul><h2 id="阅读">阅读</h2><ul><li><a href="https://sspai.com/post/60466" target="_blank" rel="noopener">How to take smart notes，方法及工具 - 少数派</a>：Zettelkasten 这种做笔记方法慢慢开始要在国内流行起来，自己已经关注差不多超过半年的时间，接下来也在计划写一篇相关的博客文章。</li><li><a href="https://beepb00p.xyz/hpi.html" target="_blank" rel="noopener">Human Programming Interface</a> 简单看来一下，利用 py 包和 Emacs 管理所有相关的个人数据，挺疯狂的。</li><li>上古论坛差不多十年前的帖子， <a href="https://www.hi-pda.com/forum/viewthread.php?tid=819978&amp;extra=&amp;authorid=1956&amp;page=1" target="_blank" rel="noopener">我的千书阅读计划 - 意欲蔓延 - Hi!PDA Hi!PDA</a> fatdragoncat 通过阅读成为一名自由职业者。帖子中介绍大量篇幅介绍如何高效阅读、锻炼、自我管理等等。在印象笔记中找到几年前自己写的笔记，现在重新整理一下相关的内容，并分享给大家。</li><li><a href="https://www.twitch.tv/videos/611050187" target="_blank" rel="noopener">AndyMatuschak - Making sense of Design Unbound vs. prior theories of collaborative design work - Twitch</a> [[Evergreen notes]]的创始人公开展示写作的过程。通过这个视频可以发现他使用的笔记软件是 [[Bear]]，看起来 Reference 和 Backlink 都是手动输入的，不过这样也符合 [[Zettelkasten]] 的原则。只是 [[Roam Research]] 这样的软件让我们变懒。</li><li>莫言获得诺贝尔文学奖发表的演讲中有一个故事：到了荒滩上，我把牛羊放开，让它们自己吃草。蓝天如海，草地一望无际，周围看不到一个人影，没有人的声音，只有鸟儿在天上鸣叫。我感到很孤独，很寂寞，心里空空荡荡。有时候，我躺在草地上，望着天上懒洋洋地飘动着的白云，脑海里便浮现出许多莫名其妙的幻象。我们那地方流传着许多狐狸变成美女的故事，我幻想着能有一个狐狸变成美女与我来作伴放牛，但她始终没有出现。但有一次，一只火红色的狐狸从我面前的草丛中跳出来时，我被吓得一屁股蹲在地上。狐狸跑没了踪影，我还在那里颤抖。有时候我会蹲在牛的身旁，看着湛蓝的牛眼和牛眼中的我的倒影。有时候我会模仿着鸟儿的叫声试图与天上的鸟儿对话，有时候我会对一棵树诉说心声。但鸟儿不理我，树也不理我。许多年后，当我成为一个小说家，当年的许多幻想，都被我写进了小说。很多人夸我想象力丰富，有一些文学爱好者，希望我能告诉他们培养想象力的秘诀，对此，我只能报以苦笑。</li></ul><h2 id="机器学习">机器学习</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/138136777" target="_blank" rel="noopener">谈谈推荐系统中的用户行为序列建模 - 知乎</a> 一篇关于用户行为序列建模的文章，基本上常用的方法都介绍了。<ul><li>和上一次 "<a href="https://zhuanlan.zhihu.com/p/125507748" target="_blank" rel="noopener">从谷歌到阿里，谈谈工业界推荐系统多目标预估的两种范式 - 知乎</a>[[机器学习实践]][[MTL]]" 属于同一个作者</li><li>目前主流推荐系统框架 [[Deep Neural Networks for YouTube Recommendations]] 中的 Matching 和 Ranking。另外可能还有规则模块。</li><li>pooling-based architecture 范式，用户行为是无序集合，使用 sum/max pooling 或各种 attention<ul><li>[[Deep Neural Networks for YouTube Recommendations]] 中将用户观看过的视频序列取到 embedding 后，做一个 mean pooling 作为用户历史兴趣的表达</li><li>Ranking 阶段：[[DIN]] target item 和行为序列的 item 做一个 attention，得到一个 weight，然后加权求和。</li><li>结合 [[Transformer]] 做 self-attention 并行的建模长序列依赖，除去用户行为序列中的噪声：[[Behavior Sequence Transformer for E-commerce Recommendation in Alibaba]]</li></ul></li><li>sequential-modeling architecture 范式，用户行为当成一个具有时间属性的序列，使用 RNN、LSTM、GRU 等<ul><li>[[Perceive Your Users in Depth: Learning Universal User Representations from Multiple E-commerce Tasks]] Property Gated LSTM</li><li><a href="https://zhuanlan.zhihu.com/p/30720579" target="_blank" rel="noopener">推荐中的序列化建模：Session-based neural recommendation - 知乎</a></li></ul></li><li>上面两种方法都是将用户行为经过 pooling/attention/rnn 的处理，聚合成用户行为序列的 embedding，再和其他的特征 concat 在一起，经过 mlp 后接 sigmod/softmax</li><li>抽取聚类出用户多峰兴趣，Capsule<ul><li>阿里 [[MIND]] 胶囊网络</li></ul></li><li>辅助损失函数<ul><li>[[DIEN]] 兴趣提取和兴趣演化，以最后一个 hidden state 做为用户兴趣的表达。兴趣提取模块，使用隐状态和下一件商品预测做二分类。不加入辅助loss，GRU 的隐变量完全受限于最终点击的 label，加入后能约束 GRU 每个隐状态表示其本身的兴趣。</li></ul></li><li>提升用户序列长度，可以带来可观的 auc 提升。[[MIMN]]</li></ul></li><li>Applying Deep Learning To Airbnb Search：一篇关于从 GBDT 模型迁移到深度模型的工业实践记录 paper。对于我这种没有经历过这种技术迭代的人来说，工业级的深度模型上线比想象中的要困难。作者们针对自己遇到的比如 listing embedding 训练不充分、如何判断 feature 的重要性等问题设计实验去验证以及给出解释。严谨的精神值得吾辈学习。</li></ul><h2 id="其他">其他</h2><p>出于对 Roam Research 开发者的不放心，已将全部文档迁移到 Obsidian。目前还在探索新的工作流，5 月分享不可避免产生拖延。另外还在寻找一种建立 Digital Garden 的方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;newsletter&quot;&gt;Newsletter&lt;/h2&gt;
&lt;p&gt;从去年开始给我一种 RSS 复兴的感觉，这个月尝试使用 Newsletter。对于创作者来说，RSS 不仅无法统计数据，也很难开展会员模式。Newsletter 通过邮箱订阅的的手段，完美解决这两个问题
      
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
  </entry>
  
  <entry>
    <title>每月分享 202004 新的尝试</title>
    <link href="https://xiang578.com/post/monthly-issue-202004.html"/>
    <id>https://xiang578.com/post/monthly-issue-202004.html</id>
    <published>2020-05-01T08:55:11.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<p>不知不觉又到更新每月分享的时间。</p><p>想写一下我为什么做这件事情？分享自己平时看到有意思的内容，现实世界认识的人，很少对我关注的内容感兴趣，所幸能借助博客超越时间和空间限制的分享。</p><p>另外一点，我希望自己能将这个系列当成一个产品去迭代，每一期都有形式和内容上的进步。这件事看起来很简单，但却需要耗费很大的精力。其实在网上看到很多人通过这种形式分享，到头来还在坚持的大概也没有多少人（比如阮一峰的科技爱好者周刊）。</p><h2 id="zettelkasten-以及-roam-research">Zettelkasten 以及 Roam Research</h2><p>Zettlekasten 是一个德语单词，意思是卡片盒。现在主要指一种记笔记的思路。Roam Research 是目前国外很火的一个笔记软件，最大的特点是实现不同笔记之间的双向链接。好几个月前就开始尝试 Zettlekasten 的方法，4 月开始才使用 Roam Research。这里先分享一些我看过的文章。</p><ul><li><a href="https://twitter.com/Tisoga/status/1244856639439515649" target="_blank" rel="noopener">Roam Research 入门指南（thread）</a></li><li><a href="https://medium.com/@dongyangvic/%E7%94%A8-roam-research-%E6%9D%A5%E6%89%93%E8%8D%89%E7%A8%BF-a3b1d3873aa4" target="_blank" rel="noopener">用 Roam Research 来打草稿 - Dongyang Vic - Medium</a></li><li>Roam Fu<ul><li><span class="citation" data-cites="RoamResearch">[Part I: My plan for using @RoamResearch for a thesis]</span>(https://twitter.com/kcorazo/status/1247260599760736256)</li><li><a href="https://twitter.com/kcorazo/status/1252669427125895169" target="_blank" rel="noopener">Part II: Using Twitter as an inter-brain zettelkasten</a></li><li><a href="https://twitter.com/kcorazo/status/1255296825566920705" target="_blank" rel="noopener">Part III: Beyond the Empire</a></li></ul></li></ul><h2 id="阅读">阅读</h2><ul><li><a href="https://www.gcores.com/articles/121924" target="_blank" rel="noopener">中文互联网中“讨论”的消亡 | 机核 GCORES</a>：从产品设计角度分析国内主流网站如何限制用户讨论，互联网缩短人与人之间的距离，但我们用来互喷。</li><li><a href="https://www.huxiu.com/article/350854.html" target="_blank" rel="noopener">互联网是人类历史的一段弯路吗？-虎嗅网</a>：又一篇深度长文，很多观点可以背下来出去装B。</li><li><a href="https://zhuanlan.zhihu.com/p/135281778" target="_blank" rel="noopener">ByteDance程序员生存指南 - 知乎</a><ul><li>有些年轻人，在结束一整天的工作后，拖着疲惫不堪的肉体回到出租屋，这时候只想躺着啥都不想干，躺在好几周没有换过的床单上点开了抖音，不一会儿就刷到了凌晨，第二天再拖着疲惫不堪的肉体上班。到了周末也没有任何心力出去玩，只能睡到中午然后随便吃点东西看看剧。没有生活，没有朋友，一晃就单身了四五年，长此以往不仅仅是肉体在亚健康和崩溃的边缘，心理健康更是会出现问题。 孤独、焦虑、易怒等等情绪时刻伴随着一个人。</li><li>每个月仅仅只有收到工资短信时可以高兴几分钟。但这笔钱并不敢轻易花出去，因为都是血汗积攒而来，付出的多自然不敢随意花出去。</li></ul></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzM0MjUyMQ==&amp;mid=2652149604&amp;idx=1&amp;sn=3c96ebfe992694e5c57affe9ef5ba33f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">构建优雅的知识创造系统</a> [[阳志平]] [[卡片写作法]]<ul><li>利用电子卡片后，如何量化每天的产出？是不是可以换算成 github 更新多少字？</li><li>卡片是自己看的，不需要分享：<strong>在卡片层级最大的误区是：分享</strong>。<strong>不少人误将卡片、文件和项目三个层级混为一谈，喜欢在卡片层级搞分享</strong>。这样每次撰写卡片时就增加了一个选择项：<strong>这张卡片我是该分享还是该存着自己看？增加的认知操作加大了认知负荷，从简单反应时变为选择反应时</strong>。所以尽可能在卡片层级少做分享。</li><li>一周、一月、一年、十年与数十年，进行自己的实践。</li></ul></li><li>所有人问俞军<ul><li>[[金字塔原理]] 和 [[学会提问]]</li><li>优秀的思维方式以及对人性和世界的底层理解</li><li>我不是说人工智能不好，而是我自己只关心我能想明白一两年内能给创造什么用户价值的产品(或技术等)，如果我想不明白这东西马上能给用户创造什么价值，我就毫无兴趣。</li><li>看《搜索研究院》页首页尾的那句话，“我们若能更妥善地搜寻资料，实在已经改变世界。”</li></ul></li><li>前百度首席科学家吴恩达谈学习<ul><li>如果你学习，两天后的周一，你不会很快的就在工作中出彩，你的老板也不会知道你花了整天的时间学习，更不会夸奖你什么。你几乎找不到任何东西可以证明你在努力学习。</li></ul></li><li><a href="https://blog.jimmylv.info/2016-07-12-pkm-again-to-innovate-my-note-system/" target="_blank" rel="noopener">再谈个人知识管理：革新我的笔记系统 | 吕立青的博客</a> 好久没有看到立青写文章<ul><li>不同软件之间，单条笔记的迁移相当于一次对知识进行提炼的过程。</li></ul></li><li><a href="https://tundrazone.com/guanyudongwuzhisendesanze/" target="_blank" rel="noopener">关于动物之森的三则 – 苔原带</a><ul><li>以前玩家渴望在游戏里杀死巨龙，飞向人马座，击败外星侵入者。而现在我们只需要在游戏里“正常”的生活就乐呵呵了。和朋友一起野餐，请朋友到家里来玩，一个人傻乎乎做开心的事情在现实世界中已经有这么高的门槛了么？又或者只是因为在游戏里做这些事情的成本足够低，多巴胺回报足够快？</li></ul></li></ul><h2 id="机器学习">机器学习</h2><ul><li>内部开始尝试 MCTS 相关的项目。正好接这个机会看一下 DeeepMind 前几年的论文 <a href>Mastering the game of Go with deep neural networks and tree search</a> 以及 <a href>Mastering the game of Go without human knowledge</a>。推荐去看一下田渊栋在知乎上 <a href="https://zhuanlan.zhihu.com/p/20607684" target="_blank" rel="noopener">AlphaGo的分析</a> ，当时他在 Facebook 参与类似围棋相关的项目。另外就是木遥的 <a href="https://36kr.com/p/5042969" target="_blank" rel="noopener">关于 AlphaGo 论文的阅读笔记</a> 有更多关于现实的思考。最后推荐 <a href="https://movie.douban.com/subject/27012433/" target="_blank" rel="noopener">阿尔法围棋</a>，记录从 DeepMind 开发 AlphaGo 到战胜李世石的全过程。有一个疑问第四局之后，他们有没有增加使用的 GPU 和 CPU？</li><li><a href="https://www.zhihu.com/question/32218407" target="_blank" rel="noopener">在你做推荐系统的过程中都遇到过什么坑？ - 知乎</a> [[推荐系统]]<ul><li>没有明确的指标：CTR，staytime，read/unread</li><li>精准推荐以及兴趣探索</li><li>线下auc涨，线上 ctr 跌</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/125507748" target="_blank" rel="noopener">从谷歌到阿里，谈谈工业界推荐系统多目标预估的两种范式 - 知乎</a>[[机器学习实践]][[多任务学习]]<ul><li>范式一：[[MMOE]] 替换 hard parameter sharing<ul><li>[[Recommending what video to watch next: a multitask ranking system]]</li></ul></li><li>范式二：任务序列依赖关系建模<ul><li>[[ESMM]]</li></ul></li><li>模型负采样，存在 CTR 漂移问题 U5Jvma3de</li></ul></li></ul><h2 id="放弃的事情emacs">放弃的事情：Emacs</h2><p>写作软件中积累一些文章的草稿，不过由于我的兴趣变化太快，很多文章还没有完成就已经被我放弃。借这个机会，展示一些有意思的东西。</p><p>程序员圈子中编辑器战争一直是一个绕不过去的话题。自己日常的工作中会使用多种编辑器：</p><ul><li>Sublime Text 简单处理文本</li><li>IDEA 处理 Scala Spark 相关的代码</li><li>PyCharm 连接 GPU 服务器处理 python 相关的代码</li><li>VS Code 本机上写 python、shell、cpp、sql 等脚本</li><li>Vim 服务器上修改文件</li></ul><p>不论选择什么编辑器，都推荐大家去看陈斌的<a href="https://github.com/redguardtoo/mastering-emacs-in-one-year-guide/blob/master/guide-zh.org" target="_blank" rel="noopener">一年成为Emacs高手 (像神一样使用编辑器)</a>。</p><p>去年底的时候，由于想尝试 <code>org-mode</code> 做任务管理（下个月再分享相关的内容），开始尝试使用 Emacs。Emacs 最大的有点是基于 Elisp 开发，软件中的每一个功能都对应一个函数，一个快捷键对应一个按键和函数的 map。修改功能和配置非常的方法。比如有人完全将 Vim 在文本操作上的功能迁移过来做成 evil 这个插件（号称所有和 Vim 中表现不同的情况都是 bug）。</p><p>Emacs 需要大量时间调教才能用起来舒心，对于初学者推荐去网上找一些成熟的配置直接使用。目前比较流行的有 Spacemacs 和 Doom emacs，这些配置维护以及使用的人很多，方便解决你遇到的各种问题。另外那些某些大佬个人分享的配置，如果你和大佬的技术栈不同，没有必要强行 clone。把它当成是一个学习素材，更好的理解 Emacs 背后的哲学。再这些基础上，成为高效的程序员的第一步，就是打造属于你自己的专门的配置文件。</p><p>使用好的编辑器是为了更快的工作。那如何更快的工作？</p><ol type="1"><li>在加快敲击键盘的速度</li><li>减少敲击键盘的次数</li><li>减少鼠标和键盘之间的切换</li></ol><p>关于 2，我在之前的文章中提到过一点，改变中文的输入方式（从全拼切换到小鹤双拼）。另外一点就是多使用快捷功能，比如 vim 里面的行号跳转。大部分软件的快捷键都是开发商配置好的，不过每一人主要使用的功能其实是完全不一样的。Emacs 中所有的快捷键可以查到定义的文件，从而进行修改。想象一种情况，为了减少我们按快捷键的次数以及难度。我们统计一段时间内使用 Emacs 各个功能的次数（插件 keyfreq），然后重新定义对应的快捷键。</p><p>由于我自己之前主要使用的是 vim，所以也给 vim 用户一个相对于合理的替换过程：</p><ol type="1"><li>当成普通的 vim 使用</li><li>逐步接触 org-mode 相关的功能</li><li>使用 emacs 其他的特性</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;不知不觉又到更新每月分享的时间。&lt;/p&gt;
&lt;p&gt;想写一下我为什么做这件事情？分享自己平时看到有意思的内容，现实世界认识的人，很少对我关注的内容感兴趣，所幸能借助博客超越时间和空间限制的分享。&lt;/p&gt;
&lt;p&gt;另外一点，我希望自己能将这个系列当成一个产品去迭代，每一期都有形式和
      
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
  </entry>
  
  <entry>
    <title>每月分享 202003</title>
    <link href="https://xiang578.com/post/monthly-issue-202003.html"/>
    <id>https://xiang578.com/post/monthly-issue-202003.html</id>
    <published>2020-03-28T08:55:11.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<h2 id="读书">读书</h2><ul><li><a href="https://book.douban.com/subject/34672176/" target="_blank" rel="noopener">呼吸 (豆瓣)</a>：这是一本由 Byte.Coffee 主播 MilkShake 🐑 推荐的一本科幻小说集（前几天看其他东西的时候学会科幻小说的英文 sci-fi）。之前看到过，小说的价值在于作者用一个故事告诉你一个道理。最喜欢的是《商人和炼金术士之门》这篇：在传统的穿越小说无法改变未来和过去的基础上，论证穿越能更深刻理解生活。书中其他探讨的几个问题也很有价值，值得一读。</li></ul><h2 id="我看">我看</h2><ul><li><a href="https://space.bilibili.com/390461123/" target="_blank" rel="noopener">徐大sao的个人空间 - 哔哩哔哩 ( ゜- ゜)つロ 乾杯~ Bilibili</a>：最近挑着看完大 sao 做饭视频，被他展示的生活激情所吸引。</li></ul><h2 id="文章">文章</h2><ul><li><a href="https://writings.stephenwolfram.com/2019/02/seeking-the-productive-life-some-details-of-my-personal-infrastructure" target="_blank" rel="noopener">Seeking the Productive Life: Some Details of My Personal Infrastructure—Stephen Wolfram Writings</a>：这篇超长的文章是 Stephen Wolfram (Mathematica CEO) 介绍自己几十年在家办公的经验，包括如何搭建一个适合自己的工作环境、如何管理文件和个人数据等等。不过这次疫情期间在家办公，大部分同事反应最大的问题是沟通效率降低。另外想想，或许是我们在这方面的思考不够。总之，互联网行业居家办公才是光明的未来。</li><li><a href="https://fs.blog/knowledge-project/naval-ravikant/" target="_blank" rel="noopener">Naval Ravikant: The Angel Philosopher</a>：AngelList 的 CEO Naval Ravikant 的播客访谈，Naval 的公司投资 Uber Twitter 等 100 多家科技公司。主要介绍 Naval 的一些哲学，文字版在 <a href="https://fs.blog/wp-content/uploads/2017/02/Naval-Ravikant-TKP.pdf" target="_blank" rel="noopener">Naval-Ravikant-TKP.pdf</a><ul><li>利用 Kindle 阅读，遇到喜欢的书，购买实体书收藏。对比书的价格，从书中学到可以改变自己人生的内容更重要。</li><li>发现一个新的博客后，会在 Archived 页面挑选几篇仔细阅读。读书时也可以使用这个技巧。</li></ul></li><li><a href="https://superorganizers.substack.com/p/how-to-build-a-learning-machine" target="_blank" rel="noopener">How to Make Yourself Into a Learning Machine - Superorganizers</a>：一位高中毕业后离开丹麦来到加拿大创业公司工作人的自我学习之路。<ul><li>自我定位 T 型人才。</li><li>兴趣面广，主题阅读。</li><li>高亮关键内容，使用 anki 记忆，相关的想法用 zettelkasten 记录。</li><li>zettelkasten 使用纯文本 + 脚本实现。（如果只记录英文 vim 和 emacs 真的是很强大的软件）</li><li>利用谷歌检索的数量判断单词的重要性</li></ul></li><li><a href="https://fortelabs.co/blog/para/" target="_blank" rel="noopener">The PARA Method: A Universal System for Organizing Digital Information - Forte Labs</a>：介绍 PARA 这种数字信息整理方法。</li><li><a href="https://tim.blog/2020/01/08/reading-recommendations/" target="_blank" rel="noopener">The Best Books and Articles I Read in 2019 – The Blog of Author Tim Ferriss</a>：一篇 2019 年阅读总结文章。对作者介绍的阅读流程比较感兴趣：<ul><li>Evernote 搭配 web clipper 收集文章。 利用 <code>***</code> 以及高亮在文章中做笔记，方便之后进行快速回顾。</li><li>阅读 Kindle 格式的电子书，定期从亚马逊官网导出高亮笔记（国内不支持）。</li><li>利用 Readwise 回顾之前提到的高亮。</li><li>阅读实体书时，写简单的索引卡片，然后将卡片拍照导入 Evernote 中。</li></ul></li></ul><h2 id="机器学习">机器学习</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/111842425" target="_blank" rel="noopener">为什么有些深度学习网络要加入Product层？ - 知乎</a>：解释为什么 MLP 只包含特征累加而有学习特征交叉的能力，后面展开讲了一些提高模型特征交叉能力的方法。</li><li><a href="https://blog.csdn.net/u011508640/article/details/72815981" target="_blank" rel="noopener">详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解_网络_nebulaf91的博客-CSDN博客</a>：看过讲 MLE 和 MAP 比较清晰的文章。刚看开头的时候，想到自己大学上过《概率论和统计》居然没有考虑过概率和统计有什么区别……</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;读书&quot;&gt;读书&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/34672176/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;呼吸 (豆瓣)&lt;/a&gt;：这是一本由 Byte.Coffe
      
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
  </entry>
  
  <entry>
    <title>每月分享 202002 「山川异域，风月同天」</title>
    <link href="https://xiang578.com/post/monthly-issue-202002.html"/>
    <id>https://xiang578.com/post/monthly-issue-202002.html</id>
    <published>2020-03-07T07:45:15.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<p>现在看来，又有多少人预测到这一次超级黑天鹅事件。</p><a id="more"></a><blockquote><p>这里记录过去一个月，我看到、想到值得分享的东西。</p></blockquote><h2 id="x03-how-instapaper-changed-my-kindle-life-for-the-better">0x03 <a href="https://www.guidingtech.com/29107/instapaper-kindle-merits/" target="_blank" rel="noopener">How Instapaper Changed My Kindle Life For the Better</a></h2><p>利用 Instapaper 定时将稍后读文章发送到 kindle 上。</p><h2 id="x02-这是我为武汉雷神山火神山医院设计的品牌形象标志logo---步行街主干道---虎扑社区">0x02 <a href="https://bbs.hupu.com/32137599.html" target="_blank" rel="noopener">这是我为武汉雷神山、火神山医院设计的品牌形象标志logo - 步行街主干道 - 虎扑社区</a></h2><p><img src="https://media.xiang578.com/15807855350677.jpg"></p><h2 id="x01-山川异域风月同天">0x01 <a href="https://weibo.com/1218287234/Is1lGejd6?type=comment#_rnd1580519347991" target="_blank" rel="noopener">“山川异域，风月同天”</a></h2><blockquote><p><span class="citation" data-cites="扎宝">@扎宝</span>：日本汉语水平考试HSK事务所捐赠给武湖北的物资，20000个口罩和一批红外体温计。 标签上写着“山川异域，风月同天”，感动[泪][泪] 求一个英文译文！ p.s. 据记载鉴真事迹的历史典籍《东征传》记载：日本长屋亲王在赠送大唐的千件袈裟上绣“山川异域，风月同天，寄诸佛子，共结来缘”偈。鉴真大师被此偈打动，决心东渡弘法。</p></blockquote><blockquote><p><span class="citation" data-cites="文冤阁大学士">@文冤阁大学士</span>：We are created to share Nature and love. 扫了下原博评转，翻得都差我好几座唐招提寺。嘻嘻。</p></blockquote><p><img src="https://media.xiang578.com/15807341522123.jpg"></p><h2 id="x00-xgboost">0x00 <a href="https://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">XGBoost</a></h2><p>春节在家，重新把这些经典的内容再拿出来多读几遍。网上写的那些总结感觉都不是很好，还是要回去看论文。说句实话，纸上谈兵这么久，居然没有跑过 xgb 的包……</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现在看来，又有多少人预测到这一次超级黑天鹅事件。&lt;/p&gt;
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
  </entry>
  
  <entry>
    <title>2019 起步</title>
    <link href="https://xiang578.com/post/2019.html"/>
    <id>https://xiang578.com/post/2019.html</id>
    <published>2020-02-04T13:44:09.000Z</published>
    <updated>2020-09-06T09:59:06.820Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>受 <a href="http://freemind.pluskid.org/" target="_blank" rel="noopener">Free Mind</a> 的影响按这种形式写年度总结</p></blockquote><p>年初的时候看到一句话：「 2019 是过去十年中最差的一年，也是未来十年中最好的一年」。和其他人一样，我害怕不确定性，不过生活除了鼓起勇气前进，还有什么其他选择。</p><h2 id="工作">工作</h2><p>完整在滴滴工作一年，自己没有太多变化，可是环境却变了很多。从年初内部会议上 Will 优化员工开始，很多同事陆续离开，从而我都快要成为团队元老……</p><p>做为食物链低端的算法工程师，工作中杂七杂八的事情干了很多。洗数据、跑模型、改工程代码、测试、上线、实验各个方面都干过。</p><p>说回来，算法还是自己的主要工具。今年用的最多的是 FM 和 GBDT，这些都是几年前的技术，但是架不住效果好，性能要求小。自己也写了一些相关的文章，可以供大家参考。</p><blockquote><p><a href="https://xiang578.com/post/fm.html">(FM) Factorization Machines | 算法花园</a></p><p><a href="https://xiang578.com/post/ftrl.html">(FTRL) Follow The Regularized Leader | 算法花园</a></p><p><a href="https://xiang578.com/post/gbdt.html">All About GBDT (1) | 算法花园</a></p><p><a href="https://xiang578.com/post/gbdt_lr.html">Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr) | 算法花园</a></p></blockquote><p>关于深度学习，在我入职前模型就基本迭代完成，今年主要探索个性化场景的解决以及模型性能优化。很遗憾，这两方面的工作目前还没有什么可以写成博客分享的。最后，自己没有参与到组内强化学习的项目中，不过还是通过李宏毅老师的相关课程了解初步的概念，争取 20 年内做一些相关的事情。</p><p>9月份开始，leader突然让我准备一些编程题目，开始去面试实习生。通过牛客网以及北邮人论坛大概收到简历60多份，我面试10多个候选人，最终通过的大概五六人，不过过来实习的也就 2 个。印证自己之前的想法，一家已经不是快速发展的公司，很难招到即懂机器学习又会做编程题的实习生。</p><p>另外想写的一点是是匿名交流。内部论坛之前有一个匿名区，后来由于一件比较有名的事情，匿名喷得太厉害，被某位海归高管以提高交流效率减少戾气所关闭（目前这位已经离职，有人开玩笑期待干掉他新公司的匿名论坛）。所幸脉脉还有职言（匿名）以及公司圈。在上面混了一年之后，越来越理解匿名交流的必要，说事。比如今年发生的延迟发年终奖，快手可以直接在内部匿名区引起宿华回复。我们的公司圈一堆人才自嗨。本质是国内环境下很难公开交流一些话题。</p><p>之前看过[一篇文章]中介绍 Google 的 TGIF：</p><blockquote><p>TGIF是Larry和Sergey在公司早期就创立的，一个全公司范围的周会。在这个周会上高管们会透露公司新项目的进展，也安排有答疑环节，员工可以询问两位创始人任何问题。TGIF毫无疑问是为了提高公司内部的透明度，但它在增强员工凝聚力的同时也对公司文化提出了挑战，最直接的就是保密问题。比如Chrome项目在公司内部的公开就是在一次TGIF上发生的，那时离Chrome的正式对外宣布早了一年多的时间。</p></blockquote><h2 id="阅读">阅读</h2><p>今年读过 <a href="https://book.douban.com/people/xiang578/collect?sort=time&amp;tags_sort=count&amp;filter=all&amp;tag=2019&amp;mode=grid" target="_blank" rel="noopener">33</a> 本书，阅读量和前几年基本持平。年底发现自己的一个坏习惯：很多书读了一半就放在那里，导致开的坑很多。应对方法也很简单：一段时间内只读一本书。而且为了提高阅读的质量，将自己读完一本书的定义从读到最后一页改成完成对这本书内容的整理。</p><p>阅读的主要工具是 kindle 和微信阅读（iPhone）。kindle 是这么多年一直使用的阅读介质，从前几年的找破解图书到现在的完全中亚购买（以目前看书的频率还不至于承受不住），长时间看电子水墨屏能减轻一些疲劳。微信阅读的特点是白领无限卡后就能全场免费读，实在是太香了。理想状态下用这两个工具读不类型的材料，微信阅读读小说以及人物传记，kindle 看需要大量抄记的书。对于需要反复阅读的内容，实体书则是最佳的选择。</p><p>分享读完觉得不错的几本书：</p><ol type="1"><li>经济学通识/薛兆丰经济学讲义：薛兆丰目前看起来风评不是很好，这两本也不是什么严肃的经济学读物。前一本书是作者专栏文章的合集，后一本是得到专栏的文字版，两本书大量的内容是重复的。书中通过现实中的例子来讲解背后的经济学原理，很适合看完之后做为饭后谈资。反正我运用书中的一些原理，给同事分析好久公司的停车场应该怎么分配车位。</li><li>银河帝国：这一套书有很多本，只看完前三本。概括起来，这本小说是以太空为背景讲政治故事，以谢顿的预言为主线，讲述基地对抗各种危机的挑战。另外书中提到看起来有多少分像统计学的心理史学，谢顿一直用这种方法预测未来，而且信徒们一直强调，预测结果不会因为个人而改变。第三本书，围绕寻找第二基地展开，把所有读者能猜的地方都写了出来，选择了一种情理之中，意料之外的结局。</li><li>人类简史/未来简史：尤瓦尔·赫拉利是前几年很火的一个历史学家。人类简史主要是按他的框架回顾从原始人类到现代人类的文明发展历程。对于我这种没有系统接受过历史学教育的人，完全是一种震撼。未来简史讨论的是人类未来的发展方向，成神。</li><li>房思琪的初戀樂園：讲述一个小女孩被文明所不齿的方式杀死的故事，最让人痛心的这女孩就是作者本人……引用最近很流行的一句话：地狱空荡荡，魔鬼在人间。</li><li>基督山伯爵：看完《了不起的盖茨比》后，老板强力推荐的爽文小说。快意恩仇，永远不要丧失对生活的期望。</li><li>临高启明：工科党神书，死于历史空无主义，最后放上来缅怀一下。</li></ol><p>2020 年开始使用 Notion 记录读书过程，点击 <a href="https://www.notion.so/ryanx/4666b7440155430880b9c9787adde5ab?v=39111f7ebd5e4be6a28d7ef712c4aebb" target="_blank" rel="noopener">看书也就图一乐</a> 查看。</p><h2 id="观影">观影</h2><p>和去年一样，看电影比起看书来更加容易，豆瓣上轻松标记 <a href="https://movie.douban.com/people/xiang578/collect?sort=time&amp;tags_sort=count&amp;filter=all&amp;tag=2018&amp;mode=grid" target="_blank" rel="noopener">60</a> 部。想想原因，打开一个视频放在那里，不用怎么理它就能结束。按类别推荐一下自己觉得不错的影视：</p><ol type="1"><li>小丑/蝙蝠侠三部曲：去年在观影中大力推荐漫威宇宙，今年看完 DC 这 4 部电影，刷新对超级英雄片的认知。蝙蝠侠：黑暗骑士在是在超级英雄的框架下对人性进行探讨。小丑展示出社会如何逼一个人成为恶魔。</li><li>黑客帝国三部曲：经典的电影，赛博朋克风格。之前的神话描述神创造了世界，在这部片子里面，这个神就变成了机器人。多少算是人工智能行业的从业者，强人工智能离我们看起来还是很远。</li><li>人生七年9：这应该是拍摄时间最慢长的纪录片，也给我们机会在几十个小时时间里面见证这些主人公 60 多年岁月。很大一个感受，除了 Nick 之外，其他人不过是重复父辈的道路，阶级跃迁又是谈何容易。</li><li>哪吒之魔童降世：即大鱼海棠之后，第二部在电影院看的动画电影。之前想过一个问题：为什么一些小说要隔一段时间就翻拍一次？简单的认为要赋予时代主题。这部片子中最喜欢的一个设定：龙族也是妖怪，镇守龙宫，其实也是镇住自己。</li><li>邪不压正：电影看到一半的时候，我就觉得自己看不懂。说回来，看这部电影有一种酣畅淋漓的感觉，节奏很快，比《一步之遥》和《让子弹飞》更快。半夜在知乎上看了很多回答之后渐渐地懂得其中的情节，蓝先生的爱国情怀，李的复仇梦想。在历史的框架下演绎，始终无法逃离历史的结果，日本人还会按照发展进入北京城。一句“异父异母的亲兄弟”就值得一看。</li></ol><h2 id="游戏">游戏</h2><p>今年新增的一个板块，自从购入 Switch 之后，开始重新接触一些游戏。</p><ol type="1"><li>隐形守护者：抗战背景下面，一个特工面对选择的游戏。所有的场景都是真实拍摄出来的，比绝大部分国内的抗战剧精美。游戏有很多个结局，当然只有符合社会主义核心价值观的才算善终。最印象深刻是第二号突然的一句：什么都是马尔可夫链。</li><li>有氧拳击/健身环大冒险：NS 上的铁人三项之二，充分发挥体感的优势，晚上下班之后健身用。不过从目前的使用频率来看，大概率和买健身卡一个性质。</li><li>塞尔达传说：旷野之息：决定买 Switch 很大程度上源于少数派中一篇关于这个游戏的介绍，大意是没有传统的等级增长，只有你真正掌握一个技巧，林克才能使用出来。这款游戏的给我带来看似无限大的空间，但也有一点遗憾，有时候遇到下雨不好攀岩时，我想让林克坐下等雨停，然后发现没有坐下的选项……</li><li>超级马力欧创作家2：大学的时候，经常看超级小桀玩这个游戏。对于我这种连普通的马里奥都要靠无敌才能通关的来说，大部分自制的地图还是有点难的。说回来，买这个游戏就是买一个青春。只可惜物是人非。</li><li>暗黑破坏神3：永恒之战版 ：Switch 上的冷饭，自己瞎玩了很久，看完所有的剧情。最终在咸鱼上买了很多强力的装备后，速通 150 层大秘境后索然无味。所以玩游戏还是不要作弊。</li></ol><h2 id="未来">未来</h2><p>世界变化太快，未来可期。</p><p>于浙江临海</p><p><a href="https://xiang578.com/post/2017.html">2017 迷茫</a> &gt;&gt; <a href="https://xiang578.com/post/2018.html">2018 探索</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;受 &lt;a href=&quot;http://freemind.pluskid.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Free Mind&lt;/a&gt; 的影响按这种形式写年度总结&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;年初的
      
    
    </summary>
    
    
      <category term="life" scheme="https://xiang578.com/tags/life/"/>
    
      <category term="book" scheme="https://xiang578.com/tags/book/"/>
    
      <category term="movie" scheme="https://xiang578.com/tags/movie/"/>
    
      <category term="game" scheme="https://xiang578.com/tags/game/"/>
    
  </entry>
  
  <entry>
    <title>All About GBDT (1)</title>
    <link href="https://xiang578.com/post/gbdt.html"/>
    <id>https://xiang578.com/post/gbdt.html</id>
    <published>2020-01-26T06:15:43.000Z</published>
    <updated>2020-09-06T09:59:06.820Z</updated>
    
    <content type="html"><![CDATA[<p>GBDT(Gradient Boosting Decision Tree) 从名字上理解包含三个部分：提升、梯度和树。它最早由 Freidman 在 <code>greedy function approximation ：a gradient boosting machine</code> 中提出。很多公司线上模型是基于 GBDT+FM 开发的，我们 Leader 甚至认为 GBDT 是传统的机器学习集大成者。断断续续使用 GBDT 一年多后，大胆写一篇有关的文章和大家分享。</p><h2 id="朴素的想法">朴素的想法</h2><p>假设有一个游戏：给定数据集 <span class="math inline">\({(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}\)</span>，寻找一个模型<span class="math inline">\({\hat y=F(x_i)}\)</span>，使得平方损失函数 <span class="math inline">\({\sum \frac{1}{2}(\hat y_i - y_i)^2}\)</span> 最小。</p><p>如果你的朋友提供一个可以使用但是不完美的模型，比如 <span class="math display">\[F(x_1)=0.8,y_1=0.9\]</span> <span class="math display">\[F(x_2)=1.4,y_2=1.3\]</span> 在如何不修改这个模型的参数情况下，提高模型效果？</p><p>一个简单的思路是：重新训练一个模型实现 <span class="math display">\[F(x_1)+h(x_1)=y_1\]</span> <span class="math display">\[F(x_2)+h(x_2)=y_2\]</span> <span class="math display">\[...\]</span> <span class="math display">\[F(x_n)+h(x_n)=y_n\]</span></p><p>换一个角度是用模型学习数据 <span class="math inline">\({(x_1,y_1-F(x_1)),(x_2,y_2-F(x_2)),...,(x_n,y_n-F(x_n))}\)</span>。得到新的模型 <span class="math inline">\({\hat y=F(x_i)+h(x_i)}\)</span>。</p><p>其中 <span class="math inline">\({y_i-F(x_i)}\)</span> 的部分被我们称之为残差，即之前的模型没有学习到的部分。重新训练模型 <span class="math inline">\({h(x)}\)</span>正是学习残差。如果多次执行上面的步骤，可以将流程描述成：</p><p><span class="math display">\[{F_0(x)}\]</span> <span class="math display">\[{F_1(x)=F_0(x)+h_1(x)}\]</span> <span class="math display">\[{F_2(x)=F_1(x)+h_2(x)}\]</span> <span class="math display">\[{...}\]</span> <span class="math display">\[{F_t(x)=F_t-1(x)+h_t(x)}\]</span></p><p>即 <span class="math inline">\({F(x;w)=\sum^T_{t=1}h_t(x;w)}\)</span>，这也就是 GBDT 。</p><h2 id="如何理解-gradient-boosting-decision-tree">如何理解 Gradient Boosting Decision Tree ?</h2><p>Gradient Boosting Decision Tree 简称 GBDT，最早由 Friedman 在论文《Greedy function approximation: a gradient boosting machine》中提出。简单从题目中理解包含三个部分内容：Gradient Descent、Boosting、Decision Tree。</p><p>Decision Tree 即决策树，利用超平面对特征空间划分来预测和分类，根据处理的任务不同分成两种：分类树和回归树。在 GBDT 算法中，用到的是 CART 即分类回归树。用数学语言来描述为 <span class="math inline">\({F=\{f(x)=w_{q(x)}\}}\)</span>，完成样本 <span class="math inline">\({x}\)</span> 到决策树叶子节点 <span class="math inline">\({q(x)}\)</span> 的映射，并将该叶子节点的权重 <span class="math inline">\({w_{q(x)}}\)</span> 赋给样本。CART 中每次通过计算 gain 值贪心来进行二分裂。</p><p>Boosting 是一种常用的集成学习方法（另外一种是 Bagging）。利用弱学习算法，反复学习，得到一系列弱分类器（留一个问题，为什么不用线性回归做为弱分类器）。然后组合这些弱分类器，构成一个强分类器。上面提到的模型 <span class="math inline">\({F(x;w)=\sum^T_{t=1}h_t(x)}\)</span> 即是一种 boosting 思路，依次训练多个 CART 树 <span class="math inline">\({h_i}\)</span>，并通过累加这些树得到一个强分类器 <span class="math inline">\({F(x;w)}\)</span>。</p><h2 id="为什么-gbdt-可行">为什么 GBDT 可行？</h2><p>在 2 中我提到 GBDT 包括三个部分并且讲述了 Boosting 和 Decison Tree。唯独没有提到 Gradient Descent，GBDT 的理论依据却恰恰和它相关。</p><p>回忆一下，Gradient Descent 是一种常用的最小化损失函数 <span class="math inline">\({L(\theta)}\)</span> 的迭代方法。</p><ul><li>给定初始值 <span class="math inline">\({\theta_0}\)</span></li><li>迭代公式：<span class="math inline">\({\theta ^t = \theta ^{t-1} + \Delta \theta}\)</span></li><li>将 <span class="math inline">\({L(\theta ^t)}\)</span> 在 <span class="math inline">\({\theta ^{t-1}}\)</span> 处进行一阶泰勒展开：<span class="math inline">\({L(\theta ^t)=L(\theta ^{t-1} + \Delta \theta) \approx L(\theta ^{t-1}) + L^\prime(\theta ^{t-1})\Delta \theta}\)</span></li><li>要使 <span class="math inline">\({L(\theta ^t) &lt; L(\theta ^{t-1}) }\)</span>，取 <span class="math inline">\({\Delta \theta = -\alpha L^\prime(\theta ^{t-1})}\)</span></li><li>其中 <span class="math inline">\({\alpha}\)</span> 是步长，可以通过 line search 确定，但一般直接赋一个很小的数。</li></ul><p>在 1 中提到的问题中，损失函数是 MSE <span class="math inline">\({L(y, F(x))=\frac{1}{2}(y_i - f(x_i))^2}\)</span>。</p><p>我们的任务是通过调整 <span class="math inline">\({F(x_1), F(x_2), ..., F(x_n)}\)</span> 最小化 <span class="math inline">\({J=\sum_i L(y_i, F(x_i))}\)</span>。</p><p>如果将 <span class="math inline">\({F(x_i)}\)</span> 当成是参数，并对损失函数求导得到 <span class="math inline">\({ \frac{\partial J}{\partial F(x_i)} = \frac{\partial \sum_i L(y_i, F(x_i))}{\partial F(x_i)} = \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} = F(x_i)-y_i}\)</span>。</p><p>可以发现，在 1 中提到的模型 <span class="math inline">\({h(x)}\)</span> 学习的残差 <span class="math inline">\({y_i-F(x_i)}\)</span>正好等于负梯度，即 <span class="math inline">\({y_i-F(x_i)=-\frac{\partial J}{\partial F(x_i)}}\)</span>。</p><p>所以，参数的梯度下降和函数的梯度下降原理上是一致的：</p><ul><li><span class="math inline">\({F_{t+1}(x_i)=F_t(x_i)+h(x_i)=F(x_i)+y_i-F(x_i)=F_t(x_i)-1\frac{\partial J}{\partial F(x_i)}}\)</span></li><li><span class="math inline">\({\theta ^t = \theta ^{t-1} + \alpha L^\prime(\theta ^{t-1})}\)</span></li></ul><h2 id="gbdt-算法流程">GBDT 算法流程</h2><p>模型 F 定义为加法模型：</p><p><span class="math display">\[{F(x;w)=\sum^{M}_{m=1} \alpha_m h_m(x;w_m) = \sum^{M}_{m=1}f_t(x;w_t)}\]</span> 其中，x 为输入样本，h 为分类回归树，w 是分类回归树的参数，<span class="math inline">\({\alpha}\)</span> 是每棵树的权重。</p><p>通过最小化损失函数求解最优模型：<span class="math inline">\({F^* = argmin_F \sum^N_{i=1}L(y_i, F(x_i))}\)</span></p><p>输入: <span class="math inline">\({(x_i,y_i),T,L}\)</span></p><ol type="1"><li>初始化：<span class="math inline">\({f_0(x)}\)</span></li><li>对于 <span class="math inline">\({t = 1 to T}\)</span> ：<ol type="1"><li>计算负梯度（伪残差）： <span class="math inline">\({ \tilde{y_i} = -[\frac{\partial L(y_i, F(x_i))}{\partial F(x)}]_{F(x)=F_{m-1}(x)} ,i=1,2,...,N}\)</span></li><li>根据 <span class="math inline">\({\tilde{y_i}}\)</span> 学习第 m 棵树： <span class="math inline">\({w^*=argmin_{w} \sum_{i=1}^N(\tilde{y_i} - h_t(x_i;w))^2}\)</span></li><li>line searcher 找步长：<span class="math inline">\({\rho^* = argmin_\rho \sum_{i=1}^{N}L(y_i, F_{t-1}(x_i)+\rho h_t(x_i;w^*))}\)</span></li><li>令 <span class="math inline">\({f_t=\rho^*h_t(x;w*)}\)</span>，更新模型：<span class="math inline">\({F_t=F_{t-1}+f_t}\)</span></li></ol></li><li>输出 <span class="math inline">\({F_T}\)</span></li></ol><p>说明：</p><ol type="1"><li>初始化 <span class="math inline">\({f_0}\)</span> 方法<ol type="1"><li>求解损失函数最小</li><li>随机初始化</li><li>训练样本的充分统计量</li></ol></li><li>每一轮拟合负梯度，而不是拟合残差，是为方便之后扩展到其他损失函数。</li><li>最小化问题中，如果有解析解，直接带入。否则，利用泰勒二阶展开，Newton Step 得到近似解。</li></ol><p>这一篇就先到这里，之后还会分享 GBDT 常用损失函数推导以及 XGboost 相关内容。如果有任何想法，都可以在留言区和我交流。</p><h2 id="reference">Reference</h2><ol type="1"><li>李航, 《统计学习方法》8.4 提升树</li><li>Freidman，greedy function approximation ：a gradient boosting machine</li><li><a href="https://zhuanlan.zhihu.com/p/73381835" target="_blank" rel="noopener">【19年ML思考笔记】GBDT碎碎念（1）谈回归树的分裂准则 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/29765582" target="_blank" rel="noopener">机器学习-一文理解GBDT的原理-20171001 - 知乎</a></li><li><a href="https://louisscorpio.github.io/2017/12/13/GBDT%E5%85%A5%E9%97%A8%E8%AF%A6%E8%A7%A3/#" target="_blank" rel="noopener">GBDT入门详解 - Scorpio.Lu|Blog</a></li><li><a href="https://stackoverflow.com/questions/45409110/why-gradient-boosting-not-working-in-linear-regression" target="_blank" rel="noopener">python - Why Gradient Boosting not working in Linear Regression? - Stack Overflow</a></li><li><a href="https://blog.csdn.net/qq_24519677/article/details/82020863" target="_blank" rel="noopener">GBDT基本原理及算法描述 - Y学习使我快乐V的博客 - CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/30711812" target="_blank" rel="noopener">GBDT的那些事儿 - 知乎</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;GBDT(Gradient Boosting Decision Tree) 从名字上理解包含三个部分：提升、梯度和树。它最早由 Freidman 在 &lt;code&gt;greedy function approximation ：a gradient boosting machi
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ml" scheme="https://xiang578.com/tags/ml/"/>
    
      <category term="gdbt" scheme="https://xiang578.com/tags/gdbt/"/>
    
  </entry>
  
  <entry>
    <title>(FTRL) Follow The Regularized Leader</title>
    <link href="https://xiang578.com/post/ftrl.html"/>
    <id>https://xiang578.com/post/ftrl.html</id>
    <published>2020-01-03T13:26:06.000Z</published>
    <updated>2020-09-06T09:59:06.820Z</updated>
    
    <content type="html"><![CDATA[<p>FTRL 是 Google 提出的一种优化算法。常规的优化方法例如梯度下降、牛顿法等属于批处理算法，每次更新需要对 batch 内的训练样本重新训练一遍。在线学习场景下，我们希望模型迭代速度越快越好。例如用户发生一次点击行为后，模型就能快速进行调整。FTRL 在这个场景中能求解出稀疏化的模型。</p><h2 id="基础知识">基础知识</h2><ul><li>L1 正则比 L2 正则可以产生更稀疏的解。</li><li>次梯度：对于 L1 正则在 <span class="math inline">\(x=0\)</span> 处不可导的情况，使用次梯度下降来解决。次梯度对应一个集合 <span class="math inline">\(\{v: v(x-x_t) \le f(x)-f(x_t)\}\)</span>，集合中的任意一个元素都能被当成次梯度。以 L1 正则为例，非零处梯度是 1 或 -1，所以 <span class="math inline">\(x=0\)</span> 处的次梯度可以取 <span class="math inline">\([-1, 1]\)</span> 之内任意一个值。</li></ul><h2 id="ftl">FTL</h2><p>FTL(Follow The Leader) 算法：每次找到让之前所有损失函数之和最小的参数。</p><p><span class="math display">\[W=argmin_W \sum^t_{i=1}F_i(W)\]</span></p><p>FTRL 中的 R 是 Regularized，可以很容易猜出来在 FTL 的基础上加正则项。</p><p><span class="math display">\[W=argmin_W \sum^t_{i=1}F_i(W) + R(W)\]</span></p><h2 id="代理函数">代理函数</h2><p>FTRL 的损失函数直接很难求解，一般需要引入一个代理损失函数 <span class="math inline">\(h(w)\)</span>。代理损失函数常选择比较容易求解析解以及求出来的解和优化原函数得到的解差距不能太大。</p><p>我们通过两个解之间的距离 Regret 来衡量效果：</p><p><span class="math display">\[\begin{array}{c}{w_{t}=\operatorname{argmin}_{w} h_{t-1}(w)} \\ {\text {Regret}_{t}=\sum_{t=1}^{T} f_{t}\left(w_{t}\right)-\sum_{t=1}^{T} f_{t}\left(w^{*}\right)}\end{array}\]</span></p><p>其中 <span class="math inline">\(w^{*}\)</span> 是直接优化 FTRL 算法得到的参数。当距离满足 <span class="math inline">\(\lim _{t \rightarrow \infty} \frac{\text {Regret}_{t}}{t}=0\)</span>，损失函数认为是有效的。其物理意义是，随着训练样本的增加，两个优化目标优化出来的参数效果越接近。</p><h2 id="推导过程">推导过程</h2><p>参数 <span class="math inline">\(w_{t+1}\)</span> 的迭代公式：</p><p><span class="math display">\[{w_{t+1}=argmin_w\{ g_{(1:t)}w + \frac{1}{2} \sum_{s=1}^t \sigma_s \lVert w - w_s \rVert ^2   + \lambda_1 \lVert W \rVert_1 + \frac{1}{2} \lambda_2 \lVert W \rVert^2 \}}\]</span></p><p>其中 <span class="math inline">\(g_{(1:t)}=\sum^{t}_{s=1}g_s\)</span>，<span class="math inline">\(g_s\)</span> 为 <span class="math inline">\(f(w_s)\)</span> 的次梯度。参数 <span class="math inline">\(\sum^t_{s=1}\sigma_s=\frac{1}{\eta _t}\)</span>，学习率 <span class="math inline">\(\eta _t = \frac{1}{\sqrt{t}}\)</span>，随着迭代轮数增加而减少。</p><p>展开迭代公式</p><p><span class="math display">\[{F(w)=  g_{(1:t)}w + \frac{1}{2} \sum_{s=1}^t \sigma_s \lVert w - w_s \rVert ^2   + \lambda_1 \lVert W \rVert_1 + \frac{1}{2} \lambda_2 \lVert W \rVert^2 }\]</span></p><p><span class="math display">\[{F(w)=  g_{(1:t)}w + \frac{1}{2} \sum_{s=1}^t \sigma_s ( w^Tw - 2w^Tw_s + w_s^Tw_s)   + \lambda_1 \lVert W \rVert_1 + \frac{1}{2} \lambda_2 \lVert W \rVert^2 }\]</span></p><p><span class="math display">\[{F(w)=  (g_{(1:t)} - \sum_{s=1}^t \sigma_s w_s)w + \frac{1}{2} (\sum_{s=1}^t \sigma_s + \lambda_2) w^Tw   + \lambda_1 \lVert W \rVert_1 + const }\]</span></p><p><span class="math display">\[{F(w)=  z_t^Tw + \frac{1}{2} (\frac{1}{\eta _t} + \lambda_2) w^Tw   + \lambda_1 \lVert W \rVert_1 + const }\]</span></p><p>其中 <span class="math inline">\({z_{t-1}=g^{(1:t-1)} - \sum_{s=1}^{t-1} \sigma_s w_s}\)</span>。</p><p>对 <span class="math inline">\(F(w)\)</span> 求偏导得到：</p><p><span class="math display">\[{z_t + (\frac{1}{\eta _t} + \lambda_2) w + \lambda_1 \partial \lvert W \rvert = 0}\]</span></p><p><span class="math inline">\(w\)</span> 和 <span class="math inline">\(z\)</span> 异号时，等式成立。</p><p>根据基础知识里面提到的对于 L1 正则利用偏导数代替无法求解的情况，得到：</p><p><span class="math display">\[\partial|W|=\left\{\begin{array}{ll}{0,} &amp; {\text { if }-1&lt;w&lt;1} \\ {1,} &amp; {\text { if } w&gt;1} \\ {-1,} &amp; {\text { if } w&lt;-1}\end{array}\right.\]</span></p><ol type="1"><li>当 <span class="math inline">\({ z_t &gt; \lambda_1}\)</span> 时，<span class="math inline">\({w_i &lt; 0}\)</span> , <span class="math inline">\({w_i = \frac{- z_t + \lambda_1 }{\frac{1}{\eta _t} + \lambda_2 }}\)</span></li><li>当 <span class="math inline">\({ z_t &lt; - \lambda_1}\)</span> 时，<span class="math inline">\({w_i &gt; 0}\)</span> , <span class="math inline">\({w_i = \frac{- z_t - \lambda_1 }{\frac{1}{\eta _t} + \lambda_2 }}\)</span></li><li>当 <span class="math inline">\({ \lvert z_t \rvert &lt; \lambda_1}\)</span> 时，当且仅当 <span class="math inline">\({w_i=0}\)</span> 成立</li></ol><p>因此可得：</p><p><span class="math display">\[w_{i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if }\left|z_{i}\right| \leq \lambda_{1}} \\ {\frac{-\left(z_{i}-\text sgn(z_i) \lambda_{1}\right)}{\eta_{t}+\lambda_{2}},} &amp; {\text { if others }}\end{array}\right.\]</span></p><h2 id="ftrl-和-sgd-的关系">FTRL 和 SGD 的关系</h2><p>将 SGD 的迭代公式写成：<span class="math inline">\({W^{t+1}=W^t - \eta _tg_t}\)</span></p><p>FTRL 迭代公式为：<span class="math inline">\({W^{t+1}=argmin_w\{ G^{(1:t)}W + \lambda_1 \lVert W \rVert_1 +\lambda_2 \frac{1}{2} \lVert W \rVert \}}\)</span></p><p>代入 <span class="math inline">\({\sum^t_{s=1}\sigma _s= \frac{1}{\eta _t}}\)</span> 到上面的公式中，得到 <span class="math inline">\({W^{t+1}=argmin_w\{ \sum_t^{s=1}g_sW + \frac{1}{2} \sum^t_{s=1}\sigma _s\lVert W - W_s \rVert_2^2 \}}\)</span></p><p>求偏导得到 <span class="math inline">\({\frac{\partial f(w)}{\partial w} = \sum^t_{s=1}g_s + \sum^t_{s=1}\sigma _s( W - W_s )}\)</span></p><p>令偏导等于 0 ：<span class="math inline">\({\sum^t_{s=1}g_s + \sum^t_{s=1}\sigma _s( W^{t+1} - W_s ) = 0}\)</span></p><p>化简得到：<span class="math inline">\({(\sum^t_{s=1}\sigma _s) W^{t+1} = \sum^t_{s=1}\sigma _s W^{s} - \sum^t_{s=1}g_s}\)</span></p><p>代入 <span class="math inline">\(\sigma\)</span>：<span class="math inline">\({\frac{1}{\eta _t} W^{t+1} = \sum^t_{s=1}\sigma _s W^{s} - \sum^t_{s=1}g_s}\)</span></p><p>根据上一个公式得出上一轮的迭代公式：<span class="math inline">\({\frac{1}{\eta _{t-1}} W^{t} = \sum^{t-1}_{s=1}\sigma _s W^{s} - \sum^{t-1}_{s=1}g_s}\)</span></p><p>两式相减：<span class="math inline">\({\frac{1}{\eta _t} W^{t+1} - \frac{1}{\eta _{t-1}} W^{t} = (\frac{1}{\eta _t} - \frac{1}{\eta _{t-1}}) W_t - g_t}\)</span></p><p>最终化简得到和 SGD 迭代公式相同的公式：<span class="math inline">\({W_{t+1} = W_t - \eta_t g_t}\)</span></p><h2 id="ftrl-工程化伪代码">FTRL 工程化伪代码</h2><p>引用自论文 <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41159.pdf" target="_blank" rel="noopener">Ad Click Prediction: a View from the Trenches</a></p><p>下面的伪代码中学习率和前面公式推导时使用的一些不一样： <span class="math inline">\(\eta_{t_{i}}=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^{t} g_{s_{i}}^{2}}}\)</span>。Facebook 在 GBDT + LR 的论文中研究过不同的学习率影响，具体可以参看博文 <a href="https://xiang578.com//post/media/gbdt_lr.html#%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%89%E6%8B%A9">Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr) | 算法花园</a>。</p><figure><img src="https://media.xiang578.com/15780559628822.jpg" alt="FTRL"><figcaption>FTRL</figcaption></figure><h2 id="例fm-使用-ftrl-优化">例：FM 使用 FTRL 优化</h2><p>FM 是工业界常用的机器学习算法，在之前博文 <a href="https://xiang578.com/post/fm.html">(FM)Factorization Machines</a> 中有简单的介绍。内部的 FTRL+FM 代码没有开源，所以也不好分析。从 <a href="https://zhuanlan.zhihu.com/p/58508137" target="_blank" rel="noopener">FM+FTRL算法原理以及工程化实现 - 知乎</a> 中找了一张 FTRL+FM 的伪代码图片。</p><p><img src="https://media.xiang578.com/15780576261639.jpg"></p><h2 id="reference">Reference</h2><ul><li><a href="https://tech.meituan.com/2016/04/21/online-learning.html" target="_blank" rel="noopener">Online Learning算法理论与实践 - 美团技术团队</a></li><li><a href="https://zhuanlan.zhihu.com/p/32694097" target="_blank" rel="noopener">FTRL公式推导 - 知乎</a></li><li><a href="https://blog.csdn.net/fangqingan_java/article/details/51020653" target="_blank" rel="noopener">每周一文】Ad Click Prediction: a View from the Trenches(2013)_机器学习,CTR,online_fangqingan_java的专栏-CSDN博客</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;FTRL 是 Google 提出的一种优化算法。常规的优化方法例如梯度下降、牛顿法等属于批处理算法，每次更新需要对 batch 内的训练样本重新训练一遍。在线学习场景下，我们希望模型迭代速度越快越好。例如用户发生一次点击行为后，模型就能快速进行调整。FTRL 在这个场景中能
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="fm" scheme="https://xiang578.com/tags/fm/"/>
    
      <category term="google" scheme="https://xiang578.com/tags/google/"/>
    
      <category term="ml" scheme="https://xiang578.com/tags/ml/"/>
    
      <category term="ftrl" scheme="https://xiang578.com/tags/ftrl/"/>
    
  </entry>
  
  <entry>
    <title>每月分享 202001 Fine-Tune Your Days</title>
    <link href="https://xiang578.com/post/monthly-issue-202001.html"/>
    <id>https://xiang578.com/post/monthly-issue-202001.html</id>
    <published>2020-01-01T08:55:11.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。</p></blockquote><h2 id="x04-你见过哪些让你目瞪口呆脑洞大开的骗局---sme情报员的回答---知乎">0x04 <a href="https://www.zhihu.com/question/41182911/answer/966701311" target="_blank" rel="noopener">你见过哪些让你目瞪口呆、脑洞大开的骗局？ - SME情报员的回答 - 知乎</a></h2><p>推荐其中的吉普赛读心术，当成脑筋急转弯来看。</p><blockquote><p>首先任选一个两位数，在心里默默记住，然后用这个两位数再依次减去它的十位和个位，最后用得数查表，找到对应的怪符号。 比如67,相应的计算就是67-6-7=54， 现在，在表中找到你心中数字经过计算后所对应的符号。</p></blockquote><p><img src="https://media.xiang578.com/15792508672842.jpg"></p><p>最后的答案都会是</p><p><img src="https://media.xiang578.com/15792508879083.jpg"></p><h2 id="x03-deep-neural-networks-for-youtube-recommendations-paper-ml">0x03 Deep Neural Networks for YouTube Recommendations #paper #ml</h2><p>Youtube 几年前的论文，最近拿过来看一下。工业界的论文最大的价值是提到的一些 tick，比如这篇论文中分析到用户对新视频的偏好，引入 example age 代表视频的上传到预测时的时间。再比如，给用户推荐视频时，考虑用户看过这个视频相关频道次数以及这个视频在用户实现中出现的次数。所以，做算法实现需要深入理解自己所处的场景。</p><p>推荐知乎上一些关于这篇论文的解读：</p><blockquote><ul><li><a href="https://zhuanlan.zhihu.com/p/25343518" target="_blank" rel="noopener">Deep Neural Network for YouTube Recommendation论文精读 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/52169807" target="_blank" rel="noopener">重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/52504407" target="_blank" rel="noopener">YouTube深度学习推荐系统的十大工程问题 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/61827629" target="_blank" rel="noopener">揭开YouTube深度推荐系统模型Serving之谜 - 知乎</a></li></ul></blockquote><h2 id="x02-fine-tune-your-days-with-the-scientific-method">0x02 Fine-Tune Your Days with the Scientific Method</h2><p>这个小标题出自 <a href="https://book.douban.com/subject/30327043/" target="_blank" rel="noopener">Make Time</a>，翻译成中文是利用科学的方法每天微调你的习惯。</p><h2 id="x01-2020-阅读看板">0x01 2020 阅读看板</h2><p>参考部分网友 Notion 的用法，搭建一个自己的阅读看板 <a href="https://www.notion.so/ryanx/4666b7440155430880b9c9787adde5ab?v=39111f7ebd5e4be6a28d7ef712c4aebb" target="_blank" rel="noopener">看书也就图一乐</a>。目前挑选出来的书远远超过前两年的阅读量，加油一起读书。</p><p>Notion 这种一个数据库 + 可选的 View 很接近我心目中任务管理软件的极限。</p><figure><img src="https://media.xiang578.com/15782399749595.jpg" alt="reading board"><figcaption>reading board</figcaption></figure><h2 id="x00-大佬们的年度总结">0x00 大佬们的年度总结</h2><p>新的一年开始时，最期待翻看大佬们的年度总结，罗列一些我觉得有总结。</p><ul><li><a href="https://yiming.dev/blog/2019/12/31/growing-a-result-driven-mindset/" target="_blank" rel="noopener">Growing a Result-Driven Mindset - Yiming Chen</a>：英文总结，Yiming 的博客给我带来学习英语并且用之来表达的动力。</li><li><a href="https://wdxmzy.com/pastfuture/year2019/2019/12/31/" target="_blank" rel="noopener">2019 总结与 2020 计划 | 小土刀 2.0</a>：从不同角度回顾自己的 2019 年。</li><li><a href="http://freemind.pluskid.org/misc/2019-summary/" target="_blank" rel="noopener">2019 时光小偷</a>：这位博主每年总结的标题都是一首歌，也是几年前看他的总结才开始尝试写自己的总结。</li><li><a href="https://zhuanlan.zhihu.com/p/100357148" target="_blank" rel="noopener">致敬时间的价值：一品十年 - 知乎</a>：和这个主题没有太大的关系，看一下其他人十年的总结，也能很好的指导自己的生活。</li><li><a href="http://zhengruonan.com/2019/12/29/2019-12-29-farewell-2019/" target="_blank" rel="noopener">2019年：下个十年路口，Farewell | Crossairplane的博客</a>：读这篇文章的时候突然想到一点，之后再看年终总结时，留言一句「新年快乐」。</li><li><a href="http://www.ztleespace.com/2020/01/01/create-vs-consume/" target="_blank" rel="noopener">Create vs. Consume - ends 2019 then starts 2020 - Ziting Li</a>：真诚的思考。</li><li><a href="https://www.hi-pda.com/forum/viewthread.php?tid=819978&amp;extra=page%3D1" target="_blank" rel="noopener">我的千书阅读计划 - 意欲蔓延 - Hi!PDA Hi!PDA</a>：fatdragoncat 13 年在 Hi!PDA 上立下愿望，这么多年过去，不知道数量上有没有达到，但是读书的收获已经改变他的生活。难得可贵这篇帖子展示他的变化过程。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;x04-你见过哪些让你目瞪口呆脑洞大开的骗局---sme情报员的回答---知乎&quot;&gt;0x04 &lt;a hre
      
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 PG PPO Q-Learing</title>
    <link href="https://xiang578.com/post/reinforce-learnning-basic.html"/>
    <id>https://xiang578.com/post/reinforce-learnning-basic.html</id>
    <published>2019-12-26T13:14:47.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<h2 id="info">Info</h2><p>课件下载：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">Hung-yi Lee - Deep Reinforcement Learning</a></p><p>课程视频：<a href="https://www.youtube.com/watch?v=z95ZYgPgXOY&amp;list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_" target="_blank" rel="noopener">DRL Lecture 1: Policy Gradient (Review) - YouTube</a></p><ul><li>Change Log<ul><li>20191226: 整理 PPO 相关资料</li><li>20191227: 整理 Q-Learning 相关资料</li><li>20200906: 拖延半年多没有整理笔记，将剩下的内容整理到单独的笔记中。</li></ul></li></ul><p>我的笔记汇总： - Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning：<a href="https://xiang578.com/post/reinforce-learnning-basic.html">李宏毅强化学习课程笔记 | 算法花园</a> - Actor Critic： -</p><h2 id="rl-基础">RL 基础</h2><p>强化学习基本定义：</p><ul><li>Actor：可以感知环境中的状态，通过执行不同的动作得到反馈的奖励，在此基础上进行学习优化。</li><li>Environment：指除 Actor 之外的所有事务，受 Actor 动作影响而改变其状态，并给 Actor 对应的奖励。</li><li>on-policy 和 off-policy 的区别在于 Actor 和 Environment 交互的策略和它自身在学习的策略是否是同一个。</li></ul><p>一些符号：</p><ul><li>State s 是对环境的描述，其状态空间是 S。</li><li>Action a 是 Actore 的行为描述，其动作空间是 A。</li><li>Policy <span class="math inline">\(\pi(a|s)=P[A_t=a|S_t=s]\)</span> 代表在给定环境状态 s 下 动作 a 的分布。</li><li>Reward <span class="math inline">\({r(s,a,s^{\prime})}\)</span> 在状态 s 下执行动作 a 后，Env 给出的打分。</li></ul><h2 id="policy-gradient">Policy Gradient</h2><p>Policy Network 最后输出的是概率。</p><p>目标：调整 actor 中神经网络 policy <span class="math inline">\(\pi(\theta)\)</span>，得到 <span class="math inline">\(a=\pi(s, \theta)\)</span>，最大化 reward。</p><p>trajectory <span class="math inline">\(\tau\)</span> 由一系列的状态和动作组成，出现这种组合的概率是 <span class="math inline">\(p_{\theta}(\tau)\)</span> 。</p><p><span class="math display">\[\begin{array}{l}{p_{\theta}(\tau)} \\ {\quad=p\left(s_{1}\right) p_{\theta}\left(a_{1} | s_{1}\right) p\left(s_{2} | s_{1}, a_{1}\right) p_{\theta}\left(a_{2} | s_{2}\right) p\left(s_{3} | s_{2}, a_{2}\right) \cdots} \\ {\quad=p\left(s_{1}\right) \prod_{l=1}^{T} p_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)}\end{array}\]</span></p><p>reward ：根据 s 和 a 计算得分 r，求和得到 R。在围棋等部分任务中，无法获得中间的 r（下完完整的一盘棋后能得到输赢的结果）。</p><p>需要计算 R 的期望 <span class="math inline">\(\bar{R}_{\theta}\)</span>，形式和 GAN 类似。如果一个动作得到 reward 多，那么就增大这个动作出现的概率。最终达到 agent 所做 policy 的 reward 一直都比较高。</p><p><span class="math display">\[\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)\]</span></p><p>强化学习中，没有 label。需要从环境中采样得到 <span class="math inline">\(\tau\)</span> 和 R，根据下面的公式去优化 agent。相当于去求一个 likelihood。</p><p><span class="math inline">\(\nabla f(x) = f(x) \frac{\nabla f(x)}{f(x)}= f(x) \nabla \log f(x)\)</span> ，这一步中用到对 log 函数进行链式求导。</p><p><span class="math display">\[\nabla \bar{R}_{\theta}=\sum_{\tau} R(\tau) \nabla p_{\theta}(\tau)\]</span></p><p><span class="math display">\[\begin{array}{l}{=E_{\left.\tau \sim p_{\theta}(\tau)[R(\tau)] \log p_{\theta}(\tau)\right]} \approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(\tau^{n}\right)} \\ {=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)}\end{array}\]</span></p><p>参数更新方法：</p><ol type="1"><li>在环境中进行采样，得到一系列的轨迹和回报。</li><li>利用状态求梯度，更新模型。如果 R 为正，增大概率 <span class="math inline">\(p_{\theta}(a_t|s_t)\)</span>, 否则减少概率。</li><li>重复上面的流程。</li></ol><p><img src="https://media.xiang578.com/15726799935625.jpg"></p><h3 id="pg-的例子">PG 的例子</h3><p>训练 actor 的过程看成是分类任务：输入 state ，输出 action。</p><p>最下面公式分别是反向传播梯度计算和 PG 的反向梯度计算，PG 中要乘以整个轨迹的 R。</p><figure><img src="https://media.xiang578.com/15752505145081.jpg" alt="PG"><figcaption>PG</figcaption></figure><p>tip 1： add a baseline</p><p>强化学习的优化和样本质量有关，避免采样不充分。Reawrd 函数变成 R-b，代表回报小于 b 的都被我们当成负样本，这样模型能去学习得分更高的动作。b 一般可以使用 R 的均值。</p><p>tip 2: assign suitable credit</p><p>一场游戏中，不论动作好坏，总会乘上相同的权重 R，这种方法是不合理的，希望每个 action 的权重不同。</p><ol type="1"><li>引入一个 discount rate，对 t 之后的动作 r 进行降权重。</li><li>利用 Advantage Function 评价状态 s 下动作 a 的好坏 critic。</li></ol><figure><img src="https://media.xiang578.com/15752505547153.jpg" alt="Assign Suitable Credit"><figcaption>Assign Suitable Credit</figcaption></figure><h2 id="ppo-proximal-policy-optimization">PPO: Proximal Policy Optimization</h2><h3 id="importance-sampling">importance sampling</h3><p>假设需要估计期望 <span class="math inline">\(E_{x~p[f(x)]}\)</span>，x 符合 p 分布，将期望写成积分的形式。由于在 P 分布下面很难采样，把问题转化到已知 q 分布上，得到在 p 分布下计算期望公式。</p><p><img src="https://media.xiang578.com/15771920437580.jpg"></p><p>上面方法得到 p 和 q 期望接近，但是方差可能相差很大，且和 <span class="math inline">\(\frac{p(x)}{q(x)}\)</span> 有关。</p><p>原分布的方差： <span class="math display">\[\operatorname{Var}_{x-p}[f(x)]=E_{x-p}\left[f(x)^{2}\right]-\left(E_{x-q}[f(x)]\right)^{2}\]</span></p><p>新分布的方差： <span class="math display">\[\begin{array}{l}{\operatorname{Var}_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}} \\ {\operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2}} \\ {=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}}\end{array}\]</span></p><p>在 p 和 q 分布不一致时，且采样不充分时，可能会带来比较大的误差。</p><figure><img src="https://media.xiang578.com/15771931618906.jpg" alt="Issue of Importance Sampling"><figcaption>Issue of Importance Sampling</figcaption></figure><h3 id="从-on-policy-到-off-policy">从 On-policy 到 Off-policy</h3><p>on-policy 时，PG 每次参数更新完成后，actor 就改变了，不能使用之前的数据，必须和环境重新互动收集数据。引入 <span class="math inline">\(p_{\theta \prime}\)</span> 进行采样，就能将 PG 转为 off-ploicy。</p><p><img src="https://media.xiang578.com/15771932374489.jpg"></p><p>和之前相比，相当于引入重要性采样，所以也有前一节中提到的重要性采样不足问题。</p><p><span class="math display">\[J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]\]</span></p><h3 id="ppotrpo">PPO/TRPO</h3><p>为了克服采样的分布与原分布差距过大的不足，PPO 引入 KL 散度进行约束。KL 散度用来衡量两个分布的接近程度。</p><p><span class="math display">\[J_{P P O}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta K L\left(\theta, \theta^{\prime}\right)\]</span></p><p>TRPO(Trust Region Policy Optimization)，要求 <span class="math inline">\(K L\left(\theta, \theta^{\prime}\right)&lt;\delta\)</span>。</p><p><span class="math display">\[J_{T R P O}^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]\]</span></p><p>KL 散度可能比较难计算，在实际中常使用 PPO2。</p><ul><li>A&gt;0，代表当前策虑表现好。需要增大 <span class="math inline">\(\pi( \theta )\)</span>，通过 clip 增加一个上限，防止 <span class="math inline">\(\pi( \theta )\)</span> 和旧分布变化太大。</li><li>A&lt;0，代表当前策虑表现差，不限制新旧分布的差异程度，只需要大幅度改变 <span class="math inline">\(\pi( \theta )\)</span>。</li></ul><p>参考 <a href="https://zhuanlan.zhihu.com/p/43114711" target="_blank" rel="noopener">【点滴】策略梯度之PPO - 知乎</a></p><p><img src="https://media.xiang578.com/15772793086357.jpg"></p><h3 id="ppo-algorithm">PPO algorithm</h3><p>系数 <span class="math inline">\(\beta\)</span> 在迭代的过程中需要进行动态调整。引入 <span class="math inline">\(KL_{max} KL_{min}\)</span>，KL &gt; KLmax，说明 penalty 没有发挥作用，增大 <span class="math inline">\(\beta\)</span>。</p><p><img src="https://media.xiang578.com/15772793200430.jpg"></p><h2 id="q-learning">Q-Learning</h2><p>value-base 方法，利用 critic 网络评价 actor 。通过状态价值函数 <span class="math inline">\(V^{\pi}(s)\)</span> 来衡量预期的期望。V 和 pi、s 相关。</p><p><img src="https://media.xiang578.com/15773668251569.jpg"></p><ol type="1"><li>Monte-Carlo MC: 训练网络使预测的 <span class="math inline">\(V^{\pi}(s_a)\)</span> 和实际完整游戏 reward <span class="math inline">\(G_a\)</span> 接近。</li><li>Temporal-difference TD: 训练网络尽量满足 <span class="math inline">\(V^{\pi}(s_t)=V^{\pi}(s_{t+1}) + r_t\)</span> 等式，两个状态之间的收益差。</li></ol><p>MC: 根据策虑 <span class="math inline">\(\pi\)</span> 进行游戏得到最后的 <span class="math inline">\(G(a)\)</span>，最终存在方差大的问题。<span class="math inline">\(\operatorname{Var}[k X]=k^{2} \operatorname{Var}[X]\)</span></p><p>TD: r 的方差比较小，<span class="math inline">\(V^{\pi}(s_{t+1})\)</span> 在采样不充分的情况下，可能不准确。</p><p><img src="https://media.xiang578.com/15773670631327.jpg"></p><h3 id="another-critic">Another Critic</h3><p>State-action value function <span class="math inline">\(Q^{\pi}(s, a)\)</span>：预测在 pi 策略下，pair(s, a) 的值。相当于假设 state 情况下强制采取 action a。</p><p><img src="https://media.xiang578.com/15773671301402.jpg"></p><p>对于非分类的方法：</p><p><img src="https://media.xiang578.com/15773671400847.jpg"></p><p>Q-Learning</p><ol type="1"><li>初始 actor <span class="math inline">\(\pi\)</span> 与环境互动</li><li>学习该 actor 对应的 Q function</li><li>找一个比 <span class="math inline">\(\pi\)</span> 好的策虑：<span class="math inline">\(\pi \prime\)</span>，满足 <span class="math inline">\(V^{\pi \prime}(s,a) \ge V^{\pi}(s,a)\)</span>, <span class="math inline">\(\pi^{\prime}(s)=\arg \max _{a} Q^{\pi}(s, a)\)</span></li></ol><p>在给定 state 下，分别代入 action，取函数值最大的 a，作为后面对该 state 时采取的 action。</p><p>证明新的策虑存在： <img src="https://media.xiang578.com/15773672909262.jpg"></p><h3 id="target-network">Target NetWork</h3><p>左右两边的网络相同，如果同时训练比较困难。简单的想法是固定右边的网络进行训练，一定次数后再拷贝左边的网络。</p><p><img src="https://media.xiang578.com/15773673947112.jpg"></p><h3 id="exploration">Exploration</h3><p>Q function 导致 actor 每次都会选择具有更大值的 action，无法准确估计某一些动作，对于收集数据而言是一个弊端。</p><ul><li>Epsilon Greedy<ul><li>小概率进行损失采样</li></ul></li><li>Boltzmann Exploration<ul><li>利用 softmax 计算选取动作的概率，然后进行采样</li></ul></li></ul><p><img src="https://media.xiang578.com/15773674189861.jpg"></p><h3 id="replay-buffer">Replay buffer</h3><p>采样之后的 <span class="math inline">\((s_t, a_t, r_t, s_{t+1})\)</span> 保存在一个 buffer 里面（可能是不同策虑下采样得到的)，每次训练从 buffer 中 sample 一个 batch。</p><p>结果：训练方法变成 off-policy。减少 RL 重复采样，充分利用数据。</p><p><img src="https://media.xiang578.com/15773675111439.jpg"></p><h3 id="typical-q-learning-algorithm">Typical Q-Learning Algorithm</h3><p>Q-Learning 流程：</p><p><img src="https://media.xiang578.com/15773677168275.jpg"></p><h3 id="double-dqn-ddqn">Double DQN DDQN</h3><ul><li>Q value 容易高估：目标值 <span class="math inline">\(r_t + maxQ(s_{t+1}, a)\)</span> 倾向于选择被高估的 action，导致 target 很大。</li><li>选动作的 Q' 和计算 value 的 Q(target network) 不同。Q 中高估 a，Q' 可能会准确估计 V 值。Q' 中高估 a ，可能不会被 Q 选中。</li></ul><p><img src="https://media.xiang578.com/15773677986325.jpg"></p><h3 id="dueling-dqn">Dueling DQN</h3><p>改 network 架构。V(s) 代表 s 所具有的价值，不同的 action 共享。 A(s,a) advantage function 代表在 s 下执行 a 的价值。最后 <span class="math inline">\(Q(s, a) = A(s, a) + V(s)\)</span>。</p><p>为了让网络倾向于使用 V（能训练这个网络），得到 A 后，要对 A 做 normalize。</p><p><img src="https://media.xiang578.com/15773680103445.jpg"></p><h3 id="prioritized-reply">Prioritized Reply</h3><p>在训练过程中，对于经验 buffer 里面的样本，TD error 比较大的样本有更大的概率被采样，即难训练的数据增大被采样的概率。</p><p><img src="https://media.xiang578.com/15773681449261.jpg"></p><h3 id="multi-step">Multi-step</h3><p>综合 MC 和 TD 的优点，训练样本按一定步长 N 进行采样。MC 准确方差大，TD 方差小，估计不准。 <img src="https://media.xiang578.com/15773682226911.jpg"></p><h3 id="noisy-net">Noisy Net</h3><p><img src="https://media.xiang578.com/15773682895872.jpg"></p><ul><li>Noise on Action：在相同状态下，可能会采取不同的动作。</li><li>Noise on Parameters：开始时加入噪声。同一个 episode 内，参数不会改变。相同状态下，动作相同。 更好探索环境。</li></ul><h3 id="distributional-q-function">Distributional Q-function</h3><p>Q 是累积收益的期望，实际上在 s 采取 a 时，最终所有得到的 reward 为一个分布 reward distribution。部分时候分布不同，可能期望相同，所以用期望来代替 reward 会损失一些信息。</p><p>Distributional Q-function 直接输出分布，均值相同时，采取方差小的方案。这种方法不会产生高估 q 值的情况。</p><p><img src="https://media.xiang578.com/15773684681347.jpg"></p><h3 id="rainbow">Rainbow</h3><p>rainbow 是各种策略的混合体。</p><p><img src="https://media.xiang578.com/15773684767629.jpg"></p><p>DDQN 影响不大。</p><p><img src="https://media.xiang578.com/15773684832549.jpg"></p><h3 id="continuous-actions">Continuous Actions</h3><p>action 是一个连续的向量，Q-learning 不是一个很好的方法。</p><p><span class="math display">\[a=\arg \max _{a} Q(s, a)\]</span></p><ol type="1"><li>从 a 中采样出一批动作，看哪个行动 Q 值最大。</li><li>使用 gradient ascent 解决最优化问题。</li><li>设计一个网络来化简过程。<ol type="1"><li><span class="math inline">\(\sum\)</span> 和 <span class="math inline">\(\mu\)</span> 是高斯分布的方差和均值，保证矩阵一定是正定。</li><li>最小化下面的函数，需要最小化 <span class="math inline">\(a - \mu\)</span>。</li></ol></li></ol><p><img src="https://media.xiang578.com/15773684903236.jpg"></p><h2 id="reference">Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/41712025" target="_blank" rel="noopener">强化学习基础知识 - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;info&quot;&gt;Info&lt;/h2&gt;
&lt;p&gt;课件下载：&lt;a href=&quot;http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hung-yi Lee
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="algorithm" scheme="https://xiang578.com/tags/algorithm/"/>
    
      <category term="reinforcementlearning" scheme="https://xiang578.com/tags/reinforcementlearning/"/>
    
  </entry>
  
  <entry>
    <title>每月分享 201912</title>
    <link href="https://xiang578.com/post/monthly-issue-201912.html"/>
    <id>https://xiang578.com/post/monthly-issue-201912.html</id>
    <published>2019-12-16T08:55:11.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。</p></blockquote><h2 id="x02.-org-mode-workflow">0x02. <a href="https://blog.jethro.dev/posts/org_mode_workflow_preview/" target="_blank" rel="noopener">Org-mode Workflow</a></h2><p>国外一名 CS 学生的 org mode workflow 教程，包括 GTD 和 Zettelkasten 两个主要的部分，分别对应时间管理和知识管理，是一份很好的参考资料。</p><h2 id="x01.-hhkb-更新">0x01. HHKB 更新</h2><blockquote class="twitter-tweet"><p lang="ja" dir="ltr">改めてとなりますが今回新しく登場したHHKBは3機種になります。それぞれの特徴をまとめたものがこちらになります。<a href="https://twitter.com/hashtag/HHKB%E3%83%9F%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener">#HHKBミートアップ</a> <a href="https://t.co/GVVxNI6H72" target="_blank" rel="noopener">pic.twitter.com/GVVxNI6H72</a></p>— HHKB OFFICIAL (<span class="citation" data-cites="PFU_HHKB">@PFU_HHKB</span>) <a href="https://twitter.com/PFU_HHKB/status/1204345452658741248?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">December 10, 2019</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>HHKB 好久之后终于更新了！不过价格也变得更贵……还是很喜欢自己 18 年买的 HHKB BT 版。不过在使用 Emacs 之后，出现没有方向键的烦恼。之前通过映射 <code>Ctrl + HJKL</code> 替代方向键，然后和 Emacs 的一些快捷键冲突……等买一个新的机械键盘。</p><h2 id="x00.-my-gtd-workflow-2019-ver.---yiming-chen-gtd">0x00. <a href="https://yiming.dev/blog/2019/05/22/my-gtd-workflow-2019-ver/" target="_blank" rel="noopener">My GTD Workflow (2019 ver.) - Yiming Chen</a> #gtd</h2><p>很少看到国人用英文写的 GTD 相关文章，年初自己也想按 Workflow 这种形式写一篇，不过一直拖到现在都没有完成。</p><ul><li>对任务设置优先级：A B C</li><li>如何设置任务优先级，对目标进行分解<ul><li>每年一月份设定年度目标</li><li>每月一号根据年度目标设定月度目标</li><li>每周日根据月度目标设定每周目标</li><li>每天早上设定当天目标</li></ul></li><li>任务安排优先级和截止日期后，可以使用四象限法则。</li><li>回顾技巧<ul><li>追求 100% 完成，可以接受 70%。</li><li>一个任务多次延迟之后，考虑是否还是重要。</li><li>如果任务还是重要，对任务进行拆分。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;x02.-org-mode-workflow&quot;&gt;0x02. &lt;a href=&quot;https://blog.
      
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
      <category term="monthly-issue" scheme="https://xiang578.com/tags/monthly-issue/"/>
    
  </entry>
  
  <entry>
    <title>2019 年软硬件指北</title>
    <link href="https://xiang578.com/post/2019-consumer-report.html"/>
    <id>https://xiang578.com/post/2019-consumer-report.html</id>
    <published>2019-12-15T13:29:38.000Z</published>
    <updated>2020-09-06T09:59:06.820Z</updated>
    
    <content type="html"><![CDATA[<p>呼吸不止，折腾不停。记录在过去的一年，自己选择的软件和硬件。去年写指南并不能指南，所以今年直接写成指北。</p><h2 id="硬件更新">硬件更新</h2><h3 id="iphone-xr-和-apple-watch-series-4">iPhone XR 和 Apple Watch Series 4</h3><p>iPhone XR 刚出来的时候，一直被吐槽是大边框。不过随着在电商网站上不断降价，越来越被当成是无边框手机……在忍受不了使用多年 iPhone 6 的卡顿，以及很难脱离 iOS 生态的现实。终于在苏宁上下单 <span class="math inline">\((Product)^{read}\)</span> 版的 XR。经过半年多的使用，这部手机实用但是不出彩。</p><figure><img src="https://media.xiang578.com/15764169455007.jpg" alt="Apple Watch 圆环图"><figcaption>Apple Watch 圆环图</figcaption></figure><p>购买 Apple Watch Series 4(AW) 理由很简单：在去公司健身房锻炼的时候，希望有一个可以记录运动数据的设备。事实 AW 自带的运动软件很不错，可以满足我的运动记录需求。但是例如 Keep 之类的第三方 App 适配不好。另外，AW 很好扩展 iPhone 和 Mac 的使用，比如可以解锁 Mac、查看 iPhone 上的信息等等。到头来，AW 还只是一块需要一天一充的电子表。</p><h3 id="nintendo-switch">Nintendo Switch</h3><p>Nintendo Switch(NS) 是任天堂在 2017 年推出的，集掌机和主机于一体的游戏机。买 NS 的理由也很简单，Mac 上游戏太少，我需要一个设备玩游戏。更深层次的来说，感觉自己的反应太慢，想通过玩游戏来锻炼快速决策能力。</p><p>简单统计一下，我在 NS 上花费的时间大概有 200 多小时，购入游戏也花费上千元……反应能力不知道有没有上去，但是享受到游戏的快乐。</p><figure><img src="https://media.xiang578.com/15764170682610.jpg" alt="任天堂就是世界的主宰"><figcaption>任天堂就是世界的主宰</figcaption></figure><p>Nintendo Switch 目前可以选的有 Nintendo Switch、Nintendo Switch 续航版、Nintendo Switch Lite。</p><h3 id="kindle-oasis-2">Kindle Oasis 2</h3><p>大概是 5 月末，通过公司内部闲置群出了使用 5 年多的 Kindle PaperWhite 2 后，从淘宝上购入美版 Kindle Oasis 2 。可惜的是，1 个月不到的时间，亚马逊推出 Kindle Oasis 3 ……</p><figure><img src="https://media.xiang578.com/kindle_2_oasis.jpg" alt="kindle oasis 2 和 kindle 2 对比图"><figcaption>kindle oasis 2 和 kindle 2 对比图</figcaption></figure><p>和 KPW2 相比，KO2 主要带来一下几个方面的提升：</p><ol type="1"><li>7寸屏幕，更高的分辨率。看的更多，看的更清晰，更加逼近纸书的感觉。</li><li>不对称设计，电池集中在一边，握持比较舒服。</li><li>金属机身。前几代 kindle 都是亚马逊祖传的类肤质塑料机身，很容易沾上油脂，这一代采用金属机身，看起来更加富有科技感。毕竟当年小米用上金属边框的时候，都敢去吹一块钢板的艺术之旅。</li><li>两个翻页实体按键，按起来比较有安全感。</li></ol><p>上面说这么多，kindle 主要功能还是看书。这几年，很多 kindle 电子书分享站都由于版权问题陆续关闭，优质的资源比较难下载。不过，去年自己办信用卡时，领了一年的 Kinle U 会员（今年又领到一年的会员），在中亚上借阅很多本小说。从体验上来说，KU 会员不能实现全场自由借，而且大部分书籍都只是滥竽充数。一对比微信读书会员就是十分实惠，多期待微信读书可以出电子书阅读器吧。</p><h2 id="软件实践">软件实践</h2><p>18 年开始，着手准备构建自己的数字化系统。19 年在前面的基础上，进行了很多迁移。</p><h3 id="信息管理">信息管理</h3><p>11 月份看到一句话：input 做的越多，知识管理越差。这个很好形容我之前的状态，在印象笔记中囤积待看的剪藏、OF 里面有很多想写的主题、MWeb 遗留大量没有写完的文章。</p><p>年中的时候，想把自己写的一些笔记更好的管理起来。最初想到的是搭建 wiki ，实现知识的网状化连接。不过市面上常用的一些个人 wiki 方案都不是很满意。最终选择 hexo 搭配一个 wiki 主题 <a href="https://github.com/zthxxx/hexo-theme-Wikitten" target="_blank" rel="noopener">Wikitten</a>。另外，后来了解到有一种基于纯文本的知识管理方案：zettelkasten。感兴趣的可以去看一下。</p><p>写日记是这么多年以来坚持的一件小事情。之前一直是在笔记本上写，后来慢慢的尝试通过印象笔记来写。2月份，订阅 Day One ，开始尝试迁移到它上面去。作为一个专业的软件，体验真的比之前的方式不知道好多少。Day One 上也有很多数据统计，多少可以拿来得瑟用。另外，自己干的一件事情就是把印象笔记中的日记慢慢转移到 Day one 。写在笔记本上的日记，也被我拍成一张又一张的照片，只不过这个迁移起来比较麻烦。</p><figure><img src="https://media.xiang578.com/15764175403738.jpg" alt="Day one 按年统计图"><figcaption>Day one 按年统计图</figcaption></figure><h3 id="任务管理">任务管理</h3><p>这个问题一直是一个大坑，花费很多时间在多个软件中试来试去。在现在这个时间点，自己开始选择混合使用 OmniFocus 和 Org mode。具体怎么搭配使用，等再坚持几个月再出来分享。不过说回来，任务管理的关键不在于软件，而在于执行。</p><h3 id="其他实践">其他实践</h3><p>下面这一些今年自己做的选择，都有一个共同的特点：从商业软件到开源项目。很多人选着使用的开源项目的出发点在于害怕商业公司无休止的使用个人隐私数据，而吸引我的主要是自由软件自由开放的精神。</p><h4 id="从-moneywiz-到-beancount">从 MoneyWiz 到 Beancount</h4><p>MoneyWiz 是在少数派上了解到记账软件，Setapp 中可以免费使用。和国内那些整天搞社区和卖理财的记账软件相比，只是纯粹的一个记账软件。Beancount 是无意中从<a href="https://www.byvoid.com/zhs/blog/beyond-the-void" target="_blank" rel="noopener">BYvoid</a>文章中了解的一款纯文本记账软件。最大的优点是扩展性强。在使用过程中，搭配一些简单的脚本，可以实现每月底花一个小时就能把这个月的开销记录明白。</p><figure><img src="https://media.xiang578.com/15764177497946.jpg" alt="Beancount fava"><figcaption>Beancount fava</figcaption></figure><h4 id="从-1password-到-keepass">从 1Password 到 KeePass</h4><p>之前看过一个结论：密码破解的难度主要在于长度而不是复杂度。所以借助密码软件辅助记忆密码是不二之选。1Password 是在去年感恩节活动中获得的长达一年的免费体验。快要到期前，没有选择转向订阅（今年感恩节活动依然是新用户 长度一年的免费使用），反而是选择开源的 KeePass。KeePass 在不同的平台上有多个客户端可以选择，目前我主要用的是 MacPass 和奇密。KeePass 中所有的密码数据都保存在一个文件中，跨平台使用只需要简单同步这个文件。</p><h4 id="从搜狗输入法到鼠须管">从搜狗输入法到鼠须管</h4><p>网上关于搜狗输入法的声讨一直不绝于耳，我也长时间忍受搜狗动不动给你跳出来的斗图功能提示。在花费一番力气，配置鼠须管后，彻底删除搜狗，详见 <a href="https://xiang578.com/post/rime.html">「Rime 鼠须管」小鹤双拼配置指南 | 算法花园</a>。另外 Mac 上自带的输入法的体验也没有那么差。</p><p>博客上和这个主题相关的文章：</p><ul><li><a href="https://xiang578.com/post/best-of-iphone-2019.html">Best of iPhone 2019 软件清单 | 算法花园</a></li><li><a href="https://xiang578.com/post/2018-consumer-report.html">2018 年消费指南 | 算法花园</a></li><li><a href="https://xiang578.com/post/iphone5s.html">iPhone软件清单 | 算法花园</a></li><li><a href="https://xiang578.com/post/mac-software.html">Mac软件清单 | 算法花园</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;呼吸不止，折腾不停。记录在过去的一年，自己选择的软件和硬件。去年写指南并不能指南，所以今年直接写成指北。&lt;/p&gt;
&lt;h2 id=&quot;硬件更新&quot;&gt;硬件更新&lt;/h2&gt;
&lt;h3 id=&quot;iphone-xr-和-apple-watch-series-4&quot;&gt;iPhone XR 和 Ap
      
    
    </summary>
    
      <category term="生活志" scheme="https://xiang578.com/categories/%E7%94%9F%E6%B4%BB%E5%BF%97/"/>
    
    
      <category term="life" scheme="https://xiang578.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>Standford CS231n 2017 课程部分总结</title>
    <link href="https://xiang578.com/post/cs231n-summary.html"/>
    <id>https://xiang578.com/post/cs231n-summary.html</id>
    <published>2019-12-04T09:58:39.000Z</published>
    <updated>2020-09-06T09:59:06.820Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>去年学习这门做的部分笔记，现在分享出来。 笔记格式有些问题，持续整理中。</p></blockquote><ul><li>大量内容参考 <a href="https://github.com/mbadry1/CS231n-2017-Summary/blob/master/README.md" target="_blank" rel="noopener">mbadry1/CS231n-2017-Summary</a></li></ul><h2 id="table-of-contents">Table of contents</h2><ul><li><a href="#standford-cs231n-2017-summary">Standford CS231n 2017 Summary</a><ul><li><a href="#table-of-contents">Table of contents</a></li><li><a href="#course-info">Course Info</a></li><li><a href="#01-introduction-to-cnn-for-visual-recognition">01. Introduction to CNN for visual recognition</a></li><li><a href="#02-image-classification">02. Image classification</a></li><li><a href="#03-loss-function-and-optimization">03. Loss function and optimization</a></li><li><a href="#04-introduction-to-neural-network">04. Introduction to Neural network</a></li><li><a href="#05-convolutional-neural-networks-cnns">05. Convolutional neural networks (CNNs)</a></li><li><a href="#06-training-neural-networks-i">06. Training neural networks I</a></li><li><a href="#07-training-neural-networks-ii">07. Training neural networks II</a></li><li><a href="#08-deep-learning-software">08. Deep learning software</a></li><li><a href="#09-cnn-architectures">09. CNN architectures</a></li></ul></li></ul><h2 id="course-info">Course Info</h2><ul><li>主页: http://cs231n.stanford.edu/</li><li>视频：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">斯坦福深度学习课程CS231N 2017中文字幕版+全部作业参考_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></li><li>大纲：<a href="http://cs231n.stanford.edu/2017/syllabus" target="_blank" rel="noopener">Syllabus | CS 231N</a></li><li>课件：<a href="http://cs231n.stanford.edu/slides/2017/" target="_blank" rel="noopener">Index of /slides/2017</a></li><li>笔记：<a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">贺完结！CS231n官方笔记授权翻译总集篇发布</a></li><li>作业仓库：<a href="https://github.com/xiang578/MachineLearning/tree/master/CS231n" target="_blank" rel="noopener">MachineLearning/CS231n at master · xiang578/MachineLearning</a></li><li>总课时: <strong>16</strong></li></ul><h2 id="introduction-to-cnn-for-visual-recognition">01. Introduction to CNN for visual recognition</h2><ul><li>视觉地出现促进了物种竞争。</li><li>ImageNet 是由李飞飞维护的一个大型图像数据集。</li><li>自从 2012 年 CNN 出现之后，图像分类的错误率大幅度下降。 神经网络的深度也从 7 层增加到 2015 年的 152 层。截止到目前，机器分类准确率已经超过人类，所以 ImageNet 也不再举办相关比赛。</li><li>CNN 在 1998 年就被提出，但是这几年才流行开来。主要原因有：1) 硬件发展，并行计算速度提到 2）大规模带标签的数据集。</li><li>Gola: Understand how to write from scratch, debug and train convolutional neural networks.</li></ul><h2 id="image-classification">02. Image classification</h2><ul><li>图像由一大堆没有规律的数字组成，无法直观的进行分类，所以存在语义鸿沟。分类的挑战有：视角变化、大小变化、形变、遮挡、光照条件、背景干扰、类内差异。<ul><li><img src="https://media.xiang578.com/2019-10-08-15387098703701.jpg"></li></ul></li><li>Data-Driven Approach<ul><li>Collect a dataset of images and labels</li><li>Use Machine Learning to train a classifier</li><li>Evaluate the classifier on new images</li></ul></li><li>图像分类流程：输入、学习、评估</li><li>图像分类数据集：<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10</a>，这个数据集包含了60000张32X32的小图像。每张图像都有10种分类标签中的一种。这60000张图像被分为包含50000张图像的训练集和包含10000张图像的测试集。</li><li>一种直观的图像分类算法：K-nearest neighbor(knn)<ul><li>为每一张需要预测的图片找到距离最近的 k 张训练集中的图片，然后选着在这 k 张图片中出现次数最多的标签做为预测图片的标签（多数表决）。</li><li>训练过程：记录所有的数据和标签 <span class="math inline">\({O(1)}\)</span></li><li>预测过程：预测给定图片的标签 <span class="math inline">\({O(n)}\)</span></li><li>Hyperparameters：k and the distance Metric</li><li>Distance Metric<ul><li>L1 distance(Manhattan Distance)</li><li>L2 distance(Euclidean Distance)</li></ul></li><li>knn 缺点<ul><li>Very slow at test time</li><li>Distance metrics on pixels are not informative</li></ul></li><li>反例：下面四张图片的 L2 距离相同<ul><li><img src="https://media.xiang578.com/2019-10-08-15387110626779.jpg" title="fig:" alt="-w622"></li></ul></li></ul></li><li>Hyperparameters: choices about the algorithm that we set ranther than learn</li><li>留一法 Setting Hyperparameters by Cross-validation:<ul><li>将数据划分为 f 个集合以及一个 test 集合，数据划分中药保证数据集的分布一致。</li><li>给定超参数，利用 f-1 个集合对算法进行训练，在剩下的一个集合中测试训练效果，重复这一个过程，直到所有的集合都当过测试集。</li><li>选择在训练集中平均表现最好的超参数。</li></ul></li><li>Linear classification: <code>Y = wX + b</code><ul><li>b 为 bias，调节模型对结果的偏好</li><li>通过最小化损失函数来，来确定 w 和 b 的值。</li></ul></li><li><strong>Linear SVM</strong>: classifier is an option for solving the image classification problem, but the curse of dimensions makes it stop improving at some point. <span class="citation" data-cites="todo">@todo</span></li><li><strong>Logistics Regression</strong>: 无法解决非线性的图像数据</li></ul><h2 id="loss-function-and-optimization">03. Loss function and optimization</h2><ul><li>通过 Loss function 评估参数质量<ul><li>比如 <span class="math display">\[L=\frac{1}{N}\sum_iL_i\left(f\left(x_i,W\right),y_i\right)\]</span></li></ul></li><li>Multiclass SVM loss 多分类支持向量机损失函数<ul><li><span class="math display">\[L_i=\sum_{j \neq y_j}\max\left(0,s_j-s_{y_i}+1\right)\]</span></li><li>这种损失函数被称为合页损失 Hinge loss</li><li>SVM 的损失函数要求正确类别的分类分数要比其他类别的高出一个边界值。</li><li>L2-SVM 中使用平方折叶损失函数<span class="math display">\[\max(0,-)^2\]</span>能更强烈地惩罚过界的边界值。但是选择使用哪一个损失函数需要通过实验结果来判断。</li><li>举例<ul><li><img src="https://media.xiang578.com/2019-10-08-15388343449162.jpg"></li><li>根据上面的公式计算：<span class="math display">\[L = \max(0,437.9-(-96.8)) + \max(0,61.95-(-96.8))=695.45\]</span></li><li>猫的分类得分在三个类别中不是最高得，所以我们需要继续优化。</li></ul></li></ul></li><li>Suppose that we found a W such that L = 0. Is this W unique?<ul><li>No! 2W is also has L = 0!</li></ul></li><li>Regularization: 正则化，向某一些特定的权值 W 添加惩罚，防止权值过大，减轻模型的复杂度，提高泛化能力，也避免在数据集中过拟合现象。<ul><li><span class="math display">\[L=\frac{1}{N}\sum_iL_i\left(f\left(x_i,W\right),y_i\right) + \lambda R(W)\]</span></li><li><code>R</code> 正则项 <span class="math display">\[\lambda\]</span> 正则化参数</li></ul></li><li>常用正则化方法<ul><li>L2<span class="math display">\[\begin{matrix} R(W)=\sum_{k}\sum_l W^2_{k,l} \end{matrix}\]</span></li><li>L1<span class="math display">\[\begin{matrix} R(W)=\sum_{k}\sum_l \left\vert W_{k,l} \right\vert \end{matrix}\]</span></li><li>Elastic net(L1 + L2): <span class="math display">\[\begin{matrix} R(W)=\sum_{k}\sum_l \beta W^2_{k,l} + \left\vert W_{k,l} \right\vert \end{matrix}\]</span></li><li>Dropout</li><li>Batch normalization</li><li>etc</li></ul></li><li>L2 惩罚倾向于更小更分散的权重向量，L1 倾向于稀疏项。</li><li>Softmax function：<ul><li><span class="math display">\[f_j(z)=\frac{e^{s_i}}{\sum e^{s_j}}\]</span></li><li>该分类器将输出向量 f 中的评分值解释为没有归一化的对数概率，通过归一化之后，所有概率之和为1。</li><li>Loss 也称交叉熵损失 cross-entropy loss <span class="math display">\[L_i = - \log\left(\frac{e^{s_i}}{\sum e^{s_j}}\right)\]</span></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure><ul><li>SVM 和 Softmax 比较<ol type="1"><li>评分，SVM 的损失函数鼓励正确的分类的分值比其他分类的分值高出一个边界值。</li><li>对数概率，Softmax 鼓励正确的分类归一化后的对数概率提高。</li><li>Softmax 永远不会满意，SVM 超过边界值就满意了。</li></ol></li><li>Optimization：最优化过程<ul><li>Follow the slope<ul><li><img src="https://media.xiang578.com/2019-10-08-15388374738605.jpg"></li></ul></li></ul></li><li>梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量，它是各个维度的斜率组成的向量。<ul><li>Numerical gradient: Approximate, slow, easy to write. (But its useful in debugging.)</li><li>Analytic gradient: Exact, Fast, Error-prone. (Always used in practice)</li><li>实际应用中使用分析梯度法，但可以用数值梯度法去检查分析梯度法的正确性。</li></ul></li><li>利用梯度优化参数的过程：<code>W = W - learning_rate * W_grad</code></li><li>learning_rate 被称为是学习率，是一个比较重要的超参数</li><li>Stochastic Gradient Descent SGD 随机梯度下降法<ul><li>每次使用一小部分的数据进行梯度计算，这样可以加快计算的速度。</li><li>每个批量中只有1个数据样本，则被称为随机梯度下降（在线梯度下降）</li></ul></li><li>图像分类任务中三大关键部分：<ol type="1"><li>评分函数</li><li>损失函数：量化某个具体参数 <span class="math inline">\({W}\)</span> 的质量</li><li>最优化：寻找能使得损失函数值最小化的参数 <span class="math inline">\({W}\)</span> 的过程</li></ol></li></ul><h2 id="introduction-to-neural-network">04. Introduction to Neural network</h2><ul><li>反向传播：在已知损失函数 <span class="math inline">\({L}\)</span> 的基础上，如何计算导数<span class="math inline">\({\nabla _WL}\)</span>？</li><li>计算图<ul><li>由于计算神经网络中某些函数的梯度很困难，所以引入计算图的概念简化运算。</li><li>在计算图中，对应函数所有的变量转换成为计算图的输入，运算符号变成图中的一个节点（门单元）。</li></ul></li><li>反向传播：从尾部开始，根据链式法则递归地向前计算梯度，一直到网络的输入端。<ul><li><img src="https://media.xiang578.com/2019-10-08-15390088806657.jpg" title="fig:" alt="-w1107"></li><li>绿色是正向传播，红色是反向传播。</li></ul></li><li>对于计算图中的每一个节点，我们需要计算这个节点上的局部梯度，之后根据链式法则反向传递梯度。</li><li>Sigmoid 函数：<span class="math inline">\({f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}}\)</span><ul><li><img src="https://media.xiang578.com/2019-10-08-15390092983570.jpg"></li><li>对于门单元 <span class="math inline">\({\frac{1}{x}}\)</span>，求导的结果是 <span class="math inline">\({-\frac{1}{x^2}}\)</span>，输入为 1.37，梯度返回值为 1.00，所以这一步中的梯度是 <span class="math inline">\({(\frac{-1}{1.37^2})*1.00=-0.53}\)</span>。</li><li>模块化思想：对 <span class="math inline">\({\sigma(x)=\frac{1}{1+e^{-x}}}\)</span> 求导的结果是 <span class="math inline">\({(1-\sigma(x))\sigma(x)}\)</span>。如果 sigmoid 表达式输入值为 1.0 时，则前向传播中的结果是 0.73。根据求导结果计算可得局部梯度是 <span class="math inline">\({(1-0.73)*0.73=0.2}\)</span>。</li></ul></li><li>Modularized implementation: forward/backwar API</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultuplyGate</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  x,y are scalars</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    z = x*y</span><br><span class="line">    self.x = x  <span class="comment"># Cache</span></span><br><span class="line">    self.y = y<span class="comment"># Cache</span></span><br><span class="line">    <span class="comment"># We cache x and y because we know that the derivatives contains them.</span></span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(dz)</span>:</span></span><br><span class="line">    dx = self.y * dz         <span class="comment">#self.y is dx</span></span><br><span class="line">    dy = self.x * dz</span><br><span class="line">    <span class="keyword">return</span> [dx, dy]</span><br></pre></td></tr></table></figure><ul><li>深度学习框架中会实现的门单元：Multiplication、Max、Plus、Minus、Sigmoid、Convolution</li><li>常用计算单元<ul><li><strong>加法门单元：</strong>把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。</li><li><strong>取最大值门单元：</strong>将梯度转给前向传播中值最大的那个输入，其余输入的值为0。</li><li><strong>乘法门单元：</strong>等值缩放。局部梯度就是输入值，但是需要相互交换，然后根据链式法则乘以输出值得梯度。</li></ul></li><li>Neural NetWorks<ul><li>(Before) Linear score function <span class="math display">\[f = Wx\]</span></li><li>(Now) 2-layer Neural NetWork <span class="math display">\[f=W_2\max(0,W_1x)\]</span></li><li>ReLU <span class="math display">\[\max(0,x)\]</span> 是激活函数，如果不使用激活函数，神经网络只是线性模型的组合，无法拟合非线性情况。</li><li>神经网络是更复杂的模型的基础组件</li></ul></li></ul><h2 id="convolutional-neural-networks-cnns">05. Convolutional neural networks (CNNs)</h2><ul><li>这一轮浪潮的开端：<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlxNet</a></li><li>卷积神经网络<ul><li>Fully Connected Layer 全连接层：这一层中所有的神经元链接在一起。</li><li>Convolution Layer：<ul><li>通过参数共享来控制参数的数量。Parameter sharing</li><li>Sparsity of connections</li></ul></li><li>卷积神经网络能学习到不同层次的输入信息</li><li>常见的神经网络结构：<code>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC</code></li><li>使用小的卷积核大小的优点：多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好地特征。而且使用的参数也会更少</li></ul></li><li>计算卷积层输出<ul><li>stride 是卷积核在移动时的步长</li><li>通用公式 (N-F)/stride + 1<ul><li>stride 1 =&gt; (7-3)/1 + 1 = 5</li><li>stride 2 =&gt; (7-3)/2 + 1 = 3</li><li>stride 3 =&gt; (7-3)/3 + 1 = 2.33</li></ul></li><li>Zero pad the border: 用零填充所有的边界，保证输入输出图像大小相同，保留图像边缘信息，提高算法性能<ul><li>步长为 1 时，需要填充的边界计算公式：(F-1)/2<ul><li>F = 3 =&gt; zero pad with 1</li><li>F = 5 =&gt; zero pad with 2</li><li>F = 7 =&gt; zero pad with 3</li></ul></li></ul></li><li>计算例子<ul><li>输入大小 <code>32*32*3</code> 卷积大小 10 5*5 stride 1 pad 2</li><li>output <code>32*32*10</code></li><li>每个 filter 的参数数量：<code>5*5*3+1 =76</code> bias</li><li>全部参数数量 76*10=760</li></ul></li></ul></li><li>卷积常用超参数设置<ul><li>卷积使用小尺寸滤波器</li><li>卷积核数量 K 一般为 2 的次方倍</li><li>卷积核的空间尺寸 F</li><li>步长 S</li><li>零填充数量 P</li></ul></li><li>Pooling layer<ul><li>降维，减少参数数量。在卷积层中不对数据做降采样</li><li>卷积特征往往对应某个局部的特征，通过池化聚合这些局部特征为全局特征</li></ul></li><li>Max pooling<ul><li>2*2 stride 2</li><li>避免区域重叠</li></ul></li><li>Average pooling</li></ul><h2 id="training-neural-networks-i">06. Training neural networks I</h2><ul><li><p>Activation functions 激活函数</p><ul><li>不使用激活函数，最后的输出会是输入的线性组合。利用激活函数对数据进行修正。</li><li><img src="https://media.xiang578.com/2019-10-08-15395012747510.jpg"></li><li>Sigmoid<ul><li>限制输出在 [0,1]区间内</li><li>firing rate</li><li>二分类输出层激活函数</li><li>Problem<ul><li>梯度消失：x很大或者很小时，梯度很小，接近于0（考虑图像中的斜率。无法得到梯度反馈。</li><li>输出不是 0 均值的数据，梯度更新效率低</li><li>exp is a bit compute expensive</li></ul></li></ul></li><li>tanh<ul><li>输出范围 [-1, 1]</li><li>0 均值</li><li>x 很大时，依然没有梯度</li><li><span class="math inline">\({f(x)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}}\)</span></li><li><span class="math inline">\({1-(tanh(x))^2}\)</span></li></ul></li><li>RELU rectified linear unit 线性修正单元<ul><li>一半空间梯度不会饱和，计算速度快，对结果又有精确的计算</li><li>不是 0 均值</li></ul></li><li>Leaky RELU<ul><li><code>leaky_RELU(x) = max(0.01x, x)</code></li><li>梯度不会消失</li><li>需要学习参数</li></ul></li><li>ELU<ul><li>比 ReLU 好用</li><li>反激活机制</li></ul></li><li>Maxout<ul><li>maxout(x) = max(w1.T<em>x + b1, w2.T</em>x + b2)</li><li>梯度不会消失</li><li>增大参数数量</li></ul></li><li>激活函数选取经验<ul><li>使用 ReLU ，但要仔细选取学习率</li><li>尝试使用 Leaky ReLU Maxout ELU</li><li>使用 tanh 时，不要抱有太大的期望</li><li>不要使用 sigmoid</li></ul></li></ul></li><li><p>数据预处理 Data Preprocessing</p><ul><li>均值减法：对数据中每个独立特征减去平均值，从几何上来看是将数据云的中心都迁移到原点。</li><li>归一化：将数据中的所有维度都归一化，使数值范围近似相等。但是在图像处理中，像素的数值范围几乎一致，所以不需要额外处理。</li></ul><p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line">X /= np.std(X, axis =<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><ul><li>图像归一化<ul><li>Subtract the mean image AlexNet<ul><li>mean image 32,32,3</li></ul></li><li>Subtract per-channel mean VGGNet<ul><li>mean along each channel = 3 numbers</li></ul></li><li>如果需要进行均值减法时，均值应该是从训练集中的图片平均值，然后训练集、验证集、测试集中的图像再减去这个平均值。</li></ul></li><li>Weight Initialization<ul><li>全零初始化<ul><li>网络中的每个神经元都计算出相同的输出，然后它们就会在反向传播中计算出相同的梯度。神经元之间会从源头上对称。</li></ul></li><li>Small random numbers<ul><li>初始化权值要非常接近 0 又不能等于 0。将权重初始化为很小的数值，以此来打破对称性</li><li>randn 函数是基于零均值和标准差的高斯分布的随机函数</li><li>W = 0.01 * np.random.rand(D,H)</li><li>问题：一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度。会减小反向传播中的“梯度信号”，在深度网络中就会出现问题。</li></ul></li><li>Xavier initialization<ul><li>W = np.random.rand(in, out) / np.sqrt(in)</li><li>校准方差，解决输入数据量增长，随机初始化的神经元输出数据的分布中的方差也增大问题。</li></ul></li><li>He initialization<ul><li>W = np.random.rand(in, out) / np.sqrt(in/2)</li></ul></li></ul></li><li>Batch normalization<ul><li>保证输入到神经网络中的数据服从标准的高斯分布</li><li>通过批量归一化可以加快训练的速度</li><li>步骤<ul><li>首先计算每个特征的平均值和平方差</li><li>通过减去平局值和除以方差对数据进行归一化</li><li><code>Result = gamma * normalizedX + beta</code><ul><li>对数据进行线性变换，相当于对数据分布进行一次移动，可以恢复数据之前的分布特征</li></ul></li></ul></li><li>BN 的好处<ul><li>加快训练速度</li><li>可以使用更快的而学习率</li><li>减少数据对初始化权值的敏感程度</li><li>相当于进行一次正则化</li></ul></li><li>BN 适用于卷积神经网络和常规的 DNN，在 RNN 和增强学习中表现不是很好</li></ul></li><li>Babysitting the Learning Provess</li><li>Hyperparameter Optimization<ul><li>Cross-validation 策略训练</li><li>小范围内随机搜索</li></ul></li></ul></li></ul><h2 id="training-neural-networks-ii">07. Training neural networks II</h2><ul><li>Optimization Algorithms:<ul><li>SGD 的问题<ul><li><code>x += - learning_rate * dx</code></li><li>梯度在某一个方向下降速度快，在其他方向下降缓慢</li><li>遇到局部最小值点，鞍点</li></ul></li><li>mini-batches GD<ul><li>Shuffling and Partitioning are the two steps required to build mini-batches</li><li>Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.</li></ul></li><li>SGD + Momentun<ul><li>动量更新：从物理学角度启发最优化问题</li><li><code>V[t+1] = rho * v[t] + dx; x[t+1] = x[t] - learningRate * V[t+1]</code></li><li>rho 被看做是动量，其物理意义与摩擦系数想类似，常取 0.9 或0.99</li><li>和 momentun 项更新方向相同的可以快速更新。</li><li>在 dx 中改变梯度方向后， rho 可以减少更新。momentun 能在相关方向加速 SGD，抑制震荡，加快收敛。</li></ul></li><li>Nestrov momentum<ul><li><img src="https://media.xiang578.com/2019-10-08-15499574039274.jpg"></li><li><code>v_prev = v; v = mu * v - learning_rate * dx; x += -mu * v_prev + (1 + mu) * v</code></li></ul></li><li>AdaGrad<ul><li><span class="math inline">\(n_t=n_{t-1}+g^2_t\)</span></li><li><span class="math inline">\(\Delta \theta _t = -\frac{\eta}{\sqrt{n_t+\epsilon}}\)</span></li><li>下面根号中会递推形成一个约束项。前期这一项比较大，能够放大梯度。后期这一项比较小，能约束梯度。</li><li>gt 的平方累积会使梯度趋向于 0</li></ul></li><li>RMSProp<ul><li>RMS 均方根</li><li>自适应学习率方法</li><li>求梯度的平方和平均数：<code>cache =  decay_rate * cache + (1 - decay_rate) * dx**2</code></li><li><code>x += - learning_rate * dx / (sqrt(cache) + eps)</code></li><li>依赖全局学习率</li></ul></li><li>Adam<ul><li>RMSProp + Momentum</li><li>It calculates an exponentially weighted average of past gradients, and stores it in variables <span class="math inline">\(v\)</span> (before bias correction) and <span class="math inline">\(v^{corrected}\)</span> (with bias correction).</li><li>It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables <span class="math inline">\(s\)</span> (before bias correction) and <span class="math inline">\(s^{corrected}\)</span> (with bias correction).</li><li>一阶到导数累积，二阶导数累积</li><li>It updates parameters in a direction based on combining information from "1" and "2".</li><li>The update rule is, for <span class="math inline">\(l = 1, ..., L\)</span>: <span class="math display">\[\begin{cases}  v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \\  v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\  s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\  s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_1)^t} \\  W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}}} + \varepsilon}  \end{cases}\]</span></li></ul>where:<ul><li>t counts the number of steps taken of Adam</li><li>L is the number of layers</li><li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters that control the two exponentially weighted averages.</li><li><span class="math inline">\(\alpha\)</span> is the learning rate</li><li><span class="math inline">\(\varepsilon\)</span> is a very small number to avoid dividing by zero</li></ul>特点：<ul><li>适用于大数据集和高维空间。</li><li>对不同的参数计算不同的自适应学习率。</li></ul></li><li>Learning decay<ul><li>学习率随着训练变化，比如每一轮在前一轮的基础上减少一半。</li><li>防止学习停止</li></ul></li><li>Second order optimization</li></ul></li><li>Regularization<ul><li>Dropout<ul><li>每一轮中随机使部分神经元失活，减少模型对神经元的依赖，增强模型的鲁棒性。</li></ul></li></ul></li><li>Transfer learning<ul><li>CNN 中的人脸识别，可以在大型的模型基础上利用少量的相关图像进行继续训练。</li></ul></li></ul><h2 id="cnn-architectures">09. CNN architectures</h2><ul><li>研究模型的方法：搞清楚每一层的输入和输出的大小关系。</li><li>LeNet - 5 [1998]<ul><li>60k 参数</li><li>深度加深，图片大小减少，通道数量增加</li><li>ac: Sigmod/tanh</li></ul></li><li>AlexNet [2012]<ul><li>(227,227,3) （原文错误）</li><li>60M 参数</li><li>LRN：局部响应归一化，之后很少使用</li></ul></li><li>VGG - 16 [2015]<ul><li>138 M</li><li>结构不复杂，相对一致，图像缩小比例和通道增加数量有规律</li></ul></li><li>ZFNet [2013]<ul><li>在 AlexNet 的基础上修改<ul><li><code>CONV1</code>: change from (11 x 11 stride 4) to (7 x 7 stride 2)</li><li><code>CONV3,4,5</code>: instead of 384, 384, 256 filters use 512, 1024, 512</li></ul></li></ul></li><li>VGG [2014]<ul><li>模型中只使用 3*3 conv：与 77 卷积有相同的感受野，而且可以将网络做得更深。比如每一层可以获取到原始图像的范围：第一层 33，第二层 55，第三层 77。</li><li>前面的卷积层参数量很少，模型中大部分参数属于底部的全连接层。</li></ul></li></ul><p><img src="https://media.xiang578.com/2019-10-08-15705415499052.jpg"></p><ul><li>GoogLeNet<ul><li>引入 <code>Inception module</code><ul><li>design a good local network topology (network within a network) and then stack these modules on top of each other</li><li>该模块可以并行计算</li><li>conv 和 pool 层进行 padding，最后将结果 concat 在一起</li></ul></li></ul></li></ul><figure><img src="https://media.xiang578.com/2019-10-08-15705420412305.jpg" alt="Reset"><figcaption>Reset</figcaption></figure><ul><li>ResNet<ul><li>目标：深层模型表现不应该差于浅层模型，解决随着网络加深，准确率下降的问题。</li><li><code>Y = (W2* RELU(W1x+b1) + b2) + X</code></li><li>如果网络已经达到最优，继续加深网络，residual mapping会被设置为 0，一直保存网络最优的情况。</li></ul></li></ul><h2 id="reference">Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam） - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;去年学习这门做的部分笔记，现在分享出来。 笔记格式有些问题，持续整理中。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;大量内容参考 &lt;a href=&quot;https://github.com/mbadry1/CS231n-2017-Summa
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ml, course" scheme="https://xiang578.com/tags/ml-course/"/>
    
  </entry>
  
  <entry>
    <title>算法花园风格清单</title>
    <link href="https://xiang578.com/post/blog-writing-checklist.html"/>
    <id>https://xiang578.com/post/blog-writing-checklist.html</id>
    <published>2019-11-03T10:03:23.000Z</published>
    <updated>2020-09-06T09:59:06.820Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>李如一在 <a href="https://www.qdaily.com/articles/1397.html" target="_blank" rel="noopener">写作风格手册</a> 中提到写作风格的作用是 「保持机构和组织内部的文体统一，提高沟通效率。」</p><p>本清单会持续更新，如果有相关的建议，可以在留言中告诉我。</p></blockquote><p>算法花园定位为个人博客，也是我和这个世界沟通的窗口。为提高读者阅读体验，参考相关文章后，推出该清单统一网站文章的基础风格。</p><h2 id="写作">写作</h2><ol type="1"><li>减少形容词使用，尽可能删除 「的」和「了」。</li><li>给出引用图片及引文来源。</li><li>文章如果发布后大幅度修改，在末尾给出版本信息。</li><li>写完文章后，整体阅读一遍。</li></ol><h2 id="排版">排版</h2><ol type="1"><li>中文、英文、数字中间加空格，数字与单位之间无需增加空格，全角标点与其他字符之间不加空格。链接前后增加空格用以区分。</li><li>不重复使用标点符号。</li><li>体中文中使用直角引号 「」以及『 』。</li><li>使用全角中文标点，数字使用半角字符。中文中出现英文部分，仍然使用中文标点。</li><li>遇到完整的英文整句、特殊名词，其內容使用半角标点。</li><li>专有名词使用正确的大小写，使用公认的缩写。</li><li>todo 如何处理图片排版和命名。</li><li>使用英文命名文档，使用 <code>-</code> 来连接。为保证搜索引擎效果，尽量不要修改文档名称。</li><li>每篇文章开头添加简单介绍 <code>&lt;!--more--&gt;</code>。</li><li>发布后，在网页中确认格式是否符合预期、链接能否点击以及图片能否展示。</li></ol><h2 id="版本">版本</h2><blockquote><p>20191103: 第一版</p></blockquote><h2 id="参考">参考</h2><ul><li><a href="https://www.qdaily.com/articles/1397.html" target="_blank" rel="noopener">写作风格手册_设计词典_好奇心日报</a></li><li><a href="https://mazhuang.org/wiki/chinese-copywriting-guidelines/" target="_blank" rel="noopener">中文文案排版指北（简体中文版） — 码志</a></li><li><a href="https://zhuanlan.zhihu.com/p/49729668" target="_blank" rel="noopener">简体中文文本排版指南 - 知乎</a></li><li><a href="https://sspai.com/post/37815" target="_blank" rel="noopener">少数派写作排版指南 - 少数派</a></li><li><a href="https://www.jianshu.com/p/8ffc3e0d11e2" target="_blank" rel="noopener">城堡制作检查清单 0.1 版 - 简书</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;李如一在 &lt;a href=&quot;https://www.qdaily.com/articles/1397.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;写作风格手册&lt;/a&gt; 中提到写作风格的作用是 「保持机构和组织内部的文体
      
    
    </summary>
    
      <category term="站务" scheme="https://xiang578.com/categories/%E7%AB%99%E5%8A%A1/"/>
    
    
      <category term="blog" scheme="https://xiang578.com/tags/blog/"/>
    
      <category term="writing" scheme="https://xiang578.com/tags/writing/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统小结</title>
    <link href="https://xiang578.com/post/ctr.html"/>
    <id>https://xiang578.com/post/ctr.html</id>
    <published>2019-10-24T13:13:35.000Z</published>
    <updated>2020-09-06T09:59:06.820Z</updated>
    
    <content type="html"><![CDATA[<p>去年看的 <a href="https://book.douban.com/subject/30416291/" target="_blank" rel="noopener">AI极简经济学</a> 中提到一个观点：AI 对企业的贡献是带来强大的推荐预测能力，比如打开手机淘宝，首屏大量的推荐商品。作者猜想未来的某一天，淘宝上的商家可以直接把预测的商品寄到你家，然后由你选择是否购买。支撑这个想法的关键是推荐系统中的点击率预测技术（CTR，Click-Through-Rate），大量的学术机构和商业公司发表相关的论文。目前所做的业务模型中，也参考了很多 CTR 中的方法。自己也看过一些这个领域的论文，并且做了一些笔记。没有太多将笔记单独整理发出来的意义，通过这一篇阅读小结，和大家分享我的学习过程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;去年看的 &lt;a href=&quot;https://book.douban.com/subject/30416291/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AI极简经济学&lt;/a&gt; 中提到一个观点：AI 对企业的贡献是带来强大的推荐预测能力，比如打开手机淘
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ctr" scheme="https://xiang578.com/tags/ctr/"/>
    
      <category term="fm" scheme="https://xiang578.com/tags/fm/"/>
    
      <category term="gbdt" scheme="https://xiang578.com/tags/gbdt/"/>
    
      <category term="dnn" scheme="https://xiang578.com/tags/dnn/"/>
    
  </entry>
  
  <entry>
    <title>(WDR) Learning to Estimate the Travel Time</title>
    <link href="https://xiang578.com/post/wdr.html"/>
    <id>https://xiang578.com/post/wdr.html</id>
    <published>2019-07-28T14:14:33.000Z</published>
    <updated>2020-09-06T09:59:06.824Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>严重申明：本篇文章所有信息从论文、网络等公开渠道中获得，不会透露滴滴地图 ETA 任何实现方法。</p></blockquote><p>这篇论文是滴滴时空数据 2018 年在 KDD 上发表的关于在 ETA 领域应用深度学习的文章，里面提到的深度学习方法大家都耳熟能详，主要是属于工业界的创新。说点题外话，<a href="https://www.zhihu.com/question/22385673/answer/522580778" target="_blank" rel="noopener">你为什么从滴滴出行离职？ - 知乎</a> 中提到一点：</p><blockquote><p>8.同年大跃进，在滴滴中高层的眼里，没有BAT。滴滴单量超淘宝指日可待，GAFA才是滴滴要赶超的对象。百度系，LinkedIn系，学院派，uber帮，联想系，MBB就算了，据说连藤校都混成了一个小圈子。。一个项目A team ，B team。一个ETA，投入了多少人力自相残杀？MAPE做到0%又如何？用户体验就爆表了吗？长期留存就高枕无忧了吗？风流总被雨打风吹去，滴滴是二龙山，三虫聚首？是不是正确的事情不知道，反正跟着公司大势所趋，升D10保平安。</p></blockquote><p>简单介绍一下背景：ETA 是 Estimate Travel Time 的缩写，中文大概能翻译成到达时间估计。这个问题描述是：在某一个时刻，估计从 A 点到 B 点需要的时间。对于滴滴，关注的是司机开车把乘客从起点送到终点需要的时间。抽象出来 ETA 就是一个时间空间信息相关的回归问题。CTR 中常用的方法都可以在这里面尝试。</p><p>对于这个问题：文章中提到一个最通用的方法 Route ETA：即在获得 A 点到 B 点路线的情况下，计算路线中每一段路的行驶时间，并且预估路口的等待时间。最终 ETA 由全部时间相加得到。这种方法实现起来很简单，也能拿到一些收益。但是仔细思考一下，没有考虑未来道路的同行状态变化情况以及路线的拓扑关系。针对这些问题，文章中提到滴滴内部也有利用 GBDT 或者 FM 的方法解决 ETA 问题，不过没有仔细写实现的方法，我也不好继续分析下去。</p><h2 id="评价指标">评价指标</h2><p>对于 ETA 问题来说，工业界和学术界常用的指标是 MAPE(mean absolute percentage error)，<span class="math inline">\({y_i}\)</span> 是司机实际从 A 点到 B 点花费的时间，<span class="math inline">\({f(x_i)}\)</span> 是 ETA 模型估计出来的时间。得到计算公式如下：</p><p><span class="math display">\[{min_f \sum_{i=1}^{N}\frac{|y_i - f(x_i)|}{y_i}}\]</span></p><p>多说一句，如果使用 GBDT 模型实现 ETA 时，这个损失函数的推导有点困难，全网也没有看见几个人推导过。</p><p>这个公式主要考虑预估时间偏差大小对用户感知体验的影响，目前我们更加关心极端 badcase 对用户的影响。</p><h2 id="特征">特征</h2><ul><li>特征：<ul><li>空间特征：路线序列、道路等级、POI等</li><li>时间特征：月份、星期、时间片等</li><li>路况特征：道路的通行速度、拥堵程度</li><li>个性化信息：司机特征、乘客特征、车辆特征</li><li>附近特征：天气、交通管制</li></ul></li></ul><h2 id="模型">模型</h2><p>WDR 模型，包含 3 个部分： - Wide Learning Models：利用交叉积学习信息，泛化能力。 - Deep Neural networks：对 sparse feature 做一次 Embedding，使用 3 层 MLP 和 ReLU 的网络。 - Long-Short Term Memory：解决 Wide &amp; Deep 没用使用路线的顺序特征，利用 LSTM 学习 link 信息以及序列信息，最后一个单元的隐藏状态作为输出。 - Regressor： 将 3 个模型的输出综合起来，作为最后的 ETA 预估。MAPE 作为损失函数，利用 BP 训练模型。</p><figure><img src="https://media.xiang578.com/15643233780326.jpg" alt="-w962"><figcaption>-w962</figcaption></figure><p>上面模型中使用的特征分类： - Dense feature：行程级别的实数特征，比如起终点球面距离、起终点 GPS 坐标等。 - Sparse feature：行程级别的离散特征，比如时间片编号、星期几、天气类型等。 - Sequential feature：link 级别的特征，实数特征直接输入模型，而离散特征先做 embedding 再输入模型。注意，这里不再是每个行程一个特征向量，而是行程中每条 link 都有一个特征向量。比如，link 的长度、车道数、功能等级、实时通行速度等。</p><h2 id="评估">评估</h2><p>包括两部分：离线评估和在线评估。</p><p>离线评估中取滴滴 2017 年北京前6个月的订单数据，分成两类 pickup （平台给司机分单后，司机开车去接乘客的过程）和 trip （司机接到乘客并前往目的地的过程）。具体数据集划分如下。</p><p><img src="https://media.xiang578.com/15643234056004.jpg"></p><p>离线使用 MAPE 来评价模型。在线评估时，为了更好的与用户体验挂钩，采用多个指标来衡量 ETA 的效果。包括： - APE20: absolute percentage error 小于 20% 的订单占比。（越大越好） - Badcase率：APE 大于 50% 或者 AE 大于 180s 的订单占比，定义为对用户造成巨大影响的情况。（越小越好） - 低估率：低估订单的比例。（越小越好）</p><p>离线结果如下图所示，说来汗颜 PTTE 和 TEMP 是什么算法我都不知道…… WD-MLP 指的是将 WDR 中的 R 部分换成 MLP 。最终 WDR 较 route-ETA 有巨大提升，而且 LSTM 引入的序列信息也在 pikcup 上提升了 0.75%。文章的最后还提出来，LSTM 也可以换成是 Attention，这样替换有什么优点和缺点留给大家思考。</p><p><img src="https://media.xiang578.com/15643234140115.jpg"></p><p>在线实验结果如下图所示，滴滴 ETA MAPE 明显小于 com1、com2、com3 ，这三家地图公司具体是哪三家，大家也能猜到吧。</p><p><img src="https://media.xiang578.com/15643234258049.jpg"></p><h2 id="eta-服务工程架构">ETA 服务工程架构：</h2><figure><img src="https://media.xiang578.com/15643234352132.jpg" alt="-w486"><figcaption>-w486</figcaption></figure><p>从上面的图中可以看出 ETA 服务工程架构主要包括三个部分：</p><ul><li>Data Aggregation：包括利用 Map Matching 将司机上传到平台的 GPS 对应到滴滴的 Map Info 中得到司机真实行驶过的路线信息，Order Context 指的是订单相关的信息，augmented Data 额外数据比如上文说的交通情况相关信息。</li><li>Offline Training：利用上一步得到的历史数据训练模型。这里可以值得一提的是，ETA 模型是和时间强相关的（节假日和工作日的数据分布明显不同），所以在文章中作者指出将拿出最新的一部分数据用来 fine-tune 训练出来的 WDR 模型。</li><li>Online Service：这里需要一个完整的模型服务系统，其他公司也有很多分享，所以原文没有多提。</li></ul><h2 id="fma-eta-estimating-travel-time-entirely-based-on-ffn-with-attention">FMA-ETA: Estimating Travel Time Entirely Based on FFN With Attention</h2><p><img src="http://image.xiang578.com//fma-eta.jpg"></p><ul><li><del>大学实验报告级别的论文</del> 简单记录一下，不详细评价</li><li>WDR 模型中 RNN 耗时长，探索基于 Attention 机制的模型</li><li>将特征分组（multi-factor）去做 Attention 效果比多头要好</li><li>实验结果给出的理由有点牵强。 &gt; The deep modules with attention achieve better results than WDR on MAE and RMSE metrics, which means attention mechanism can help to extract features and sole the long-range dependencies in long sequence.</li><li>说预测时延减少，也没有提供线上数据。</li><li>最后，不公开代码、不公开数据、SOTA 是 WDR，图一乐。</li></ul><h2 id="总结">总结</h2><p>从上面简单的介绍来看，ETA 可以使用 CTR 和 NLP 领域的很多技术，大有可为。最后，滴滴 ETA 团队持续招人中（社招、校招、日常实习等），感兴趣者快快和我联系。</p><h2 id="参考">参考</h2><blockquote><ul><li><a href="https://www.leiphone.com/news/201808/EmRne91YDwwNCl4A.html" target="_blank" rel="noopener">KDD 2018：滴滴提出WDR模型显著提升ETA预测精度 | 雷锋网</a></li><li><a href="http://www.semocean.com/lbs%e5%b7%a5%e4%b8%9a%e7%95%8ceta%e5%ba%94%e7%94%a8%e5%8f%8a%e6%bb%b4%e6%bb%b4wdr%e6%8a%80%e6%9c%af/" target="_blank" rel="noopener">LBS工业界ETA应用及滴滴WDR技术 – Semocean</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;严重申明：本篇文章所有信息从论文、网络等公开渠道中获得，不会透露滴滴地图 ETA 任何实现方法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这篇论文是滴滴时空数据 2018 年在 KDD 上发表的关于在 ETA 领域应用深度学习的文章，里面提到的深
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="lstm" scheme="https://xiang578.com/tags/lstm/"/>
    
      <category term="widedeep" scheme="https://xiang578.com/tags/widedeep/"/>
    
      <category term="didi" scheme="https://xiang578.com/tags/didi/"/>
    
  </entry>
  
</feed>
