<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>算法花园</title>
  
  
  <link href="https://blog.xiang578.com/atom.xml" rel="self"/>
  
  <link href="https://blog.xiang578.com/"/>
  <updated>2023-03-25T10:05:26.000Z</updated>
  <id>https://blog.xiang578.com/</id>
  
  <author>
    <name>Ryen Xiang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>webmention 实现参考</title>
    <link href="https://blog.xiang578.com/note/641ea60f-a31e-4979-9327-c50509c146e3.html"/>
    <id>https://blog.xiang578.com/note/641ea60f-a31e-4979-9327-c50509c146e3.html</id>
    <published>2023-03-25T10:05:26.000Z</published>
    <updated>2023-03-25T10:05:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://indiewebify.me/">IndieWebify.Me - a guide to getting you on the IndieWeb</a><br /><a href="https://kwaa.dev/indieweb">IndieWeb, Webmentions | ./kwaa.dev</a>: <a href="https://kwaa.dev/about">藍+85CD</a> 在 <a href="https://github.com/importantimport/urara">Urara</a> 博客中实现过程记录<br /><a href="https://geekplux.com/posts/webmention">Refactor your blog comments system with Webmention.io</a>: GeekPlux 使用 twitter + webmention 实现博客评论系统<br /><a href="https://kiko.io/post/Hexo-and-the-IndieWeb/">Hexo and the IndieWeb - kiko.io</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://indiewebify.me/&quot;&gt;IndieWebify.Me - a guide to getting you on the IndieWeb&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://kwaa.dev/indieweb&quot;&gt;I</summary>
      
    
    
    
    <category term="note" scheme="https://blog.xiang578.com/categories/note/"/>
    
    
  </entry>
  
  <entry>
    <title>【时间序列预测】Are Transformers Effective for Time Series Forecasting?</title>
    <link href="https://blog.xiang578.com/post/are-transformers-effective-for-time-series-forecasting.html"/>
    <id>https://blog.xiang578.com/post/are-transformers-effective-for-time-series-forecasting.html</id>
    <published>2023-03-18T13:53:32.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>香港中文大学曾爱玲文章，在长时间序列预测问题上使用线性模型打败基于 Transformer 的模型，并对已有模型的能力进行实验分析（灵魂7问，强烈推荐好好读一下！）。</p><span id="more"></span><h2 id="核心贡献"><a class="markdownIt-Anchor" href="#核心贡献"></a> 核心贡献</h2><ul><li>质疑基于 Transformer 的序列预测模型在长时间序列预测任务（LTSF，long-term time series forecasting） 的有效性<ul><li>大部分模型无法从长序列中抽取时序信息（实验中预测误差并没有随着历史窗口增大而减少）</li><li>大部分模型相对于 baseline（rnn类）的提升是将预测方法从 IMS 替换成 DMS 得到的。</li></ul></li><li>提出只有 one-layer linear model： <code>LTSF-Linear</code>，通过 DMS 方式预测效果超过之前模型。</li><li>对已有模型的能力进行实验分析（建模长输入的能力、对时间顺序的敏感性、位置编码、子序列 embdding 影响、模型性能）</li></ul><h2 id="核心问题"><a class="markdownIt-Anchor" href="#核心问题"></a> 核心问题</h2><ul><li>时间序列预测问题主要形式是：已知前 t 时间的特征，然后预测未来一段时间的结果。如果需要预测的时间很长，被称之为长时间序列预测。<ul><li><code>IMS</code> iterated multi-step (IMS) forecasting，每次预测一个结果，迭代多次得到对未来一段时间的预测结果（自回归）。结果方差比较小，但是预测过程中误差会累积。</li><li><code>DMS</code> direct multistep (DMS) forecasting，一步得到对于未来一段时间的预测结果。常在需要预测未来时间很长或者很难训练无偏单步预测模型时使用。</li></ul></li><li>Transformer 前提假设是成对元素之间有语义相关性，self-attention 机制本质是元素置换不变，建模时序关系能力很大程度上取决于与输入标记相关联的位置编码。<ul><li>时序数据基本上没有点对点的语义相关性，需要模型能提取连续点之间的时序关系。Time Series Transformer 通过位置编码以及序列编码提供部分顺序信息，但是由于置换不变自注意力机制的存在必然存在时序信息丢失问题。</li></ul></li></ul><h2 id="相关工作"><a class="markdownIt-Anchor" href="#相关工作"></a> 相关工作</h2><h3 id="time-series-transformer-框架"><a class="markdownIt-Anchor" href="#time-series-transformer-框架"></a> Time Series Transformer 框架</h3><p><img src="https://media.xiang578.com/202303182247135.png" alt="" /></p><ul><li><code>Time series decomposition</code> 对输入序列进行分解，后续更好预测。<ul><li><a href="/post/autoformer.html">Autoformer</a> 在 seasonal-trend decomposition 中通过 moving average 建模 trend 趋势项。</li><li><a href="/post/FEDformer.html">FEDformer</a> 通过 MoE 策略将移动平均提取的趋势和各种大小核进行混合。</li></ul></li><li>对输入特征的 emebdding 策略：<ul><li>原始时间序列的顺序 local positional information</li><li>全局时间信息，比如hierarchical timestamps (week, month, year) 和 agnostic timestamps(holidays and events)</li><li>injecting several embeddings, like a fixed positional encoding, a channel projection embedding, and learnable temporal embeddings into the input sequence.</li><li>temporal embeddings with a temporal convolution layer or learnable timestamps  are introduced</li></ul></li><li>self-attention schemes 让 attention 矩阵计算尽可能稀疏</li><li>Decoders 改进原始 Transformer 的自回归模式。</li></ul><h2 id="解决方案"><a class="markdownIt-Anchor" href="#解决方案"></a> 解决方案</h2><h3 id="ltsf-linear"><a class="markdownIt-Anchor" href="#ltsf-linear"></a> LTSF-Linear</h3><ul><li>通过一层神经网对过去信息加权得到未来预测结果。</li><li>不同变量间共享参数并且不对 spatial correlations 建模</li></ul><p><img src="https://media.xiang578.com/202303182310362-LTSF-Linear.png" alt="" /></p><h3 id="dlinear"><a class="markdownIt-Anchor" href="#dlinear"></a> DLinear</h3><ul><li>D 对应 Decomposition scheme，将输入数据分解成 trend 和 remaninder 两部分<ul><li>trend 通过移动平均核得到</li><li>remaninder 是输入数据减去 trend 结果得到</li></ul></li><li>在数据集有明显趋势性的时候该方法能提升预测效果</li></ul><h3 id="nlinear"><a class="markdownIt-Anchor" href="#nlinear"></a> NLinear</h3><p>部分数据集的训练数据和测试数据存在分布偏移（下图中 b），无法使用训练集的均值和方差进行归一化。NLinear 将输入序列每一个值减去该序列最后一个值，然后输入序列过完线性层后加回被减去的值得到最后预测结果。</p><p><img src="https://media.xiang578.com/202303182316694-lsft-dis-shift.png" alt="" /></p><h2 id="实验结论"><a class="markdownIt-Anchor" href="#实验结论"></a> 实验结论</h2><ul><li><code>Repeat*</code> 指用输入序列最后一个特征做为未来一段时间的预测，模型如果效果比这个查，说明模型错误预测了趋势……</li><li>NLiner 和 DLinear 的结果比 Linear 好，说明处理分布偏移和分解趋势-周期特征的重要性。</li><li>FEDformer 相比其他 Transformer 方法好，是因为它采用经典的时间序列分析技术，不太依赖自注意力机制。</li></ul><p><img src="https://media.xiang578.com/202303182322387-ltsf-linear-result.png" alt="" /></p><h3 id="定性分析"><a class="markdownIt-Anchor" href="#定性分析"></a> 定性分析</h3><ul><li>过去 96 预测未来 336 时，下图 a 和 c，部分 Tranformer 模型根本无法预测未来数据的取值范围和偏差。</li><li>在非周期数据（图b，汇率）Tranformer 模型几乎无法预测适当的趋势。</li></ul><p><img src="https://media.xiang578.com/202303182331993-ltsf-f3-fix.png" alt="" /></p><h3 id="transformer-类模型进一步分析灵魂七问"><a class="markdownIt-Anchor" href="#transformer-类模型进一步分析灵魂七问"></a> Transformer 类模型进一步分析（灵魂七问）</h3><h4 id="能否从长序列中提取时序信息"><a class="markdownIt-Anchor" href="#能否从长序列中提取时序信息"></a> 能否从长序列中提取时序信息？</h4><ul><li>历史窗口（look-back window） 越长（输入信息越多），预测效果应该越好。</li><li>下图 x 轴对应不同历史窗口长度，可以看到随着输入信息变多，LTSF-Linear 方法预测结果越来越准，但是大部分 Transformer 模型 mse 并没有太多变化，作者猜测可能模型过拟合噪音而不能获得时序信息。</li></ul><p><img src="https://media.xiang578.com/202303192214654-ltsf-linear-f4.png" alt="" /></p><h4 id="能从长序列预测可以学到什么"><a class="markdownIt-Anchor" href="#能从长序列预测可以学到什么"></a> 能从长序列预测可以学到什么？</h4><ul><li>历史窗口中的动态时间会显著影响短期预测的精度，而长期预测仅依赖模型是否很好捕捉趋势和周期性。即预测时间跨度越长，历史窗口本身影响越小。</li><li>预测未来 720 步，下面两种取历史窗口方法:<ul><li><code>Close</code> 前 96 步输入</li><li><code>Far</code> 前 192 步到 前 97 步输入</li></ul></li><li>fedformer 和 autoformer 在两种方法下预测效果几乎相同，说明模型只能从相邻时间序列中捕获到类似的时序信息。</li></ul><p><img src="https://media.xiang578.com/202303192209473-ltsf-t3.png" alt="" /></p><h4 id="self-attention-scheme-是否有效"><a class="markdownIt-Anchor" href="#self-attention-scheme-是否有效"></a> self-attention scheme 是否有效？</h4><ul><li>对 Informer 结构进行消融实验，下表从左到右网络结构越来越简单，然后效果基本上逐步提升。看起来注意力机制和其他复杂模块在 LTSF 上没有正向效果。<ul><li><code>Att.-Linear</code> 用 linear layer 替换  self-attention layer</li><li><code>Embed+Linear</code> 再去除其他模块</li></ul></li></ul><p><img src="https://media.xiang578.com/202303192202216-ltsf-t4.png" alt="" /></p><h4 id="模型可以保留时序信息吗"><a class="markdownIt-Anchor" href="#模型可以保留时序信息吗"></a> 模型可以保留时序信息吗？</h4><ul><li>self-attention 本质上不考虑空间关系（无视元素的位置信息） permutation-invariant，通过 positional and temporal embedding 模型还是丢失时间信息。</li><li>三种数据处理方式<ul><li><code>Shuf</code> 输入数据顺序全部打乱</li><li><code>Half-ex</code> 输入数据分成前后两部分，交换这两部分的顺序。</li></ul></li><li>两个实验表明 Transformers  不能很好保留时序信息<ul><li>在 Exchange 汇率数据集，三个 Tranformer 模型在这三种数据处理方式下，预测结果的 mse 基本上都都接近，可能仅保留有限的时间关系，但是最终过拟合了。Linear 模型自然建模顺序并且使用更少的参数避免过度拟合。</li><li>在 ETH1 数据集，FEDformer 建模时考虑时间序列偏差，提出了数据中明显的时间信息，所以在 shuf 组 mse 下降非常快。Informer 根本没有建模这方面信息。</li></ul></li></ul><p><img src="https://media.xiang578.com/202303190007289-ltsf-t5.png" alt="" /></p><h4 id="不同-embedding-策略的作用"><a class="markdownIt-Anchor" href="#不同-embedding-策略的作用"></a> 不同 embedding 策略的作用？</h4><ul><li>依次删除模型中的 Position embedding、global time stamp embedding 和同时删除这两个 temebdding。</li><li>Informer的预测误差在没有位置嵌入（wo/Pos.）的情况下大幅增加。在没有时间戳嵌入（wo/Temp.）的情况下，随着预测长度的增加，Informer的性能将逐渐受损。<ul><li>论文中给的原因没有看懂：Informer uses a single time step for each token, it is necessary to introduce temporal information in tokens.</li></ul></li><li>Autoformer 和 FEDformer 通过一系列时间戳去编码时间信息，所以删除 pos 后效果可能变得更好。</li><li>Autoformer 去除时间 embedding 后效果明显变差。</li><li>FEDformer 通过 frequency-enhanced module 使模型有 temporal inductive bias，删除 pos 和 temp 影响不是很大。</li></ul><p><img src="https://media.xiang578.com/202303182353101-ltsf-t6.png" alt="" /></p><h4 id="训练数据大小是现在模型的限制因素"><a class="markdownIt-Anchor" href="#训练数据大小是现在模型的限制因素"></a> 训练数据大小是现在模型的限制因素？</h4><ul><li>问题：是不是训练数据量太少导致 Transformer 类模型表现差？</li><li>在交通数据上用 Ori <code>17544*0.7 hour</code> 数据和  Short <code>365＊24=8760 hour</code> 数据对比：<ul><li>基本上是训练数据小的时候，预测误差小。证明训练数据不是限制 Transformer 类模型表现的原因。</li><li>导致这种现象的原因可能是 short 对应全年数据，有更清晰的时间特征趋势。</li></ul></li></ul><p><img src="https://media.xiang578.com/202303182349885-ltsf-t7.png" alt="" /></p><h4 id="性能真的是长时间预测任务中优先级最高的事情吗"><a class="markdownIt-Anchor" href="#性能真的是长时间预测任务中优先级最高的事情吗"></a> 性能真的是长时间预测任务中优先级最高的事情吗？</h4><ul><li>两个问题：Transformer 模型的推理时间和内存消耗是否增加？现在的 GPU 显存能否满足任务和模型需求。</li><li>下图可以看到大部分 Transformer 模型参数量和原始 Tranformer 没有太大区别，但是推理时间可能超过原来……<ul><li>另外在通过 96 步预测 720 步任务上，原始 Tranformer 的参数 GPU 也放得下……</li></ul></li></ul><p><img src="https://media.xiang578.com/202303182345491-ltsf-t8.png" alt="" /></p><h2 id="可解释性"><a class="markdownIt-Anchor" href="#可解释性"></a> 可解释性</h2><p>附录中简单写了一些，由于模型是线性模型，不同输入位置的权重可以直接用来解释模型如何生效以及反应特点。</p><ul><li>金融数据可以观察到越靠近未来时刻的特征权重越大，代表它们对预测值的贡献更大。</li><li>交通数据能观察到明显的周期性。</li></ul><h2 id="读后总结"><a class="markdownIt-Anchor" href="#读后总结"></a> 读后总结</h2><ul><li>灵魂七问提供了很好设计对比实验的思路</li><li>读论文需要更加仔细思考，之前根本没有想过 informer 这种点注意力其实在时间序列预测上的有效性。</li><li>transformer 在时序预测的应用还是值得进一步思考！</li></ul><h2 id="ref"><a class="markdownIt-Anchor" href="#ref"></a> Ref</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/569194246">Are Transformers Effective for Time Series Forecasting? - 知乎 (zhihu.com)</a></li><li><a href="https://www.bilibili.com/video/BV14j411A72V/">Transformer对时序预测真的有效吗? | AAAI’23 Oral_哔哩哔哩_bilibili</a></li><li><a href="https://zhuanlan.zhihu.com/p/521329803">Transformer用于时间序列预测任务真的有效吗？ - 知乎 (zhihu.com)</a></li><li><a href="https://readpaper.com/paper/4628140664303927297">Are Transformers Effective for Time Series Forecasting?-论文阅读讨论-ReadPaper</a></li><li><a href="https://zhuanlan.zhihu.com/p/612092817">CTR率预测One Epoch现象 / LTSF场景质疑Transformer - 知乎 (zhihu.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/544622984">DLinear：Are Transformers Effective for Time Series Forecasting? - 知乎 (zhihu.com)</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;香港中文大学曾爱玲文章，在长时间序列预测问题上使用线性模型打败基于 Transformer 的模型，并对已有模型的能力进行实验分析（灵魂7问，强烈推荐好好读一下！）。&lt;/p&gt;</summary>
    
    
    
    <category term="智能路" scheme="https://blog.xiang578.com/categories/%E6%99%BA%E8%83%BD%E8%B7%AF/"/>
    
    
    <category term="Paper" scheme="https://blog.xiang578.com/tags/Paper/"/>
    
    <category term="Transformer" scheme="https://blog.xiang578.com/tags/Transformer/"/>
    
    <category term="Time Series Forecasting" scheme="https://blog.xiang578.com/tags/Time-Series-Forecasting/"/>
    
  </entry>
  
  <entry>
    <title>【滴滴 HierETA】Interpreting Trajectories from Multiple Views A Hierarchical Self-Attention Network for Estimating the Time of Arrival</title>
    <link href="https://blog.xiang578.com/post/hiereta.html"/>
    <id>https://blog.xiang578.com/post/hiereta.html</id>
    <published>2023-03-12T14:14:33.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>滴滴和华南理工在 2022 年 KDD 上发表的 ETA 论文，从多个视角解释轨迹，引入 Hierarchical Self-Attention Network 方法进行建模，最终在滴滴内部数据集上获得指标提升。</p><span id="more"></span><h2 id="关键信息"><a class="markdownIt-Anchor" href="#关键信息"></a> 关键信息</h2><ul><li>论文地址：<a href="https://dl.acm.org/doi/10.1145/3534678.3539051">Interpreting Trajectories from Multiple Views: A Hierarchical Self-Attention Network for Estimating the Time of Arrival | Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</a></li><li>关键字：网约车 ETA、轨迹挖掘、Self-Attention</li></ul><h2 id="背景信息"><a class="markdownIt-Anchor" href="#背景信息"></a> 背景信息</h2><p>下图是一个行程的轨迹（trajectory）示意图，任务是预测整个行程的到达时间。作者提出三种视图：</p><ul><li><code>Intersection-view</code> 对应现实世界中的路口，其属性包括：红绿灯等待时间、交通灯数量、历史车通过流量。</li><li><code>link-view</code> 连接两个路口的路，其静态属性包括：是否收费、道路宽度、道路等级。</li><li><code>segment-view</code> 人工将 link 打断成多个小段，用来表示细粒度的局部交通情况（红绿灯前一条路可能不是全部拥堵，用segment来表示比link来表示更合理），但是在表征道路网络结构方面并不完全。</li></ul><p><img src="https://media.xiang578.com/202303122106126-ETA.png" alt="" /></p><h2 id="核心问题"><a class="markdownIt-Anchor" href="#核心问题"></a> 核心问题</h2><ul><li>传统 ETA 方法采用分治策略，将一个轨迹拆分成多个小段（segment-view），然后累加每个小段预测结果得到最终 ETA（实验结果中的 Route-ETA），显然这种策略会导致较大的累积误差。</li><li>多视图（Intersection、link、segment）建模困难，常规方法使用 segment 建模，不考虑 link，没有对同一个 link 多个段之间的一致性进行建模。</li><li>link 和 Intersection 的属性不一致，很难使用同一个网络去建模。</li></ul><h2 id="相关工作"><a class="markdownIt-Anchor" href="#相关工作"></a> 相关工作</h2><p>文章中引用的相关工作主要包括三个方面：交通流预估（traffic flow prediction）、到达时间预估（travel time estimation）、自注意力机制。</p><ul><li>GMAN：基于图的多注意力机制来预测交通状况<ul><li>图学习通常会受到不相关的空间领域的负面影响（关注的区域越大月明显），并且这种影响会在训练过程中传播。</li><li>图建模被限制在很小的区域范围，在大规模城市系统中存在不足</li></ul></li><li>DeepTTE：原始 GPS 轨迹序列 + geo convolutional network + LSTM</li><li><a href="/post/wdr.html">(WDR) Learning to Estimate the Travel Time</a>：Wide &amp; Deep &amp; RNN</li><li>ConSTGAT 和  CompactETA 图建模 ETA</li><li>DeepGTT： 深度生成模型学习 ETA 分布</li><li>HetETA ：multi-relational network 学习时空数据表示</li><li>TTPNet：利用张量分解和图 embedding 从历史轨迹中学习速度和 emebdding</li></ul><h2 id="核心贡献"><a class="markdownIt-Anchor" href="#核心贡献"></a> 核心贡献</h2><ol><li>利用三个视图的层次关系对道路底层结构进行建模</li><li>分层自自注意力网络 <code>Hierarchical Self-Attention Network</code></li><li>自适应自注意力网络 <code>Adaptive Self-Attention Network</code></li></ol><h2 id="解决方案"><a class="markdownIt-Anchor" href="#解决方案"></a> 解决方案</h2><p>模型结构分成三部分：Segment Encoder、Joint Link-Intersection Encoder 和 Hierachy-Aware Attention Decoder</p><p><img src="https://media.xiang578.com/202303122134091-hiereta-arch.png" alt="" /></p><h3 id="segment-encoder"><a class="markdownIt-Anchor" href="#segment-encoder"></a> Segment Encoder</h3><p>这个编码器主要作用是对同一个 link 的 sgement 进行时空建模：</p><ul><li>segment 特征是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msubsup><mi>x</mi><mi>j</mi><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">[x^s_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1447720000000001em;vertical-align:-0.394772em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span></span></span></span>，全局特征是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>r</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">x_r]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></li><li>利用 BiLSTM 处理 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msubsup><mi>x</mi><mi>j</mi><mi>s</mi></msubsup><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>r</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x^s_j|x_r]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1447720000000001em;vertical-align:-0.394772em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>，正向和反向结果 concat 成 segment 的表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>H</mi><mi>j</mi><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">H^s_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.078102em;vertical-align:-0.394772em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span></span></span></span>。</li><li>同一个 link 内 segement 记作 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mi>s</mi></msup><mo>=</mo><mrow><mo fence="true">[</mo><msubsup><mi>H</mi><mn>1</mn><mi>s</mi></msubsup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>H</mi><mi>n</mi><mi>s</mi></msubsup><mo fence="true">]</mo></mrow><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">H^s=\left[H_1^s, \ldots, H_n^s\right] \in \mathbb{R}^{n \times d_s}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4518920000000004em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24810799999999997em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4530000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">]</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></li><li>计算 j-th segment 和 link 内其他 segment 的全局相似度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><msub><mi>P</mi><mi>j</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>Q</mi><mi>j</mi></msub><msup><mi>K</mi><mi>T</mi></msup></mrow><msub><msqrt><mi>d</mi></msqrt><mi>s</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">G P_j=\frac{Q_j K^T}{\sqrt{d}_s}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">G</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.7386849999999998em;vertical-align:-0.588em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.150685em;"><span style="top:-2.5335085em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.937845em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mathnormal mtight">d</span></span></span><span style="top:-2.8978450000000002em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,-221l0 -0c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47zM834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.102155em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.13395928571428573em;"><span style="top:-2.3264164285714286em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17358357142857145em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.50732em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9190928571428572em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.588em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li><li>计算 j-th segment 相邻 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi></mrow><annotation encoding="application/x-tex">\omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span></span></span></span> 个 segment 计算局部相似度 <code>local sematic pattern</code>  ，这一步是捕获局部 segment 的依然，对拥堵转移进行建模。<ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><msub><mi>P</mi><mi>j</mi></msub><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>G</mi><msub><mi>P</mi><mi>j</mi></msub><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">∣</mi><mi>j</mi><mo>−</mo><mi>k</mi><mi mathvariant="normal">∣</mi><mo>≤</mo><mi>ω</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi mathvariant="normal">∞</mi><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext> otherwise </mtext></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">L P_j(k)= \begin{cases}G P_j(k), &amp; |j-k| \leq \omega \\ -\infty, &amp; \text { otherwise }\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span><span class="mpunct">,</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">−</span><span class="mord">∞</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord"> otherwise </span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li></ul></li><li>用门控机制平衡全局和局部相似度的结果<ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>F</mi><mi>j</mi><mi>s</mi></msubsup><mo>=</mo><mrow><mo fence="true">(</mo><mn>1</mn><mo>−</mo><msub><mi>z</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><mo>⊙</mo><mi mathvariant="normal">Att</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mi>G</mi><msub><mi>P</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><mo>+</mo><msub><mi>z</mi><mi>j</mi></msub><mo>⊙</mo><mi mathvariant="normal">Att</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mi>L</mi><msub><mi>P</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">F_j^s=\left(1-z_j\right) \odot \operatorname{Att}\left(G P_j\right)+z_j \odot \operatorname{Att}\left(L P_j\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.078102em;vertical-align:-0.394772em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mop"><span class="mord mathrm">A</span><span class="mord mathrm">t</span><span class="mord mathrm">t</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">G</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8694379999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mop"><span class="mord mathrm">A</span><span class="mord mathrm">t</span><span class="mord mathrm">t</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></li><li>控制参数通过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>j</mi></msub><mo>=</mo><mi>σ</mi><mrow><mo fence="true">(</mo><msub><mi>W</mi><mi>h</mi></msub><msubsup><mi>H</mi><mi>j</mi><mi>s</mi></msubsup><mo>+</mo><msub><mi>W</mi><mi>g</mi></msub><mi>A</mi><mi>t</mi><mi>t</mi><mrow><mo fence="true">(</mo><mi>G</mi><msub><mi>P</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><mo>+</mo><msub><mi>W</mi><mi>l</mi></msub><mi>A</mi><mi>t</mi><mi>t</mi><mrow><mo fence="true">(</mo><mi>L</mi><msub><mi>P</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><mo>+</mo><msub><mi>b</mi><mi>z</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">z_j=\sigma\left(W_h H_j^s+W_g A t t\left(G P_j\right)+W_l A t t\left(L P_j\right)+b_z\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.244772em;vertical-align:-0.394772em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord mathnormal">A</span><span class="mord mathnormal">t</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">G</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">A</span><span class="mord mathnormal">t</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span> 学习</li></ul></li><li>所有 link 的 encoder 参数共享以及并行计算</li></ul><h3 id="joint-link-intersection-encoder"><a class="markdownIt-Anchor" href="#joint-link-intersection-encoder"></a> Joint Link-Intersection Encoder</h3><p>上一个解码器主要在 segment-view 上建模，缺少对 link 之间一致性建模，Joint Link-Intersection Encoder 考虑 link 和 intersection 交替出现的特点（一条路连接两个路口），同时在 link-view 和 intersection-view 维度建模，实现通过 segment-view 捕捉局部交通信息， link-intersection context 捕捉道路属性：</p><ul><li>加权融合 segment 表示得到 link 表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>l</mi></msubsup><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>γ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msubsup><mi>h</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">x_i^l=\sum_{j=1}^n \gamma_{i j} h_{i j}^s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.107772em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.24011em;vertical-align:-0.43581800000000004em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05556em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span></span></span></span><ul><li>权重计算方法 [[Attention]] <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>γ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mo><mi mathvariant="normal">softmax</mi><mo>⁡</mo></mo><mi>j</mi></msub><mrow><mo fence="true">(</mo><msub><mi>W</mi><mi>γ</mi></msub><msubsup><mi>h</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>s</mi></msubsup><mo>+</mo><msub><mi>b</mi><mi>γ</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma_{i j}=\operatorname{softmax}_j\left(W_\gamma h_{i j}^s+b_\gamma\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.05556em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.244772em;vertical-align:-0.394772em;"></span><span class="mop"><span class="mop"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05556em;">γ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05556em;">γ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></li></ul></li><li>得到 link 和 intersections 的表示后，分别用两个不同的 BiLSTM 编码（主要让两个向量长度相同）得到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>H</mi><mi>i</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">{H^l_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.107772em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>H</mi><mi>i</mi><mi>c</mi></msubsup></mrow><annotation encoding="application/x-tex">{H^c_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.941994em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span></span></span></span></span>，concat 在一起得到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover accent="true"><mi>H</mi><mo>^</mo></mover><mi>i</mi><mi>l</mi></msubsup><mo>=</mo><mrow><mo fence="true">[</mo><msubsup><mi>H</mi><mi>i</mi><mi>l</mi></msubsup><mo>∣</mo><msubsup><mi>H</mi><mi>i</mi><mi>c</mi></msubsup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{H}_i^l=\left[H_i^l \mid H_i^c\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.205434em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">]</span></span></span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover accent="true"><mi>H</mi><mo>^</mo></mover><mi>i</mi><mi>l</mi></msubsup><mo>=</mo><mrow><mo fence="true">[</mo><msubsup><mi>H</mi><mi>i</mi><mi>l</mi></msubsup><mo>∣</mo><msubsup><mi>H</mi><mi>i</mi><mi>c</mi></msubsup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{H}_i^l=\left[H_i^l \mid H_i^c\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.205434em;vertical-align:-0.258664em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">[</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">]</span></span></span></span></span></span> 经过 self-attention layer + resnet + ln 得到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mrow><mo fence="true">{</mo><msubsup><mi>h</mi><mi>i</mi><mi>l</mi></msubsup><mo fence="true">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow><annotation encoding="application/x-tex">\left\{h_i^l\right\}_{i=1}^m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.304002em;vertical-align:-0.39971em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">{</span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">}</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.904292em;"><span style="top:-2.3002900000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.3029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.39971em;"><span></span></span></span></span></span></span></span></span></span></li><li>考虑到相邻 link 之间的交通影响更加弱和稀疏，作者没有计算不同 link 之间的 local pattern，从而避免过拟合。</li></ul><h3 id="hierachy-aware-attention-decoder-层次感知注意力解码器"><a class="markdownIt-Anchor" href="#hierachy-aware-attention-decoder-层次感知注意力解码器"></a> Hierachy-Aware Attention Decoder 层次感知注意力解码器</h3><p>一个行程中不同的 sub-route 对于 eta 贡献是不一样的（比如拥堵的路口和道路对时间预估影响更大），引入层次感知注意力来计算最终 ETA：</p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">R</mi><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>λ</mi><mo stretchy="false">)</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msubsup><mi>h</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>s</mi></msubsup><mo>+</mo><mi>λ</mi><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>β</mi><mi>i</mi></msub><msubsup><mi>h</mi><mi>i</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{R}=(1-\lambda) \sum_{i=1}^m \sum_{j=1}^n \alpha_{i j} h_{i j}^s+\lambda \sum_{i=1}^m \beta_i h_i^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathcal">R</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.0651740000000007em;vertical-align:-1.4137769999999998em;"></span><span class="mord mathnormal">λ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714392em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>h</mi><mi>s</mi></msup></mrow><annotation encoding="application/x-tex">h^s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>h</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">h^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span> 分别是 segment 和 link-intersection 的表示</li><li>link 的注意力 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>i</mi></msub><mo>=</mo><mi><munder><mo><mi mathvariant="normal">softmax</mi><mo>⁡</mo></mo><mi>i</mi></munder></mi><mrow><mo fence="true">(</mo><msup><mi>f</mi><mi>l</mi></msup><mrow><mo fence="true">(</mo><msubsup><mi>h</mi><mi>i</mi><mi>l</mi></msubsup><mo separator="true">,</mo><msup><mi>x</mi><mi>r</mi></msup><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\beta_i=\underset{i}{\operatorname{softmax}}\left(f^l\left(h_i^l, x^r\right)\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.577664em;vertical-align:-0.727664em;"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.372336em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.727664em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span>，且 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mi>l</mi></msup><mrow><mo fence="true">(</mo><msubsup><mi>h</mi><mi>i</mi><mi>l</mi></msubsup><mo separator="true">,</mo><msup><mi>x</mi><mi>r</mi></msup><mo fence="true">)</mo></mrow><mo>=</mo><msup><mi>v</mi><mi>T</mi></msup><mi>tanh</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>w</mi><mn>1</mn></msub><msubsup><mi>h</mi><mi>i</mi><mi>l</mi></msubsup><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msup><mi>x</mi><mi>r</mi></msup><mo>+</mo><mi>b</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">f^l\left(h_i^l, x^r\right)=v^T \tanh \left(w_1 h_i^l+w_2 x^r+b\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">tanh</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">b</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span> （xr 是全局特征）</li><li>计算 segment 之间注意力 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi><munder><mo><mi mathvariant="normal">softmax</mi><mo>⁡</mo></mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></munder></mi><mrow><mo fence="true">(</mo><msub><mi>β</mi><mi>i</mi></msub><msup><mi>f</mi><mi>s</mi></msup><mrow><mo fence="true">(</mo><msubsup><mi>h</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>s</mi></msubsup><mo separator="true">,</mo><msup><mi>x</mi><mi>r</mi></msup><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha_{i j}=\underset{(i, j)}{\operatorname{softmax}}\left(\beta_i f^s\left(h_{i j}^s, x^r\right)\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.8159999999999998em;vertical-align:-0.966em;"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-2.3089999999999997em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.966em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> 是 link 重要性，实现在计算 segement 之间的重要性时，模型也能参考 link 重要性。</li></ul><p>Hierachy-Aware Attention Decoder 让整个模型自适应选择不同表示粒度中最相关的特征（可以是几个 link 权重大，还是几个）</p><h2 id="实验结论"><a class="markdownIt-Anchor" href="#实验结论"></a> 实验结论</h2><p>从北京的 mae 上来看比其他方法好 5s 以上，更进一步分析，里程比较长的单提升效果更明显。</p><p><img src="https://media.xiang578.com/202303122127226-hiereta-result.png" alt="" /></p><p>消融实验方法：有无 local 和 global 特征、有无路况信息、由于层次化结构等。mae 超过 1s 可以认为变化很大。</p><p><img src="https://media.xiang578.com/202303122130436-ablation-hiereta.png" alt="" /></p><h2 id="读后总结"><a class="markdownIt-Anchor" href="#读后总结"></a> 读后总结</h2><ul><li>层次化建模然后在利用 link 的 attention 对 segment 的 attention score 进行调整挺有意思的</li><li>遗憾没有线上实验结果，以及没有开放相关数据集，感觉 ETA 任务各家都有自己的数据集，然后不同方法之间没有太强横向可比性</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;滴滴和华南理工在 2022 年 KDD 上发表的 ETA 论文，从多个视角解释轨迹，引入 Hierarchical Self-Attention Network 方法进行建模，最终在滴滴内部数据集上获得指标提升。&lt;/p&gt;</summary>
    
    
    
    <category term="智能路" scheme="https://blog.xiang578.com/categories/%E6%99%BA%E8%83%BD%E8%B7%AF/"/>
    
    
    <category term="eta" scheme="https://blog.xiang578.com/tags/eta/"/>
    
    <category term="self-attention" scheme="https://blog.xiang578.com/tags/self-attention/"/>
    
    <category term="KDD" scheme="https://blog.xiang578.com/tags/KDD/"/>
    
    <category term="Paper" scheme="https://blog.xiang578.com/tags/Paper/"/>
    
    <category term="didi" scheme="https://blog.xiang578.com/tags/didi/"/>
    
  </entry>
  
  <entry>
    <title>【随想集】07 新年快乐</title>
    <link href="https://blog.xiang578.com/post/thinking-07.html"/>
    <id>https://blog.xiang578.com/post/thinking-07.html</id>
    <published>2023-02-13T14:30:00.000Z</published>
    <updated>2023-03-28T03:58:52.116Z</updated>
    
    <content type="html"><![CDATA[<p>距离上次分享已经快一个月了，这段时间我度过了长达两周的春节假期，现在是时候写点东西了！</p><span id="more"></span><blockquote><p>卷首语：决定自己人生的那些重要时刻，并没有风雨交加电闪雷鸣。上天从来不屑给于凡人征兆，这些时刻平凡普通，仿佛就和过去无数个日日夜夜一样无聊而烦闷。</p></blockquote><h2 id="阅读"><a class="markdownIt-Anchor" href="#阅读"></a> 阅读</h2><ul><li>本周读了一篇 Uber 关于 ETA 的论文，简单整理文章中主要内容写成博客：<a href="https://blog.xiang578.com/post/deepreta.html">DeeprETA: An ETA Post-processing System at Scale</a>。希望今年能养成读论文，写公开的笔记的习惯。</li><li>这个月在读一本效率类工具书 <a href="https://book.douban.com/subject/26854756/">最有生產力的一年 (豆瓣) (douban.com)</a>，作者用 1 年时间进行一系列的实验，记录和探讨什么方法可以提升自己的生产力。有一些方法其实自己之前也尝试过，比如每天做 3 件重要的事情、每天只使用 1 小时手机等，很可惜没有坚持下来。</li></ul><h2 id="影音"><a class="markdownIt-Anchor" href="#影音"></a> 影音</h2><ul><li>满江红：大量情节翻转，部分剧情存在逻辑硬伤。几乎是全部场景都在大院里面拍摄，所以根本没有去电影院看的必要。</li><li>流浪地球：诚意满满，大场面，满足幻想，部分中国独有的精神。由于先看的满江红，看这部片的时候还睡了一会儿，更有趣的是电影院里面还有人打呼噜了……</li><li>扬名立万：剧本杀形式电影，案中案，最后有开放结局的意思。</li><li>飞越13号房：互动影视游戏，看异灵术直播云的，改编自杨永信事件。剧情比较简单，和隐形的守护者相比没有太多反转和暗线。</li><li>三体电视剧：第一部《地球往事》的影视化，腾讯出品，十分忠于原著。部分场景和配乐非常震撼：比如三体游戏中<a href="https://www.bilibili.com/video/BV1Qx4y177qZ/">秦始皇出场的「天有鲲鹏，展翅苍穹。六合一扫，四海统」</a>。</li><li>狂飙：看完电视剧和同名小说，讲得是扫黑除恶的故事。高启强从买鱼佬成长为黑社会老大的过程非常刺激，最后为了过审等原因，强行改了很多剧情，感觉不是很过瘾。</li></ul><h2 id="新工具"><a class="markdownIt-Anchor" href="#新工具"></a> 新工具</h2><ul><li><a href="https://elk.zone/home">鹿鸣 (elk.zone)</a> 一个灵巧的 Mastodon web 客户端，模仿 twitter 样式。<ul><li>去年就接触到 mastodon，上个月 mask 一系列操作，大量推友开始考虑迁移到 mastodon 上。我的帐号 <a href="https://douchi.space/web/@xiang578">@xiang578@douchi.space</a></li><li>可以参考这篇文章 <a href="https://medium.com/@PeiLun/%E8%A8%AD%E5%AE%9A-ifttt-%E5%90%8C%E6%AD%A5-twitter-%E5%88%B0-mastodon-1010ee3798bb">設定 IFTTT 同步 Twitter 到 Mastodon. 事前準備 | by Pei-lun | Medium</a> 将推文同步到长毛象。</li></ul></li><li><a href="https://www.stayinsession.com/">Session - Pomodoro focus timer with analytics (stayinsession.com)</a> 番茄钟工具，5 美元一个月，不太值得。SetApp 中有提供。</li><li><a href="https://umami.is/">umami</a>：网站访问量统计工具，可以用 docker 部署，也能使用官方提供的 umami cloud 服务（目前免费中），和 Google Analytics 相比更加注重隐私。</li><li><a href="https://github.com/sethyuan/logseq-plugin-another-embed">sethyuan/logseq-plugin-another-embed: This plugin provides 2 extra ways to embed blocks/pages. (github.com)</a>： logseq 插件，实现类似于 roam 的 embed 展现 ui。</li><li><a href="Miniflux.net">Minflux</a>：开源的轻量级 RSS 工具，可以用 docker 部署，在线版本 15 天试用，15 美元一年。默认不抓取全文，也没有 Inoreader 那样的双栏模式。</li><li><a href="https://www.skywind.me/blog/archives/2229">别让 cd 浪费你的时间 - Skywind Inside</a> autojump 替代，实现终端内快速目录跳转，使用 lua 语言实现，所以能感觉到速度飞快！</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;距离上次分享已经快一个月了，这段时间我度过了长达两周的春节假期，现在是时候写点东西了！&lt;/p&gt;</summary>
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
  </entry>
  
  <entry>
    <title>【Uber ETA】DeeprETA An ETA Post-processing System at Scale</title>
    <link href="https://blog.xiang578.com/post/deepreta.html"/>
    <id>https://blog.xiang578.com/post/deepreta.html</id>
    <published>2023-02-12T14:14:33.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章充满工业界风格，介绍 Uber 如何构建基于深度学习的 ETA 系统。在 Uber App 中，ETA 主要服务网约车和外卖两大业务，基于业务发展产生出一些细分场景的 ETA 需求（pick-up、drop-off）。技术挑战在于偏航（系统预估路线和司机真实路线不同）、不同场景数据分布不同、不同场景对 ETA 诉求不同，所以他们主要目标是构建高效以及泛用的 ETA 系统。</p><span id="more"></span><h2 id="背景介绍"><a class="markdownIt-Anchor" href="#背景介绍"></a> 背景介绍</h2><blockquote><p>为了更充分了解相关背景知识，也可以看看我之前写的滴滴 ETA 论文解读 <a href="https://blog.xiang578.com/post/wdr.html">(WDR) Learning to Estimate the Travel Time)</a></p></blockquote><p>ETA (Estimated Time of Arrival，预估到达时间)：给定时刻下，预估从一个起点到终点所需要的时间。ATA (Actual Time Of Arrival，真实到达时间)：给定时刻下，真实从一个起点到终点花费的时间。ETA 任务的目标是最小化 ETA 和 ATA 的差距，一般用 MAPE 或 MAE 来衡量。</p><p><img src="https://media.xiang578.com/202302121823437-uber-eta-processing.png" alt="Uber ETA Posting-processing System" /></p><p>上图是 Uber ETA Posting-processing System，主要包括四部分：</p><ul><li><code>Maps &amp; Traffic</code>： Map 指将整个真实世界用图数据（点和边）来建模。在给定起点和终点的情况下，路径规划系统规划的路线可以用一系列边来表示。Traffic 指交通状态，路况团队会根据前一段时间收集到的 gps 信号，预计后面一段时间内真实世界中道路的拥堵程度和通行速度。</li><li><code>Routing Engine</code>：根据路径规划系统规划的路线以及路况发布的道路通行速度计算出一个规则 ETA（Routing Engine ETA, RE-ETA）</li><li><code>ML Model</code>：本文介绍的深度学习 DeeprETA 模型，该模型预估 RE-ETA 和 ATA 的残差。</li><li><code>Routing APIs</code>：通过 api 接口给其他业务提供 ETA 服务</li></ul><h2 id="deepreta"><a class="markdownIt-Anchor" href="#deepreta"></a> DeeprETA</h2><p>遵循 <code>More Embeddings, Fewer Layers</code> 原则，设计预测效率更高的模型，主要包括 embdding module  和 2-Layer Module 两个模块。</p><p><img src="https://media.xiang578.com/202302122107777-deepreta-architecture.png" alt="deepreta-architecture" /></p><h3 id="embedding-module"><a class="markdownIt-Anchor" href="#embedding-module"></a> Embedding module</h3><p>模型特征主要分成三类：Continuous Features、Categorical Features 以及 Geospatial Features，具体特征如下：</p><p><img src="https://media.xiang578.com/202302121908948-deepreta-feature.png" alt="Feature Tabble" /></p><h4 id="categorical-features"><a class="markdownIt-Anchor" href="#categorical-features"></a> Categorical Features</h4><p>所有类别特征都 embedding 化</p><h4 id="continuous-features"><a class="markdownIt-Anchor" href="#continuous-features"></a> Continuous Features</h4><p>数值特征先分桶离散化，然后转成 embeddine。作者他们尝试发现 quantile buckets（等频分桶，比如每 20% 样本分一个同） 比 equal-width buckets（等距分桶）效果更好，猜测其可能原因是等频分桶更有效保留信息（We suspect that quantile buckets perform well because they maximize entropy: for any fixed number of buckets, quantile buckets convey the most information (in bits) about the original feature value compared to any other bucketing scheme.）。</p><h4 id="geospatial-features"><a class="markdownIt-Anchor" href="#geospatial-features"></a> Geospatial Features</h4><p>Geospatial Features 主要指终点和起点的经纬度坐标，通过 geo hashing + multiple feature hashing 转换成 embdding。</p><p><img src="https://media.xiang578.com/202302122052993-multi-resolution-geohashes.png" alt="multi-resolution-geohashes" /></p><ul><li>通过 Geohash 将经纬度转换成固定长度的字符串<ul><li>将 <code>lat, lng</code> 转换成 <code>[0,1]</code> 范围内浮点数</li><li>放大上一步得到的浮点数到 <code>[0, 2e32]</code> 转换成 32 位整数</li><li>将 lat 和 lng 对应的两个 32 整数合并成一个 64 位整数</li><li>利用 base32 对 64 位整数进行编码得到一个长度为 <code>5u</code> bits 的字符串</li></ul></li><li>通过 Feature hasing 将字符串转换成 index<ul><li><code>Exact indexing</code> 每个字符串转成单独 embdding。geohash 精度越高，该方法内存消耗越大。</li><li><code>Multiple feature hashing</code> feature hashing 将多个字符串转换成一个 index。考虑到hash冲突以及格子内订单的热度，同一个字符串使用多个 hash 函数处理得到多个 index。</li></ul></li></ul><h3 id="2-layer-module"><a class="markdownIt-Anchor" href="#2-layer-module"></a> 2-layer Module</h3><p>2-layer Module 分别是 Interaction layer 和 Calibration layer。模型 ETA 可以表示成：</p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mover accent="true"><mi>r</mi><mo>^</mo></mover><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">j</mi></mrow></msub><mo>=</mo><msub><mover accent="true"><mi>f</mi><mo>^</mo></mover><mn>2</mn></msub><mrow><mo fence="true">(</mo><mover accent="true"><mi>f</mi><mo>^</mo></mover><mrow><mo fence="true">(</mo><msub><mi>X</mi><msub><mi>i</mi><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi></mrow></msub></msub><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mo>+</mo><msub><mover accent="true"><mi>b</mi><mo>^</mo></mover><mi>j</mi></msub><mrow><mo fence="true">(</mo><msub><mi>X</mi><msub><mi>i</mi><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi></mrow></msub></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{r}_{\mathrm{ij}}=\hat{f}_2\left(\hat{f}\left(X_{i_{\mathrm{emb}}}\right)\right)+\hat{b}_j\left(X_{i_{\mathrm{type}}}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.317502em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">i</span><span class="mord mathrm mtight">j</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.08332999999999999em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.08332999999999999em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.55em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">e</span><span class="mord mathrm mtight">m</span><span class="mord mathrm mtight">b</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25586em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.3078899999999998em;vertical-align:-0.35001em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">b</span></span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166400000000005em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">t</span><span class="mord mathrm mtight" style="margin-right:0.01389em;">y</span><span class="mord mathrm mtight">p</span><span class="mord mathrm mtight">e</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34731999999999996em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></span></p><p>其中 f 代表 Interaction layer，f2 代表全连层，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">b_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 代表 Calibration layer （第 j 种类型 ETA 的偏置）。</p><h4 id="interaction-layer"><a class="markdownIt-Anchor" href="#interaction-layer"></a> Interaction layer</h4><p>所有特征在经过 Embedding module 后。都被转换成相同长度的 embdding，然后经过 Interaction layer 进行特征交叉。尝试过主流的网络结构后，作者选择的是 self-attention 结构，更进一步引入 Linear self-attention 解决 attention 矩阵计算速度慢的问题。</p><p><img src="https://media.xiang578.com/202302122114998-one-interaction-layer.png" alt="one-interaction-layer" /></p><h4 id="calibration-layer"><a class="markdownIt-Anchor" href="#calibration-layer"></a> Calibration layer</h4><p>针对不同业务数据分布特点，Calibration layer 根据ETA请求类型（网约车/配送、pickup/dropoff、长短单）使用全连接层对上一步模型结果进行校准（对预测结果进行整体偏移）。另外，作者也提到这一层也可以设计成 MMoE 结构。</p><p><img src="https://media.xiang578.com/202302122129047-uber-model-strucure.png" alt="uber-model-strucure" /></p><h3 id="asymmetric-huber-loss-function"><a class="markdownIt-Anchor" href="#asymmetric-huber-loss-function"></a> Asymmetric Huber loss function</h3><p>不同类型 ETA 任务需要用不同的指标来评估，比如预估价格需要 ETA mean erros，配送 ETA 利用类似于 95th 更加合理。所以作者设计出一种非对称的 Huber Loss Fucntion：</p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mrow><mo fence="true">(</mo><mi>ω</mi><mo separator="true">,</mo><mi>δ</mi><mo separator="true">,</mo><mi mathvariant="normal">Θ</mi><mo separator="true">;</mo><mrow><mo fence="true">(</mo><mi mathvariant="bold-italic">q</mi><mo separator="true">,</mo><msub><mi>y</mi><mn>0</mn></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mi>y</mi><mo fence="true">)</mo></mrow><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>ω</mi><mi mathvariant="script">L</mi><mrow><mo fence="true">(</mo><mi>δ</mi><mo separator="true">,</mo><mi mathvariant="normal">Θ</mi><mo separator="true">;</mo><mrow><mo fence="true">(</mo><mi mathvariant="bold-italic">q</mi><mo separator="true">,</mo><msub><mi>y</mi><mn>0</mn></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mi>y</mi><mo fence="true">)</mo></mrow><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>y</mi><mo>&lt;</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>ω</mi><mo stretchy="false">)</mo><mi mathvariant="script">L</mi><mrow><mo fence="true">(</mo><mi>δ</mi><mo separator="true">,</mo><mi mathvariant="normal">Θ</mi><mo separator="true">;</mo><mrow><mo fence="true">(</mo><mi mathvariant="bold-italic">q</mi><mo separator="true">,</mo><msub><mi>y</mi><mn>0</mn></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mi>y</mi><mo fence="true">)</mo></mrow><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>y</mi><mo>≥</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}\left(\omega, \delta, \Theta ;\left(\boldsymbol{q}, y_0\right), y\right)= \begin{cases}\omega \mathcal{L}\left(\delta, \Theta ;\left(\boldsymbol{q}, y_0\right), y\right), &amp; y&lt;\hat{y} \\ (1-\omega) \mathcal{L}\left(\delta, \Theta ;\left(\boldsymbol{q}, y_0\right), y\right), &amp; y \geq \hat{y}\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal">L</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">Θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">q</span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="mord"><span class="mord mathcal">L</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">Θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">q</span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="mclose">)</span><span class="mord"><span class="mord mathcal">L</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">Θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">q</span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mrow><mo fence="true">(</mo><mi>δ</mi><mo separator="true">,</mo><mi mathvariant="normal">Θ</mi><mo separator="true">;</mo><mrow><mo fence="true">(</mo><mi mathvariant="bold-italic">q</mi><mo separator="true">,</mo><msub><mi>y</mi><mn>0</mn></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mi>y</mi><mo fence="true">)</mo></mrow><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">∣</mi><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi><mo>&lt;</mo><mi>δ</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>δ</mi><mi mathvariant="normal">∣</mi><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>δ</mi><mn>2</mn></msup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">∣</mi><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi><mo>≥</mo><mi>δ</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}\left(\delta, \Theta ;\left(\boldsymbol{q}, y_0\right), y\right)= \begin{cases}\frac{1}{2}(y-\hat{y})^2, &amp; |y-\hat{y}|&lt;\delta \\ \delta|y-\hat{y}|-\frac{1}{2} \delta^2, &amp; |y-\hat{y}| \geq \delta\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal">L</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">Θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord"><span class="mord boldsymbol" style="margin-right:0.03704em;">q</span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mpunct">,</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi></mrow><annotation encoding="application/x-tex">\Theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Θ</span></span></span></span> 代表模型参数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\omega \in[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> 控制模型对高低估倾向（比如准时赔场景，如果模型低估严重会带来大量投诉）, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta&gt;0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> 控制异常值的容忍程度，值越大对异常值越不敏感。</p><h3 id="train"><a class="markdownIt-Anchor" href="#train"></a> Train</h3><ul><li>每周训练模型</li><li>Adam</li><li>Relative cosine annealing learning rate scheduler</li></ul><h2 id="实验效果"><a class="markdownIt-Anchor" href="#实验效果"></a> 实验效果</h2><h3 id="评估指标"><a class="markdownIt-Anchor" href="#评估指标"></a> 评估指标</h3><p>主要有 mean absolute error (MAE), 50th percentile absolute error (p50 error) and 95th percentile absolute error (p95 error)。实验结果都是计算三个指标相对于 RE-ETA 的提升。比如 MAE 的计算方式改成：</p><p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo fence="true">∣</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mn>0</mn><mi>i</mi></mrow></msub><mo fence="true">∣</mo></mrow><mo>−</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo fence="true">∣</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo fence="true">∣</mo></mrow></mrow><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo fence="true">∣</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mn>0</mn><mi>i</mi></mrow></msub><mo fence="true">∣</mo></mrow></mrow></mfrac><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{\frac{1}{n} \sum_{i=1}^n\left|y_i-\hat{y}_{0 i}\right|-\frac{1}{n} \sum_{i=1}^n\left|y_i-\hat{y}\right|}{\frac{1}{n} \sum_{i=1}^n\left|y_i-\hat{y}_{0 i}\right|} .</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.660216em;vertical-align:-1.080108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.580108em;"><span style="top:-2.2648919999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.7350000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.080108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">.</span></span></span></span></span></p><h3 id="结果"><a class="markdownIt-Anchor" href="#结果"></a> 结果</h3><ul><li>DeeprETANet 比 XGBoost ResNet HammockNet 等方法都有明显的效果提升。</li><li>在 p95 指标上 DeeprETANet 和无 feature hashing DeeprETANet 提升相类似，可能说明 geospatial embeddings 能在 type case 上有提升，但是不能改善极端错误的情况。</li></ul><p><img src="https://media.xiang578.com/202302122153936-deepretaresult.png" alt="deepretaresult" /></p><h3 id="embedding-analysis"><a class="markdownIt-Anchor" href="#embedding-analysis"></a> Embedding Analysis</h3><p>这一部分作者用 t-SNE (将 embedding 降维到 2维）对 geospatial embeddings 和 temporal embedding 进行分析。</p><h4 id="temporal-embedding"><a class="markdownIt-Anchor" href="#temporal-embedding"></a> temporal embedding</h4><p>两幅图分别是配送和网约车的 embedding 分布，深色代表 weekend，浅色代表 weekday，从图上看有局部连续性，但是没有明确的周末或工作日效应（深色和浅色没有明显聚集）</p><p><img src="https://media.xiang578.com/202302122201409-t-sne-time-embedding.png" alt="time-embedding" /></p><h3 id="geospatial-embeddings"><a class="markdownIt-Anchor" href="#geospatial-embeddings"></a> geospatial embeddings</h3><p>geospatial embeddings 有局部聚集性，猜测是相邻的位置有相似的表示。配送和网约车学习到的 embedding 不同。</p><p><img src="https://media.xiang578.com/202302122207120-geospatial-embeddings.png" alt="geospatial embeddings" /></p><h2 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h2><ul><li><a href="https://www.youtube.com/watch?v=CJTitzj0qBo">How Uber Predicts Arrival Times - Xinyu Hu and Olcay Cirit | Stanford MLSys #64 - YouTube</a></li><li><a href="https://www.uber.com/en-JP/blog/deepeta-how-uber-predicts-arrival-times/">DeepETA: How Uber Predicts Arrival Times Using Deep Learning | Uber Blog</a></li><li><a href="https://ithelp.ithome.com.tw/articles/10302600">Day 17 Uber 要如何估計司機和外送的抵達時間？（上）- Self attention 介紹</a>：简单讲了 self-attention 的计算过程</li><li><a href="https://ithelp.ithome.com.tw/articles/10303293">Day 18 Uber 要如何估計司機和外送的抵達時間？（下）- DeeprETANet</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;本篇文章充满工业界风格，介绍 Uber 如何构建基于深度学习的 ETA 系统。在 Uber App 中，ETA 主要服务网约车和外卖两大业务，基于业务发展产生出一些细分场景的 ETA 需求（pick-up、drop-off）。技术挑战在于偏航（系统预估路线和司机真实路线不同）、不同场景数据分布不同、不同场景对 ETA 诉求不同，所以他们主要目标是构建高效以及泛用的 ETA 系统。&lt;/p&gt;</summary>
    
    
    
    <category term="智能路" scheme="https://blog.xiang578.com/categories/%E6%99%BA%E8%83%BD%E8%B7%AF/"/>
    
    
    <category term="eta" scheme="https://blog.xiang578.com/tags/eta/"/>
    
    <category term="uber" scheme="https://blog.xiang578.com/tags/uber/"/>
    
  </entry>
  
  <entry>
    <title>【随想集】06 一元复始，重新出发</title>
    <link href="https://blog.xiang578.com/post/thinking-06.html"/>
    <id>https://blog.xiang578.com/post/thinking-06.html</id>
    <published>2023-01-08T08:48:00.000Z</published>
    <updated>2023-03-28T03:58:52.116Z</updated>
    
    <content type="html"><![CDATA[<p>距离上一次写随想集已经过去一年……趁现在是的一年开始，重新捡起这个习惯来写点东西。</p><span id="more"></span><h2 id="生活"><a class="markdownIt-Anchor" href="#生活"></a> 生活</h2><p>自从去年国内放开后，大家陆续感染了新冠。随着最近一段时间开始去公司上班，我也不可避免的中招：</p><ul><li>周四(0105)，喉咙开始痛。</li><li>周五(0106)，喉咙还是痛，体温也到 37.5 摄氏度，晃一下脑袋都能感觉到头晕，然后整天基本上都是躺在床上发呆或者睡觉，不过测抗原还没有事。</li><li>周六(0107)，喉咙痛好一些，开始咳嗽，吃了一些 999 强力琵琶露。</li><li>周日(0108)，测出抗原阳了，然后其他症状基本上消失。</li></ul><h2 id="阅读"><a class="markdownIt-Anchor" href="#阅读"></a> 阅读</h2><ul><li><a href="https://notes.xiang578.com/#/page/%40%E7%BF%A6%E5%95%86%EF%BC%9A%E6%AE%B7%E5%91%A8%E4%B9%8B%E5%8F%98%E4%B8%8E%E5%8D%8E%E5%A4%8F%E6%96%B0%E7%94%9F">翦商：殷周之变与华夏新生</a><ul><li>前一段时间挺火的历史小说，讲得是夏商周的历史。上古史的参考文献很少，作者亮点在于结合这几十年的考古成果（二里头、殷墟、周原等等）进行创造。翦商出自诗经「后稷之孙，实维太王，居岐之阳，实始翦商。」，意思是周灭商。为什么周要消灭商？商代遗址挖掘发现和甲骨文破译，商人生活中使用大量的人牲进行祭祀（比如房屋用人奠基，墓葬用人陪葬……），而且随着时间推移，祭祀规模越来越大，手段越来越残酷。作者推测周族可能长时间从事给商人捕捉祭祀用的人牲工作，随着周文王被拘和伯邑考被杀，周商矛盾激化，周武王开始伐商。周公旦在武王驾崩后，辅佐成王，创建礼乐制度。后世流传的文献没有直接记录商朝活人祭祀，可能是周公毁掉相关的档案，改写历史。</li></ul></li><li>读库 2105：垄断的困境<ul><li>评论尸去年写的长文，围绕互联网相关垄断进行讨论。首先，定义有效垄断（建立在可行的商业模式上）以及无效垄断（任何在短期和长期均无法赚钱的垄断，都应破视为商业上的无效垄断）。一言蔽之，企业垄断的目的是赚钱。<ul><li>无效垄断的例子正是我现在的东家：滴滴，占有网约车市场绝大部分份额却还一直在亏损，产生这种情况更多是竞争对手不想竞争，而不是不能与其竞争。</li><li>有效垄断的例子则是当年的淘宝屏蔽百度对商品详情页面的抓取，淘宝的推荐位一直是阿里重要的收入来源，其他公司也开始学着屏蔽百度爬虫，互联网变得更加割裂。</li></ul></li><li>产生大量无效垄断的根因是资本的无序扩张。</li><li>通过计算垄断的成本收益来衡量互联网企业的垄断是否合理。</li><li>最后作者还是举例了 多抓鱼，元气森林，shenin 等例子来讨论如何在巨头垄断的局面下进行创新。</li></ul></li></ul><h2 id="利器"><a class="markdownIt-Anchor" href="#利器"></a> 利器</h2><ul><li><a href="https://logseq.com/">Logseq</a> 去年 9 月份开始，觉得 Roam Research 更新速度太慢，又迁移到 Logseq。为了使用同步功能，也一直是  Backers 用户（ 5$ 每月）。</li><li><a href="https://readwise.io/i/ryen05">Readwise</a> 高亮管理服务，用来复习 kindle，微信读书，简悦等服务上的高亮。另外也开始用它家的 <a href="https://readwise.io/read">Readwise Reader</a> 阅读 epub 和 newsletter。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;距离上一次写随想集已经过去一年……趁现在是的一年开始，重新捡起这个习惯来写点东西。&lt;/p&gt;</summary>
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
    <category term="logseq" scheme="https://blog.xiang578.com/tags/logseq/"/>
    
    <category term="covid-19" scheme="https://blog.xiang578.com/tags/covid-19/"/>
    
    <category term="readwise" scheme="https://blog.xiang578.com/tags/readwise/"/>
    
  </entry>
  
  <entry>
    <title>【随想集】05 重新使用 Logseq</title>
    <link href="https://blog.xiang578.com/post/thinking-05.html"/>
    <id>https://blog.xiang578.com/post/thinking-05.html</id>
    <published>2022-01-09T11:00:11.000Z</published>
    <updated>2023-03-28T03:58:52.116Z</updated>
    
    <content type="html"><![CDATA[<p>新年快乐！不知不觉距离上次写博客过去 4 个多月，去年的年终总结还不知道如何动笔，先尝试写一篇简单的随想集热热身。</p><span id="more"></span><p>这两年新的笔记软件层出不穷，目前全部私人笔记使用 Roam Research 记录，但是 Roam 过去一年没有什么大更新，更别提移动端。另外为了搭建数字花园，使用 Obsidian 管理部分可以公开的笔记，再通过 MkDocs 生成 html 文件发布（谁让 Obsidian Publish 每月费用高达 8 刀）。</p><p>前几天听 <a href="https://twitter.com/tiensonqin">Tienson Qin</a> 采访 <a href="https://www.xiaoyuzhoufm.com/episode/61c6cccac525309a443b5b36?s=eyJ1IjogIjYxNmFlZTVhZTBmNWU3MjNiYjhjNzE4MCJ9">跟 Logseq 的创造者天生聊聊 Logseq 的故事 - ByteTalk | 小宇宙</a> 后，重新对 Logseq 产生兴趣（刚出的时候体验过，开源+弱化版Roam）。Logseq 支持直接导入 Roam json 文件，我大概有 2k+ 笔记，大小在 6 MB 左右。直接迁移过去有一些细节问题，以至于我发了 <a href="https://github.com/logseq/logseq/issues/3684">Issue #3684：导入 roam json 数据后部分 blockref 无法正确显示</a>，在开发者（不知道是不是）帮助下完美解决。不过体验几个小时后，总感觉 0.5.6 版本很卡，随便进行点击操作，需要 5s 以上才会有反馈……无奈只好放弃迁移。</p><p>这次体验过程中 logseq 插件 <a href="https://github.com/debanjandhar12/logseq-anki-sync">debanjandhar12/logseq-anki-sync: An Logseq to Anki Flashcards integration plugin.</a> 给我留下深刻的印象，只需点击插件 icon 就可以通过 Anki Connect 将笔记中的卡片同步到 Anki 中（单向更新）。使用 Roam 时，我都是将需要制卡的笔记统一整理到一个文档中，然后使用 <a href="https://github.com/shiqi-lu/roam2anki">shiqi-lu/roam2anki</a> 转换成csv 文件，最后手动导入 Anki。可见本地客户端想象空间更加强大！</p><p>Logseq 本身有 Publish 功能，可以配置将全部或有 <code>public:: true</code> 属性的笔记一起导出成单个 html 文件，然后借助静态网站托管工具发布。每次都进行这样的操作也挺麻烦的，幸好前几天看到有人发布 <a href="https://github.com/pengx17/logseq-publish">pengx17/logseq-publish: Logseq Publish Action</a> ，从而实现笔记数据推送到 github 后，自动编译 html 文件并发布。不过笔记数据存在 private repo 中，而我没有购买 <code>Github Pro</code> 服务（这回不是钱的问题，只是找不到理由说服我自己），所以无法直接使用 Github Pages 功能。退而求其次，借助 <a href="https://vercel.com/">Vercel</a> 发布导出的网页。如果你对我公开笔记感兴趣，可以访问：<a href="https://notes.xiang578.com/#/page/Contents">算法花园・笔记</a>。</p><p>这周也看了一些 Supermemo 相关的文章和视频，等再深入体验一下和大家分享。</p><p>最后，推荐一些这两周我看过觉得不错的文章：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/453340208">【ML专栏】谈一种复用CART训练中丢弃信息的改进方案 - 知乎</a>：之前一位同事的文章，对 GBDT 分裂过程的小改进。</li><li><a href="https://blog.k8s.li/2021.html">时光痕迹：2021 年总结 | Reimu’s blog</a></li><li><a href="https://freemind.pluskid.org/misc/2021-summary/">2021 世界终结前一天</a>：pluskid 的年终总结，他已经坚持写了十多年。去年也进行过推荐</li><li><a href="https://wdxmzy.com/pastfuture/year2021/2021/12/31/">2021 总结与 2022 计划 | 小土刀 2.0</a></li><li><a href="https://github.com/saveweb/review-2021">saveweb/review-2021: 今年，你写年终总结了吗？</a>：年终总结大合集，一次看到爽！</li><li><a href="https://mp.weixin.qq.com/s?__biz=MzA4MjkwMDcyOQ==&amp;mid=2647743768&amp;idx=1&amp;sn=7e8bb4538413b1225e22dceb97ca4df0&amp;chksm=87db5e8eb0acd7987c00bd9177aa732e8c4c9ffb33829d5684fe45db8f7a300e90cd1203fb76&amp;mpshare=1&amp;scene=1&amp;srcid=1224Zq6Af0OpusK6GOSQjurt&amp;sharer_sharetime=1640357186656&amp;sharer_shareid=b9cd91a2d6fed9ed64a4a90526f5e1c6#rd">工程化你的习惯</a>：介绍三个习惯记录系统，自己会在今年尝试复刻 Ali Yahya 的 walrOS。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;新年快乐！不知不觉距离上次写博客过去 4 个多月，去年的年终总结还不知道如何动笔，先尝试写一篇简单的随想集热热身。&lt;/p&gt;</summary>
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
    <category term="logseq" scheme="https://blog.xiang578.com/tags/logseq/"/>
    
  </entry>
  
  <entry>
    <title>【随想集】04 曾经我也想过一了百了</title>
    <link href="https://blog.xiang578.com/post/thinking-04.html"/>
    <id>https://blog.xiang578.com/post/thinking-04.html</id>
    <published>2021-08-14T11:00:11.000Z</published>
    <updated>2023-03-28T03:58:52.116Z</updated>
    
    <content type="html"><![CDATA[<p>开始前先推荐一首日文歌曲 <a href="https://www.bilibili.com/video/BV1oW411b7hx">中岛美嘉 - 曾经我也想过一了百了</a> 以及中文填词 <a href="https://www.bilibili.com/video/BV1uW41117yV">【小宁子】曾经我也想过一了百了》中文版 feat. 绝之望的马沙 （僕が死のうと思ったのは）</a>。第一个链接的视频里，中岛美嘉在接近于失聪的状态唱这一首歌。可以注意细节，有时候她的脚在跺地板找节奏。</p><span id="more"></span><p>差不多两个月时间没有写随想集，自己又折腾出一个网站 <a href="https://note.xiang578.com/">Hi - 算法花园・笔记</a>，用来分享一些不成熟的想法。</p><h2 id="图论-算法花园笔记"><a class="markdownIt-Anchor" href="#图论-算法花园笔记"></a> <a href="https://note.xiang578.com/CP/%E5%9B%BE%E8%AE%BA/">图论 - 算法花园・笔记</a></h2><p>实习的时候做过将CH中的搜索模块从双向 Dijikstra 改成双向 A*，很大一部分精力是在尝试启发函数。后来去面试时，有人给了一个启发函数，问为什么不行？<br />最近听专门做图论同事分享时，他提到一句所有图论新算法结果需要和dijkstra一致，可能这就是答案。</p><p>之前做题目时，喜欢的最短路算法是 SPFA 。前几天为了面试一个候选人，特地去 oi wiki 上回顾了一下其他最短路算法，spfa的优势在于使用队列模拟优先队列，一般出题人不特殊构造图，基本上会比 Dijikstra 快。可惜当时没有改一版试试。<br />说回来，工业界看起来只有几家公司有图论方面的需求，但也能让一些人做到极致。</p><h2 id="我眼中的技术深度"><a class="markdownIt-Anchor" href="#我眼中的技术深度"></a> <a href="https://mp.weixin.qq.com/s/fN1GcC1vwZqe-UtCGBO9gg">我眼中的技术深度</a></h2><p>前几天听陶文老师参与录制播客<a href="https://pythonhunter.org/episodes/taowen-vol1">Ep 29. 架构设计与 12fallacy（上） - 捕蛇者说</a>，想起他之前在内网写的文章。作为一名中台部门的算法工程师，其实面临的问题和业务部门的软件工程师挺类似的。</p><p><img src="https://media.xiang578.com/taowen-deep-of-tech.png" alt="" /></p><p>他将工程师的主要工作是定义成：构建一个可持续运行的 solution 去解决用户的一个 problem。深度来自：运营维护这个 solution 如何降低成本以及洞察到 problem 的 new solution。你的竞争力来自：</p><ul><li>为什么在这个点上，我做过的工作证明了比其他同事要更强。</li><li>为什么这个能力是当前公司需要的，也就是所谓的收益。</li></ul><h2 id="ise-grand-shrine"><a class="markdownIt-Anchor" href="#ise-grand-shrine"></a> Ise Grand Shrine</h2><p>很难想象核心模型的训练代码，是一个 10 多人修修补补，长达1 千行的 py 文件（驼峰命名、匈牙利命名、下划线命名混合，多行类似的代码以最后一行生效等）。忍受三年之后，感觉自己有能力去重构一把。特意将分支命名为 ise_gradn_shrine，致敬之前听到的伊势神社（每一代工人之间为传递知识，二十年原地重建一次神社）。并将下面这一段话放到 README 中。</p><blockquote><p>Japan’s Ise Grand Shrine is an extraordinary example in that genre. Every 20 years, caretakers completely tear down the shrine and build it anew. The wooden shrine has been rebuilt again and again for 1,200 years. Locals want to make sure that they don’t ever forget the production knowledge that goes into constructing the shrine. There’s a very clear sense that the older generation wants to teach the building techniques to the younger generation: “I will leave these duties to you next time.”</p></blockquote><h2 id="旧文重发2017-wf感想的校内版本-知乎-zhihucom"><a class="markdownIt-Anchor" href="#旧文重发2017-wf感想的校内版本-知乎-zhihucom"></a> <a href="https://zhuanlan.zhihu.com/p/203222250">「旧文重发」：2017 WF感想的校内版本 - 知乎 (zhihu.com)</a></h2><p>这两个月一直在强迫自己参加 LeetCode 的双周赛、周赛，现在的水平大概勉强能写出 3 题。赛后会去看一下其他人的题解，很多方法自己是绝对想不到的。机缘巧合看到戴老师的文章，自己之前也没有好好参加 rating 比赛，缺少一份面对真实的勇气。有时候点开 LeetCode 比赛在想，如果这场我一题都不会怎么办？可能唯一的选择是在电脑前坐满两个小时。最后文章里面提到要培养思维，还在上学时被人家评价「只会暴力」，这也是我欠缺的，所以现在打完比赛都会进行简单复盘，自己我感觉挺有收获的，极端方法应该是录屏分析。程序设计竞赛对于我意味着什么？可能山就在那里。</p><blockquote><p>个人实力保证队伍下限，队伍配合决定队伍的上限。<br />有趣的是，可能是受我之前的错误观念的影响，我们学校这么些年来真正高分 rating 选手确实是少之又少，很多 final 选手甚至不爱打线上比赛（例如 luyi 学长）。即使参加比赛，也总是喜欢开小号，开黑，而不是认认真真的一路做到底。开小号有个什么坏处呢？赢了是我吊，输了不丢人，这种心态就不是个正常打比赛的心态，等到了关键时候，不能开小号了，心理就可能承受不住压力崩溃。<br />所以做一道题，如果做不出来，看题解不丢人，但要仔细想想：为什么自己想不出来？解题的切入点应该怎么分析出来？以后遇到类似思路、类似条件的题目应该出哪里下手？要及早建立自己的规范的思维方式和体系，这样无论是在竞赛中，还是在以后的学习、工作中，都能发挥重要的作用。</p></blockquote><h2 id="电影心灵捕手-西恩关于真实感"><a class="markdownIt-Anchor" href="#电影心灵捕手-西恩关于真实感"></a> 电影《心灵捕手》 西恩关于真实感</h2><blockquote><p>你只是个孩子，你根本不晓得你在说什麽。   问你艺术，你可能会提出艺术书籍中的粗浅论调，有关米开朗基罗，你知道很多，他的满腔政治热情，他与教皇相交莫逆，和他的耽于性爱，你对他很清楚吧？但你知道西斯汀教堂的气味吗？你没试过站在那儿，昂首眺望天花板上的名画吧？你肯定未见过吧？</p></blockquote><h2 id="如何阅读"><a class="markdownIt-Anchor" href="#如何阅读"></a> 如何阅读？</h2><ul><li><a href="https://einverne.github.io/post/2021/05/is-reading-the-only-important-thing.html">读书是否是唯一重要的事？ | Verne in GitHub (einverne.github.io)</a>：读书与实践的关系。</li><li><a href="https://www.zhuoxi.me/2020/12/%E5%88%B0%E5%BA%95%E8%A6%81%E5%A6%82%E4%BD%95%E6%9C%89%E6%95%88%E8%AF%BB%E4%B9%A6%E5%91%A2-2021%E5%B9%B4%E8%AF%BB%E4%B9%A6%E8%AE%A1%E5%88%92-%E6%9C%80%E5%A4%9A%E5%8F%AA%E8%AF%BB50%E6%9C%AC/">到底要如何有效读书呢 - 2021年读书计划， 最多只读50本 (zhuoxi.me)</a></li></ul><h2 id="尾巴"><a class="markdownIt-Anchor" href="#尾巴"></a> 尾巴</h2><p>推特上冲浪时看到一个问题「数学是发明还是发现？」</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;开始前先推荐一首日文歌曲 &lt;a href=&quot;https://www.bilibili.com/video/BV1oW411b7hx&quot;&gt;中岛美嘉 - 曾经我也想过一了百了&lt;/a&gt; 以及中文填词 &lt;a href=&quot;https://www.bilibili.com/video/BV1uW41117yV&quot;&gt;【小宁子】曾经我也想过一了百了》中文版 feat. 绝之望的马沙 （僕が死のうと思ったのは）&lt;/a&gt;。第一个链接的视频里，中岛美嘉在接近于失聪的状态唱这一首歌。可以注意细节，有时候她的脚在跺地板找节奏。&lt;/p&gt;</summary>
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
  </entry>
  
  <entry>
    <title>【随想集】03 你当像鸟飞往你的山</title>
    <link href="https://blog.xiang578.com/post/thinking-03.html"/>
    <id>https://blog.xiang578.com/post/thinking-03.html</id>
    <published>2021-06-05T11:00:11.000Z</published>
    <updated>2023-03-28T03:58:52.116Z</updated>
    
    <content type="html"><![CDATA[<p>本期的标题取自《圣经》，当然大部分人可能更熟悉这个名字是 <a href="https://book.douban.com/subject/33440205/">Educated:A Memoir</a> 中文版的书名。这本书主要讲的是一个大山女孩克服各种困难成为一名博士的故事，虽然我还没有看过，但还是推荐大家去读一读……</p><span id="more"></span><h2 id="教育"><a class="markdownIt-Anchor" href="#教育"></a> 教育</h2><p>如果没有记错，明天又是一年一度高考的日子。一个神奇的现象，大部分人都觉得自己考差了。几年前我的想法是，高考前有无限的可能，高考后几乎没有什么选择。当然这些也只是我的牢骚，说回来，至少高考提供意义上的公平。借这个机会写一写教育。</p><h3 id="一群穷孩子的人生实验"><a class="markdownIt-Anchor" href="#一群穷孩子的人生实验"></a> <a href="https://mp.weixin.qq.com/s/9N_F6k2a2Tu-o76F3-Q0IQ">一群穷孩子的人生实验</a></h3><p>中国版罗森塔尔实验，十几年前，几个老师在北京务工人员的子女中挑选出一批差不多上小学的孩子组成青云班，参考类似八中少年班的方式，希望能培养成「品行高尚、具有国际视野、富有人文与创新精神的精英人才」。</p><p>不知道在户籍更加宽松的地方会不会有不一样的结果？</p><blockquote><p>第一届青云班最终的成绩就是这样了：有北京户口的孩子顺利地上了北大或出了国，剩下的一部分去了西澳大学，还有几位磕磕绊绊，最终在国内上了本科或专科，极少的人只读到高中或没了下落。</p></blockquote><p>优质教育是一个昂贵的商品。这个项目坚持十多年，最后的投入资金远超 5000 万。</p><blockquote><p>直到他们高中毕业后，孙文华与几位第一届青云班的同学吃饭，喝了一些酒，才讲出了平时他不该说的话。他说随着孩子们慢慢长大，他不再像最开始那样希望他们出人头地，对社会做很大贡献，他的要求在逐渐降低，希望他们首先是作为一个人，一个积极的人，然后还能过得幸福快乐，就足够了——项目逐渐往前走，他发现教育确实是一件吃力不讨好的事情，花钱、耗时间，而且也许并不会有一个让别人很满意的结果。</p></blockquote><p>罗森塔尔在 1966 年得出结论：教师对学生积极的态度，会让他们朝着正确的方向改进。很高兴，我在初三也体会过这种感觉。</p><blockquote><p>孩子们高中毕业时，刘正奎告诉了他们故事的另一个真相——当年选拔他们时的智商测试并不严格，是「矮子里拔高个儿」。青云班孩子们的智商大概在120左右，比普通人稍高，但与北京八中少儿班人均130的智商水平，还是有很大差距，「哪里有那么多超常的孩子？」但他接着说，让人欣慰的是，当被告知有天分时，孩子们就真的可以做到很不错的程度。而学生们听了这话，并不真正感到失望，而是开起了玩笑：「校长你忽悠我们！」<br />实际上，他们早已在漫长过程里修正了对自我的认识。他们确实度过过一个「被选中」的阶段，觉得可以通过努力，成为伟大的、和别人都不一样的人。但后来慢慢发现，大家本来都是不一样的，伟大和成功也没有确切标准，而且——外部环境对一个人的影响太大了。</p></blockquote><p>这是其中一个孩子对这个项目的评价。我也挺喜欢这种人生突然被某样东西改变的故事。</p><blockquote><p>从外界看，这可能是一个非常失败的投资，是一个不太成功的教育案例。但我觉得它是成功的，它教会了我们，如何做一个向上的人、一个正直的人。</p></blockquote><h3 id="inspire-a-generation"><a class="markdownIt-Anchor" href="#inspire-a-generation"></a> Inspire a generation</h3><p>这个是 2012 年伦敦奥运会的口号，中文是「激励一代人」。之前读过一篇文章 <a href="https://cf020031308.github.io/blog/why-i-quit-my-job-for-furthur-study">为何我要辞职读研 | cf020031308.github.io</a>，其中写到两段：</p><blockquote><p>写这文章时，女儿一岁半，预计她小学时会参加我的毕业礼。我希望用包括这几年在内的自己的行动，教会她终生学习、终生奋斗的道理。我很遗憾相比我继续工作，她这些年能获得的教育资源一定更差，但我仍认为将这股精神给到她会是更加宝贵的。<br />前面提到我生自县城，也提到我曾经的没志气、没见识、没规划，很遗憾这可能是大多数小城青年的通病。三十岁了还去读书，虽然只是为了自己与自己的小家，但终究会在家族中树立起一个标杆，拓宽后辈的眼界，动摇长辈的成见，多多少少算是对我们既定命运的一点反抗吧。</p></blockquote><p>对于大部分人来说，教育是一个人的事情，而是几代人的希望。我可能无法激励其他人，但是能给家族里面年轻一代一些关于求学路上的思考。不过从目前来看，每次回家看见几个表弟在疯狂打「王者荣耀」之类的游戏，我都无动于衷。还是希望有一天，能像 <a href="http://xiaohanyu.me/posts/2017-02-13-about-education/">教育对人的改变有多大？</a> 这个故事中主人公那样帮助弟弟考上大学吧。</p><p>最后，其实有空也可以观察身边的同事。</p><blockquote><p>就拿互联网公司来说，外部看到的一些粉丝很多的人顶多是中层级别的，然后这些人离顶端的扎克伯克，马云之间有着大量的资深总监，vp之类的人物，他们不算出名，但是却有着极高的身价，大量时间投入在解决实在问题，这些人的经验基本上只会在熟人圈子里传播，亲戚的孩子之类的。</p></blockquote><h2 id="平行世界"><a class="markdownIt-Anchor" href="#平行世界"></a> 平行世界</h2><p>小径分岔的花园，间谍小说，剧情很巧妙。我们生活的世界就像小径分岔的花园，每到一个岔入口，你走上不同的路，就失去走进其他未来的可能。</p><p>彗星来的那一夜，低成本科幻片。如果多个平行世界之间相互干涉，似乎找不到完美解，不可避免落入黑森林法则？</p><p>笛卡尔的哲学中有一句，我们的世界是众多可能的世界之中最好的一个。</p><h2 id="职场人如何系统做好知识管理"><a class="markdownIt-Anchor" href="#职场人如何系统做好知识管理"></a> <a href="https://youcangogogo.com/live_pc/l_60b49afce4b0017651a08b91">职场人如何系统做好知识管理</a></h2><p>少楠老师前一段时间线上分享，大部分理念都在之前提到过。这个讲座是一个很好的实例，产品总监关于「知识管理」应该有怎么样的认知？</p><ul><li>知识管理的定义：知识是⽤于⽣产的信息（有意义的信息）。管理就是制定决策。知识管理是获取、分配和有效利⽤知识的过程。</li><li>知识管理四个过程：<ul><li>获取：明确自己的母题（关注什么领域），形成知识的复利。<ul><li>这一点自己其实做的挺差的。之前有人和我说，感觉你什么都懂。可能后面半句「但是什么东西都只懂一些」顾及我的面子没有明说。</li></ul></li><li>记录：保持一定的输入难度。也推崇 Zettelkasten 的理念，每天练习写原子化的卡片。</li><li>整理：不进行传统意义上的分类，让知识进行生长。然后建立索引（Zettelkasten 中的大纲笔记）让知识结构涌现出来。</li><li>输出：输出是做出正确的决策。输出不是为了关注，而是为了得到反馈。</li></ul></li><li>需要通过自己的尝试形成一套知识管理的系统。</li></ul><p>最后少楠老师答疑时，抛出一个问题：离开一家公司你带走的是什么？</p><h2 id="重铸荣光"><a class="markdownIt-Anchor" href="#重铸荣光"></a> 重铸荣光</h2><p>前一段时间 MSI 比赛，LCK DWG 中单选手 showmaker 在 S10 宣传视频中说的文案反复在论坛上被引用：</p><blockquote><p>那一年的决赛，是SSG对阵SKT，最终比分3-0。当时我看见FAKER颓坐在椅子上泣不成声，这个画面我永生难忘。<br />那一刻我在想，如果我能成为一名职业选手，我一定要赢下所有。如今奖杯就在眼前，我必须考虑这会不会是我此生仅有的机会。<br />我相信LCK能有过去的霸主地位，FAKER功不可没。重铸LCK的荣光，我辈义不容辞!</p></blockquote><h2 id="尾巴"><a class="markdownIt-Anchor" href="#尾巴"></a> 尾巴</h2><p>我在内部 IM 给一位离职同事发的最后一条消息也是 「你当像鸟飞往你的山」。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本期的标题取自《圣经》，当然大部分人可能更熟悉这个名字是 &lt;a href=&quot;https://book.douban.com/subject/33440205/&quot;&gt;Educated:A Memoir&lt;/a&gt; 中文版的书名。这本书主要讲的是一个大山女孩克服各种困难成为一名博士的故事，虽然我还没有看过，但还是推荐大家去读一读……&lt;/p&gt;</summary>
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
  </entry>
  
  <entry>
    <title>【随想集】02 我曾刻意远离人群</title>
    <link href="https://blog.xiang578.com/post/Thinking-02.html"/>
    <id>https://blog.xiang578.com/post/Thinking-02.html</id>
    <published>2021-05-31T11:00:11.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>开始之前推荐一首歌 <a href="https://music.163.com/#/song?id=16139381">3055 - Ólafur Arnalds</a>。</p><span id="more"></span><h2 id="1-我曾刻意远离人群"><a class="markdownIt-Anchor" href="#1-我曾刻意远离人群"></a> 1. 我曾刻意远离人群</h2><p>五一前，遭受打击，开始陷入自我怀疑中。想起吴岭南在「无闻西东」中的一段台词：</p><blockquote><p>当我在你们这个年纪，有段时间，我远离人群，独自思索，我的人生到底应该怎样度过？某日，我偶然去图书馆，听到泰戈尔的演讲，而陪同在泰戈尔身边的人，是当时最卓越的一群人，这些人站在那里，自信而笃定，那种从容让我十分羡慕。而泰戈尔，正在讲“对自己的真实”有多么重要，那一刻，我从思索生命意义的羞耻感中，释放出来。原来这些卓越的人物，也认为花时间思考这些，谈论这些，是重要的。今天，我把泰戈尔的诗介绍给你们，希望你们在今后的岁月里，不要放弃对生命的思索，对自己的真实。</p></blockquote><h3 id="请给我五月"><a class="markdownIt-Anchor" href="#请给我五月"></a> <a href="https://mp.weixin.qq.com/s/lB2whRNySiZrBLWHIEpkUA">请给我五月</a> 以及 <a href="https://mp.weixin.qq.com/s?__biz=MzIyNzA5ODg5NA==&amp;mid=2647828222&amp;idx=1&amp;sn=97a2bb46a3416ff2dadc13c3a4978f01&amp;chksm=f0406e9fc737e7892dd99b511d4ace483703ad322b7baa46e2be8327de9b6b35a755ea37b969&amp;scene=21#wechat_redirect">逃离韩料店</a></h3><p>Hayami，拼多多 PM（写出这个多少怀疑自己是时间太多从而导致什么都不想干），一直觉得她的文字很温暖。这两篇文章分别是她在 23 岁以及 24 岁生日所写，有时候读其他人的记录相当于给自己照镜子。5 月我也很长时间怀疑人生没有意义，甚至读到哲学史中的「所有形而上学的问题都没有答案」时还有一些激动。后来渐渐明白，自己更大的问题是不知道如何做到自己认为的人生意义。阅读不是解决孤独的方法，创造才是，所以才鼓起勇气重新写下这篇随想集。</p><p>用创作来「抵抗」惯性、「抵抗」生活日复一日的琐屑：</p><ul><li>当你觉得什么事情都提不起兴趣的时候，不要让惯性吞没了自己，去做点什么吧，至少做点什么事情。</li><li>洛克菲勒的信中说，克服绝望的方式只有一种，那就是持续（用自己的行动和努力）创造出各种可能性以跨越障碍。</li><li>如果想去建立自己的社会关系，想去结识丰满有趣的人类，最有效的方法就是成为他们想要认识的人。而成为他们想要认识的人，就是要去创作。</li><li>豆瓣上有个很热门的广播提问—「大家生活的支点究竟是什么呢？（换句话说，究竟是因为什么东西所以愿意承受和忍耐生活中无穷无尽的困难和苦难呢？）」<ul><li>排名分先后：想象力，创作，爱。简言之，是自己信仰的价值体系，包括超越性的审美和智性的情感。</li></ul></li></ul><h2 id="2-我到了参加葬礼的年纪"><a class="markdownIt-Anchor" href="#2-我到了参加葬礼的年纪"></a> 2. 我到了参加葬礼的年纪</h2><p>外公在 4 月 30 日选择离开我们，对他的印象永远停留今年春节。5 月 10 日葬礼前，一直在家里休假，特地去看一些和死亡相关的内容。</p><h3 id="寻梦环游记"><a class="markdownIt-Anchor" href="#寻梦环游记"></a> 寻梦环游记</h3><p>人的一生，要死去三次：</p><ul><li>第一次，当你的心跳停止，呼吸消逝，你在生物学上被宣告了死亡；</li><li>第二次，当你下葬，人们穿着黑衣出席你的葬礼，他们宣告，你在这个社会上不复存在，你悄然离去；</li><li>第三次，是这个世界上最后一个记得你的人，把你忘记，于是，你就真正的死去，整个宇宙都不再和你有关。</li></ul><h3 id="人生总有一刻我们会开始思考死亡-知乎"><a class="markdownIt-Anchor" href="#人生总有一刻我们会开始思考死亡-知乎"></a> <a href="https://zhuanlan.zhihu.com/p/24640592">人生总有一刻，我们会开始思考死亡 - 知乎</a></h3><p>于是我发现，面对死亡最终可能只有两种方法：</p><ul><li>一种是将自己与一些更宏大的东西联系起来：一个数学定理、一本文学著作、一件艺术作品或一种恒久的信仰。马尔克斯与康德靠《百年孤独》与《[[纯粹理性批判]]》遗世独立，米开朗基罗把《创世纪》和《最后的审判》印刻在西斯廷大教堂里，供千万后朝拜——他们肉身虽灭，但精神不朽——反正建筑是永远戳在那儿的。</li><li>还有一种就是，生活在当下的每个瞬间里，不烦扰过去、不担忧将来。</li></ul><h3 id="lambda-reading-这里蕴涵着世界和灵魂的救赎以及关于死亡的读书笔记"><a class="markdownIt-Anchor" href="#lambda-reading-这里蕴涵着世界和灵魂的救赎以及关于死亡的读书笔记"></a> <a href="https://rizi.me/#2019%E5%B9%B410%E6%9C%8830%E6%97%A5%E5%91%A8%E4%B8%89%E4%B8%8B%E5%8D%8804%3A02%3A39:%E3%8B%A1%20%E7%AD%89%E5%BE%85%E4%BB%96%E7%9A%84%E6%AD%BB%E4%BA%A1%202019%E5%B9%B410%E6%9C%8830%E6%97%A5%E5%91%A8%E4%B8%89%E4%B8%8B%E5%8D%8804%3A02%3A39">Lambda-Reading 这里蕴涵着世界和灵魂的救赎以及关于死亡的读书笔记</a></h3><p>临终者的五大遗憾、《天堂的证据》、《陪伴生命》（我从临终病人眼中看到的幸福）、《与神对话》、《耶鲁大学公开课：死亡》、《死亡如此多情》（百位临床医生口述的临终事件）、《向死而生》</p><h3 id="people-die-but-long-live-github-laike9ms-blog"><a class="markdownIt-Anchor" href="#people-die-but-long-live-github-laike9ms-blog"></a> <a href="https://laike9m.com/blog/people-die-but-long-live-github,122/">People Die, but Long Live GitHub - laike9m’s blog</a></h3><blockquote><p>目前来看，我认为 GitHub 是在百年尺度上存储信息并让其能被访问的<strong>唯一</strong>途径。</p></blockquote><p><a href="https://joearms.github.io/#2013-11-21%20My%20favorite%20Erlang%20Program">Joe’s Blog — a non-linear personal web notebook</a> 临终前把博客文章用 TiddlyWiki 保存在 Github，并且写到「I hope the blog will be readable in 100+ years time.」也许 Github 是我们这个时代的数字公墓（不老阁）。</p><h3 id="城堡-17很高兴遇见你-语雀-yuquecom"><a class="markdownIt-Anchor" href="#城堡-17很高兴遇见你-语雀-yuquecom"></a> <a href="https://www.yuque.com/gaohui-bdaa2/chengbao/lug9lw">城堡 17：很高兴遇见你…… · 语雀 (yuque.com)</a></h3><p>我和解的方式是，也曾经让外公在某些时刻高兴过。</p><h3 id="迪拜孝子-郭麒麟-阎鹤祥_哔哩哔哩_bilibili"><a class="markdownIt-Anchor" href="#迪拜孝子-郭麒麟-阎鹤祥_哔哩哔哩_bilibili"></a> <a href="https://www.bilibili.com/video/BV1XX4y1V7bG">【迪拜孝子】 郭麒麟 阎鹤祥_哔哩哔哩_bilibili</a></h3><p>郭麒麟在《欢乐戏剧人》一个相声的链接，这个节目改编自传统相声《福寿全》。大概情节是，一个人用钱忽悠另外一个人穿麻带孝，给号称死去的老富翁当孝子。现在想起来，这是我第一次关于葬礼的练习。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;开始之前推荐一首歌 &lt;a href=&quot;https://music.163.com/#/song?id=16139381&quot;&gt;3055 - Ólafur Arnalds&lt;/a&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
  </entry>
  
  <entry>
    <title>【随想集】01 我讲不清楚事情怎么办？</title>
    <link href="https://blog.xiang578.com/post/Thinking-01.html"/>
    <id>https://blog.xiang578.com/post/Thinking-01.html</id>
    <published>2021-03-14T11:00:11.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>重新定期写一些阅读分享，暂时命名为「随想集」。</p><span id="more"></span><h2 id="我讲不清楚事情怎么办-语雀"><a class="markdownIt-Anchor" href="#我讲不清楚事情怎么办-语雀"></a> <a href="https://www.yuque.com/lyndon/daylesson/tyxoqs">我讲不清楚事情，怎么办？ · 语雀</a></h2><p>这是一篇关于职场沟通的文章，如何把事情说清楚是一项很重要的技能。作者给了三点建议：</p><ul><li><strong>说人话，要用比喻的方式，把复杂的概念讲清楚。</strong></li><li><strong>定调子，我讲的第一句话，一定很重要，如果能用通俗的概念开场，才会有兴趣听下去。</strong></li><li><strong>看反馈，不要只顾着讲，或者担心有留白会尴尬，所以一直拼命讲。</strong></li></ul><p>结合自己的经历，讲事情的时候还要考虑听众的背景，对其他 RD 说的就不能和 PM 一样。最后，高情商可以时不时问一下：我说清楚了吗？低情商可以问：你听明白了吗？</p><h2 id="请不要神化双链笔记-少数派"><a class="markdownIt-Anchor" href="#请不要神化双链笔记-少数派"></a> <a href="https://sspai.com/post/65273">请不要神化双链笔记 - 少数派</a></h2><p>Zettelkasten、双链这些概念差不多火了一年多的时间，作者对目前的现象进行思考和批评。</p><p>首先分析大部分介绍双链笔记文章的逻辑：</p><ol><li>卢曼自下而上地生成卡片</li><li>卢曼重视在笔记之间建立联系</li><li>现在的双向链接就是卢曼卡片盒笔记法的软件实现</li><li>得益于卡片盒笔记法，卢曼从公务员成为德国当代重要的社会学家，因此这种扁平化的组织形式是十分有效的。</li></ol><p>看起来是很完备的逻辑论证扁平化的双链笔记如何有用的过程，但是却忽视了最基础的类比可能并不成立的前提。作者提出的观点是：Zettelkasten 这样的信息组织形式是为了消除笔记内容和搜索的不确定性而努力。你需要先积累一定程度的笔记到，然后再去设计更细粒度的、结构化的笔记结构。最终才能明确笔记原则，理清自身期许 。</p><h2 id="为什么有些人总被人评价逻辑性很强而有些人被评价逻辑弱鸡-即刻app"><a class="markdownIt-Anchor" href="#为什么有些人总被人评价逻辑性很强而有些人被评价逻辑弱鸡-即刻app"></a> <a href="https://m.okjike.com/originalPosts/604289315b8fd40011e1fbee">为什么有些人总被人评价逻辑性很强？而有些人被评价逻辑弱鸡？ - 即刻App</a></h2><p>作者将分成三个层次来说：没逻辑，有体系以及逻辑链。由于认知能力有限，没太明白后面两部分的关系。<br />没逻辑这个部分，作者给出的例子很形象：有些人的思考路径是，嗯，我觉得1很重要，2也很重要，对，还有3和4，我想想，对，还有5，嗯5比2重要一点。更糟的是，过两个小时，你再问他这个问题，他会回答235，丢掉了14，可能又多了一个67。所以如何才能避免？</p><h2 id="在做算法工程师的道路上你掌握了什么概念或技术使你感觉自我提升突飞猛进-知乎"><a class="markdownIt-Anchor" href="#在做算法工程师的道路上你掌握了什么概念或技术使你感觉自我提升突飞猛进-知乎"></a> <a href="https://www.zhihu.com/question/436874654/answer/1746629056">在做算法工程师的道路上，你掌握了什么概念或技术使你感觉自我提升突飞猛进？ - 知乎</a> 以及  <a href="https://www.zhihu.com/question/446374101/answer/1756122452">工作之后，顶会还重要吗？ - 知乎</a></h2><p>算法是一个残酷的竞技场。比起软件工程师，淘汰更加残酷，被击败，主动离队。</p><p>顶会论文是跨越公司、跨越国家、跨越年龄的硬通货。</p><p>（希望年前写的论文能中 KDD）<br />（20210609: 被 Reject，等 AAAI）</p><h2 id="换脑游戏试播集"><a class="markdownIt-Anchor" href="#换脑游戏试播集"></a> <a href="https://www.acfun.cn/v/ac25767536">《换脑游戏》试播集：）</a></h2><p>我当时的评论：很喜欢这一期播客。整体听完，对小约翰老师的印象变成了「吾道不孤」，好像知识星球都不能加入了……主播提的那么多问题，引发自己思考的是你有哪一种焦虑？john 的善于利用自己的焦虑提供了一种新的思路。最后，john 提到英语老师讲题目的就只有一个人举手的场景，我也遇到过，可能当时真的想知道问题答案驱动我去举手吧。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;重新定期写一些阅读分享，暂时命名为「随想集」。&lt;/p&gt;</summary>
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
  </entry>
  
  <entry>
    <title>《会饮篇》读书笔记</title>
    <link href="https://blog.xiang578.com/post/symposium.html"/>
    <id>https://blog.xiang578.com/post/symposium.html</id>
    <published>2021-01-10T12:35:29.000Z</published>
    <updated>2023-03-28T03:58:52.116Z</updated>
    
    <content type="html"><![CDATA[<p>《会饮篇》记录阿伽通获得酒神节悲剧比赛第一名后，苏格拉底等人在庆祝宴会中「赞美爱神」的比赛过程。实际上记录与会人七种不同的爱情观。最后，酒神狄奥尼索斯决定出当晚的获胜者。</p><span id="more"></span><h2 id="裴卓勇气之爱"><a class="markdownIt-Anchor" href="#裴卓勇气之爱"></a> 裴卓：勇气之爱</h2><p>讲述者被人们视为弱小的被爱之人，美少年裴卓 Phaedrus，爱可以给我们勇气！</p><ul><li>为人们要想过美好正当的生活，必须终生遵循一个指导原则，这并不能完全依靠血统，也不能靠威望、财富，只有靠爱才能办到。这原则是什么呢？就是：厌恶丑恶的，爱慕美好的。</li><li>如果一个情人在准备做一件丢人的坏事，或者在受人凌辱而怯懦不敢抵抗，这时他被人看见了就会觉得羞耻，但是被父亲、朋友或其他人看见还远远不如被爱人看见那样羞到无地自容。爱人被情人发现他做坏事，情形也是如此。</li><li>阿喀琉斯为情人复仇。母亲叮嘱他，杀死仇人赫格多尔之后自己也会死。最后还是奋不顾身杀死仇人，自己被射中脚踵而死。</li><li>“最爱的人心中对我们的期望。「目标的意义」”</li></ul><h2 id="包萨尼亚精神之爱"><a class="markdownIt-Anchor" href="#包萨尼亚精神之爱"></a> 包萨尼亚：精神之爱</h2><p>讲述者是一个成年的同性恋，阿伽通的情人包萨尼亚 Pausanias，被人们视为追求肉欲的人。爱神爱有天地之分，天之爱是精神之爱，是崇高的爱，只有男男之间才有，是美好的追求；地之爱是肉体之爱，是普遍的低级的爱，也就是大多数人的爱，只有肉欲的爱。</p><ul><li>凡间阿莆若狄德引起的爱神确实也是凡俗的，它不分皂白地奔赴它的目的。这种爱情只限于下等人。它的对象可以是娈童，也可以是女子；它所眷恋的是肉体而不是灵魂；最后，它只选择愚蠢的对象，因为它只贪图达到目的，不管达到目的的方式美丑。</li><li>至于天上的那位的出身却与女的无关，只是由男子生的，所以其爱情对象只是少年男子。其次，她的年纪较大，所以不至于荒淫放荡。她只鼓舞人们把爱情专注在男性对象上，因为这种对象生来就比较坚强，比较聪明。</li><li>他们说，与其暗爱，不如明爱，所爱的人应当在门第和品德上都很高尚，美还在其次。人们对情人都给予极大的鼓励，不认为他在做不体面的事；人们把追求爱情的胜利看成光荣，把这方面的失败看成羞耻。为了争取胜利，他可以做出种种离奇的事。</li><li>这种少年男子一定要显现理性，也就是腮帮上长胡须的时候，才能成为爱的对象。我想情人之所以要等爱人达到这种年龄之后才钟爱他，是由于存心要和他终生相守，不是要利用他的年幼无知来哄骗他，碰到另外一个可以宠爱的对象时就把他扔掉。<ul><li>当时社会能接受：年长男性(智慧，情人)和少年男孩(12-17，肉体，爱人)之间的恋爱关系。包萨尼亚和阿伽通年纪都大了。</li></ul></li><li>我认为道理是这样：这件事并不是十分单纯的，像我开头说的那样。单就它本身来看，它无所谓美，也无所谓丑；做的方式美它就美，做的方式丑它就丑。丑的方式就是拿卑鄙的方式来对付卑鄙的对象，美的方式就是拿高尚的方式来对付高尚的对象。所谓卑鄙的对象就是上面说的凡俗的情人，爱肉体过于爱灵魂的。他所爱的东西不是始终不变的，所以他的爱情也不能始终不变。一旦肉体的颜色衰败了，他就远走高飞，毁弃从前的一切信誓。然而钟爱优美品德的情人却不然，他的爱情是始终不变的，因为他所爱的东西也是始终不变的。</li><li>我们的习俗定了两条规矩，头一条是：迅速接受情人是可耻的，应该经过一段时间，因为时间对于许多事物常常是最好的考验；第二条是：受金钱引诱或政治威胁而委身于人是可耻的，无论是不敢抵抗威胁而投降，还是贪图财产和地位，全都一样。因为这些势力、名位和金钱都不是持久不变的，高尚的友谊当然不能由此产生。</li><li>按照我们这里的规矩，如果一个人肯侍候另外一个人，目的在于得到那个人的帮助在爱智或其他品德上更进一步，这种卑躬屈节并不卑鄙，也不能指为谄媚。<ul><li>这里有两条规矩，一条是关于少年男子的爱情，一条是关于学问道德的追求，应该合而为一；如果合而为一，爱人眷恋情人就是一件美事。所以，情人和爱人来往，就各有各的指导原则。情人的原则是爱人对自己既然表现殷勤，自己就应该在一切方面为他效劳；爱人的原则是情人既然使自己在学问道德方面有长进，自己就应该尽量拿恩情来报答。一方面乐于拿学问道德来施教，一方面乐于在这些方面受益，只有在这两条原则合而为一时，爱人眷恋情人才是一件美事，如若不然，它就不美。如果是为了增进学问道德，纵然完全失败也没有什么可耻；如果是为了其他的目的，不管失败与否都是可耻的。假如一个少年男子以为他的情人很富，为了贪财就去眷恋他，后来发现自己看错了，其实他很穷，无利可图。其眷恋还是很可耻的，因为这种行为揭穿了他的性格，证明他这个人为了金钱可以侍候任何人、做出任何事来，这当然是很不光彩的。</li></ul></li><li>我们谈爱情的时候，事实上，我们讨论的正是真实的自己。</li></ul><h2 id="鄂吕克锡马柯功利之爱"><a class="markdownIt-Anchor" href="#鄂吕克锡马柯功利之爱"></a> 鄂吕克锡马柯：功利之爱</h2><p>讲述者是医生鄂吕克锡马柯，理性大于感性，是技术主义的代表。爱情是和谐的，是有规律的，是有用的，我们可以通过其中的规律得到爱情，也可以通过爱情得到别的有用的东西。</p><ul><li>爱情克制论</li></ul><h2 id="阿里斯多芬缺憾之爱"><a class="markdownIt-Anchor" href="#阿里斯多芬缺憾之爱"></a> 阿里斯多芬：缺憾之爱</h2><p>讲述者是喜剧家阿里斯多芬，而喜剧总是悲剧的内核。最初有三种人，男男人、女女人和男女人，像球一样。因为得罪宙斯，人被劈成两半，天生就该追求自己的另一半。不论肉体还是精神，我们都应该追求自己的另一半，以此互补，否则我们孤独，有缺点，不完美。</p><ul><li>所以我们每人都是人的一半，是一种合起来才成为全体的东西。所以每个人都经常在寻求自己的另一半。全人类只有一条幸福之路，就是实现自己的爱，找到恰好和自己配合的爱人，总之，还原到自己的本来面目。</li><li>对爱情最浪漫的解释。</li></ul><h2 id="阿伽通完美之爱"><a class="markdownIt-Anchor" href="#阿伽通完美之爱"></a> 阿伽通：完美之爱</h2><p>讲述者是悲剧诗人阿伽通，是十分优秀的人。我们要追求完美，爱神就是完美的，爱情也是完美的，所以我们要追求爱，而且爱是有益于他人的，是可以从满溢之处流向匮乏之处的(对应开头评价苏格拉底的智慧)，可以让大家一起变的完美。</p><ul><li>前面的人赞美爱神，其实都是在赞美其索给予的幸福，而不是真正地赞美爱神本身。</li></ul><h2 id="苏格拉底朝圣之爱"><a class="markdownIt-Anchor" href="#苏格拉底朝圣之爱"></a> 苏格拉底：朝圣之爱</h2><p>讲述者是苏格拉底，从狄欧蒂玛口中听到的爱。爱神不是完美的，介于不死的神和会死的人之间的精灵，介于美和丑之间，介于无知和有知之间。追求爱不是为了爱本身，是为了爱延伸的东西，是在追求爱情过程中得到的美好、是追求到以后收获的智慧和快乐，美好和永恒。</p><ul><li>和阿伽通的对话<ul><li>爱神不仅公正，而且审慎。大家公认审慎是节制快感和情欲的力量。世界上没有一种快感比爱情本身还要强烈。一切快感都比不上爱情，就是因为它们都受爱神节制，而爱神是它们的统治者。爱神既然统治着快感和情欲，岂不是最审慎的吗？</li><li>爱不也是这样：一个人既然爱一件东西，就是还没有那样东西；他盼望它，就是盼望他现在有它，或者将来有它。是不是？”</li><li>所以总起来说，在这种和其他情况下，一个盼望的人所盼望的是他缺少的、还没有到手的，总之是他所没有的，是本身不存在的，不在他那里的；只有这样的东西才是他所盼望的、他所爱的。</li><li>爱神首先是对某某东西的爱，其次是对他所欠缺的东西的爱。是不是？</li><li>正确的意见而说不出所以然来，就不是有知识（因为没有根据的不能算知识），却也不是无知（因为有正确内容的不能叫无知）。所以很明显，正确的意见就是介于智慧和无知之间的东西。</li></ul></li><li>她说：「说来话长，我还是给你说说吧。当初阿莆若狄德诞生的时候，诸神举行宴会，出席的有智谋女神梅蒂的儿子丰饶神波若。他们宴饮结束时，匮乏神贝尼娅来向他们作节日例行的行乞，站在门口。波若多喝了几杯琼浆（因为那时还没有酒），就走进宙斯的花园，昏昏沉沉地睡着了。贝尼娅由于贫乏，很想和波若生个孩子，于是和他睡在一起，怀下了爱若。爱若也成了阿莆若狄德的随从和仆人，因为他是在阿莆若狄德的生日投的胎，生性爱美的东西，而阿莆若狄德是很美的。」</li><li>她说：‘这正是我要启发你的第二点，苏格拉底。爱神就是这个样子，就是这样产生的。他是奔赴美的东西的，像你说的那样。假如有人问我们：爱者从美的东西得到什么呢，苏格拉底和狄欧蒂玛？或者问得更明确一点：那盼望美的东西的是盼望什么呢？你怎么回答？’</li><li>那我给你说清楚点。苏格拉底啊，所有的人都会生育，凭借身体或灵魂生育，到了一定的年龄，就为本性所推动，迫不及待地要求生育。可是他们不能在丑的东西里、只能在美的东西里生育。男人和女人结合就是生育。怀胎、生育是一件神圣的事，是会死的凡夫身上的不朽的因素。但是这件事不可能在不协调的情况下实现，丑的东西与神圣的事情不协调，只有美的东西才与它协调。所以美是引导和帮助生育的女神和决定命运的女神。因为这个缘故，那生育能力旺盛的一遇到美的对象就立刻欢欣鼓舞，精神焕发，同它交配生子；如果遇到丑的，就垂头丧气，毫无兴趣，避开它不去生育，宁愿把沉重的包袱背下去。因此那充满生育的种子和欲望的一遇到美的对象就欣喜若狂，是由于它可以结束他的巨大痛苦。</li><li>你就不会觉得奇怪了。因为和上面说的完全一样，那会死的东西也是力求能够永远存在和不朽。要达到不朽，全凭生殖，以新的代替旧的。每一个个体的生物，虽然我们说它一生之中始终是同一个东西。</li><li>美者，这时，就只有大愚不解的人才会不明白一切形体中的美是同一个美了。明白了这一点，他就成了爱一切美好形体的人，把他的热情从专注于某一形体推广到一切，因为他把那种专注一点看成渺小的、微不足道的。再则，他必须把灵魂的美看得大大优于形体的美，如果有一个人灵魂值得称赞，即便形貌较次，那也足够了，他也应该对这个人表示爱慕之情，加以照顾，他心里想出来发表的那些美好的话语可以使青年奋发向上，他这样做也使他自己遍览人们各种行动中以及各种风俗习惯中的美，从而见到美是到处贯通的，就把形体的美看成甚为微末的了。可是他必须从各种行动向前更进一步，达到知识，这样就见到知识的美。</li><li>至于爱的方面，情形也是这样。一般说来，凡属对于好东西、对于幸福的企盼，都是每个人心中最大的、强烈的爱。然而其余的那些在某一个方面有所追求的，无论是谋求获利的，喜爱体育的，还是爱智慧的，我们都不说他们在爱，不说他们是钟爱者；只有那些以某种方式发挥作用的喜好者，才占有全体的名称，我们说他们在爱，称他们为情人或钟爱者。</li><li>一个人如果一直接受爱的教育，按照这样的次序一一观察各种美的东西，直到这门爱的学问的结尾，就会突然发现一种无比奇妙的美者，即美本身。</li><li>爱其实就是寻找真正的美的过程。</li><li>苏格拉底的颂辞是全篇三大段的中段，也是全篇精义所在。它本身分两部分：和阿伽通的对话以及和狄欧蒂玛的对话。<ul><li>在和阿伽通的对话里，他说明了：<ul><li>（一）爱情必有对象；</li><li>（二）钟爱者还没有得到所爱的对象；</li><li>（三）爱情就是想占有所爱对象那一个欲望；</li><li>（四）爱情的对象既然是美，如阿伽通所说的，它就还缺乏美，“爱神是美的”一说不能成立；</li><li>（五）美善同一，所以爱神也不是善的。</li></ul></li><li>这样苏格拉底就把阿伽通的一篇大文章完全推翻了。接着他说他的爱情学问是从女巫狄欧蒂玛那里领教来的。他原来和阿伽通一般见解，她纠正了他。她使他明白：<ul><li>（一）爱神是介乎美丑、善恶、有知与无知、神与人之间的一种精灵，是丰富和贫乏的统一，总之，就是一个哲学家；</li><li>（二）爱情就是想凡是美的善的永远归自己所有那一个欲望；</li><li>（三）爱情的目的是在美的对象中传播种子，凭它孕育生殖，达到凡人所能享有的不朽：生殖就是以新代旧，种族与个体都时时刻刻在生灭流转中。这种生殖可以是身体的，也可以是心灵的。诗人、立法者、教育者以及一切创造者都是心灵方面的生殖者；</li><li>（四）爱情的深密教，也就是达到哲学极境的四大步骤。</li></ul></li></ul></li></ul><h2 id="阿尔基比亚德畸形之爱"><a class="markdownIt-Anchor" href="#阿尔基比亚德畸形之爱"></a> 阿尔基比亚德：畸形之爱</h2><p>醉酒阿尔基比亚德（对应开头提到的酒神）的胡言，他爱着苏格拉底却又自惭形秽。爱情的美好没有让他变美好，反而让他与美好背道而驰，离爱越来越远，他矛盾而纠结，爱在折磨着他。</p><h2 id="尾声"><a class="markdownIt-Anchor" href="#尾声"></a> 尾声</h2><p>柏拉图的爱情观：爱是阶梯式的，肉体之爱会让精神之爱更崇高，二者不能单一存在，否则偏向肉体则堕落，偏向精神则虚渺。(寻找本源的行为。)</p><h2 id="ref"><a class="markdownIt-Anchor" href="#ref"></a> Ref</h2><ul><li><a href="https://zh.wikipedia.org/wiki/%E6%9C%83%E9%A3%B2%E7%AF%87">会饮篇 - 维基百科，自由的百科全书</a></li><li><a href="https://www.zhihu.com/question/22519668">如何理解柏拉图的《会饮篇》？ - 知乎</a><ul><li>我希望你的目的地是星辰大海</li></ul></li><li><a href="https://www.bilibili.com/video/BV1zT4y1K7sm">【罗翔老师直播课堂】共读柏拉图的《会饮篇》_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a><ul><li>我们所有的学习都是在回忆</li><li>阿尔基比亚德在看到苏格拉底身上看到他原初的美好，现在的匮乏，将来的美好。<ul><li>“人最大的痛苦是无法跨越知道和做到的鸿沟”</li><li>最终在战争中背叛了雅典</li></ul></li><li>有追求者介于有知和无知之间，神有知，所以不需要追求，愚人无知，所以不知道要去追求，只有介于二者之间的人才会去追求知识追求美好，这就是追求者，每一个为梦想努力的人就是这样的人。</li><li>爱他人就是爱自己。因为爱他人时，你爱上的那些优点和美好，其实都印证了自身。也许过去自己拥有这些美好如今失去了，所以我们爱；也许现在我们正拥有这些美好，我们惺惺相惜所以爱；也许我们看到了将来的自己，自己想要成为的样子，想要拥有的特质，所以我们爱。自恋所恋，其实是恋着自身过去、现在、将来的美好。</li><li>所以上次在读书会的时候我给学生说了这样一段话：<ul><li>我说很少有那个词会像爱一样被庸俗对待，但是爱是可以承载真正的严肃和祟高，我们爱因为我们匮乏，我们爱因为我们希望超越每日的锱铢必较，在爱中我们放弃了自我，我们发现了自我，我们希望重塑我们的自我。</li><li>所以整个会饮篇，他提醒我们肉体的爱当然是有意义的，但人不要沉迷于肉体的爱，人要扶级而上，去追求灵魂的爱，去追求更好的爱。因为我们要回忆起我们失落的美好，我们就可以告别，我们当下的平庸苟且，我们要攀登美善的阶梯，一步一步往上爬。</li><li>这样，我们才能够达到一种爱的升华，我们才能在一个具体的人中，去发现我们，对于所有人美好的期待，就是这种抽象的爱。因为具体的人，我们在他身上看到了，我们想看到的样子，我们希望变得美好，当我们变得美好的时候，我们又会和具体的人，在具体的人身上，我们又会经营我们的美好。</li></ul></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;《会饮篇》记录阿伽通获得酒神节悲剧比赛第一名后，苏格拉底等人在庆祝宴会中「赞美爱神」的比赛过程。实际上记录与会人七种不同的爱情观。最后，酒神狄奥尼索斯决定出当晚的获胜者。&lt;/p&gt;</summary>
    
    
    
    <category term="文渊阁" scheme="https://blog.xiang578.com/categories/%E6%96%87%E6%B8%8A%E9%98%81/"/>
    
    
    <category term="book" scheme="https://blog.xiang578.com/tags/book/"/>
    
  </entry>
  
  <entry>
    <title>2020 挑战</title>
    <link href="https://blog.xiang578.com/post/2020.html"/>
    <id>https://blog.xiang578.com/post/2020.html</id>
    <published>2021-01-01T08:19:43.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>2020 年最后一天下午，坐在工位上想着「今年有什么遗憾？」想到最大遗憾：这个世界变不回原来的样子。超级黑天鹅「Covid 2019」或多或少改变我们生活，新时代就这样拉开大幕。在这个背景下，承受来自各个方面的压力，各种「挑战」也随之而来。在这一篇总结中，我会通过关键字回顾自己过去一年。</p><h2 id="一-工作"><a class="markdownIt-Anchor" href="#一-工作"></a> 一 工作</h2><p>没有太多变化，还是选择留在滴滴。年初获得机会参与「晋升」答辩，在老板和同事的帮助下，异常艰难地升到下一个级别。两点感悟：需要做有价值的事情以及有把做过的事情说清楚的能力。之后参与内部一个比较大的探索项目，费了很大力气上线后，12 月底开始和同事一起写相关的论文，希望能中 KDD 2021。</p><p>晋升之后，日常工作中会接触到更多的人，如何与其他人进行有效的沟通成为自己新的挑战。后来也有机会指导实习生，不过没有多久就离开了。简单总结自己变化：做人家分配的事情，到自己思考如何做事情，再到分配事情给其他人做。这些变化中，我经历长时间不适应的感受。大学参与编程比赛培养的是靠外部刺激和目标驱动，工作中需要的是你不断拓展边界，打破旧的认知和规则。</p><h2 id="二-技术能力"><a class="markdownIt-Anchor" href="#二-技术能力"></a> 二 技术能力</h2><p>分成两个部分：尝试一些工程的开发，改线上代码差不多和呼吸一样自然。但是觉得自己总体上来说写的东西还是偏向简单，没有太多的设计和思考在里面。下半年很多时候是在修自己上半年的 bug……算法学习差不多是跟随业务需求进行，看得大部分还是 CTR 相关内容。另外也在 b站看了一半统计机器学习的内容。做的业务不像 CTR、NLP 这样有明确的界限，所以也没有机会去深入一个方向去实践，大部分时候浅尝辄止。</p><h2 id="三-程序设计"><a class="markdownIt-Anchor" href="#三-程序设计"></a> 三 程序设计</h2><p>不知道告别多久之后，下半年尝试想把这个东西捡起来。说回来，我之间问一个进过 world final 的候选人，工作中有没有用过相关的算法，他也是摇头。按自己目前的水平，每周日上午写 LeetCode 周赛是很奇妙的体验。每次都是一场孤独的挑战，只有调整好自己的思维方式，才能把自己从苦海中解脱出来。做这件事情的意义大概是山就在那里。多少年之后，我也体会到当年没有拼尽全力的遗憾。</p><p>9 月份，在周神的组织下，我们组成一个队伍参加 Leetcode 举办的秋季编程大赛。刚开始我们都是信心满满地，比赛开始之后就是无尽的折磨，最后就是以三题结束比赛。原来他们这么久不训练也不会写题目……希望等自己学校举办区域赛的时候还能打星参与一次。</p><h2 id="四-生活"><a class="markdownIt-Anchor" href="#四-生活"></a> 四 生活</h2><p>11 月份遇到房东卖房，从原来住的房子里面搬出来，跑到回龙观和同事一起合租。最大变化是通勤时间的变长，挤过几天地铁后，立马决定打拼车上班，另外忍受几次下班后打车的痛苦，也开始 20:30 自费打车。所以多了很多坐车的时光，一般早上都会带一个阅读器去看书。有一次堵在后厂村路上，看着《禅与摩托车维修艺术》，恨自己没有摩托车。晚上回来的时候，大概率是听播客。似乎比之前把节省下来的时间拿去冲浪好很一些。</p><h2 id="五-知识管理"><a class="markdownIt-Anchor" href="#五-知识管理"></a> 五 知识管理</h2><blockquote><p>投入在 Input 上的时间越多，知识管理越差。</p></blockquote><p>这个差不多是这几年一直在思考的话题（时间管理方法，我看清 GTD 本质之后，就不太纠结）。去年年末开始关注 Zettelkasten，当时还是在借助 <code>sublime_zk</code> 进行折腾。2 月份的时候，了解到一个工具 Roam Research，开始慢慢把自己的笔记迁移过去。几个月之后，Zettelkasten 和双向链接笔记在国内火了起来。给我最大的两点改变是：</p><ol><li>区分自己写的笔记和收藏的文章（印象笔记就是因为剪藏太多变成垃圾堆）。</li><li>思考不同内容之间的关系。</li></ol><p>很遗憾，这一年很大一部分精力停留在把之前散落在不同软件中的笔记汇聚到 Roam 中。新的一年尝试把更多的内容分享出来。</p><p>另外一方面，真正记住内容还需要不断回忆（给你选择记忆的机会），比较知名的软件就是 anki。有一段时间经常制作卡片，但是没能融入自己的系统，也没有把这个习惯坚持下来。</p><h2 id="六-分享"><a class="markdownIt-Anchor" href="#六-分享"></a> 六 分享</h2><p><img src="https://media.xiang578.com/blog-analytics-2019.jpg" alt="2020 博客访问量统计" /></p><p>从访问数据上来看，比起去年博客的流量快增加超过 5 倍。统计数据中，流量大概是 8 月中旬开始的，最大的原因是完成百度和 Google 对博客的收录。大部分流量来自于一篇文章（已被自己手动删除）。不过得到启示，某一些小众的关键字，如果能做到百度的前几名，获得的流量还是很可观的。</p><p>今年没有写多少正经的东西，最大的亮点是差不多用8个月的时间尝试摸索分享每个月阅读相关的总结「Never Reading」。这些文章内容来自 Roam Reasearch 阅读笔记。最终被自己停掉的原因是，形式上并不优美，可能阅读时，都只是一些没有上下文的关键词或短句。最好的形式还是自己将阅读的内容按主题写成文章进行分享。</p><p>放弃写每月的阅读总结后，自己觉得笔记-博客文章中应该还有一个中介介质。用现在时髦的话来说，是数字花园。苦于目前还没有发现如何快速将部分 Roam 笔记公开展示出来，9 月份的时候模仿 <a href="https://onetwo.ren/wiki/#:Index">Meme of LinOnetwo — 林一二的模因和想法</a>也搭建一个 <a href="https://wiki.xiang578.com/">算法花园・Xanadu — 吾辈心中亦有惑，坐！</a>，想着分享一些读书和读论文的笔记。大概过一个多月，差不多就放弃这个网站……目前还是等待更高效导出 Roam 笔记制作站点的方案出现。</p><p>最后，也开始在 Twitter <a href="https://twitter.com/xiang578">Ryen Xiang(@xiang578)</a>进行分享。很遗憾，之前和人交流太少，写的东西除了错别字多以外，没有太强的逻辑性，可能很难理解………这一些都是在新的一年中需要克服的。</p><h2 id="七-阅读"><a class="markdownIt-Anchor" href="#七-阅读"></a> 七 阅读</h2><p>阅读的挑战是：为了什么而读？思考如何去做阅读笔记？主力阅读软件是微信读书，不仅仅是无限会员全场畅读，更重要的是导出的笔记格式都比 kindle 方便整理。全年读过 <a href="https://book.douban.com/people/xiang578/collect?sort=time&amp;tags_sort=count&amp;filter=all&amp;tag=2020&amp;mode=grid">36</a> 本书，平均下来一个月 3 本书，和年初计划每周一本书，还是有很大的差距。推荐几本 5 星的图书：</p><ul><li><a href="https://book.douban.com/subject/35092383/">把自己作为方法</a>：项飙的三场访谈录，一次感受知识分子可爱以及真诚的机会。</li><li><a href="https://book.douban.com/subject/27167270/">像哲学家一样生活</a>：介绍哲学流派斯多葛的书，一种人间哲学。</li><li><a href="https://book.douban.com/subject/21570668/">会饮篇</a>：苏格拉底等人关于爱的讨论。</li><li><a href="https://book.douban.com/subject/26831789/">穷查理宝典</a>：查理芒格 11 篇演讲的合集。</li><li><a href="https://book.douban.com/subject/33438811/">为什么</a>：因果推理入门科普书，不过没有机会应用到业务中。</li><li><a href="https://book.douban.com/subject/35013197/">深度学习推荐系统</a>：介绍深度学习在推荐系统的的应用，大而全。</li><li><a href="https://book.douban.com/subject/25757313/">红楼梦脂评汇校本</a>：市面上口碑比较好的红楼梦版本。</li></ul><p>2021 年的阅读目标还是每周一本书，然后每月挑一本书写出读书笔记在博客中发表。</p><h2 id="八-观影"><a class="markdownIt-Anchor" href="#八-观影"></a> 八 观影</h2><p>今年在豆瓣上标记看过电影只有 <a href="https://movie.douban.com/people/xiang578/collect?start=0&amp;sort=rating&amp;rating=all&amp;tag=2020&amp;mode=list&amp;filter=all">40</a> 个条目，仅是去年的 2/3。可能现在把更多空闲时间花费在看 B 站各种 UP 主的视频中，仔细想想还是需要去欣赏艺术。</p><ul><li><a href="https://movie.douban.com/subject/33447642/">沉默的真相</a>/<a href="https://movie.douban.com/subject/33404425/">隐秘的角落</a>：不错的故事和拍摄，但是紫金陈的原著一言难尽。</li><li><a href="https://movie.douban.com/subject/1900841/">窃听风暴 (豆瓣)</a>：从一个简单的故事告诉你为什么某些制度不行。有一种听不懂德语的遗憾。</li><li><a href="https://movie.douban.com/subject/10533913/">头脑特工队 (豆瓣)</a>：今日简史中推荐反传统科幻片。 完全是五星，迪斯尼实在是太有创意了。</li><li><a href="https://movie.douban.com/subject/27012433/">阿尔法围棋 (豆瓣)</a>：尝试调研 MCTS 时补充看的纪录片，当年的旁观者，再回头已经成为参与者。</li><li><a href="https://movie.douban.com/subject/27010768/">寄生虫 (豆瓣)</a>：小孩子早已经看穿一切。剧情安排真实精彩。</li></ul><h2 id="九-游戏"><a class="markdownIt-Anchor" href="#九-游戏"></a> 九 游戏</h2><p>动物森友会：上半年最火的游戏。曾经的QQ农场爱好者，不可避免在同事的带领下，成为一名光荣的岛主。100 多个小时候之后，就没有重新上过岛……<br />Ingress：一款手机端的 LBS 游戏，你扮演一名特工，完成游戏中的任务探索现实世界。自己最初希望通过这款游戏多出去走走。11 月和 12 月，为了做任务，多次出去暴走 20+ 公里！不过，已经变成匆匆寻找关键打卡地点，很多时候错过旁边的风景。<br />Celeste：IGN 满分作品，号称是治疗抑郁的。横版平台跳跃游戏，和马里奥相比多了一个冲刺键。最大优点是死亡之后复活特别快，第一关还没有通过，我差不多就死了 1k 次。</p><h2 id="十-消费指北"><a class="markdownIt-Anchor" href="#十-消费指北"></a> 十 消费指北</h2><p>克制自己的消费，不单独写一篇文章。</p><p><code>ErgoDox EZ</code>：红轴，分体人体工程学键盘，反正看到的同事都觉得很酷。<br /><code>Inkpad X</code>：为了用微信读书，买的10寸安卓平板。分辨率只有 200 ppi，字体调大一些没有太大影响。不过没有实体翻页键还是挺难受的。微信阅读也没有怎么对这个设备优化。最近小米又出一个 pro 阅读器，也可以考虑。<br /><code>DEVONthink pro</code>：增强型文件管理器，主要用来替代印象笔记。虽然贵，但是好用。<br /><code>Offscreen</code>： 记录手机屏幕时间、打开次数等，不断提醒你今天沉迷手机多久。<br /><code>产品沉思录</code>：邮件组，有关互联网、科技和人文，提供不错的信息来源。可以先从精选集判断是否值得入手。</p><h2 id="尾声"><a class="markdownIt-Anchor" href="#尾声"></a> 尾声</h2><p>在迈入 2021 之际，给自己留下一些 New Year Challenge:</p><ol><li>认识你自己</li><li>变得勇敢</li><li>成为生产者</li></ol><p>于北京回龙观</p><p><a href="https://xiang578.com/post/2017.html">2017 迷茫</a> &gt;&gt; <a href="https://xiang578.com/post/2018.html">2018 探索</a> &gt;&gt; <a href="https://xiang578.com/post/2019.html">2019 起步</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;2020 年最后一天下午，坐在工位上想着「今年有什么遗憾？」想到最大遗憾：这个世界变不回原来的样子。超级黑天鹅「Covid 2019」或多或少改变我们生活，新时代就这样拉开大幕。在这个背景下，承受来自各个方面的压力，各种「挑战」也随之而来。在这一篇总结中，我会通过关键字回顾</summary>
      
    
    
    
    
    <category term="life" scheme="https://blog.xiang578.com/tags/life/"/>
    
    <category term="book" scheme="https://blog.xiang578.com/tags/book/"/>
    
    <category term="game" scheme="https://blog.xiang578.com/tags/game/"/>
    
    <category term="movie" scheme="https://blog.xiang578.com/tags/movie/"/>
    
    <category term="didiglobal" scheme="https://blog.xiang578.com/tags/didiglobal/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出 BERT 源代码之 BertModel 类</title>
    <link href="https://blog.xiang578.com/post/all-about-bert-code.html"/>
    <id>https://blog.xiang578.com/post/all-about-bert-code.html</id>
    <published>2020-10-03T12:56:43.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>国庆节前突然对如何计算 BERT 的参数量感兴趣，不过一直看不明白网上的计算过程，索性下载 BERT 源代码阅读一番。这篇文章记录阅读 BertModel 类（核心代码实现）时写的一些笔记，反正我也是纸上谈兵，所以不需要太关注数据处理和 Finetune 相关部分，最后附上计算 BERT 参数量的过程仅供参考。</p><span id="more"></span><p>代码地址：<a href="https://github.com/google-research/bert/blob/master/modeling.py">bert/modeling.py at master · google-research/bert</a></p><h2 id="bertconfig"><a class="markdownIt-Anchor" href="#bertconfig"></a> BertConfig</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertConfig</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Configuration for `BertModel`.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">               vocab_size,</span></span><br><span class="line"><span class="params">               hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">               num_hidden_layers=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">               num_attention_heads=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">               intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">               hidden_act=<span class="string">&quot;gelu&quot;</span>,</span></span><br><span class="line"><span class="params">               hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">               attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">               max_position_embeddings=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">               type_vocab_size=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">               initializer_range=<span class="number">0.02</span></span>):</span><br><span class="line">    self.vocab_size = vocab_size</span><br><span class="line">    self.hidden_size = hidden_size</span><br><span class="line">    self.num_hidden_layers = num_hidden_layers</span><br><span class="line">    self.num_attention_heads = num_attention_heads</span><br><span class="line">    self.hidden_act = hidden_act</span><br><span class="line">    self.intermediate_size = intermediate_size</span><br><span class="line">    self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">    self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">    self.max_position_embeddings = max_position_embeddings</span><br><span class="line">    self.type_vocab_size = type_vocab_size</span><br><span class="line">    self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">from_dict</span>(<span class="params">cls, json_object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Constructs a `BertConfig` from a Python dictionary of parameters.&quot;&quot;&quot;</span></span><br><span class="line">    config = BertConfig(vocab_size=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> (key, value) <span class="keyword">in</span> six.iteritems(json_object):</span><br><span class="line">      config.__dict__[key] = value</span><br><span class="line">    <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">from_json_file</span>(<span class="params">cls, json_file</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Constructs a `BertConfig` from a json file of parameters.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(json_file, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">      text = reader.read()</span><br><span class="line">    <span class="keyword">return</span> cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">to_dict</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Serializes this instance to a Python dictionary.&quot;&quot;&quot;</span></span><br><span class="line">    output = copy.deepcopy(self.__dict__)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">to_json_string</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Serializes this instance to a JSON string.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(self.to_dict(), indent=<span class="number">2</span>, sort_keys=<span class="literal">True</span>) + <span class="string">&quot;\n&quot;</span></span><br></pre></td></tr></table></figure><p><code>BertConfig</code> 类包含模型参数、几个读取和存储参数的方法。</p><p><code>@classmethod</code> 代表类方法，不需要实例化就可以调用类中的方法。参考其他的文件可以发现它的使用是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)</span><br></pre></td></tr></table></figure><p>主要参数有：</p><ul><li><code>vocab_size</code>: 词表大小</li><li><code>hidden_size</code>: Size of the encoder layers and the pooler layer. 词向量 embedding 大小</li><li><code>num_hidden_layers</code>: Number of hidden layers in the Transformer encoder. 层数</li><li><code>num_attention_heads</code>: Number of attention heads for each attention layer in<br />the Transformer encoder. 多头数量</li><li><code>intermediate_size</code>: The size of the “intermediate” (i.e., feed-forward)<br />layer in the Transformer encoder. FFN 中间层的大小</li><li><code>hidden_act</code>: The non-linear activation function (function or string) in the<br />encoder and pooler. 激活函数</li><li><code>hidden_dropout_prob</code>: The dropout probability for all fully connected<br />layers in the embeddings, encoder, and pooler. dropout 参数</li><li><code>attention_probs_dropout_prob</code>: The dropout ratio for the attention<br />probabilities.</li><li><code>max_position_embeddings</code>: position embedding 的最大值 (e.g., 512 or 1024 or 2048).</li><li><code>type_vocab_size</code>: next sentence prediction 中的 Segment A 和 Segment B，默认大小是 2</li><li><code>initializer_range</code>: The stdev of the truncated_normal_initializer for<br />initializing all weight matrices.<br />“”&quot;</li></ul><h2 id="embedding_lookup"><a class="markdownIt-Anchor" href="#embedding_lookup"></a> embedding_lookup</h2><p>根据 <code>input_ids</code> 生成词向量 <code>embedding table</code> 以及对应的 <code>input_id_embeddings</code>。简单一点理解就是向量从 <code>[batch_size, seq_size]</code> 到 <code>[batch_size, seq_size，embedding_size]</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">embedding_lookup</span>(<span class="params">input_ids,</span></span><br><span class="line"><span class="params">                     vocab_size,</span></span><br><span class="line"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                     word_embedding_name=<span class="string">&quot;word_embeddings&quot;</span>,</span></span><br><span class="line"><span class="params">                     use_one_hot_embeddings=<span class="literal">False</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span></span><br><span class="line"><span class="string">      ids.</span></span><br><span class="line"><span class="string">    vocab_size: int. Size of the embedding vocabulary.</span></span><br><span class="line"><span class="string">    embedding_size: int. Width of the word embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Embedding initialization range.</span></span><br><span class="line"><span class="string">    word_embedding_name: string. Name of the embedding table.</span></span><br><span class="line"><span class="string">    use_one_hot_embeddings: bool. If True, use one-hot method for word</span></span><br><span class="line"><span class="string">      embeddings. If False, use `tf.gather()`.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">  <span class="comment"># num_inputs].</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">  <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">  <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [-<span class="number">1</span>])</span><br><span class="line">  <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[<span class="number">0</span>:-<span class="number">1</span>] + [input_shape[-<span class="number">1</span>] * embedding_size])</span><br><span class="line">  <span class="keyword">return</span> (output, embedding_table)</span><br></pre></td></tr></table></figure><p>从 embedding_table 取 input_ids 对应的 embedding 有两种方法：</p><ul><li>矩阵乘法：先通过 input_ids 构造出 one_hot 矩阵，然后和 embedding_table 相乘得到结果。</li><li><code>tf.gather</code> 根据 input_ids 取 embedding_table 对应行的结果。和 <code>tf.nn.embedding_lookup</code> 方法类似。具体原理可以参考 <a href="https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do">python - What does tf.nn.embedding_lookup function do? - Stack Overflow</a></li></ul><p>看网上的解释，定义两种方法主要是不同设备（CPU、GPU、TPU）运算速度导致的。</p><h2 id="embedding_postprocessor"><a class="markdownIt-Anchor" href="#embedding_postprocessor"></a> embedding_postprocessor</h2><p>embedding_postprocessor 将 <code>token embeddings</code> <code>segmentation embeddings</code> <code>position embeddings</code> 三个向量相加得到最终的输入向量。</p><p><img src="https://media.xiang578.com/bert-input-embeddings.png" alt="" /></p><ul><li><code>token embeddings</code> 对应单词 embedding</li><li><code>segmentation embeddings</code> 代表单词来自哪个句子，在 Next Sentence Prediction 任务中使用。</li><li><code>position embeddings</code> 位置 embedding。在「Attention is all your need」论文中，Google 生成 position embedding 的方法是一个花里胡哨 cos/sin 公式，这一次换成训练 position embedding。猜测在之前的论文中，输入的 seq len 可能长短不一，导致部分 position embedding 训练不充分。BERT 中强行定死 seq len。</li><li>最后直接将三个 embedding 相加，可能对新人来说也有点迷惑。我自己的理解是，物理中多个不同波长的波叠加，是可以通过方法区分的。所以三个 embedding 相加，模型也能学到差异。<ul><li>知乎这个问题<a href="https://www.zhihu.com/question/374835153">为什么 Bert 的三个 Embedding 可以进行相加</a>可以提供更加严谨的理由。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">embedding_postprocessor</span>(<span class="params">input_tensor,</span></span><br><span class="line"><span class="params">                            use_token_type=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                            token_type_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                            token_type_vocab_size=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">                            token_type_embedding_name=<span class="string">&quot;token_type_embeddings&quot;</span>,</span></span><br><span class="line"><span class="params">                            use_position_embeddings=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                            position_embedding_name=<span class="string">&quot;position_embeddings&quot;</span>,</span></span><br><span class="line"><span class="params">                            initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                            max_position_embeddings=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">                            dropout_prob=<span class="number">0.1</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Performs various post-processing on a word embedding tensor.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      embedding_size].</span></span><br><span class="line"><span class="string">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      Must be specified if `use_token_type` is True.</span></span><br><span class="line"><span class="string">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for token type ids.</span></span><br><span class="line"><span class="string">    use_position_embeddings: bool. Whether to add position embeddings for the</span></span><br><span class="line"><span class="string">      position of each token in the sequence.</span></span><br><span class="line"><span class="string">    position_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for positional embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initialization.</span></span><br><span class="line"><span class="string">    max_position_embeddings: int. Maximum sequence length that might ever be</span></span><br><span class="line"><span class="string">      used with this model. This can be longer than the sequence length of</span></span><br><span class="line"><span class="string">      input_tensor, but cannot be shorter.</span></span><br><span class="line"><span class="string">    dropout_prob: float. Dropout probability applied to the final output tensor.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float tensor with same shape as `input_tensor`.</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: One of the tensor shapes or input values is invalid.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 是否使用有 segmentation embeddings</span></span><br><span class="line">  <span class="keyword">if</span> use_token_type:</span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;`token_type_ids` must be specified if&quot;</span></span><br><span class="line">                       <span class="string">&quot;`use_token_type` is True.&quot;</span>)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    <span class="comment"># segmentation vocab 大小一般是 2，所以使用 one-hot 速度比较快</span></span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [-<span class="number">1</span>])</span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      <span class="comment"># Since the position embedding table is a learned variable, we create it</span></span><br><span class="line">      <span class="comment"># using a (long) sequence length `max_position_embeddings`. The actual</span></span><br><span class="line">      <span class="comment"># sequence length might be shorter than this, for faster training of</span></span><br><span class="line">      <span class="comment"># tasks that do not have long sequences.</span></span><br><span class="line">      <span class="comment">#</span></span><br><span class="line">      <span class="comment"># So `full_position_embeddings` is effectively an embedding table</span></span><br><span class="line">      <span class="comment"># for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span></span><br><span class="line">      <span class="comment"># sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span></span><br><span class="line">      <span class="comment"># perform a slice.</span></span><br><span class="line">      <span class="comment"># position embedding 可以通过学习得到，然后可能输入句子的长度没有到达 512。使用 tf.slice 取对应的向量速度比较快。大小是[seq_length, width]</span></span><br><span class="line">      position_embeddings = tf.<span class="built_in">slice</span>(full_position_embeddings, [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                     [seq_length, -<span class="number">1</span>])</span><br><span class="line">      num_dims = <span class="built_in">len</span>(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Only the last two dimensions are relevant (`seq_length` and `width`), so</span></span><br><span class="line">      <span class="comment"># we broadcast among the first dimensions, which is typically just</span></span><br><span class="line">      <span class="comment"># the batch size.</span></span><br><span class="line">      <span class="comment"># word embedding 的大小是 [batch_size, seq_length, width]，上一步取出的 position embedding 大小是 [seq_length, width]，需要对后面一个矩阵进行广播。</span></span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_dims - <span class="number">2</span>):</span><br><span class="line">        position_broadcast_shape.append(<span class="number">1</span>)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width]) <span class="comment"># 大小为 [1, seq_length, width] </span></span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      <span class="comment"># 通过 broadcast 相加                          </span></span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>三个 embedding 向量相加后，还会过一个 <code>layer_norm_and_dropout</code> 层，都是标准的，没有什么特殊。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dropout</span>(<span class="params">input_tensor, dropout_prob</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Perform dropout.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor.</span></span><br><span class="line"><span class="string">    dropout_prob: Python float. The probability of dropping out a value (NOT of</span></span><br><span class="line"><span class="string">      *keeping* a dimension as in `tf.nn.dropout`).</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A version of `input_tensor` with dropout applied.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">if</span> dropout_prob <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> dropout_prob == <span class="number">0.0</span>:</span><br><span class="line">    <span class="keyword">return</span> input_tensor</span><br><span class="line"></span><br><span class="line">  output = tf.nn.dropout(input_tensor, <span class="number">1.0</span> - dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layer_norm</span>(<span class="params">input_tensor, name=<span class="literal">None</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Run layer normalization on the last dimension of the tensor.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> tf.contrib.layers.layer_norm(</span><br><span class="line">      inputs=input_tensor, begin_norm_axis=-<span class="number">1</span>, begin_params_axis=-<span class="number">1</span>, scope=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">layer_norm_and_dropout</span>(<span class="params">input_tensor, dropout_prob, name=<span class="literal">None</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Runs layer normalization followed by dropout.&quot;&quot;&quot;</span></span><br><span class="line">  output_tensor = layer_norm(input_tensor, name)</span><br><span class="line">  output_tensor = dropout(output_tensor, dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output_tensor</span><br></pre></td></tr></table></figure><h2 id="create_attention_mask_from_input_mask"><a class="markdownIt-Anchor" href="#create_attention_mask_from_input_mask"></a> create_attention_mask_from_input_mask</h2><p>create_attention_mask_from_input_mask 用来构造 attention 时的 mask 矩阵（padding 的单词不参与计算 attention socre）。输入向量 <code>[batch_size, from_seq_length, ...]</code> 和 <code>[batch_size, to_seq_length]</code> 输出向量 <code>[batch_size, from_seq_length, to_seq_length]</code>。</p><p>偷个例子来举：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from_tensor = tf.constant([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">1</span>]]) <span class="comment"># 中间的 0 代表 padding 的结果</span></span><br><span class="line">to_mask = tf.constant([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]) <span class="comment"># 和 from_tensor 对应。如果 1 代表对应位置有词，如果 0 代表对应位置是 padding 的。</span></span><br><span class="line"></span><br><span class="line">to_mask = tf.cast(tf.reshape(to_mask, [<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>]), tf.float32)</span><br><span class="line"><span class="comment"># print(to_mask_2)</span></span><br><span class="line">broadcast_ones = tf.ones(</span><br><span class="line">      shape=[<span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span>], dtype=tf.float32)</span><br><span class="line">mask = broadcast_ones * to_mask</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># print(sess.run(to_mask))</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(mask))</span><br></pre></td></tr></table></figure><p>最后的结果是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[[1. 1. 1. 0. 0.] #第一个词可以和前三个词计算 attention</span><br><span class="line">  [1. 1. 1. 0. 0.]</span><br><span class="line">  [1. 1. 1. 0. 0.]</span><br><span class="line">  [1. 1. 1. 0. 0.]</span><br><span class="line">  [1. 1. 1. 0. 0.]]</span><br><span class="line"></span><br><span class="line"> [[1. 1. 1. 1. 1.]</span><br><span class="line">  [1. 1. 1. 1. 1.]</span><br><span class="line">  [1. 1. 1. 1. 1.]</span><br><span class="line">  [1. 1. 1. 1. 1.]</span><br><span class="line">  [1. 1. 1. 1. 1.]]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_attention_mask_from_input_mask</span>(<span class="params">from_tensor, to_mask</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Create 3D attention mask from a 2D tensor mask.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].</span></span><br><span class="line"><span class="string">    to_mask: int32 Tensor of shape [batch_size, to_seq_length].</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length, to_seq_length].</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">  from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  to_shape = get_shape_list(to_mask, expected_rank=<span class="number">2</span>)</span><br><span class="line">  to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  to_mask = tf.cast(</span><br><span class="line">      tf.reshape(to_mask, [batch_size, <span class="number">1</span>, to_seq_length]), tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We don&#x27;t assume that `from_tensor` is a mask (although it could be). We</span></span><br><span class="line">  <span class="comment"># don&#x27;t actually care if we attend *from* padding tokens (only *to* padding)</span></span><br><span class="line">  <span class="comment"># tokens so we create a tensor of all ones.</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># `broadcast_ones` = [batch_size, from_seq_length, 1]</span></span><br><span class="line">  broadcast_ones = tf.ones(</span><br><span class="line">      shape=[batch_size, from_seq_length, <span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Here we broadcast along two dimensions to create the mask.</span></span><br><span class="line">  <span class="comment"># 广播得到最后的 mask 矩阵 [batch_size, from_seq_length, to_seq_length]</span></span><br><span class="line">  mask = broadcast_ones * to_mask</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><h2 id="transformer_model"><a class="markdownIt-Anchor" href="#transformer_model"></a> transformer_model</h2><p>顾名思议 BERT 最核心的 Multi-headed, multi-layer<br />Transformer 实现过程。Attention is all you need 中的实现在 <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py">链接</a></p><p>一个 Transformer 的示意图：</p><p><img src="https://media.xiang578.com/transformer-encoder.png" alt="" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transformer_model</span>(<span class="params">input_tensor,</span></span><br><span class="line"><span class="params">                      attention_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                      hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                      num_hidden_layers=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                      num_attention_heads=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                      intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">                      intermediate_act_fn=gelu,</span></span><br><span class="line"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                      do_return_all_layers=<span class="literal">False</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Multi-headed, multi-layer Transformer from &quot;Attention is All You Need&quot;.</span></span><br><span class="line"><span class="string">  This is almost an exact implementation of the original Transformer encoder.</span></span><br><span class="line"><span class="string">  See the original paper:</span></span><br><span class="line"><span class="string">  https://arxiv.org/abs/1706.03762</span></span><br><span class="line"><span class="string">  Also see:</span></span><br><span class="line"><span class="string">  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      seq_length], with 1 for positions that can be attended to and 0 in</span></span><br><span class="line"><span class="string">      positions that should not be. 就是前面 create_attention_mask_from_input_mask 产出的结果</span></span><br><span class="line"><span class="string">    hidden_size: int. Hidden size of the Transformer.</span></span><br><span class="line"><span class="string">    num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads in the Transformer.</span></span><br><span class="line"><span class="string">    intermediate_size: int. The size of the &quot;intermediate&quot; (a.k.a., feed</span></span><br><span class="line"><span class="string">      forward) layer.</span></span><br><span class="line"><span class="string">    intermediate_act_fn: function. The non-linear activation function to apply</span></span><br><span class="line"><span class="string">      to the output of the intermediate/feed-forward layer.</span></span><br><span class="line"><span class="string">    hidden_dropout_prob: float. Dropout probability for the hidden layers.</span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: float. Dropout probability of the attention</span></span><br><span class="line"><span class="string">      probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the initializer (stddev of truncated</span></span><br><span class="line"><span class="string">      normal).</span></span><br><span class="line"><span class="string">    do_return_all_layers: Whether to also return all layers or just the final</span></span><br><span class="line"><span class="string">      layer.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span></span><br><span class="line"><span class="string">    hidden layer of the Transformer.</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: A Tensor shape or parameter is invalid.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="comment"># 最终输出的 hidden_size 能被 num_attention_heads 整除</span></span><br><span class="line">  <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span></span><br><span class="line">        <span class="string">&quot;heads (%d)&quot;</span> % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 定义 attention 每个输出的头的大小</span></span><br><span class="line">  <span class="comment"># 最后结果 concat 之后和原始输入大小相同。</span></span><br><span class="line">  attention_head_size = <span class="built_in">int</span>(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The Transformer performs sum residuals on all layers so the input needs</span></span><br><span class="line">  <span class="comment"># to be the same as the hidden size.</span></span><br><span class="line">  <span class="comment"># Transformer 中有残差连接，所以输入和输出 embedding size 要相同</span></span><br><span class="line">  <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;The width of the input tensor (%d) != hidden size (%d)&quot;</span> %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">  <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">  <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">  <span class="comment"># help the optimizer.</span></span><br><span class="line">  <span class="comment"># TPU 不擅长 reshape 操作，所以把所有的 3D tensor 变成 2D tensor</span></span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="comment"># 遍历多层</span></span><br><span class="line">  <span class="keyword">for</span> layer_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_hidden_layers):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;layer_%d&quot;</span> % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;attention&quot;</span>):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;self&quot;</span>):</span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(attention_heads) == <span class="number">1</span>:</span><br><span class="line">          attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">          <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">          <span class="comment"># concat 多头的结果</span></span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">        <span class="comment"># with `layer_input`. 加上残差</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          <span class="comment"># dropout 和 layer_norm</span></span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># 全连接层</span></span><br><span class="line">      <span class="comment"># The activation is only applied to the &quot;intermediate&quot; hidden layer.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;intermediate&quot;</span>):</span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    </span><br><span class="line">      <span class="comment"># 变回原来的大小，才能加上残差</span></span><br><span class="line">      <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;output&quot;</span>):</span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 是不是要输出中间结果</span></span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure><h2 id="attention_layer"><a class="markdownIt-Anchor" href="#attention_layer"></a> attention_layer</h2><p><code>attention_layer</code> 中实现 self-attention 和 multi-head，细节在 「Attention is all your need」里面有。query_layer 由 from_tensor 得到，key_layer 和 value_layer 由 to_tensor 得到。由于是 self-attention-encoder，from_tensor 和 to_tensor 相同。</p><p>示意图：</p><p><img src="https://media.xiang578.com/self-attention-and-multi-head.png" alt="" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention_layer</span>(<span class="params">from_tensor,</span></span><br><span class="line"><span class="params">                    to_tensor,</span></span><br><span class="line"><span class="params">                    attention_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    num_attention_heads=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                    size_per_head=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">                    query_act=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    key_act=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    value_act=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">                    initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                    do_return_2d_tensor=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                    batch_size=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    from_seq_length=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                    to_seq_length=<span class="literal">None</span></span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Performs multi-headed attention from `from_tensor` to `to_tensor`.</span></span><br><span class="line"><span class="string">  This is an implementation of multi-headed attention based on &quot;Attention</span></span><br><span class="line"><span class="string">  is all you Need&quot;. If `from_tensor` and `to_tensor` are the same, then</span></span><br><span class="line"><span class="string">  this is self-attention. Each timestep in `from_tensor` attends to the</span></span><br><span class="line"><span class="string">  corresponding sequence in `to_tensor`, and returns a fixed-with vector.</span></span><br><span class="line"><span class="string">  This function first projects `from_tensor` into a &quot;query&quot; tensor and</span></span><br><span class="line"><span class="string">  `to_tensor` into &quot;key&quot; and &quot;value&quot; tensors. These are (effectively) a list</span></span><br><span class="line"><span class="string">  of tensors of length `num_attention_heads`, where each tensor is of shape</span></span><br><span class="line"><span class="string">  [batch_size, seq_length, size_per_head].</span></span><br><span class="line"><span class="string">  Then, the query and key tensors are dot-producted and scaled. These are</span></span><br><span class="line"><span class="string">  softmaxed to obtain attention probabilities. The value tensors are then</span></span><br><span class="line"><span class="string">  interpolated by these probabilities, then concatenated back to a single</span></span><br><span class="line"><span class="string">  tensor and returned.</span></span><br><span class="line"><span class="string">  In practice, the multi-headed attention are done with transposes and</span></span><br><span class="line"><span class="string">  reshapes rather than actual separate tensors.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    from_tensor: float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      from_width].</span></span><br><span class="line"><span class="string">    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].</span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size,</span></span><br><span class="line"><span class="string">      from_seq_length, to_seq_length]. The values should be 1 or 0. The</span></span><br><span class="line"><span class="string">      attention scores will effectively be set to -infinity for any positions in</span></span><br><span class="line"><span class="string">      the mask that are 0, and will be unchanged for positions that are 1.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads.</span></span><br><span class="line"><span class="string">    size_per_head: int. Size of each attention head.</span></span><br><span class="line"><span class="string">    query_act: (optional) Activation function for the query transform.</span></span><br><span class="line"><span class="string">    key_act: (optional) Activation function for the key transform.</span></span><br><span class="line"><span class="string">    value_act: (optional) Activation function for the value transform.</span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: (optional) float. Dropout probability of the</span></span><br><span class="line"><span class="string">      attention probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initializer.</span></span><br><span class="line"><span class="string">    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size</span></span><br><span class="line"><span class="string">      * from_seq_length, num_attention_heads * size_per_head]. If False, the</span></span><br><span class="line"><span class="string">      output will be of shape [batch_size, from_seq_length, num_attention_heads</span></span><br><span class="line"><span class="string">      * size_per_head].</span></span><br><span class="line"><span class="string">    batch_size: (Optional) int. If the input is 2D, this might be the batch size</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor` and `to_tensor`.</span></span><br><span class="line"><span class="string">    from_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor`.</span></span><br><span class="line"><span class="string">    to_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `to_tensor`.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span></span><br><span class="line"><span class="string">      true, this will be of shape [batch_size * from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]).</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: Any of the arguments or tensor shapes are invalid.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">transpose_for_scores</span>(<span class="params">input_tensor, batch_size, num_attention_heads,</span></span><br><span class="line"><span class="params">                           seq_length, width</span>):</span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]) <span class="comment">#[batch_size,  num_attention_heads, seq_length, width]</span></span><br><span class="line">    <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">len</span>(from_shape) != <span class="built_in">len</span>(to_shape):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">&quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">len</span>(from_shape) == <span class="number">3</span>:</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">elif</span> <span class="built_in">len</span>(from_shape) == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(</span><br><span class="line">          <span class="string">&quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span></span><br><span class="line">          <span class="string">&quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot;</span></span><br><span class="line">          <span class="string">&quot;must all be specified.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">  <span class="comment">#   B = batch size (number of sequences) </span></span><br><span class="line">  <span class="comment">#   F = `from_tensor` sequence length 输入单词长度</span></span><br><span class="line">  <span class="comment">#   T = `to_tensor` sequence length 输出单词长度</span></span><br><span class="line">  <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">  <span class="comment">#   H = `size_per_head`</span></span><br><span class="line"></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor) <span class="comment"># [B*F，hidden_size=N*H]</span></span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor) <span class="comment"># [B*T，head_size=N*H]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B*F, N*H] 从 from_tensor 得到 query_layer</span></span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=<span class="string">&quot;query&quot;</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B*T, N*H]</span></span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=<span class="string">&quot;key&quot;</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B*T, N*H]</span></span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=<span class="string">&quot;value&quot;</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 计算多头调整 tensor shape，都是为了方便计算.变成 [batch_size,  num_attention_heads, seq_length, width]</span></span><br><span class="line">  <span class="comment"># `query_layer` = [B, N, F, H]</span></span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B, N, T, H]</span></span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw</span></span><br><span class="line">  <span class="comment"># attention scores.</span></span><br><span class="line">  <span class="comment"># `attention_scores` = [B, N, F, T] =&gt; [F, H] * [H, T] = [F, T]</span></span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 <span class="number">1.0</span> / math.sqrt(<span class="built_in">float</span>(size_per_head))) <span class="comment"># 经典缩小 scroe 值，防止落到 softmask 梯度饱和区</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 处理 padding 部分的 score 值， padding 为 0 的在对应的位置上加上 -10000.0， 这样求 exp 之后就是一个接近于 0 的值</span></span><br><span class="line">  <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># `attention_mask` = [B, 1, F, T]</span></span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span></span><br><span class="line">    <span class="comment"># masked positions, this operation will create a tensor which is 0.0 for</span></span><br><span class="line">    <span class="comment"># positions we want to attend and -10000.0 for masked positions.</span></span><br><span class="line">    adder = (<span class="number">1.0</span> - tf.cast(attention_mask, tf.float32)) * -<span class="number">10000.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">    <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">  <span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">  <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, N, T, H]</span></span><br><span class="line">  value_layer = tf.transpose(value_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># attention 之后的结果</span></span><br><span class="line">  <span class="comment"># `context_layer` = [B, N, F, H]</span></span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, F, N, H]</span></span><br><span class="line">  context_layer = tf.transpose(context_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_2d_tensor:</span><br><span class="line">    <span class="comment"># `context_layer` = [B*F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># `context_layer` = [B, F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> context_layer</span><br></pre></td></tr></table></figure><h2 id="bertmodel-构造类"><a class="markdownIt-Anchor" href="#bertmodel-构造类"></a> BertModel 构造类</h2><p>init 方法就是将上面的内容串联起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">               config,</span></span><br><span class="line"><span class="params">               is_training,</span></span><br><span class="line"><span class="params">               input_ids,</span></span><br><span class="line"><span class="params">               input_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               token_type_ids=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               use_one_hot_embeddings=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               scope=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Constructor for BertModel.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      config: `BertConfig` instance.</span></span><br><span class="line"><span class="string">      is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">        whether dropout will be applied.</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">        embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">      scope: (optional) variable scope. Defaults to &quot;bert&quot;.</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">      ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">        is invalid.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    config = copy.deepcopy(config)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">      config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">      config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">    <span class="comment"># 处理 embedding</span></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">&quot;bert&quot;</span>):</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embeddings&quot;</span>):</span><br><span class="line">        <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">        (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            vocab_size=config.vocab_size,</span><br><span class="line">            embedding_size=config.hidden_size,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            word_embedding_name=<span class="string">&quot;word_embeddings&quot;</span>,</span><br><span class="line">            use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">        <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">        self.embedding_output = embedding_postprocessor(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            use_token_type=<span class="literal">True</span>,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">            token_type_embedding_name=<span class="string">&quot;token_type_embeddings&quot;</span>,</span><br><span class="line">            use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">            position_embedding_name=<span class="string">&quot;position_embeddings&quot;</span>,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">            dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;encoder&quot;</span>):</span><br><span class="line">        <span class="comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span></span><br><span class="line">        <span class="comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span></span><br><span class="line">        <span class="comment"># for the attention scores. 获得 attention_mask</span></span><br><span class="line">        attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">            input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the stacked transformer. 计算 transformer 的结果</span></span><br><span class="line">        <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line">        self.all_encoder_layers = transformer_model(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            hidden_size=config.hidden_size,</span><br><span class="line">            num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">            num_attention_heads=config.num_attention_heads,</span><br><span class="line">            intermediate_size=config.intermediate_size,</span><br><span class="line">            intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">            hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            do_return_all_layers=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.sequence_output = self.all_encoder_layers[-<span class="number">1</span>]</span><br><span class="line">      <span class="comment"># The &quot;pooler&quot; converts the encoded sequence tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">      <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">      <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">      <span class="comment"># 分类任务取第一个 [CLS] 对应的 embedding 值</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;pooler&quot;</span>):</span><br><span class="line">        <span class="comment"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line">        first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">        self.pooled_output = tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation=tf.tanh,</span><br><span class="line">            kernel_initializer=create_initializer(config.initializer_range))</span><br></pre></td></tr></table></figure><h2 id="模型使用"><a class="markdownIt-Anchor" href="#模型使用"></a> 模型使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Already been converted into WordPiece token ids</span></span><br><span class="line">input_ids = tf.constant([[<span class="number">31</span>, <span class="number">51</span>, <span class="number">99</span>], [<span class="number">15</span>, <span class="number">5</span>, <span class="number">0</span>]])</span><br><span class="line">input_mask = tf.constant([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">token_type_ids = tf.constant([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">config = modeling.BertConfig(vocab_size=<span class="number">32000</span>, hidden_size=<span class="number">512</span>,</span><br><span class="line">  num_hidden_layers=<span class="number">8</span>, num_attention_heads=<span class="number">6</span>, intermediate_size=<span class="number">1024</span>)</span><br><span class="line"><span class="comment"># 调用模型</span></span><br><span class="line">model = modeling.BertModel(config=config, is_training=<span class="literal">True</span>,</span><br><span class="line">  input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span><br><span class="line">label_embeddings = tf.get_variable(...)</span><br><span class="line">pooled_output = model.get_pooled_output()</span><br><span class="line">logits = tf.matmul(pooled_output, label_embeddings)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="bert-参数量计算"><a class="markdownIt-Anchor" href="#bert-参数量计算"></a> Bert 参数量计算</h2><p>回到写这篇文章的起点，最后通过计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>E</mi><mi>R</mi><msub><mi>T</mi><mrow><mi>B</mi><mi>A</mi><mi>S</mi><mi>E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">BERT_{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的参数量，加深对模型的理解。论文介绍 Layer = 12，Hidden Size = 768，multi head = 12，参数量是 110M 左右。</p><p>总的计算公式为 <code>(30522 + 512 + 2)*768 + 768*2 + (3*768*64*12 + 3*64*12 + 64*768*12 + 768 + 768 + 768 + 768*3072 + 3072 + 3072*768 + 768 + 768 + 768) * 12 = 108891648</code></p><ul><li>embedding 部分 <code>(30522 + 512 + 2)*768 + 768*2</code><ul><li>embedding size = 768</li><li>单词数仅有 30522，比起 CTR 几千万的物品还是少很多。</li><li>position size = 512</li><li>sentence size = 2</li><li>三个 embedding 相加后 Norm 的参数 2</li></ul></li><li>multi attention 部分 <code>(3*768*64*12 + 3*64*12 + 64*768*12 + 768 + 768 + 768 + 768*3072 + 3072 + 3072*768 + 768 + 768 + 768) * 12</code><ul><li>一共是 12 层，对应 12 个 Transformer</li><li><code>3*768*64*12 + 3*64*12</code> 12 个 multi-head 对应的 Q K V 参数</li><li><code>64*768*12 + 768 + 768 + 768</code> multi-head 结果 concat 之后接的全连接层参数以及后面的 norm</li><li><code>768*3072 + 3072 + 3072*768 + 768 + 768 + 768</code> FFN 以及 norm 的参数</li></ul></li></ul><h2 id="ref"><a class="markdownIt-Anchor" href="#ref"></a> Ref</h2><ul><li><a href="http://fancyerii.github.io/2019/03/09/bert-codes/">BERT代码阅读 - 李理的博客</a></li><li><a href="https://lsc417.com/2020/06/19/bert_parameter/">BERT encoder参数量计算 | 417’s blog</a></li><li><a href="https://zhuanlan.zhihu.com/p/69106080">BERT源码分析PART I - 知乎</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;国庆节前突然对如何计算 BERT 的参数量感兴趣，不过一直看不明白网上的计算过程，索性下载 BERT 源代码阅读一番。这篇文章记录阅读 BertModel 类（核心代码实现）时写的一些笔记，反正我也是纸上谈兵，所以不需要太关注数据处理和 Finetune 相关部分，最后附上计算 BERT 参数量的过程仅供参考。&lt;/p&gt;</summary>
    
    
    
    <category term="智能路" scheme="https://blog.xiang578.com/categories/%E6%99%BA%E8%83%BD%E8%B7%AF/"/>
    
    
    <category term="nlp" scheme="https://blog.xiang578.com/tags/nlp/"/>
    
    <category term="transformer" scheme="https://blog.xiang578.com/tags/transformer/"/>
    
    <category term="google" scheme="https://blog.xiang578.com/tags/google/"/>
    
    <category term="bert" scheme="https://blog.xiang578.com/tags/bert/"/>
    
    <category term="code" scheme="https://blog.xiang578.com/tags/code/"/>
    
  </entry>
  
  <entry>
    <title>【Never Reading】 202008 选择记忆</title>
    <link href="https://blog.xiang578.com/post/Never-Reading-202008.html"/>
    <id>https://blog.xiang578.com/post/Never-Reading-202008.html</id>
    <published>2020-09-13T15:55:11.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>某一刻突然意识到可以选择自己的记忆，我顿悟了。</p><span id="more"></span><h2 id="roam-cn-聚会"><a class="markdownIt-Anchor" href="#roam-cn-聚会"></a> Roam CN 聚会</h2><p>8 月 29 日参与由 <a href="https://twitter.com/JESSCATE93">Jessie@FG</a>发起一次北京 RoamCN 微信群聚会，其他到场的还有 <a href="https://twitter.com/pimgeek">pimgeek</a>、<a href="http://flynngao.github.io/about">Flynn</a> 等 7 位群友。聚会中大家讨论和知识管理、Roam Research 相关的内容。记录一些给我灵感的内容：</p><h3 id="anki"><a class="markdownIt-Anchor" href="#anki"></a> Anki</h3><p>做为一个知识管理爱好者，很难没有听说过 Anki。这一次线下的相关内容讨论，给我留下的最大感受就是本期标题的由来「选择记忆」。Flynn 介绍在 Michael Nielsen 的 <a href="http://augmentingcognition.com/ltm.html">Augmenting Long-term Memory</a> ，一直坚持使用 Anki，他举的例子是「可以记住全部的正则匹配」。和绝大部分人一样，我记忆内容的方式就是看一遍，然后等到要用的时候，再去查相关的内容。Anki 可以自己制作需要记忆内容的卡片，这个过程不就是选择自己的记忆吗？其实几年前也看过 Michael Nielsen 的文章，可能是当时境界太低，没有理解选择记忆对自己的重要性。现在觉得有几点很重要：</p><ol><li>自己制作卡片，网上下载的卡片没有灵魂。</li><li>卡片尽量原子化，使用 QA 形式。原子化保证没一张可以快速过去，QA 形式费曼技巧，防止假懂。</li><li>坚持使用。</li></ol><p>等到自制卡片能到达 1k 张，再来具体分享自己的体验。</p><p>Michael Nielsen 的文章实在是太长了，先分享第一部分的摘录：</p><ul><li>之前看过中文翻译但是没有实践 <a href="https://zhuanlan.zhihu.com/p/65131722">【三万字长文】量子物理学家是如何使用 Anki 的？ - 知乎</a></li><li>Solomon Shereshevsky 以超级记忆力闻名</li><li>memex 外部记忆机器，汇总全部个人资料<ul><li>[[Douglas Engelbart]] augmentation of human intelligence</li><li>[[Ted Nelson]] [[Project Xanadu]]</li><li>Tim Berners-Lee world wide web</li></ul></li><li>Anki makes memory a <strong>choice</strong>, rather than a haphazard event, to be left to chance.</li><li>But, as we shall see, there are already powerful ideas about personal memory systems based solely on the structuring and presentation of information. 从信息组织和展示的角度入手。</li><li>Anki 卡片之间的复习间隔时间<ul><li>一张卡片在 20 年间需要花费 4-7 分钟去记忆</li></ul></li><li>制作卡片的标准，Anki make memory a choice：你可以选择自己记忆的内容<ul><li>if memorizing a fact seems worth 10 minutes of my time in the future, then I do it</li><li>superseding the first, if a fact seems striking then into Anki it goes, regardless of whether it seems worth 10 minutes of my future time or not.</li></ul></li><li>尝试使用 Anki 很不容易，通过学习 Unix 命令掌握需要的技巧。可以把之前 [[Vim 实用技巧]] 拆解到 Anki 中。</li><li>用 QA  的形式来使用 Anki</li><li>Using Anki to thoroughly read a research paper in an unfamiliar field<ul><li>[[AlphaGo]]还记得多少？</li><li>举例的问题<ul><li>“What’s the size of a Go board?”;</li><li>“Who plays first in Go?”;</li><li>“How many human game positions did AlphaGo learn from?”;</li><li>“Where did AlphaGo get its training data?”;</li><li>“What were the names of the two main types of neural network AlphaGo used?”</li></ul></li><li>多轮记录问题</li><li>通过一年之后阅读 [[AlphaGo Zero]] 检验记忆效果</li><li>I find Anki works much better when used in service to some personal creative project.</li><li>为了一个明确目标去设置 Anki 的问题，没有温度。<ul><li>when I’m reading in support of some creative project, I ask much better Anki questions.</li></ul></li></ul></li><li>Using Anki to do shallow reads of papers [[how to read a paper]](10 to 60 minutes Ankifying a paper)<ul><li>基于主题去阅读论文，读重要的论文。</li><li>选择性阅读，记录关键。I’ll add to Anki questions about the core claims, core questions, and core ideas of the paper.</li><li>一篇论文 5 到 20 个问题。</li><li>避免 Anki 化有误导性的条目，仔细选择提问的方法。<ul><li>“What does Jones 2011 claim is the average age at which physics Nobelists made their prizewinning discovery, over 1980-2011?” (Answer: 48).</li><li>“Which paper claimed that physics Nobelists made their prizewinning discovery at average age 48, over the period 1980-2011?” (Answer: Jones 2011).</li></ul></li><li>Ankifying figures：知道某张图存在，然后参考。<ul><li>曲线大概走向？</li><li>图表细节</li></ul></li></ul></li><li>Syntopic reading using Anki<ul><li>从 key paper 出发，best 5-10 paper，普通的论文也有助于认识整个领域。</li><li>Anki 从设计上不是为了创造性工作而生。</li><li>Anki 创造理解新领域的机会。</li></ul></li><li>More patterns of Anki use<ul><li><a href="https://www.supermemo.com/en/archives1990-2015/articles/20rules">Effective learning: Twenty rules of formulating knowledge</a></li><li>原子化 <strong>Make most Anki questions and answers as atomic as possible</strong><ul><li>How to create a soft link from linkname to filename?<ul><li><code>ln -s filename linkname</code></li><li>根据答案分解成两部分。</li></ul></li><li>回答错原子化问题，能更清楚在哪一方面不足。</li></ul></li><li>追求大师级使用 <strong>Anki use is best thought of as a virtuoso skill, to be developed:</strong><ul><li>软件简单，但是功能强大。</li><li>你就是创造者。</li></ul></li><li>一个卡组 <strong>Use one big deck</strong><ul><li>不同知识混合在一起，会产生意想不到的结果。</li></ul></li><li>避免临时感兴趣的话题 <strong>Avoid orphan questions</strong></li><li><strong>95% of Anki’s value comes from 5% of the features</strong></li><li><strong>Using Anki for APIs, books, videos, seminars, conversations, the web, events, and places</strong><ul><li>研讨会记录 3 个左右高质量的问题。</li><li>有选择记忆 unmindfully Ankifying everything in sight is a bad habit</li><li>Ankify things that serve your long-term goals</li><li>Anki 之前读过的书或者论文</li></ul></li><li>避免判断题</li><li>创造性工作需要内化理解部分知识。<ul><li>知识的流畅度 Fluency matters in thinking。</li></ul></li><li>为什么 Anki 不火？<strong>If personal memory systems are so great, why aren’t they more widely used?</strong><ul><li>人们更喜欢临时报佛脚。In experimental research on memory, people consistently underestimate the gains that come from distributing their study in a manner similar to Anki. Instead, they prefer last-minute cramming, and believe it produces better results, though many studies show it does not.</li><li>理想难度原理 The psychologist Robert Bjork has suggested the “principle of desirable difficulty”, the idea that memories are maximally strengthened if tested when we’re on the verge of forgetting them. This suggests that an efficient memory system will intrinsically be somewhat difficult to use. Human beings have a complex relationship to difficult activities, and often dislike performing them, unless strongly motivated (in which case they may become pleasurable).</li><li>用好很难。Systems such as Anki are challenging to use well, and easy to use poorly.</li></ul></li></ul></li></ul><p>最后，Michael Nielsen 在文章中提到 AlphaGo，分享一个这个相关的例子：</p><p><a href="https://twitter.com/xiang578/status/1300068256045129728">RyenX 算法花园 在 Twitter: “几周前和大老板开会，他突然问我们一个问题，总结一下就是知识层次。 第一层：alpha go 和 alpha zero 的策略网络是如何训练的？（go 学习获胜的下法，zero 学习分布） 第二层：为什么会产出这样的区别？ 第三层：那么这和我们的业务中什么类似？” / Twitter</a></p><h3 id="zettelkasten"><a class="markdownIt-Anchor" href="#zettelkasten"></a> Zettelkasten</h3><p>最近中文圈比较火的概念，我也多次在 「Never Reading」分享相关的内容。Flynn 在聚会中也分享自己之前写过的一篇文章 <a href="http://flynngao.github.io/2020/07/18/zettelkasten-1">拆解Zettelkasten | 卡片盒知识管理体系实践反思 - Flynn</a>，摘录相关的内容：</p><ul><li>工作流和笔记方式</li><li>Zettelkasten 认可思考的非线性特征，给出了一种脱离现有顺序框架的方式。</li><li><strong>本来就没有一个完整的知识管理工作流。</strong> 如何判断<ul><li>是否在各种情景下都可以使用有效方式捕捉素材？</li><li>是否对待读材料有合适的管理方式？是否能够不定期清空待读材料？</li><li>是否建立了自己的关注领域清单，并且建立对应的项目？</li><li>是否知道如何为任何项目快速建立下一步行动？</li><li>是否有种合理的方式检验自己对知识的理解？</li><li>是否确认现有正在执行系统/习惯能可以长期执行？</li><li>是否确定现有系统在遭遇中断甚至奔溃之后可以很快恢复？</li></ul></li><li>[[Evergreen Note]] 比 Permanent Notes 更合适<ul><li>以能够直接公布为目的逻辑完备的小文章，并且需要不停的更新他们之间的可能存在的逻辑关系，共同点，冲突点。<ul><li>笔记可以直接公开，比如 [[Andy Matuschak]]</li></ul></li><li>写作当做最重要的事情</li><li>引用 [[Simon Eskildsen]] <a href="https://superorganizers.substack.com/p/how-to-build-a-learning-machine">How to Make Yourself Into a Learning Machine - Superorganizers</a> 以及 <a href="https://notes.andymatuschak.org/z7kEFe6NfUSgtaDuUjST1oczKKzQQeQWk4Dbc">notes.andymatuschak.org</a><ul><li><img src="https://media.xiang578.com/flynn-andy.png" alt="" /></li><li><img src="https://media.xiang578.com/flynn-zk.png" alt="" /></li></ul></li></ul></li><li>从他的理解来看，[[Obsidian]] 比 [[Roam]] 更合适，以及在 RoamCN 聚会时提到的，需要对文字数量进行限制。[[Progressive Summarization]] 和 [[Evergreen Note]] 的生长概念类似。</li></ul><h3 id="有限与无限的游戏"><a class="markdownIt-Anchor" href="#有限与无限的游戏"></a> 有限与无限的游戏</h3><blockquote><p>有本书对我蛮有影响的——叫做《有限与无限的游戏》。有限游戏在边界内玩，无限游戏却是在和边界，也就是和“规则”玩，探索改变边界本身。实际上只有一个无限游戏，那就是你的人生，死亡是不可逾越的边界。与之相比，其他的边界并不是那么重要了。<br />人人网、美团网、饭否网创始人王兴</p></blockquote><p>有限游戏有明确的游戏结束目标，无限游戏的目标是让游戏一直进行下去。生活中的很多方面更多接近于无限游戏。比如工作之后的学习，没有明确的目标引导，更多是去保持这个习惯。</p><p>本期中脱不花的四个面试题中的驱动力也和这两个游戏有关。文章中介绍到谷歌之前从编程比赛中寻找程序员（回想到自己大学参加 ACM 经历），这些人习惯与在边界内做事情（外部刺激和目标）。而谷歌希望的人更多能根据自己的兴趣不断地拓展边界，打破旧有规则。结合到自己的工作中，很多任务没有之前编程时那么明确的目标，更多时候是依赖自己去寻找问题和解决问题。</p><p>另外 <a href="https://mp.weixin.qq.com/s/0tUYITGjZjFb7JLU4F-MUw">谁在驱动滴滴发动机？</a> 中也提到张博最近在看 《有限与无限的游戏》，互联网下半场可能各个大佬更希望自己的企业基业长青。</p><h3 id="其他"><a class="markdownIt-Anchor" href="#其他"></a> 其他</h3><p>还有部分提及的内容还没有时间看，记录一下关键字和大家分享：active recall、spaced repetition、learn how to learn、nstigation habit、Atomic Habits、Mini Habits。</p><h2 id="阅读"><a class="markdownIt-Anchor" href="#阅读"></a> 阅读</h2><h3 id="个人成长知乎上最神奇的专业领域"><a class="markdownIt-Anchor" href="#个人成长知乎上最神奇的专业领域"></a> 「个人成长」，知乎上最神奇的「专业领域」</h3><ul><li>厉害的标准<ul><li>在一个职级体系中通过竞争的手段达到了高位。</li><li>留下传世的精品</li><li>让足够多的人改善自己的生活</li></ul></li></ul><h3 id="产品沉思录-product-thinkingvol20200816把自己作为方法"><a class="markdownIt-Anchor" href="#产品沉思录-product-thinkingvol20200816把自己作为方法"></a> [[产品沉思录]] Product Thinking】Vol.20200816：[[把自己作为方法]]</h3><ul><li>他用「<strong>蜂鸟般悬浮</strong>」来描述当下中国人焦虑的现状</li><li>反而对这种「温州乡绅式」的做事方式</li><li>他还提到了一种温柔而坚定地力量 —— <strong>认命不认输</strong><ul><li>所以关键是要把自己所在的社会位置想透。在这个现实下如何去<strong>超脱自己的角色，和强大的社会和历史力量持续的较劲，不认输的较劲。</strong></li></ul></li><li>Temporary Social Media<ul><li>事实证明我还是想的肤浅了、Snap 这两篇文章，是从现有社交网络的设计模式出发，从中寻找被忽略掉的边缘行为，然后反思为何社交网络会的「默认值」会被设计为「记录一切」，而这背后会带来什么样的代价。</li><li>铭记每一刻，但也带来各种挖坟的效果。</li><li>所有的事情都能被记录的时候，往往意味着什么事情都不重要。</li><li>所以 Snap 看似用有限的时间和形式，反而促进你能更好的记住某个故事和感觉，而不是像档案馆一样收集你所有的，几乎不会回溯的过往。</li></ul></li><li>温故知新：如何巧妙地达到你沟通的目的？<ul><li>文中关于沟通的定义值得着重记录 —— Ronald B. Adler与Russell F. Proctor将沟通定义为：<strong>“（沟通是）一种交流的过程，参与者经由交换信息而建立关系。”</strong></li><li>不过另一个有趣的地方是关于沟通深度的，即不是两个人在一起聊得越多越好，而是聊得是否是<strong>对自己意义重大的事件，或者是极为隐私的事件</strong>。</li></ul></li></ul><h3 id="回顾我学心理学的这8年经历-discovery-hipda-hipda"><a class="markdownIt-Anchor" href="#回顾我学心理学的这8年经历-discovery-hipda-hipda"></a> <a href="https://www.hi-pda.com/forum/viewthread.php?tid=2760589&amp;extra=page%3D1">回顾我学心理学的这8年经历 - Discovery - Hi!PDA Hi!PDA</a> #[[fatdragoncat]]</h3><ul><li>沙盘游戏展示个人心理状态</li><li>为了改变这个现状，我用了一个最简单粗暴的办法，就是你不理我，那我就去跟随你，参加你参加的一切活动</li><li>我一改过去记者生涯养成的晚上到处约饭的习惯，下班准点回家，周末绝不加班，而且开始主动参加老婆的各种活动，有时是到公园一群妈妈带娃的聚会，有时是做蛋糕，有时是去不同妈妈家聚餐，我本身是极端内向的人，而且前面也说过，我其实是有社交恐惧症的，所以参加这些活动每次都会迅速让我极端疲劳</li><li>移情(transference)来访者对分析者产生的一种强烈的情感，将自己过去对生活中某些重要人物的情感会太多投射到分析者身上的过程。</li><li>反移情(counter-transference)移情是咨询师把对生活中某个重要人物的情感、态度和属性转移到了来访者身上。</li><li>从那时起我就知道了，自己真正渴望的就是一个温暖的家，我想服务的客户，也是渴望家的温暖的人，而不是只想疗愈自己，而不顾伴侣死活的人。</li><li>我不想再听一帮只是花钱找人诉苦，却不肯承担家庭责任的人说废话了</li><li>我的心理咨询工作终结了，我的婚姻也over了，我以为往后的岁月，就是漂泊江湖，四海为家了。</li><li>于是我开始研究，怎么才能背一个双肩背，就能一边工作，一边生活，然后就撞上了知识付费时代的来临。</li><li>心理咨询教练很难，但是保持自己的心理健康可能很容易。</li></ul><h3 id="观点得到ceo脱不花面试一个人你只问ta这四个问题就够了"><a class="markdownIt-Anchor" href="#观点得到ceo脱不花面试一个人你只问ta这四个问题就够了"></a> <a href="https://mp.weixin.qq.com/s/ks6JKet-GR81QZbqxHgOCw">【观点】得到CEO脱不花：面试一个人，你只问ta这四个问题就够了 ！</a></h3><ul><li>[[面试]] 后来 HR 和 Mentor 告诉我日常要，多积累一些面试相关的题目，这样进去之前就知道，通过什么问题考核什么方面。从此收集一些面试题就成了习惯，而在这个过程中关注点也从对专业内容的考核，变成了对人本身的考核，也变成了对自己的向内思索：这些问题，自己会如何回答呢？</li><li>[[少楠]] 的面试题<ul><li>有哪些事情是别人做起来觉得很难很无趣，但是你自己却乐此不彼的坚持了很久。</li><li>最近半年有什么观点改变了你的认知或者行为方式，为什么？</li><li>你迄今为止做过的，最让你有成就感的一件事是什么？</li></ul></li><li>[[脱不花]] 面试题<ul><li>**驱动力：**如果你突然有半个月的带薪休假，只有一个条件，就是必须研究一个事儿，你会研究什么?<ul><li>主要考察的是内驱力，擅长比赛的要靠外部刺激和目标(原文中举例谷歌从编程比赛中寻找程序员)，在边界内做事。<ul><li>所以我对工作的不适应，除了不了解机器学习，另外一方面来自于大学靠的是外部刺激和目标。现在需要尝试的是突破业务理解？</li></ul></li><li>而内驱力更能适应变化，他们能根据自己的兴趣不断地拓展边界，打破旧有规则</li><li>和最终在谷歌成功概率相关的问题：你几岁开始拥有自己的电脑？</li><li>谷歌的这两条经验,本质上都指向一个问题:一个人的驱动力是从哪里来的?是来自他自己的兴趣或者对自己的要求,还是来自外界什么人给他设定的标准？</li><li>瞬间反应和回答问题的思路。研究的标的、怎么使用这些时间、达到什么目标？</li><li>因为了解个人的内驱力高低,基本上就能判断出来这个人未来应对变化的能力。当环境对他提出新的要求时,他的抗压性强不强,能不能主动适应变化,就能从这个问题中反映出来了。</li></ul></li><li>**期望值：**你正在做的事，行业里最顶尖的人或公司是谁，他们是怎么做的?<ul><li>主要是看眼界如何，视野开阔与否。能不能把内驱力转化为行动力</li><li>对自己领域的理解</li><li>对高手的定义决定了他的认知，定义的过程能看到研究的过程及人脉情况</li><li>关键是如何定义和标杆的差异，以及差异如何形成及缩短方式。</li></ul></li><li><strong>人际</strong>：你在此之前的人生经历中，做过什么重要的取舍?<ul><li>如果你做重要取舍都是一个人，是不是验证你是一个孤独的人？</li><li>核心是看决策机制的形成，了解进退感和分寸，是否有清晰的边界意识。</li><li>为什么做出这种选择？出发点是什么？为什么这个时候做？你能不能清晰地界定选择的代价是什么？在你做出这些选择的前后都发生了什么，分别怎么解决的？</li><li>重大决策都会有一两个关键人的影响，关注是否有其他相关人被提及，以及这些人在决策中的角色，来判断他的关系网络</li><li>啰嗦的人无法带大队伍，因为边界不清晰</li></ul></li><li>**反思：**针对你刚才提到过的这件事，如果你有机会能重新做一遍，会有哪些地方不一样?<ul><li>反思能力，自己经历过的事情是否有清醒的觉察，评估他对待机会的敏感度。</li><li>对过往是否有总结和复盘，以及对机会的敏感度。颗粒度越细，反思越深。</li><li>面试中很难靠一面之词分清哪些是团队的水平，哪些是个人的贡献。通过反思，根据提到的颗粒度，能够判断，他在项目中究竟起到了多大的作用。</li></ul></li><li>对一个人的内驱力、关系建立能力、目标感和反思能力都有充分的了解能更好帮助做出准确的判断。</li><li>很幽默以及不温和</li><li>最后推荐了 [[奈飞文化手册]]</li></ul></li></ul><h2 id="商业"><a class="markdownIt-Anchor" href="#商业"></a> 商业</h2><ul><li><a href="https://mp.weixin.qq.com/s/0tUYITGjZjFb7JLU4F-MUw">谁在驱动滴滴发动机？</a><ul><li>平台治理</li><li>[[有限与无限的游戏]]</li><li>叶杰平来滴滴的两道面试题：<ul><li>“一道题目，跟出行里抢单、派单相关，问我能不能抽象成数学问题。”</li><li>能不能把抢单到派单问题，具体建立成一套算法模型？</li><li>纪念一下人已经走了。</li></ul></li></ul></li><li>失去字节技术中台支持的 TikTok，还会是曾经那个 TikTok 吗？ [[The Information]]<ul><li>美国政府：威胁国家安全和违反数据隐私。</li><li>国内以什么理由封杀部分软件？美国是实行对等的权力吗？中国和美国之间的战争一部分，没有一个人是无辜的。另外，没有看到国内政府在这一件事情上发表任何的声明？</li><li>联系一下，这件事情对滴滴的国际化会产生什么样的影响？滴滴会不会被更加宽容的处理？还是不要抱有幻想？</li><li>从监管层到潜在买家，试图改写 TikTok 未来的角色变多了[[The Information]]</li></ul></li></ul><h2 id="算法"><a class="markdownIt-Anchor" href="#算法"></a> 算法</h2><p>这个月看起来，看得论文和文章比较少。ai-labs 单机 93w QPS 的模型承保我好几点的笑点。</p><h3 id="滴滴kdd2020论文六-滴滴公开eta新系统线上推理速度进入微秒时代"><a class="markdownIt-Anchor" href="#滴滴kdd2020论文六-滴滴公开eta新系统线上推理速度进入微秒时代"></a> <a href="https://mp.weixin.qq.com/s/quJq0bvTs0qpwYUglQrUfw">滴滴KDD2020论文(六) | 滴滴公开ETA新系统，线上推理速度进入微秒时代</a> [[CompactETA]]  #ETA</h3><ul><li>之前写过他们原来的 ETA 模型文章： <a href="https://xiang578.com/post/wdr.html">(WDR) Learning to Estimate the Travel Time | 算法花园</a>。</li><li>数据稀疏<ul><li>空间稀疏：link 历史数据少<ul><li>基于路况分布来度量不同 link 的相似性，利用 metric learning 进行训练</li></ul></li><li>时间稀疏：相邻时段的 embedding 设置共享参数，使得相邻时段的 embedding 更加相似</li></ul></li><li>如何解决线上预测耗时？<ul><li>[[GAN]] 替代 [[LSTM]]，link 之间的依赖关系通过学习路网的拓扑结构来建立</li><li>位置编码：保持序列信息</li><li>查表 + g 然后过 MLP</li><li><img src="https://media.xiang578.com/compacteta.png" alt="" /></li></ul></li></ul><h3 id="算法工程师技术路线图-知乎"><a class="markdownIt-Anchor" href="#算法工程师技术路线图-知乎"></a> <a href="https://zhuanlan.zhihu.com/p/192633890">算法工程师技术路线图 - 知乎</a></h3><ul><li>[[Python]]：[[Learn Python the Hard Way]] [[流畅的 Python]]<ul><li>能读懂 panads、sklearn 等包的源代码</li></ul></li><li>[[Scala]]<ul><li>Spark快速大数据分析</li><li>[[Scala函数式编程]]</li><li>[[冒号课堂]]</li></ul></li><li>[[cpp]] 能够读懂[[LightGBM]]里对于tweedie loss的相关定义代码。</li><li><a href="https://refactoringguru.cn/design-patterns">常用设计模式有哪些？</a></li></ul><h2 id="embedding-技术的非端到端学习方法-知乎"><a class="markdownIt-Anchor" href="#embedding-技术的非端到端学习方法-知乎"></a> <a href="https://zhuanlan.zhihu.com/p/188569580">Embedding 技术的非端到端学习方法 - 知乎</a></h2><ul><li>下载记录变成一个 session</li><li>随机游走扩充数据</li><li>#Airbnb 全局 context [[Real-time Personalization using Embeddings for Search Ranking at Airbnb]]</li><li>同一个类别的随机负样本，分类成本不高。</li><li><img src="https://media.xiang578.com//tencent-app-store-embedding.png" alt="" /></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;某一刻突然意识到可以选择自己的记忆，我顿悟了。&lt;/p&gt;</summary>
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 Imitation Learning</title>
    <link href="https://blog.xiang578.com/post/reinforce-learnning-basic-imitation-learning.html"/>
    <id>https://blog.xiang578.com/post/reinforce-learnning-basic-imitation-learning.html</id>
    <published>2020-09-06T15:14:47.000Z</published>
    <updated>2023-03-28T03:58:52.116Z</updated>
    
    <content type="html"><![CDATA[<p>我的笔记汇总：</p><ul><li><a href="https://xiang578.com/post/reinforce-learnning-basic.html">Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html">Actor Critic</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html">Sparse Reward</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html">Imitation Learning</a></li></ul><p>apprenticeship learning</p><ol><li>无法从环境中获得 reward。</li><li>某些任务中很难定义 reward。</li><li>人为设计的奖励可能导致意外的行为。</li></ol><p>学习专家的行为。</p><h2 id="behavior-cloning"><a class="markdownIt-Anchor" href="#behavior-cloning"></a> Behavior Cloning</h2><p>监督学习，但是样本有限。</p><p>Dataset Aggregation</p><ol><li>通过行为克隆得到 actor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\pi_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li><li>利用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\pi_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和环境交互得到一些新的样本</li><li>由专家对上一步采样得到的样本进行标注</li><li>利用新得到的样本训练 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\pi_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li></ol><p>如果机器的学习能力有限，可能复制专家多余无用的动作。监督学习无法区分哪些是需要学习、哪些是需要忽视的行为。</p><h3 id="miss-match"><a class="markdownIt-Anchor" href="#miss-match"></a> Miss match</h3><p>监督学习中，我们假设训练数据和测试数据有相同的分布。Behavior Cloning 中可能分布不同。</p><p><img src="https://media.xiang578.com/15733541795048.jpg" alt="" /></p><h2 id="inverse-reinfofcement-learning"><a class="markdownIt-Anchor" href="#inverse-reinfofcement-learning"></a> Inverse Reinfofcement Learning</h2><p>反向强化学习<br />没有 reward 函数，通过专家和环境互动学到一个 reward function，然后再训练 actor。<br /><img src="https://media.xiang578.com/15733545279563.jpg" alt="" /></p><p>类似于 GAN 的训练方法（actor 换成 generator，reward function 换成 discriminator）。<br />学到 actor 的 pi 后，调整 reward function，保证专家的行为得分大于学到的行为。</p><p><img src="https://media.xiang578.com/15733546273431.jpg" alt="" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我的笔记汇总：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://xiang578.com/post/reinforce-learnning-basic.html&quot;&gt;Policy Gradient、PPO: Proximal Policy Optimization</summary>
      
    
    
    
    <category term="智能路" scheme="https://blog.xiang578.com/categories/%E6%99%BA%E8%83%BD%E8%B7%AF/"/>
    
    
    <category term="algorithm" scheme="https://blog.xiang578.com/tags/algorithm/"/>
    
    <category term="Reinforcement Learning" scheme="https://blog.xiang578.com/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 Sparse Reward</title>
    <link href="https://blog.xiang578.com/post/reinforce-learnning-basic-sparse-reward.html"/>
    <id>https://blog.xiang578.com/post/reinforce-learnning-basic-sparse-reward.html</id>
    <published>2020-09-06T14:14:47.000Z</published>
    <updated>2023-03-28T03:58:52.116Z</updated>
    
    <content type="html"><![CDATA[<p>我的笔记汇总：</p><ul><li><a href="https://xiang578.com/post/reinforce-learnning-basic.html">Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html">Actor Critic</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html">Sparse Reward</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html">Imitation Learning</a></li></ul><h2 id="reward-shaping"><a class="markdownIt-Anchor" href="#reward-shaping"></a> Reward Shaping</h2><p>如果 reward 分布非常稀疏的时候，actor 会很难学习，所以刻意设计 reward 引导模型学习。</p><h3 id="curiosity-intrinsic-curiosity-module-icm"><a class="markdownIt-Anchor" href="#curiosity-intrinsic-curiosity-module-icm"></a> Curiosity Intrinsic Curiosity module (ICM)</h3><p>在原来 Reward 函数的基础上，引入 ICM 函数。ICM 鼓励模型去探索新的动作。最后 ICM 和 Reward 和越大越好。</p><p><img src="https://media.xiang578.com/15732008339393.jpg" alt="" /></p><p>鼓励探索新动作之后，会导致系统风险变大。对比预测的下一个状态和真正的状态的差异程度进行抑制。</p><p><img src="https://media.xiang578.com/15732012078318.jpg" alt="" /></p><p>Feature Ext 对状态进行抽取，过滤没有意义的内容。<br />Network 1 预测下一个状态，然后再和真实状态计算 diff 程度。<br />Network 2 预测 action，和真实的 action 进行对比。如果两个 action 接近，说明 f 可以进行特征提取。重要程度计算。</p><p><img src="https://media.xiang578.com/15732013249053.jpg" alt="" /></p><h3 id="curriculum-learning"><a class="markdownIt-Anchor" href="#curriculum-learning"></a> Curriculum Learning</h3><p>规划学习路线，从简单任务学习。</p><p>Reverse Curriculum Generation</p><p><img src="https://media.xiang578.com/15732019855111.jpg" alt="" /></p><p><img src="https://media.xiang578.com/15732020592680.jpg" alt="" /></p><h3 id="hierarchical-reinforcement-learning"><a class="markdownIt-Anchor" href="#hierarchical-reinforcement-learning"></a> Hierarchical Reinforcement Learning</h3><p>对 agent 分层，高层负责定目标，分配给底层 agent 执行。如果低一层的agent没法达到目标，那么高一层的agent会受到惩罚（高层agent将自己的愿景传达给底层agent）。</p><p>如果一个agent到了一个错误的目标，那就假设最初的目标本来就是一个错误的目标（保证已经实现的成果不被浪费）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我的笔记汇总：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://xiang578.com/post/reinforce-learnning-basic.html&quot;&gt;Policy Gradient、PPO: Proximal Policy Optimization</summary>
      
    
    
    
    <category term="智能路" scheme="https://blog.xiang578.com/categories/%E6%99%BA%E8%83%BD%E8%B7%AF/"/>
    
    
    <category term="algorithm" scheme="https://blog.xiang578.com/tags/algorithm/"/>
    
    <category term="Reinforcement Learning" scheme="https://blog.xiang578.com/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 Actor Critic</title>
    <link href="https://blog.xiang578.com/post/reinforce-learnning-basic-actor-critic.html"/>
    <id>https://blog.xiang578.com/post/reinforce-learnning-basic-actor-critic.html</id>
    <published>2020-09-06T13:14:47.000Z</published>
    <updated>2023-03-28T03:58:52.116Z</updated>
    
    <content type="html"><![CDATA[<p>我的笔记汇总：</p><ul><li><a href="https://xiang578.com/post/reinforce-learnning-basic.html">Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html">Actor Critic</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html">Sparse Reward</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html">Imitation Learning</a></li></ul><h2 id="actor-critic"><a class="markdownIt-Anchor" href="#actor-critic"></a> Actor Critic</h2><h3 id="policy-gradient"><a class="markdownIt-Anchor" href="#policy-gradient"></a> policy gradient</h3><ul><li>给定在某个 state 采取某个 action 的概率。</li><li>baseline b 的作用是保证 reward 大的样本有更大的概率被采样到。</li><li>从当前时间点累加 reward，并且当前 action 对后面的 reward 影响很小，添加折扣系数。</li><li>PG 效果受到采样数量和质量影响。</li></ul><p><img src="https://media.xiang578.com/15731332476541.jpg" alt="" /></p><h3 id="q-learning"><a class="markdownIt-Anchor" href="#q-learning"></a> Q-learning</h3><p>状态价值函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mi>π</mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V^{\pi}(s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span><br />状态行动价值函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>π</mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q^{\pi}(s,a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span></p><p><img src="https://media.xiang578.com/15731335120798.jpg" alt="" /></p><h3 id="actor-critic-2"><a class="markdownIt-Anchor" href="#actor-critic-2"></a> Actor-Critic</h3><p>用 V 和 Q 替换 PG 中的累积 reward 和 baseline。新的模型需要训练两个网络，比较困难。</p><p><img src="https://media.xiang578.com/15731338724158.jpg" alt="" /></p><h3 id="advantage-actor-critic"><a class="markdownIt-Anchor" href="#advantage-actor-critic"></a> Advantage Actor-Critic</h3><p>用 V 去替代 Q，能降低模型整体方差（MC 到 TD)。最下面两个公式转化是由实验得到。</p><p><img src="https://media.xiang578.com/15731340046945.jpg" alt="" /></p><p>训练过程：</p><p><img src="https://media.xiang578.com/15731341766172.jpg" alt="" /></p><p>tip:</p><ol><li>actor 和 critic 具有相同的输入 s，可以共享部分网络结构。</li><li>output entropy 作为 pi 的正则项，entropy 越大采样效果越好。</li></ol><p><img src="https://media.xiang578.com/15731342994690.jpg" alt="" /></p><h3 id="asynchronous-advantage-acotr-critic-a3c"><a class="markdownIt-Anchor" href="#asynchronous-advantage-acotr-critic-a3c"></a> Asynchronous Advantage Acotr-Critic A3C</h3><ol><li>利用多个 worker 去训练。</li><li>每个 worker 复制主模型的参数。</li><li>每个模型单独采样，并且计算梯度。</li><li>更新全局参数。</li></ol><h3 id="pathwise-derivative-policy-gradient"><a class="markdownIt-Anchor" href="#pathwise-derivative-policy-gradient"></a> Pathwise derivative policy gradient</h3><p>该网络不仅仅告诉 actor 某一个 action 的好坏，还告诉 actor 应该返回哪一个 action。</p><p><img src="https://media.xiang578.com/15731346701840.jpg" alt="" /></p><p>将这个 actor 返回的 action 和 state 一起输入到一个固定的 Q，利用梯度上升更新 actor。</p><p><img src="https://media.xiang578.com/15731348191409.jpg" alt="" /></p><p>完整的训练过程和 conditional GAN 类似， actor 是 generator，Q 是 discriminator。</p><p><img src="https://media.xiang578.com/15731350424130.jpg" alt="" /></p><p>算法：</p><ol><li>action 由训练的 actor 决定</li><li>利用 s 和 a 更新 Q<br /><img src="https://media.xiang578.com/15731351315860.jpg" alt="" /></li></ol><h3 id="gan-和-ac-方法对比"><a class="markdownIt-Anchor" href="#gan-和-ac-方法对比"></a> GAN 和 AC 方法对比</h3><p><img src="https://media.xiang578.com/15731353632450.jpg" alt="" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我的笔记汇总：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://xiang578.com/post/reinforce-learnning-basic.html&quot;&gt;Policy Gradient、PPO: Proximal Policy Optimization</summary>
      
    
    
    
    <category term="智能路" scheme="https://blog.xiang578.com/categories/%E6%99%BA%E8%83%BD%E8%B7%AF/"/>
    
    
    <category term="algorithm" scheme="https://blog.xiang578.com/tags/algorithm/"/>
    
    <category term="Reinforcement Learning" scheme="https://blog.xiang578.com/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>【Never Reading】 202007 互联网商业模式</title>
    <link href="https://blog.xiang578.com/post/Never-Reading-202007.html"/>
    <id>https://blog.xiang578.com/post/Never-Reading-202007.html</id>
    <published>2020-08-08T15:55:11.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>不知不觉中每月分享已经进行半年，不过前 6 期都没有想到取什么名字。上期的标题「Never Reading」来自稍后读列表名称，仔细一想不正好成为每月分享的名字吗？而且还有致敬「Λ-Reading」的成分。</p><h2 id="互联网商业模式"><a class="markdownIt-Anchor" href="#互联网商业模式"></a> 互联网商业模式</h2><p>在 <a href="http://xiang578.com/post/monthly-issue-202006.html">202006 Never Reading</a> 中摘录过「即刻半月刊」的一段内容：</p><ul><li>所谓“商业模式”其实指的是这家公司的“价值创造模式”，即用什么样的模型创造了更多价值。<ul><li>世界上现存所有的商业模式无非三种，一是[[边际效应]]（规模效应/协同效应），二是[[双边效应]]，三是梅特卡夫[[网络效应]]。不同的价值创造模型，带来不同的增长动力，继而带来不同的货币化方法。</li><li>滴滴是什么模型？</li></ul></li></ul><p>当时没有找到上面这一段内容的解释，只是觉得有点神奇就记录下来。这个月收听「三五环」中刘飞和少楠关于交易平台两期内容（14、17），其中提到许小年教授的一本书「商业的本质和互联网」。在书中详细的介绍商业平台的效应，感兴趣的可以找来详细的阅读。</p><ul><li>[[规模效应]]：[[边际成本]]越低和边际收益越高</li><li>[[协同效应]]：依赖于品种增加带来的 1+1 &gt; 2<ul><li>百货公司拥有协同效应</li><li>#problem 协同效应失败的例子？</li></ul></li><li>[[双边市场效应]]<ul><li>双边供需的进入，都会有正外部性。公式：V=k<em>m</em>n</li><li>电商平台的效应弱：需要平台来管控质量，即变成了单边的；滴滴的效应强：司机和乘客的增加，都会带来正向效应（但边际收益未必持续提升）。</li></ul></li><li>[[梅特卡夫效应]]<ul><li>一个网络的价值与用户量的平方成正比。与常见网络效应的概念基本相同。任何用户的进入，都会有正外部性。</li><li>社交网络</li><li>曾李青定律：V=k*n²/r²（r 受 T、S、I、C 影响）。</li></ul></li></ul><p>读完之后，滴滴是什么模型这个问题就迎刃而解。这个月 有一个比较火视频  <a href="https://www.bilibili.com/video/BV1UT4y1j7Ht">复盘出行大战：被BAT选中的滴滴，如何完成垄断霸业_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a>，介绍滴滴创业的故事。做为一名内部人士，也被里面的内容给震撼到。之前沈南鹏在吴海波的采访中说过一句话「未来十年看滴滴」。记录自己看到的三个细节：</p><ul><li>滴滴接受腾讯投资后，程维和王刚彻底关上和阿里的联系。</li><li>和快的补贴大战中：马化腾建议每次补贴金额在 12-20 中间的一个随机数</li><li>和 uber 大战中：腾讯封禁 uber 在微信上的微信号</li></ul><p>有一个梦想是能看到程维的传记，之前特意查过他的花名 —— 常遇春。知乎热榜这个月出现过一个奇怪的问题，里面有一段引用：</p><blockquote><p>王保保这个“奇男子”的称号，是朱元璋给的，评价较帝国双壁之一常遇春更高，含金量十足：朱元璋曾大会诸将，问道：“天下奇男子谁也？”诸将都说：“[[常遇春]]将不过万人，横行无敌，真奇男子。”明太祖笑曰：“遇春虽人杰，吾得而臣之。吾不能臣王保保，其人奇男子也。”（《明史》-《扩廓帖木儿传》）</p></blockquote><h2 id="商业"><a class="markdownIt-Anchor" href="#商业"></a> 商业</h2><ul><li>阿里很强调价值观，但[[阿里巴巴]]首先是一家商业组织。</li><li>上学时，[[黄峥]]就意识到了机会可贵：“我在上学时就意识到几个事：一是寒门出贵子是小概率事件，大部分富二代，尤其是官二代非常优秀。二是田忌赛马，在整体资源劣势的情况下可以创造出局部优势，进而有机会获得整个战役的胜利。基于此，平凡人可以成就不凡事。第三是钱是工具，不是目的。”</li><li>趣头条本质上是一款游戏产品，只不过披着内容信息流的外衣。<ul><li>不过这一些信息流产品只有学习用户兴趣，真正的影响用户行为还是要靠基于强化学习开发的游戏。</li></ul></li><li>字节系广告变现涉及4个角色：内容消费者，内容生产者，平台，广告主；阿里系广告变现涉及3个角色：消费者，商家，平台；所以阿里的广告变现链条更短，一定程度上也是效率更高的原因。多一个角色，多一份成本，最终效果要多乘一个参数。</li><li><a href="https://medium.learningbyshipping.com/apples-relentless-strategy-and-execution-7544a76aa26">Apple’s Relentless Strategy, Execution, and Point of View | by Steven Sinofsky | Jun, 2020 | Learning By Shipping</a>  Steven Sinofsky 是前微软 Windows 业务部门的总裁评价苹果架构迁移。<ul><li>其实没有仔细看这一篇文章。震惊到我的是，这位兄弟管理过 20000 人的工程团队？不知道现在国内有哪一些公司的 CTO 能管理这么多的人。</li></ul></li><li><a href="https://sspai.com/post/61174">从苹果的系统更新，理解设计中的「控制」与「自由」 - 少数派</a><ul><li>雪城大学建筑学教授理查德·洛萨（Rhichard Rosa）认为**“设计的本质即是在于控制与自由。”**这句话非常简洁但直接地介入了设计的核心——设计行为为混乱与无序赋予秩序，使之得以承载可控的人类行为；但设计也一定不是绝对的控制，它一定要为使用者预留一些自由。</li><li>设计师对美学的自信会落脚在对细节的强制：贝聿铭设计京都东郊的美秀美术馆，下雨天需要领取相应颜色的雨伞才能进入。</li><li><strong>有些人说：“消费者想要什么就给他们什么。”</strong> <strong>人们不知道想要什么，直到你把它摆在他们面前</strong></li><li><strong>设计一定需要控制，但一定不是绝对的控制。平台的搭建者需要预留一些自由给开发者以及用户，对秩序的全盘接手最终会导致自下而上民意的反弹。</strong></li><li>[[设计的哲学]]，设计的背后是不是需要有哲学？一味满足用户需求的软件是不是会变得十分的复杂？比如 Emacs 之类自由度高的软件？</li></ul></li><li><a href="https://www.jianshu.com/p/03080891e4ba">顶级PM的产品观：王慧文看行业-合集 - 简书</a><ul><li>互联网 AB 面：A类是供给和履约在线上，B类是供给和履约在线下。</li><li>B类又可以分为：以SKU为中心的供给B1和以Location为中心的服务B2。</li><li>A 类能力体现在产品设计领域，体现在用户理解上，体现在对于通讯、社交以及内容把握上。</li><li>B1里面，主要体现在对于品类的理解，对于供应链的理解，对于定价的理解。</li><li>B2里面，如果你们去盘点一下B2的公司，他们总体来说有一个比较共有的特征，大规模的线下团队。</li><li>是否有大规模的线下团队是B1和B2一个很大的差别。</li><li>在LBS的方向上，中国和美国的企业这样的差距是怎么样发生的？大概有四个因素决定：人力成本、人口密度、人口规模、代际竞争。</li></ul></li></ul><p><img src="https://media.xiang578.com/a-b1-b2.png" alt="互联网 AB 面" /></p><h2 id="阅读"><a class="markdownIt-Anchor" href="#阅读"></a> 阅读</h2><ul><li>理解世界的一个有效方法是，在人生的某个阶段，把任何之前视为理所当然的事情全都重新研究与思考一遍，并弄清楚它们运行的真正起源与机理。在这个过程中，自问的问题越基础、越显得不需要去质疑，收获往往就会越多——人为什么要每天吃三顿饭、买东西为什么要花钱、书籍和文章为什么会存在——真理通常就藏在这些大多数人想都不会想的事情里。[[张潇雨]]<ul><li>「银河系漫游指」里面有一句：任何在我出生时已经有的科技都是稀松平常的世界本来秩序的一部分。</li></ul></li><li>什么是第一原理？<ul><li>[[亚里士多德]] 在[[形而上学]]中提出这个哲学概念，指「公理：无法再分、无法证明且不证自明的命题。」</li><li>“第一原理”本身并不是什么原理，它只是个简称。准确地说，应该叫做“从第一原理推理”（reasoning by first principle），是<strong>分析问题，找出其不可继续拆分的根本原因，即第一原理，再从第一原理反推出解决方案的思考方式。</strong></li><li>第一原理是先验</li></ul></li></ul><h2 id="算法"><a class="markdownIt-Anchor" href="#算法"></a> 算法</h2><ul><li><a href="https://www.tanglei.name/blog/shuffle-algorithm.html">面试官：会玩牌吧？给我讲讲洗牌算法和应用场景吧！ | 唐磊的个人博客</a> [[洗牌算法]]<ul><li>之前的分享中写房租分配 <a href="https://xiang578.com/post/week-issue-9.html">每周分享第 9 期：拼多多 | 算法花园</a></li><li>保证每次的概率是相同</li><li>#[[problem]]实现以下算法：一组数，每次不放回抽样，得到一个随机序列。白板编程。分析时间空间复杂度。<ul><li>follow up：能否时间O(n)完成，能否空间O(1)完成。</li></ul></li></ul></li><li><a href="https://www.youtube.com/watch?v=733m6qBH-jI">Stanford CS230: Deep Learning | Autumn 2018 | Lecture 8 - Career Advice / Reading Research Papers - YouTube</a>：Ag 在课程中介绍如何阅读论文，又一次感受到大佬的真诚。<ul><li>主题阅读<ul><li>收集资料</li><li>列出一个 list ，标注阅读进度。挑选有价值的论文阅读。</li><li>5-20 初步了解</li><li>50-100 很好理解前沿工作</li></ul></li><li>如何阅读论文<ul><li>多遍阅读</li><li>第一遍：标题，摘要，图片</li><li>第二遍：简介、结论、图片相关材料</li><li>第三遍：进入论文主体部分，但是可以跳过数学，明白每个参数的含义。</li><li>第四遍：阅读整篇文章，跳过没意义的部分(内容过时，没有火起来过)。</li></ul></li><li>阅读时思考的问题<ul><li>作者试图解决什么问题？</li><li>研究方法的关键是什么？（最具有开创性）</li><li>哪些东西可以为你所用？</li><li>有哪些参考文献可以继续跟进？</li></ul></li><li>最后发现一篇相关实践文章： <a href="https://towardsdatascience.com/how-you-should-read-research-papers-according-to-andrew-ng-stanford-deep-learning-lectures-98ecbd3ccfb3">How You Should Read Research Papers According To Andrew Ng (Stanford Deep Learning Lectures) | by Richmond Alake | Jul, 2020 | Towards Data Science</a></li></ul></li><li>另外一篇和论文阅读有关的文章：<a href="https://zhuanlan.zhihu.com/p/163227375">沈向洋、华刚：读科研论文的三个层次、四个阶段与十个问题 - 知乎</a><ul><li>三个层次：速度、精读与研读</li><li>四个阶段：Passive Reading、Active Reading、Critical Reading、Creative Reading</li></ul></li></ul><p>这就是本期的 「Never-Reading」，我们下个月再见。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;不知不觉中每月分享已经进行半年，不过前 6 期都没有想到取什么名字。上期的标题「Never Reading」来自稍后读列表名称，仔细一想不正好成为每月分享的名字吗？而且还有致敬「Λ-Reading」的成分。&lt;/p&gt;
&lt;h2 id=&quot;互联网商业模式&quot;&gt;&lt;a class=&quot;ma</summary>
      
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
  </entry>
  
  <entry>
    <title>【每月分享】 202006 Never Reading</title>
    <link href="https://blog.xiang578.com/post/monthly-issue-202006.html"/>
    <id>https://blog.xiang578.com/post/monthly-issue-202006.html</id>
    <published>2020-07-14T15:55:11.000Z</published>
    <updated>2023-03-28T03:58:52.112Z</updated>
    
    <content type="html"><![CDATA[<p>这一份 6 月的阅读总结来的有一点晚。前几年一直断断续续在实践 GTD，人的拖延症超乎想象，学到的一个经验是 「Now or Never」。所以，这个月将自己的阅读列表取名为「Never Reading」。</p><h2 id="笔记方法精进"><a class="markdownIt-Anchor" href="#笔记方法精进"></a> 笔记方法精进</h2><p>Roam Research 引起现在这一波 Backlink 笔记软件浪潮，本月依然阅读一些和笔记方法相关的文章。</p><h3 id="我的-zettelkasten-卡片盒笔记法实践-吕立青的博客"><a class="markdownIt-Anchor" href="#我的-zettelkasten-卡片盒笔记法实践-吕立青的博客"></a> 我的 Zettelkasten 卡片盒笔记法实践 | 吕立青的博客</h3><p><a href="https://blog.jimmylv.info/2020-06-03-zettelkasten-in-action/">文章链接</a></p><p>上个月介绍过，自己已经从 Roam Research 迁移到 Obsidian，然后就看到吕立青这一篇基于 Obsidian 的 Zettelkasten 实践。文章中将卡片盒笔记分拆分成四步：</p><blockquote><p>1️⃣ 第一步：必须用自己的话写笔记卡片，以确保你将来能够理解。</p><p>2️⃣ 第二步：无论何时添加新笔记，主动查找可链接到的已有笔记。</p><p>3️⃣ 第三步：通过添加新记录并联系起来，延续这一系列的连续思考。</p><p>4️⃣ 第四步：使用 Anki 间隔重复加深记忆，主动由大脑触发远程联想。</p></blockquote><p>搞笑地是，在他写出这篇文章没有多久之后，已经开始尝试往 Roam Research 上迁移……另外，为了更好在国内推广，他参与发起 <a href="roam-cn.github.io">roam/cn</a> 组织，从英文世界翻译一些推特、视频以及分享一些个人的案例。</p><h3 id="zettelkasten-note-taking-in-10-minutes"><a class="markdownIt-Anchor" href="#zettelkasten-note-taking-in-10-minutes"></a> Zettelkasten note-taking in 10 minutes</h3><p>两条原则：</p><ul><li>Don’t try to get this method perfect from the get go.</li><li>The advanced practices are useful only when you’ve got close to 1000 notes</li></ul><h3 id="my-productivity-app-for-the-past-12-years-has-been-a-single-txt-file"><a class="markdownIt-Anchor" href="#my-productivity-app-for-the-past-12-years-has-been-a-single-txt-file"></a> My productivity app for the past 12 years has been a single .txt file</h3><p>12 年间使用一个 txt 进行任务管理方法分享。作者提到 to do list 变成 what done list 的过程，每天晚上将日历中第二天的代办事项整理到 txt 中，第二天顺手记录任务相关的信息（比如讨论出的结论，或者得到的信息）。结合自己使用经历，OmniFocus 是一个 to do list，基于纯文本的任务管理方式（org-mode 或 taskpaper）更容易成为 what done list。</p><h2 id="商业"><a class="markdownIt-Anchor" href="#商业"></a> 商业</h2><ul><li><a href="https://36kr.com/p/736767189897093">滴滴重踩油门_详细解读_最新资讯_热点事件_36氪</a> 不开玩笑地说，看完这里面的分析，我才理解公司很多的战略。</li><li><a href="https://www.notion.so/c074ab234e2b4dbb9143a48faceec031">淘宝宣战拼多多的前夜：吕晋杰、陈琪、徐易容和葛永昌的至暗时刻</a> 和很多同学聊天，17 年找工作的时候低估了拼多多的潜力。<ul><li>淘宝 PC 转移动互联网时，流量入口从网页短 300 个减少到手淘 app 48 个。是不是能解释淘宝这几年涌现出那么多的推荐系统相关的问题。</li><li>电视广告投放策略以及如何理解流量暴涨，拼多多上一直很火爆的「百亿补贴」</li><li>任何人都可以成为英雄，哪怕是做了一件不起眼的事情。</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/146363597">快手的普通，抖音的美好，算法的价值观 - 知乎</a><ul><li>算法没有价值观，算法只会实现设计者的意图。</li><li>主动降低算法效率，是为了实现某些短期无法衡量的业务目标。</li><li>抖音将流量集中在头部。</li><li>快手整体的点击率被牺牲，普通人的流量被保证。</li><li>抖音说记录美好生活，快手说记录世界记录你。</li></ul></li></ul><h2 id="阅读"><a class="markdownIt-Anchor" href="#阅读"></a> 阅读</h2><h3 id="人生总有一刻我们会开始思考死亡"><a class="markdownIt-Anchor" href="#人生总有一刻我们会开始思考死亡"></a> 人生总有一刻，我们会开始思考死亡</h3><p>茨威格在《人类的群星闪耀时》所说的那种幸运是什么 —— 「最大的幸运，莫过于在年富力强的时候，发现了自己的使命」</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/24640592">人生总有一刻，我们会开始思考死亡 - 知乎</a> [[张潇雨]]<br />于是我发现，面对死亡最终可能只有两种方法。</p><ul><li>一种是将自己与一些更宏大的东西联系起来：一个数学定理、一本文学著作、一件艺术作品或一种恒久的信仰。马尔克斯与康德靠《百年孤独》与《纯粹理性批判》遗世独立，米开朗基罗把《创世纪》和《最后的审判》印刻在西斯廷大教堂里，供千万后朝拜——他们肉身虽灭，但精神不朽——反正建筑是永远戳在那儿的。</li><li>还有一种就是，生活在当下的每个瞬间里，不烦扰过去、不担忧将来。</li></ul></blockquote><h3 id="即刻半月刊"><a class="markdownIt-Anchor" href="#即刻半月刊"></a> 即刻半月刊</h3><p>6.18？即刻重新开放，之前对这个社区没有太多印象，尝试关注一些人之后，信息流的质量也不错。「即刻半月刊」是某些爱好者挑选的一些即友发言集合。摘录一些我觉得有意思的内容。</p><ul><li>老罗带货首卖小米的中性笔：数量太少。之前看到过一个分析，抖音自己做电商最大的困难点在于没有用户的物流地址。简单想一下，用户可能在填写地址这一步流失。</li><li>@Rey_L 我在判断哪家公司的产品会换灰色皮肤的时候，基本都猜对了hhh，滴滴头条一定会做，京东会做，淘宝也许会做，拼多多肯定不做。</li><li>微信小程序减少 App Store 在国内的下载量。不过大部分巨头的小程序还是想要从微信中引流的。</li><li>底下有回复cite了一个working paper，大意说的是名校（elite education，这里用的是211高校与非211高校的断点回归）能够给学生带来的起薪上 30-45% 的提升，其原因在于名校当中的social network，elite的peer普遍家庭条件更好，成绩也更好。上网课带来不了这种social connection…所以说不单只是知识改变命运，环境也很重要</li><li>所谓“商业模式”其实指的是这家公司的“价值创造模式”，即用什么样的模型创造了更多价值。<ul><li>世界上现存所有的商业模式无非三种，一是[[边际效应]]（规模效应/协同效应），二是[[双边效应]]，三是梅特卡夫[[网络效应]]。不同的价值创造模型，带来不同的增长动力，继而带来不同的货币化方法。</li><li>滴滴是什么模型？</li></ul></li><li>A/B test只能做模型的小优化，但找不到大的绝对的增长点，所以除了推荐系统里的参数，即刻的很多产品决策希望尽量远离A/B test</li><li>张小龙的饭否，张一鸣的微博，黄峥的微信公众号，很可惜这几个都成了过去形态。现在只剩下王兴的饭否了。</li><li>一个反向小思考：兴趣社交刚需归刚需，社交app的本质还是信息分发。RSS订阅20周年，分发的有效性也不是什么相似推荐，^<sup>而是真正提供专注力的阅读</sup>^。因为个性推荐的本质就是专注力。信息廉价甩卖的今天，打开微博广场/抖音主页/朋友圈（广告），用户最需要的不是新鲜事而是减少噪音的阅读器，人永远看到自己想看到的，产品顺水推舟不也是更讨巧，所以需要设计者克制。沟通永远是双向的，当用户感到一个舒适被接纳的舆论场所，也会更加有表达欲。</li><li>芒果系这么多年都在做一件事，就是把握这个时代的情绪。[[乘风破浪的姐姐们]]</li></ul><h2 id="机器学习"><a class="markdownIt-Anchor" href="#机器学习"></a> 机器学习</h2><p>本月 KDD 2020 的文章应该已经放出，推荐阅读 Airbnb 深度模型实践相关的文章 Managing Diversity in Airbnb Search 以及 Improving Deep Learning For Airbnb Search。 7 月份希望能写一篇博客分享自己的阅读笔记。</p><ul><li><a href="https://www.zhihu.com/question/389912594">神经网络中对需要concat的特征进行线性变换然后相加是否好于直接concat? - 知乎</a> [[机器学习]]<ul><li>concat 没有信息融合，也没有信息缺失</li><li>add 不同向量之间的权重相同，会导致信息缺失。待融合的特征具有相同分布，或特征属于同一类，直接相加才有可能提升模型性能。</li><li>concat + MLP 能够自动学习不同 channel 的权重，MLP 能引入非线性。利于通道间的信息融合，计算量大。</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/24851814">【机器学习】Bootstrap详解 - 知乎</a>：随机森林里面用到的数据重采样方法，老板要求训练两个有差异模型时给我推荐的方法。</li><li><a href="https://zhuanlan.zhihu.com/p/148729018">BERT 可解释性-从&quot;头&quot;说起 - 知乎</a>：蛮有意思的对 BERT 组件效果分析，这位作者举的例子有些蹭热点。</li><li><a href="https://zhuanlan.zhihu.com/p/63982470">都9102年了，别再用Adam + L2 regularization了 - 知乎</a>：说明为什么要用 AdamW。另外推荐一下，之前一位同事写的 AdamW 实现：<a href="https://zhuanlan.zhihu.com/p/40814046">L2正则=Weight Decay？并不是这样 - 知乎</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这一份 6 月的阅读总结来的有一点晚。前几年一直断断续续在实践 GTD，人的拖延症超乎想象，学到的一个经验是 「Now or Never」。所以，这个月将自己的阅读列表取名为「Never Reading」。&lt;/p&gt;
&lt;h2 id=&quot;笔记方法精进&quot;&gt;&lt;a class=&quot;mar</summary>
      
    
    
    
    <category term="随想集" scheme="https://blog.xiang578.com/categories/%E9%9A%8F%E6%83%B3%E9%9B%86/"/>
    
    
    <category term="weekly" scheme="https://blog.xiang578.com/tags/weekly/"/>
    
  </entry>
  
</feed>
