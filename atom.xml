<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>算法花园</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xiang578.com/"/>
  <updated>2020-03-28T09:07:31.367Z</updated>
  <id>https://xiang578.com/</id>
  
  <author>
    <name>RyenX</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>每月分享 202003</title>
    <link href="https://xiang578.com/post/monthly-issue-202003.html"/>
    <id>https://xiang578.com/post/monthly-issue-202003.html</id>
    <published>2020-03-28T08:55:11.000Z</published>
    <updated>2020-03-28T09:07:31.367Z</updated>
    
    <content type="html"><![CDATA[<h2 id="读书">读书</h2><ul><li><a href="https://book.douban.com/subject/34672176/" target="_blank" rel="noopener">呼吸 (豆瓣)</a>：这是一本由 Byte.Coffee 主播 MilkShake 🐑 推荐的一本科幻小说集（前几天看其他东西的时候学会科幻小说的英文 sci-fi）。之前看到过，小说的价值在于作者用一个故事告诉你一个道理。最喜欢的是《商人和炼金术士之门》这篇：在传统的穿越小说无法改变未来和过去的基础上，论证穿越能更深刻理解生活。书中其他探讨的几个问题也很有价值，值得一读。</li></ul><h2 id="我看">我看</h2><ul><li><a href="https://space.bilibili.com/390461123/" target="_blank" rel="noopener">徐大sao的个人空间 - 哔哩哔哩 ( ゜- ゜)つロ 乾杯~ Bilibili</a>：最近挑着看完大 sao 做饭视频，被他展示的生活激情所吸引。</li></ul><h2 id="文章">文章</h2><ul><li><a href="https://writings.stephenwolfram.com/2019/02/seeking-the-productive-life-some-details-of-my-personal-infrastructure" target="_blank" rel="noopener">Seeking the Productive Life: Some Details of My Personal Infrastructure—Stephen Wolfram Writings</a>：这篇超长的文章是 Stephen Wolfram (Mathematica CEO) 介绍自己几十年在家办公的经验，包括如何搭建一个适合自己的工作环境、如何管理文件和个人数据等等。不过这次疫情期间在家办公，大部分同事反应最大的问题是沟通效率降低。另外想想，或许是我们在这方面的思考不够。总之，互联网行业居家办公才是光明的未来。</li><li><a href="https://fs.blog/knowledge-project/naval-ravikant/" target="_blank" rel="noopener">Naval Ravikant: The Angel Philosopher</a>：AngelList 的 CEO Naval Ravikant 的播客访谈，Naval 的公司投资 Uber Twitter 等 100 多家科技公司。主要介绍 Naval 的一些哲学，文字版在 <a href="https://fs.blog/wp-content/uploads/2017/02/Naval-Ravikant-TKP.pdf" target="_blank" rel="noopener">Naval-Ravikant-TKP.pdf</a><ul><li>利用 Kindle 阅读，遇到喜欢的书，购买实体书收藏。对比书的价格，从书中学到可以改变自己人生的内容更重要。</li><li>发现一个新的博客后，会在 Archived 页面挑选几篇仔细阅读。读书时也可以使用这个技巧。</li></ul></li><li><a href="https://superorganizers.substack.com/p/how-to-build-a-learning-machine" target="_blank" rel="noopener">How to Make Yourself Into a Learning Machine - Superorganizers</a>：一位高中毕业后离开丹麦来到加拿大创业公司工作人的自我学习之路。<ul><li>自我定位 T 型人才。</li><li>兴趣面广，主题阅读。</li><li>高亮关键内容，使用 anki 记忆，相关的想法用 zettelkasten 记录。</li><li>zettelkasten 使用纯文本 + 脚本实现。（如果只记录英文 vim 和 emacs 真的是很强大的软件）</li><li>利用谷歌检索的数量判断单词的重要性</li></ul></li><li><a href="https://fortelabs.co/blog/para/" target="_blank" rel="noopener">The PARA Method: A Universal System for Organizing Digital Information - Forte Labs</a>：介绍 PARA 这种数字信息整理方法。</li><li><a href="https://tim.blog/2020/01/08/reading-recommendations/" target="_blank" rel="noopener">The Best Books and Articles I Read in 2019 – The Blog of Author Tim Ferriss</a>：一篇 2019 年阅读总结文章。对作者介绍的阅读流程比较感兴趣：<ul><li>Evernote 搭配 web clipper 收集文章。 利用 <code>***</code> 以及高亮在文章中做笔记，方便之后进行快速回顾。</li><li>阅读 Kindle 格式的电子书，定期从亚马逊官网导出高亮笔记（国内不支持）。</li><li>利用 Readwise 回顾之前提到的高亮。</li><li>阅读实体书时，写简单的索引卡片，然后将卡片拍照导入 Evernote 中。</li></ul></li></ul><h2 id="机器学习">机器学习</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/111842425" target="_blank" rel="noopener">为什么有些深度学习网络要加入Product层？ - 知乎</a>：解释为什么 MLP 只包含特征累加而有学习特征交叉的能力，后面展开讲了一些提高模型特征交叉能力的方法。</li><li><a href="https://blog.csdn.net/u011508640/article/details/72815981" target="_blank" rel="noopener">详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解_网络_nebulaf91的博客-CSDN博客</a>：看过讲 MLE 和 MAP 比较清晰的文章。刚看开头的时候，想到自己大学上过《概率论和统计》居然没有考虑过概率和统计有什么区别……</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;读书&quot;&gt;读书&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/34672176/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;呼吸 (豆瓣)&lt;/a&gt;：这是一本由 Byte.Coffe
      
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
  </entry>
  
  <entry>
    <title>每月分享 202002 「山川异域，风月同天」</title>
    <link href="https://xiang578.com/post/monthly-issue-202002.html"/>
    <id>https://xiang578.com/post/monthly-issue-202002.html</id>
    <published>2020-03-07T07:45:15.000Z</published>
    <updated>2020-03-28T09:07:31.367Z</updated>
    
    <content type="html"><![CDATA[<p>现在看来，又有多少人预测到这一次超级黑天鹅事件。</p><a id="more"></a><blockquote><p>这里记录过去一个月，我看到、想到值得分享的东西。</p></blockquote><h2 id="x03-how-instapaper-changed-my-kindle-life-for-the-better">0x03 <a href="https://www.guidingtech.com/29107/instapaper-kindle-merits/" target="_blank" rel="noopener">How Instapaper Changed My Kindle Life For the Better</a></h2><p>利用 Instapaper 定时将稍后读文章发送到 kindle 上。</p><h2 id="x02-这是我为武汉雷神山火神山医院设计的品牌形象标志logo---步行街主干道---虎扑社区">0x02 <a href="https://bbs.hupu.com/32137599.html" target="_blank" rel="noopener">这是我为武汉雷神山、火神山医院设计的品牌形象标志logo - 步行街主干道 - 虎扑社区</a></h2><p><img src="/file/15807855350677.jpg"></p><h2 id="x01-山川异域风月同天">0x01 <a href="https://weibo.com/1218287234/Is1lGejd6?type=comment#_rnd1580519347991" target="_blank" rel="noopener">“山川异域，风月同天”</a></h2><blockquote><p><span class="citation" data-cites="扎宝">@扎宝</span>：日本汉语水平考试HSK事务所捐赠给武湖北的物资，20000个口罩和一批红外体温计。 标签上写着“山川异域，风月同天”，感动[泪][泪] 求一个英文译文！ p.s. 据记载鉴真事迹的历史典籍《东征传》记载：日本长屋亲王在赠送大唐的千件袈裟上绣“山川异域，风月同天，寄诸佛子，共结来缘”偈。鉴真大师被此偈打动，决心东渡弘法。</p></blockquote><blockquote><p><span class="citation" data-cites="文冤阁大学士">@文冤阁大学士</span>：We are created to share Nature and love. 扫了下原博评转，翻得都差我好几座唐招提寺。嘻嘻。</p></blockquote><p><img src="/file/15807341522123.jpg"></p><h2 id="x00-xgboost">0x00 <a href="https://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">XGBoost</a></h2><p>春节在家，重新把这些经典的内容再拿出来多读几遍。网上写的那些总结感觉都不是很好，还是要回去看论文。说句实话，纸上谈兵这么久，居然没有跑过 xgb 的包……</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现在看来，又有多少人预测到这一次超级黑天鹅事件。&lt;/p&gt;
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
  </entry>
  
  <entry>
    <title>2019 起步</title>
    <link href="https://xiang578.com/post/2019.html"/>
    <id>https://xiang578.com/post/2019.html</id>
    <published>2020-02-04T13:44:09.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>受 <a href="http://freemind.pluskid.org/" target="_blank" rel="noopener">Free Mind</a> 的影响按这种形式写年度总结</p></blockquote><p>年初的时候看到一句话：「 2019 是过去十年中最差的一年，也是未来十年中最好的一年」。和其他人一样，我害怕不确定性，不过生活除了鼓起勇气前进，还有什么其他选择。</p><h2 id="工作">工作</h2><p>完整在滴滴工作一年，自己没有太多变化，可是环境却变了很多。从年初内部会议上 Will 优化员工开始，很多同事陆续离开，从而我都快要成为团队元老……</p><p>做为食物链低端的算法工程师，工作中杂七杂八的事情干了很多。洗数据、跑模型、改工程代码、测试、上线、实验各个方面都干过。</p><p>说回来，算法还是自己的主要工具。今年用的最多的是 FM 和 GBDT，这些都是几年前的技术，但是架不住效果好，性能要求小。自己也写了一些相关的文章，可以供大家参考。</p><blockquote><p><a href="https://xiang578.com/post/fm.html">(FM) Factorization Machines | 算法花园</a></p><p><a href="https://xiang578.com/post/ftrl.html">(FTRL) Follow The Regularized Leader | 算法花园</a></p><p><a href="https://xiang578.com/post/gbdt.html">All About GBDT (1) | 算法花园</a></p><p><a href="https://xiang578.com/post/gbdt_lr.html">Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr) | 算法花园</a></p></blockquote><p>关于深度学习，在我入职前模型就基本迭代完成，今年主要探索个性化场景的解决以及模型性能优化。很遗憾，这两方面的工作目前还没有什么可以写成博客分享的。最后，自己没有参与到组内强化学习的项目中，不过还是通过李宏毅老师的相关课程了解初步的概念，争取 20 年内做一些相关的事情。</p><p>9月份开始，leader突然让我准备一些编程题目，开始去面试实习生。通过牛客网以及北邮人论坛大概收到简历60多份，我面试10多个候选人，最终通过的大概五六人，不过过来实习的也就 2 个。印证自己之前的想法，一家已经不是快速发展的公司，很难招到即懂机器学习又会做编程题的实习生。</p><p>另外想写的一点是是匿名交流。内部论坛之前有一个匿名区，后来由于一件比较有名的事情，匿名喷得太厉害，被某位海归高管以提高交流效率减少戾气所关闭（目前这位已经离职，有人开玩笑期待干掉他新公司的匿名论坛）。所幸脉脉还有职言（匿名）以及公司圈。在上面混了一年之后，越来越理解匿名交流的必要，说事。比如今年发生的延迟发年终奖，快手可以直接在内部匿名区引起宿华回复。我们的公司圈一堆人才自嗨。本质是国内环境下很难公开交流一些话题。</p><p>之前看过[一篇文章]中介绍 Google 的 TGIF：</p><blockquote><p>TGIF是Larry和Sergey在公司早期就创立的，一个全公司范围的周会。在这个周会上高管们会透露公司新项目的进展，也安排有答疑环节，员工可以询问两位创始人任何问题。TGIF毫无疑问是为了提高公司内部的透明度，但它在增强员工凝聚力的同时也对公司文化提出了挑战，最直接的就是保密问题。比如Chrome项目在公司内部的公开就是在一次TGIF上发生的，那时离Chrome的正式对外宣布早了一年多的时间。</p></blockquote><h2 id="阅读">阅读</h2><p>今年读过 <a href="https://book.douban.com/people/xiang578/collect?sort=time&amp;tags_sort=count&amp;filter=all&amp;tag=2019&amp;mode=grid" target="_blank" rel="noopener">33</a> 本书，阅读量和前几年基本持平。年底发现自己的一个坏习惯：很多书读了一半就放在那里，导致开的坑很多。应对方法也很简单：一段时间内只读一本书。而且为了提高阅读的质量，将自己读完一本书的定义从读到最后一页改成完成对这本书内容的整理。</p><p>阅读的主要工具是 kindle 和微信阅读（iPhone）。kindle 是这么多年一直使用的阅读介质，从前几年的找破解图书到现在的完全中亚购买（以目前看书的频率还不至于承受不住），长时间看电子水墨屏能减轻一些疲劳。微信阅读的特点是白领无限卡后就能全场免费读，实在是太香了。理想状态下用这两个工具读不类型的材料，微信阅读读小说以及人物传记，kindle 看需要大量抄记的书。对于需要反复阅读的内容，实体书则是最佳的选择。</p><p>分享读完觉得不错的几本书：</p><ol type="1"><li>经济学通识/薛兆丰经济学讲义：薛兆丰目前看起来风评不是很好，这两本也不是什么严肃的经济学读物。前一本书是作者专栏文章的合集，后一本是得到专栏的文字版，两本书大量的内容是重复的。书中通过现实中的例子来讲解背后的经济学原理，很适合看完之后做为饭后谈资。反正我运用书中的一些原理，给同事分析好久公司的停车场应该怎么分配车位。</li><li>银河帝国：这一套书有很多本，只看完前三本。概括起来，这本小说是以太空为背景讲政治故事，以谢顿的预言为主线，讲述基地对抗各种危机的挑战。另外书中提到看起来有多少分像统计学的心理史学，谢顿一直用这种方法预测未来，而且信徒们一直强调，预测结果不会因为个人而改变。第三本书，围绕寻找第二基地展开，把所有读者能猜的地方都写了出来，选择了一种情理之中，意料之外的结局。</li><li>人类简史/未来简史：尤瓦尔·赫拉利是前几年很火的一个历史学家。人类简史主要是按他的框架回顾从原始人类到现代人类的文明发展历程。对于我这种没有系统接受过历史学教育的人，完全是一种震撼。未来简史讨论的是人类未来的发展方向，成神。</li><li>房思琪的初戀樂園：讲述一个小女孩被文明所不齿的方式杀死的故事，最让人痛心的这女孩就是作者本人……引用最近很流行的一句话：地狱空荡荡，魔鬼在人间。</li><li>基督山伯爵：看完《了不起的盖茨比》后，老板强力推荐的爽文小说。快意恩仇，永远不要丧失对生活的期望。</li><li>临高启明：工科党神书，死于历史空无主义，最后放上来缅怀一下。</li></ol><p>2020 年开始使用 Notion 记录读书过程，点击 <a href="https://www.notion.so/ryanx/4666b7440155430880b9c9787adde5ab?v=39111f7ebd5e4be6a28d7ef712c4aebb" target="_blank" rel="noopener">看书也就图一乐</a> 查看。</p><h2 id="观影">观影</h2><p>和去年一样，看电影比起看书来更加容易，豆瓣上轻松标记 <a href="https://movie.douban.com/people/xiang578/collect?sort=time&amp;tags_sort=count&amp;filter=all&amp;tag=2018&amp;mode=grid" target="_blank" rel="noopener">60</a> 部。想想原因，打开一个视频放在那里，不用怎么理它就能结束。按类别推荐一下自己觉得不错的影视：</p><ol type="1"><li>小丑/蝙蝠侠三部曲：去年在观影中大力推荐漫威宇宙，今年看完 DC 这 4 部电影，刷新对超级英雄片的认知。蝙蝠侠：黑暗骑士在是在超级英雄的框架下对人性进行探讨。小丑展示出社会如何逼一个人成为恶魔。</li><li>黑客帝国三部曲：经典的电影，赛博朋克风格。之前的神话描述神创造了世界，在这部片子里面，这个神就变成了机器人。多少算是人工智能行业的从业者，强人工智能离我们看起来还是很远。</li><li>人生七年9：这应该是拍摄时间最慢长的纪录片，也给我们机会在几十个小时时间里面见证这些主人公 60 多年岁月。很大一个感受，除了 Nick 之外，其他人不过是重复父辈的道路，阶级跃迁又是谈何容易。</li><li>哪吒之魔童降世：即大鱼海棠之后，第二部在电影院看的动画电影。之前想过一个问题：为什么一些小说要隔一段时间就翻拍一次？简单的认为要赋予时代主题。这部片子中最喜欢的一个设定：龙族也是妖怪，镇守龙宫，其实也是镇住自己。</li><li>邪不压正：电影看到一半的时候，我就觉得自己看不懂。说回来，看这部电影有一种酣畅淋漓的感觉，节奏很快，比《一步之遥》和《让子弹飞》更快。半夜在知乎上看了很多回答之后渐渐地懂得其中的情节，蓝先生的爱国情怀，李的复仇梦想。在历史的框架下演绎，始终无法逃离历史的结果，日本人还会按照发展进入北京城。一句“异父异母的亲兄弟”就值得一看。</li></ol><h2 id="游戏">游戏</h2><p>今年新增的一个板块，自从购入 Switch 之后，开始重新接触一些游戏。</p><ol type="1"><li>隐形守护者：抗战背景下面，一个特工面对选择的游戏。所有的场景都是真实拍摄出来的，比绝大部分国内的抗战剧精美。游戏有很多个结局，当然只有符合社会主义核心价值观的才算善终。最印象深刻是第二号突然的一句：什么都是马尔可夫链。</li><li>有氧拳击/健身环大冒险：NS 上的铁人三项之二，充分发挥体感的优势，晚上下班之后健身用。不过从目前的使用频率来看，大概率和买健身卡一个性质。</li><li>塞尔达传说：旷野之息：决定买 Switch 很大程度上源于少数派中一篇关于这个游戏的介绍，大意是没有传统的等级增长，只有你真正掌握一个技巧，林克才能使用出来。这款游戏的给我带来看似无限大的空间，但也有一点遗憾，有时候遇到下雨不好攀岩时，我想让林克坐下等雨停，然后发现没有坐下的选项……</li><li>超级马力欧创作家2：大学的时候，经常看超级小桀玩这个游戏。对于我这种连普通的马里奥都要靠无敌才能通关的来说，大部分自制的地图还是有点难的。说回来，买这个游戏就是买一个青春。只可惜物是人非。</li><li>暗黑破坏神3：永恒之战版 ：Switch 上的冷饭，自己瞎玩了很久，看完所有的剧情。最终在咸鱼上买了很多强力的装备后，速通 150 层大秘境后索然无味。所以玩游戏还是不要作弊。</li></ol><h2 id="未来">未来</h2><p>世界变化太快，未来可期。</p><p>于浙江临海</p><p><a href="https://xiang578.com/post/2017.html">2017 迷茫</a> &gt;&gt; <a href="https://xiang578.com/post/2018.html">2018 探索</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;受 &lt;a href=&quot;http://freemind.pluskid.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Free Mind&lt;/a&gt; 的影响按这种形式写年度总结&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;年初的
      
    
    </summary>
    
    
      <category term="life" scheme="https://xiang578.com/tags/life/"/>
    
      <category term="book" scheme="https://xiang578.com/tags/book/"/>
    
      <category term="movie" scheme="https://xiang578.com/tags/movie/"/>
    
      <category term="game" scheme="https://xiang578.com/tags/game/"/>
    
  </entry>
  
  <entry>
    <title>All About GBDT (1)</title>
    <link href="https://xiang578.com/post/gbdt.html"/>
    <id>https://xiang578.com/post/gbdt.html</id>
    <published>2020-01-26T06:15:43.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<p>GBDT(Gradient Boosting Decision Tree) 从名字上理解包含三个部分：提升、梯度和树。它最早由 Freidman 在 <code>greedy function approximation ：a gradient boosting machine</code> 中提出。很多公司线上模型是基于 GBDT+FM 开发的，我们 Leader 甚至认为 GBDT 是传统的机器学习集大成者。断断续续使用 GBDT 一年多后，大胆写一篇有关的文章和大家分享。</p><h2 id="朴素的想法">朴素的想法</h2><p>假设有一个游戏：给定数据集 <span class="math inline">\({(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}\)</span>，寻找一个模型<span class="math inline">\({\hat y=F(x_i)}\)</span>，使得平方损失函数 <span class="math inline">\({\sum \frac{1}{2}(\hat y_i - y_i)^2}\)</span> 最小。</p><p>如果你的朋友提供一个可以使用但是不完美的模型，比如 <span class="math display">\[F(x_1)=0.8,y_1=0.9\]</span> <span class="math display">\[F(x_2)=1.4,y_2=1.3\]</span> 在如何不修改这个模型的参数情况下，提高模型效果？</p><p>一个简单的思路是：重新训练一个模型实现 <span class="math display">\[F(x_1)+h(x_1)=y_1\]</span> <span class="math display">\[F(x_2)+h(x_2)=y_2\]</span> <span class="math display">\[...\]</span> <span class="math display">\[F(x_n)+h(x_n)=y_n\]</span></p><p>换一个角度是用模型学习数据 <span class="math inline">\({(x_1,y_1-F(x_1)),(x_2,y_2-F(x_2)),...,(x_n,y_n-F(x_n))}\)</span>。得到新的模型 <span class="math inline">\({\hat y=F(x_i)+h(x_i)}\)</span>。</p><p>其中 <span class="math inline">\({y_i-F(x_i)}\)</span> 的部分被我们称之为残差，即之前的模型没有学习到的部分。重新训练模型 <span class="math inline">\({h(x)}\)</span>正是学习残差。如果多次执行上面的步骤，可以将流程描述成：</p><p><span class="math display">\[{F_0(x)}\]</span> <span class="math display">\[{F_1(x)=F_0(x)+h_1(x)}\]</span> <span class="math display">\[{F_2(x)=F_1(x)+h_2(x)}\]</span> <span class="math display">\[{...}\]</span> <span class="math display">\[{F_t(x)=F_t-1(x)+h_t(x)}\]</span></p><p>即 <span class="math inline">\({F(x;w)=\sum^T_{t=1}h_t(x;w)}\)</span>，这也就是 GBDT 。</p><h2 id="如何理解-gradient-boosting-decision-tree">如何理解 Gradient Boosting Decision Tree ?</h2><p>Gradient Boosting Decision Tree 简称 GBDT，最早由 Friedman 在论文《Greedy function approximation: a gradient boosting machine》中提出。简单从题目中理解包含三个部分内容：Gradient Descent、Boosting、Decision Tree。</p><p>Decision Tree 即决策树，利用超平面对特征空间划分来预测和分类，根据处理的任务不同分成两种：分类树和回归树。在 GBDT 算法中，用到的是 CART 即分类回归树。用数学语言来描述为 <span class="math inline">\({F=\{f(x)=w_{q(x)}\}}\)</span>，完成样本 <span class="math inline">\({x}\)</span> 到决策树叶子节点 <span class="math inline">\({q(x)}\)</span> 的映射，并将该叶子节点的权重 <span class="math inline">\({w_{q(x)}}\)</span> 赋给样本。CART 中每次通过计算 gain 值贪心来进行二分裂。</p><p>Boosting 是一种常用的集成学习方法（另外一种是 Bagging）。利用弱学习算法，反复学习，得到一系列弱分类器（留一个问题，为什么不用线性回归做为弱分类器）。然后组合这些弱分类器，构成一个强分类器。上面提到的模型 <span class="math inline">\({F(x;w)=\sum^T_{t=1}h_t(x)}\)</span> 即是一种 boosting 思路，依次训练多个 CART 树 <span class="math inline">\({h_i}\)</span>，并通过累加这些树得到一个强分类器 <span class="math inline">\({F(x;w)}\)</span>。</p><h2 id="为什么-gbdt-可行">为什么 GBDT 可行？</h2><p>在 2 中我提到 GBDT 包括三个部分并且讲述了 Boosting 和 Decison Tree。唯独没有提到 Gradient Descent，GBDT 的理论依据却恰恰和它相关。</p><p>回忆一下，Gradient Descent 是一种常用的最小化损失函数 <span class="math inline">\({L(\theta)}\)</span> 的迭代方法。</p><ul><li>给定初始值 <span class="math inline">\({\theta_0}\)</span></li><li>迭代公式：<span class="math inline">\({\theta ^t = \theta ^{t-1} + \Delta \theta}\)</span></li><li>将 <span class="math inline">\({L(\theta ^t)}\)</span> 在 <span class="math inline">\({\theta ^{t-1}}\)</span> 处进行一阶泰勒展开：<span class="math inline">\({L(\theta ^t)=L(\theta ^{t-1} + \Delta \theta) \approx L(\theta ^{t-1}) + L^\prime(\theta ^{t-1})\Delta \theta}\)</span></li><li>要使 <span class="math inline">\({L(\theta ^t) &lt; L(\theta ^{t-1}) }\)</span>，取 <span class="math inline">\({\Delta \theta = -\alpha L^\prime(\theta ^{t-1})}\)</span></li><li>其中 <span class="math inline">\({\alpha}\)</span> 是步长，可以通过 line search 确定，但一般直接赋一个很小的数。</li></ul><p>在 1 中提到的问题中，损失函数是 MSE <span class="math inline">\({L(y, F(x))=\frac{1}{2}(y_i - f(x_i))^2}\)</span>。</p><p>我们的任务是通过调整 <span class="math inline">\({F(x_1), F(x_2), ..., F(x_n)}\)</span> 最小化 <span class="math inline">\({J=\sum_i L(y_i, F(x_i))}\)</span>。</p><p>如果将 <span class="math inline">\({F(x_i)}\)</span> 当成是参数，并对损失函数求导得到 <span class="math inline">\({ \frac{\partial J}{\partial F(x_i)} = \frac{\partial \sum_i L(y_i, F(x_i))}{\partial F(x_i)} = \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} = F(x_i)-y_i}\)</span>。</p><p>可以发现，在 1 中提到的模型 <span class="math inline">\({h(x)}\)</span> 学习的残差 <span class="math inline">\({y_i-F(x_i)}\)</span>正好等于负梯度，即 <span class="math inline">\({y_i-F(x_i)=-\frac{\partial J}{\partial F(x_i)}}\)</span>。</p><p>所以，参数的梯度下降和函数的梯度下降原理上是一致的：</p><ul><li><span class="math inline">\({F_{t+1}(x_i)=F_t(x_i)+h(x_i)=F(x_i)+y_i-F(x_i)=F_t(x_i)-1\frac{\partial J}{\partial F(x_i)}}\)</span></li><li><span class="math inline">\({\theta ^t = \theta ^{t-1} + \alpha L^\prime(\theta ^{t-1})}\)</span></li></ul><h2 id="gbdt-算法流程">GBDT 算法流程</h2><p>模型 F 定义为加法模型：</p><p><span class="math display">\[{F(x;w)=\sum^{M}_{m=1} \alpha_m h_m(x;w_m) = \sum^{M}_{m=1}f_t(x;w_t)}\]</span> 其中，x 为输入样本，h 为分类回归树，w 是分类回归树的参数，<span class="math inline">\({\alpha}\)</span> 是每棵树的权重。</p><p>通过最小化损失函数求解最优模型：<span class="math inline">\({F^* = argmin_F \sum^N_{i=1}L(y_i, F(x_i))}\)</span></p><p>输入: <span class="math inline">\({(x_i,y_i),T,L}\)</span></p><ol type="1"><li>初始化：<span class="math inline">\({f_0(x)}\)</span></li><li>对于 <span class="math inline">\({t = 1 to T}\)</span> ：<ol type="1"><li>计算负梯度（伪残差）： <span class="math inline">\({ \tilde{y_i} = -[\frac{\partial L(y_i, F(x_i))}{\partial F(x)}]_{F(x)=F_{m-1}(x)} ,i=1,2,...,N}\)</span></li><li>根据 <span class="math inline">\({\tilde{y_i}}\)</span> 学习第 m 棵树： <span class="math inline">\({w^*=argmin_{w} \sum_{i=1}^N(\tilde{y_i} - h_t(x_i;w))^2}\)</span></li><li>line searcher 找步长：<span class="math inline">\({\rho^* = argmin_\rho \sum_{i=1}^{N}L(y_i, F_{t-1}(x_i)+\rho h_t(x_i;w^*))}\)</span></li><li>令 <span class="math inline">\({f_t=\rho^*h_t(x;w*)}\)</span>，更新模型：<span class="math inline">\({F_t=F_{t-1}+f_t}\)</span></li></ol></li><li>输出 <span class="math inline">\({F_T}\)</span></li></ol><p>说明： 1. 初始化 <span class="math inline">\({f_0}\)</span> 方法 1. 求解损失函数最小 2. 随机初始化 3. 训练样本的充分统计量 2. 每一轮拟合负梯度，而不是拟合残差，是为方便之后扩展到其他损失函数。 3. 最小化问题中，如果有解析解，直接带入。否则，利用泰勒二阶展开，Newton Step 得到近似解。</p><p>这一篇就先到这里，之后还会分享 GBDT 常用损失函数推导以及 XGboost 相关内容。如果有任何想法，都可以在留言区和我交流。</p><h2 id="reference">Reference</h2><ol type="1"><li>李航, 《统计学习方法》8.4 提升树</li><li>Freidman，greedy function approximation ：a gradient boosting machine</li><li><a href="https://zhuanlan.zhihu.com/p/73381835" target="_blank" rel="noopener">【19年ML思考笔记】GBDT碎碎念（1）谈回归树的分裂准则 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/29765582" target="_blank" rel="noopener">机器学习-一文理解GBDT的原理-20171001 - 知乎</a></li><li><a href="https://louisscorpio.github.io/2017/12/13/GBDT%E5%85%A5%E9%97%A8%E8%AF%A6%E8%A7%A3/#" target="_blank" rel="noopener">GBDT入门详解 - Scorpio.Lu|Blog</a></li><li><a href="https://stackoverflow.com/questions/45409110/why-gradient-boosting-not-working-in-linear-regression" target="_blank" rel="noopener">python - Why Gradient Boosting not working in Linear Regression? - Stack Overflow</a></li><li><a href="https://blog.csdn.net/qq_24519677/article/details/82020863" target="_blank" rel="noopener">GBDT基本原理及算法描述 - Y学习使我快乐V的博客 - CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/30711812" target="_blank" rel="noopener">GBDT的那些事儿 - 知乎</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;GBDT(Gradient Boosting Decision Tree) 从名字上理解包含三个部分：提升、梯度和树。它最早由 Freidman 在 &lt;code&gt;greedy function approximation ：a gradient boosting machi
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ml" scheme="https://xiang578.com/tags/ml/"/>
    
      <category term="gdbt" scheme="https://xiang578.com/tags/gdbt/"/>
    
  </entry>
  
  <entry>
    <title>(FTRL) Follow The Regularized Leader</title>
    <link href="https://xiang578.com/post/ftrl.html"/>
    <id>https://xiang578.com/post/ftrl.html</id>
    <published>2020-01-03T13:26:06.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<p>FTRL 是 Google 提出的一种优化算法。常规的优化方法例如梯度下降、牛顿法等属于批处理算法，每次更新需要对 batch 内的训练样本重新训练一遍。在线学习场景下，我们希望模型迭代速度越快越好。例如用户发生一次点击行为后，模型就能快速进行调整。FTRL 在这个场景中能求解出稀疏化的模型。</p><h2 id="基础知识">基础知识</h2><ul><li>L1 正则比 L2 正则可以产生更稀疏的解。</li><li>次梯度：对于 L1 正则在 <span class="math inline">\(x=0\)</span> 处不可导的情况，使用次梯度下降来解决。次梯度对应一个集合 <span class="math inline">\(\{v: v(x-x_t) \le f(x)-f(x_t)\}\)</span>，集合中的任意一个元素都能被当成次梯度。以 L1 正则为例，非零处梯度是 1 或 -1，所以 <span class="math inline">\(x=0\)</span> 处的次梯度可以取 <span class="math inline">\([-1, 1]\)</span> 之内任意一个值。</li></ul><h2 id="ftl">FTL</h2><p>FTL(Follow The Leader) 算法：每次找到让之前所有损失函数之和最小的参数。</p><p><span class="math display">\[W=argmin_W \sum^t_{i=1}F_i(W)\]</span></p><p>FTRL 中的 R 是 Regularized，可以很容易猜出来在 FTL 的基础上加正则项。</p><p><span class="math display">\[W=argmin_W \sum^t_{i=1}F_i(W) + R(W)\]</span></p><h2 id="代理函数">代理函数</h2><p>FTRL 的损失函数直接很难求解，一般需要引入一个代理损失函数 <span class="math inline">\(h(w)\)</span>。代理损失函数常选择比较容易求解析解以及求出来的解和优化原函数得到的解差距不能太大。</p><p>我们通过两个解之间的距离 Regret 来衡量效果：</p><p><span class="math display">\[\begin{array}{c}{w_{t}=\operatorname{argmin}_{w} h_{t-1}(w)} \\ {\text {Regret}_{t}=\sum_{t=1}^{T} f_{t}\left(w_{t}\right)-\sum_{t=1}^{T} f_{t}\left(w^{*}\right)}\end{array}\]</span></p><p>其中 <span class="math inline">\(w^{*}\)</span> 是直接优化 FTRL 算法得到的参数。当距离满足 <span class="math inline">\(\lim _{t \rightarrow \infty} \frac{\text {Regret}_{t}}{t}=0\)</span>，损失函数认为是有效的。其物理意义是，随着训练样本的增加，两个优化目标优化出来的参数效果越接近。</p><h2 id="推导过程">推导过程</h2><p>参数 <span class="math inline">\(w_{t+1}\)</span> 的迭代公式：</p><p><span class="math display">\[{w_{t+1}=argmin_w\{ g_{(1:t)}w + \frac{1}{2} \sum_{s=1}^t \sigma_s \lVert w - w_s \rVert ^2   + \lambda_1 \lVert W \rVert_1 + \frac{1}{2} \lambda_2 \lVert W \rVert^2 \}}\]</span></p><p>其中 <span class="math inline">\(g_{(1:t)}=\sum^{t}_{s=1}g_s\)</span>，<span class="math inline">\(g_s\)</span> 为 <span class="math inline">\(f(w_s)\)</span> 的次梯度。参数 <span class="math inline">\(\sum^t_{s=1}\sigma_s=\frac{1}{\eta _t}\)</span>，学习率 <span class="math inline">\(\eta _t = \frac{1}{\sqrt{t}}\)</span>，随着迭代轮数增加而减少。</p><p>展开迭代公式</p><p><span class="math display">\[{F(w)=  g_{(1:t)}w + \frac{1}{2} \sum_{s=1}^t \sigma_s \lVert w - w_s \rVert ^2   + \lambda_1 \lVert W \rVert_1 + \frac{1}{2} \lambda_2 \lVert W \rVert^2 }\]</span></p><p><span class="math display">\[{F(w)=  g_{(1:t)}w + \frac{1}{2} \sum_{s=1}^t \sigma_s ( w^Tw - 2w^Tw_s + w_s^Tw_s)   + \lambda_1 \lVert W \rVert_1 + \frac{1}{2} \lambda_2 \lVert W \rVert^2 }\]</span></p><p><span class="math display">\[{F(w)=  (g_{(1:t)} - \sum_{s=1}^t \sigma_s w_s)w + \frac{1}{2} (\sum_{s=1}^t \sigma_s + \lambda_2) w^Tw   + \lambda_1 \lVert W \rVert_1 + const }\]</span></p><p><span class="math display">\[{F(w)=  z_t^Tw + \frac{1}{2} (\frac{1}{\eta _t} + \lambda_2) w^Tw   + \lambda_1 \lVert W \rVert_1 + const }\]</span></p><p>其中 <span class="math inline">\({z_{t-1}=g^{(1:t-1)} - \sum_{s=1}^{t-1} \sigma_s w_s}\)</span>。</p><p>对 <span class="math inline">\(F(w)\)</span> 求偏导得到：</p><p><span class="math display">\[{z_t + (\frac{1}{\eta _t} + \lambda_2) w + \lambda_1 \partial \lvert W \rvert = 0}\]</span></p><p><span class="math inline">\(w\)</span> 和 <span class="math inline">\(z\)</span> 异号时，等式成立。</p><p>根据基础知识里面提到的对于 L1 正则利用偏导数代替无法求解的情况，得到：</p><p><span class="math display">\[\partial|W|=\left\{\begin{array}{ll}{0,} &amp; {\text { if }-1&lt;w&lt;1} \\ {1,} &amp; {\text { if } w&gt;1} \\ {-1,} &amp; {\text { if } w&lt;-1}\end{array}\right.\]</span></p><ol type="1"><li>当 <span class="math inline">\({ z_t &gt; \lambda_1}\)</span> 时，<span class="math inline">\({w_i &lt; 0}\)</span> , <span class="math inline">\({w_i = \frac{- z_t + \lambda_1 }{\frac{1}{\eta _t} + \lambda_2 }}\)</span></li><li>当 <span class="math inline">\({ z_t &lt; - \lambda_1}\)</span> 时，<span class="math inline">\({w_i &gt; 0}\)</span> , <span class="math inline">\({w_i = \frac{- z_t - \lambda_1 }{\frac{1}{\eta _t} + \lambda_2 }}\)</span></li><li>当 <span class="math inline">\({ \lvert z_t \rvert &lt; \lambda_1}\)</span> 时，当且仅当 <span class="math inline">\({w_i=0}\)</span> 成立</li></ol><p>因此可得： <span class="math display">\[w_{i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if }\left|z_{i}\right| \leq \lambda_{1}} \\ {\frac{-\left(z_{i}-\text sgn(z_i) \lambda_{1}\right)}{\eta_{t}+\lambda_{2}},} &amp; {\text { if others }}\end{array}\right.\]</span></p><h2 id="ftrl-和-sgd-的关系">FTRL 和 SGD 的关系</h2><p>将 SGD 的迭代公式写成：<span class="math inline">\({W^{t+1}=W^t - \eta _tg_t}\)</span></p><p>FTRL 迭代公式为：<span class="math inline">\({W^{t+1}=argmin_w\{ G^{(1:t)}W + \lambda_1 \lVert W \rVert_1 +\lambda_2 \frac{1}{2} \lVert W \rVert \}}\)</span></p><p>代入 <span class="math inline">\({\sum^t_{s=1}\sigma _s= \frac{1}{\eta _t}}\)</span> 到上面的公式中，得到 <span class="math inline">\({W^{t+1}=argmin_w\{ \sum_t^{s=1}g_sW + \frac{1}{2} \sum^t_{s=1}\sigma _s\lVert W - W_s \rVert_2^2 \}}\)</span></p><p>求偏导得到 <span class="math inline">\({\frac{\partial f(w)}{\partial w} = \sum^t_{s=1}g_s + \sum^t_{s=1}\sigma _s( W - W_s )}\)</span></p><p>令偏导等于 0 ：<span class="math inline">\({\sum^t_{s=1}g_s + \sum^t_{s=1}\sigma _s( W^{t+1} - W_s ) = 0}\)</span></p><p>化简得到：<span class="math inline">\({(\sum^t_{s=1}\sigma _s) W^{t+1} = \sum^t_{s=1}\sigma _s W^{s} - \sum^t_{s=1}g_s}\)</span></p><p>代入 <span class="math inline">\(\sigma\)</span>：<span class="math inline">\({\frac{1}{\eta _t} W^{t+1} = \sum^t_{s=1}\sigma _s W^{s} - \sum^t_{s=1}g_s}\)</span></p><p>根据上一个公式得出上一轮的迭代公式：<span class="math inline">\({\frac{1}{\eta _{t-1}} W^{t} = \sum^{t-1}_{s=1}\sigma _s W^{s} - \sum^{t-1}_{s=1}g_s}\)</span></p><p>两式相减：<span class="math inline">\({\frac{1}{\eta _t} W^{t+1} - \frac{1}{\eta _{t-1}} W^{t} = (\frac{1}{\eta _t} - \frac{1}{\eta _{t-1}}) W_t - g_t}\)</span></p><p>最终化简得到和 SGD 迭代公式相同的公式：<span class="math inline">\({W_{t+1} = W_t - \eta_t g_t}\)</span></p><h2 id="ftrl-工程化伪代码">FTRL 工程化伪代码</h2><p>引用自论文 <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41159.pdf" target="_blank" rel="noopener">Ad Click Prediction: a View from the Trenches</a></p><p>下面的伪代码中学习率和前面公式推导时使用的一些不一样： <span class="math inline">\(\eta_{t_{i}}=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^{t} g_{s_{i}}^{2}}}\)</span>。Facebook 在 GBDT + LR 的论文中研究过不同的学习率影响，具体可以参看博文 <a href="https://xiang578.com//post/media/gbdt_lr.html#%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%89%E6%8B%A9">Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr) | 算法花园</a>。</p><figure><img src="/file/15780559628822.jpg" alt="FTRL"><figcaption>FTRL</figcaption></figure><h2 id="例fm-使用-ftrl-优化">例：FM 使用 FTRL 优化</h2><p>FM 是工业界常用的机器学习算法，在之前博文 <a href="https://xiang578.com/post/fm.html">(FM)Factorization Machines</a> 中有简单的介绍。内部的 FTRL+FM 代码没有开源，所以也不好分析。从 <a href="https://zhuanlan.zhihu.com/p/58508137" target="_blank" rel="noopener">FM+FTRL算法原理以及工程化实现 - 知乎</a> 中找了一张 FTRL+FM 的伪代码图片。</p><p><img src="/file/15780576261639.jpg"></p><h2 id="reference">Reference</h2><ul><li><a href="https://tech.meituan.com/2016/04/21/online-learning.html" target="_blank" rel="noopener">Online Learning算法理论与实践 - 美团技术团队</a></li><li><a href="https://zhuanlan.zhihu.com/p/32694097" target="_blank" rel="noopener">FTRL公式推导 - 知乎</a></li><li><a href="https://blog.csdn.net/fangqingan_java/article/details/51020653" target="_blank" rel="noopener">每周一文】Ad Click Prediction: a View from the Trenches(2013)_机器学习,CTR,online_fangqingan_java的专栏-CSDN博客</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;FTRL 是 Google 提出的一种优化算法。常规的优化方法例如梯度下降、牛顿法等属于批处理算法，每次更新需要对 batch 内的训练样本重新训练一遍。在线学习场景下，我们希望模型迭代速度越快越好。例如用户发生一次点击行为后，模型就能快速进行调整。FTRL 在这个场景中能
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="fm" scheme="https://xiang578.com/tags/fm/"/>
    
      <category term="google" scheme="https://xiang578.com/tags/google/"/>
    
      <category term="ml" scheme="https://xiang578.com/tags/ml/"/>
    
      <category term="ftrl" scheme="https://xiang578.com/tags/ftrl/"/>
    
  </entry>
  
  <entry>
    <title>每月分享 202001 Fine-Tune Your Days</title>
    <link href="https://xiang578.com/post/monthly-issue-202001.html"/>
    <id>https://xiang578.com/post/monthly-issue-202001.html</id>
    <published>2020-01-01T08:55:11.000Z</published>
    <updated>2020-03-28T09:07:31.367Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。</p></blockquote><h2 id="x04-你见过哪些让你目瞪口呆脑洞大开的骗局---sme情报员的回答---知乎">0x04 <a href="https://www.zhihu.com/question/41182911/answer/966701311" target="_blank" rel="noopener">你见过哪些让你目瞪口呆、脑洞大开的骗局？ - SME情报员的回答 - 知乎</a></h2><p>推荐其中的吉普赛读心术，当成脑筋急转弯来看。</p><blockquote><p>首先任选一个两位数，在心里默默记住，然后用这个两位数再依次减去它的十位和个位，最后用得数查表，找到对应的怪符号。 比如67,相应的计算就是67-6-7=54， 现在，在表中找到你心中数字经过计算后所对应的符号。</p></blockquote><p><img src="/file/15792508672842.jpg"></p><p>最后的答案都会是</p><p><img src="/file/15792508879083.jpg"></p><h2 id="x03-deep-neural-networks-for-youtube-recommendations-paper-ml">0x03 Deep Neural Networks for YouTube Recommendations #paper #ml</h2><p>Youtube 几年前的论文，最近拿过来看一下。工业界的论文最大的价值是提到的一些 tick，比如这篇论文中分析到用户对新视频的偏好，引入 example age 代表视频的上传到预测时的时间。再比如，给用户推荐视频时，考虑用户看过这个视频相关频道次数以及这个视频在用户实现中出现的次数。所以，做算法实现需要深入理解自己所处的场景。</p><p>推荐知乎上一些关于这篇论文的解读：</p><blockquote><ul><li><a href="https://zhuanlan.zhihu.com/p/25343518" target="_blank" rel="noopener">Deep Neural Network for YouTube Recommendation论文精读 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/52169807" target="_blank" rel="noopener">重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/52504407" target="_blank" rel="noopener">YouTube深度学习推荐系统的十大工程问题 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/61827629" target="_blank" rel="noopener">揭开YouTube深度推荐系统模型Serving之谜 - 知乎</a></li></ul></blockquote><h2 id="x02-fine-tune-your-days-with-the-scientific-method">0x02 Fine-Tune Your Days with the Scientific Method</h2><p>这个小标题出自 <a href="https://book.douban.com/subject/30327043/" target="_blank" rel="noopener">Make Time</a>，翻译成中文是利用科学的方法每天微调你的习惯。</p><h2 id="x01-2020-阅读看板">0x01 2020 阅读看板</h2><p>参考部分网友 Notion 的用法，搭建一个自己的阅读看板 <a href="https://www.notion.so/ryanx/4666b7440155430880b9c9787adde5ab?v=39111f7ebd5e4be6a28d7ef712c4aebb" target="_blank" rel="noopener">看书也就图一乐</a>。目前挑选出来的书远远超过前两年的阅读量，加油一起读书。</p><p>Notion 这种一个数据库 + 可选的 View 很接近我心目中任务管理软件的极限。</p><figure><img src="/file/15782399749595.jpg" alt="reading board"><figcaption>reading board</figcaption></figure><h2 id="x00-大佬们的年度总结">0x00 大佬们的年度总结</h2><p>新的一年开始时，最期待翻看大佬们的年度总结，罗列一些我觉得有总结。</p><ul><li><a href="https://yiming.dev/blog/2019/12/31/growing-a-result-driven-mindset/" target="_blank" rel="noopener">Growing a Result-Driven Mindset - Yiming Chen</a>：英文总结，Yiming 的博客给我带来学习英语并且用之来表达的动力。</li><li><a href="https://wdxmzy.com/pastfuture/year2019/2019/12/31/" target="_blank" rel="noopener">2019 总结与 2020 计划 | 小土刀 2.0</a>：从不同角度回顾自己的 2019 年。</li><li><a href="http://freemind.pluskid.org/misc/2019-summary/" target="_blank" rel="noopener">2019 时光小偷</a>：这位博主每年总结的标题都是一首歌，也是几年前看他的总结才开始尝试写自己的总结。</li><li><a href="https://zhuanlan.zhihu.com/p/100357148" target="_blank" rel="noopener">致敬时间的价值：一品十年 - 知乎</a>：和这个主题没有太大的关系，看一下其他人十年的总结，也能很好的指导自己的生活。</li><li><a href="http://zhengruonan.com/2019/12/29/2019-12-29-farewell-2019/" target="_blank" rel="noopener">2019年：下个十年路口，Farewell | Crossairplane的博客</a>：读这篇文章的时候突然想到一点，之后再看年终总结时，留言一句「新年快乐」。</li><li><a href="http://www.ztleespace.com/2020/01/01/create-vs-consume/" target="_blank" rel="noopener">Create vs. Consume - ends 2019 then starts 2020 - Ziting Li</a>：真诚的思考。</li><li><a href="https://www.hi-pda.com/forum/viewthread.php?tid=819978&amp;extra=page%3D1" target="_blank" rel="noopener">我的千书阅读计划 - 意欲蔓延 - Hi!PDA Hi!PDA</a>：fatdragoncat 13 年在 Hi!PDA 上立下愿望，这么多年过去，不知道数量上有没有达到，但是读书的收获已经改变他的生活。难得可贵这篇帖子展示他的变化过程。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;x04-你见过哪些让你目瞪口呆脑洞大开的骗局---sme情报员的回答---知乎&quot;&gt;0x04 &lt;a hre
      
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记</title>
    <link href="https://xiang578.com/post/reinforce-learnning-basic.html"/>
    <id>https://xiang578.com/post/reinforce-learnning-basic.html</id>
    <published>2019-12-26T13:14:47.000Z</published>
    <updated>2020-03-28T09:07:31.367Z</updated>
    
    <content type="html"><![CDATA[<h2 id="info">Info</h2><p>课件下载：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">Hung-yi Lee - Deep Reinforcement Learning</a></p><p>课程视频：<a href="https://www.youtube.com/watch?v=z95ZYgPgXOY&amp;list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_" target="_blank" rel="noopener">DRL Lecture 1: Policy Gradient (Review) - YouTube</a></p><ul><li>Change Log<ul><li>20191226: 整理 PPO 相关资料</li><li>20191227: 整理 Q-Learning 相关资料</li></ul></li></ul><h2 id="rl-基础">RL 基础</h2><p>强化学习基本定义：</p><ul><li>Actor：可以感知环境中的状态，通过执行不同的动作得到反馈的奖励，在此基础上进行学习优化。</li><li>Environment：指除 Actor 之外的所有事务，受 Actor 动作影响而改变其状态，并给 Actor 对应的奖励。</li><li>on-policy 和 off-policy 的区别在于 Actor 和 Environment 交互的策略和它自身在学习的策略是否是同一个。</li></ul><p>一些符号：</p><ul><li>State s 是对环境的描述，其状态空间是 S。</li><li>Action a 是 Actore 的行为描述，其动作空间是 A。</li><li>Policy <span class="math inline">\(\pi(a|s)=P[A_t=a|S_t=s]\)</span> 代表在给定环境状态 s 下 动作 a 的分布。</li><li>Reward <span class="math inline">\({r(s,a,s^{\prime})}\)</span> 在状态 s 下执行动作 a 后，Env 给出的打分。</li></ul><h2 id="policy-gradient">Policy Gradient</h2><p>Policy Network 最后输出的是概率。</p><p>目标：调整 actor 中神经网络 policy <span class="math inline">\(\pi(\theta)\)</span>，得到 <span class="math inline">\(a=\pi(s, \theta)\)</span>，最大化 reward。</p><p>trajectory <span class="math inline">\(\tau\)</span> 由一系列的状态和动作组成，出现这种组合的概率是 <span class="math inline">\(p_{\theta}(\tau)\)</span> 。</p><p><span class="math display">\[\begin{array}{l}{p_{\theta}(\tau)} \\ {\quad=p\left(s_{1}\right) p_{\theta}\left(a_{1} | s_{1}\right) p\left(s_{2} | s_{1}, a_{1}\right) p_{\theta}\left(a_{2} | s_{2}\right) p\left(s_{3} | s_{2}, a_{2}\right) \cdots} \\ {\quad=p\left(s_{1}\right) \prod_{l=1}^{T} p_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)}\end{array}\]</span></p><p>reward ：根据 s 和 a 计算得分 r，求和得到 R。在围棋等部分任务中，无法获得中间的 r（下完完整的一盘棋后能得到输赢的结果）。</p><p>需要计算 R 的期望 <span class="math inline">\(\bar{R}_{\theta}\)</span>，形式和 GAN 类似。如果一个动作得到 reward 多，那么就增大这个动作出现的概率。最终达到 agent 所做 policy 的 reward 一直都比较高。</p><p><span class="math display">\[\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)\]</span></p><p>强化学习中，没有 label。需要从环境中采样得到 <span class="math inline">\(\tau\)</span> 和 R，根据下面的公式去优化 agent。相当于去求一个 likelihood。</p><p><span class="math inline">\(\nabla f(x) = f(x) \frac{\nabla f(x)}{f(x)}= f(x) \nabla \log f(x)\)</span> ，这一步中用到对 log 函数进行链式求导。</p><p><span class="math display">\[\nabla \bar{R}_{\theta}=\sum_{\tau} R(\tau) \nabla p_{\theta}(\tau)\]</span></p><p><span class="math display">\[\begin{array}{l}{=E_{\left.\tau \sim p_{\theta}(\tau)[R(\tau)] \log p_{\theta}(\tau)\right]} \approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(\tau^{n}\right)} \\ {=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)}\end{array}\]</span></p><p>参数更新方法：</p><ol type="1"><li>在环境中进行采样，得到一系列的轨迹和回报。</li><li>利用状态求梯度，更新模型。如果 R 为正，增大概率 <span class="math inline">\(p_{\theta}(a_t|s_t)\)</span>, 否则减少概率。</li><li>重复上面的流程。</li></ol><p><img src="/file/15726799935625.jpg"></p><h3 id="pg-的例子">PG 的例子</h3><p>训练 actor 的过程看成是分类任务：输入 state ，输出 action。</p><p>最下面公式分别是反向传播梯度计算和 PG 的反向梯度计算，PG 中要乘以整个轨迹的 R。</p><figure><img src="/file/15752505145081.jpg" alt="PG"><figcaption>PG</figcaption></figure><p>tip 1： add a baseline</p><p>强化学习的优化和样本质量有关，避免采样不充分。Reawrd 函数变成 R-b，代表回报小于 b 的都被我们当成负样本，这样模型能去学习得分更高的动作。b 一般可以使用 R 的均值。</p><p>tip 2: assign suitable credit</p><p>一场游戏中，不论动作好坏，总会乘上相同的权重 R，这种方法是不合理的，希望每个 action 的权重不同。</p><ol type="1"><li>引入一个 discount rate，对 t 之后的动作 r 进行降权重。</li><li>利用 Advantage Function 评价状态 s 下动作 a 的好坏 critic。</li></ol><figure><img src="/file/15752505547153.jpg" alt="Assign Suitable Credit"><figcaption>Assign Suitable Credit</figcaption></figure><h2 id="ppo-proximal-policy-optimization">PPO: Proximal Policy Optimization</h2><h3 id="importance-sampling">importance sampling</h3><p>假设需要估计期望 <span class="math inline">\(E_{x~p[f(x)]}\)</span>，x 符合 p 分布，将期望写成积分的形式。由于在 P 分布下面很难采样，把问题转化到已知 q 分布上，得到在 p 分布下计算期望公式。</p><p><img src="/file/15771920437580.jpg"></p><p>上面方法得到 p 和 q 期望接近，但是方差可能相差很大，且和 <span class="math inline">\(\frac{p(x)}{q(x)}\)</span> 有关。</p><p>原分布的方差： <span class="math display">\[\operatorname{Var}_{x-p}[f(x)]=E_{x-p}\left[f(x)^{2}\right]-\left(E_{x-q}[f(x)]\right)^{2}\]</span></p><p>新分布的方差： <span class="math display">\[\begin{array}{l}{\operatorname{Var}_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}} \\ {\operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2}} \\ {=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}}\end{array}\]</span></p><p>在 p 和 q 分布不一致时，且采样不充分时，可能会带来比较大的误差。</p><figure><img src="/file/15771931618906.jpg" alt="Issue of Importance Sampling"><figcaption>Issue of Importance Sampling</figcaption></figure><h3 id="从-on-policy-到-off-policy">从 On-policy 到 Off-policy</h3><p>on-policy 时，PG 每次参数更新完成后，actor 就改变了，不能使用之前的数据，必须和环境重新互动收集数据。引入 <span class="math inline">\(p_{\theta \prime}\)</span> 进行采样，就能将 PG 转为 off-ploicy。</p><p><img src="/file/15771932374489.jpg"></p><p>和之前相比，相当于引入重要性采样，所以也有前一节中提到的重要性采样不足问题。</p><p><span class="math display">\[J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]\]</span></p><h3 id="ppotrpo">PPO/TRPO</h3><p>为了克服采样的分布与原分布差距过大的不足，PPO 引入 KL 散度进行约束。KL 散度用来衡量两个分布的接近程度。</p><p><span class="math display">\[J_{P P O}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta K L\left(\theta, \theta^{\prime}\right)\]</span></p><p>TRPO(Trust Region Policy Optimization)，要求 <span class="math inline">\(K L\left(\theta, \theta^{\prime}\right)&lt;\delta\)</span>。</p><p><span class="math display">\[J_{T R P O}^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]\]</span></p><p>KL 散度可能比较难计算，在实际中常使用 PPO2。</p><ul><li>A&gt;0，代表当前策虑表现好。需要增大 <span class="math inline">\(\pi( \theta )\)</span>，通过 clip 增加一个上限，防止 <span class="math inline">\(\pi( \theta )\)</span> 和旧分布变化太大。</li><li>A&lt;0，代表当前策虑表现差，不限制新旧分布的差异程度，只需要大幅度改变 <span class="math inline">\(\pi( \theta )\)</span>。</li></ul><p>参考 <a href="https://zhuanlan.zhihu.com/p/43114711" target="_blank" rel="noopener">【点滴】策略梯度之PPO - 知乎</a></p><p><img src="/file/15772793086357.jpg"></p><h3 id="ppo-algorithm">PPO algorithm</h3><p>系数 <span class="math inline">\(\beta\)</span> 在迭代的过程中需要进行动态调整。引入 <span class="math inline">\(KL_{max} KL_{min}\)</span>，KL &gt; KLmax，说明 penalty 没有发挥作用，增大 <span class="math inline">\(\beta\)</span>。</p><p><img src="/file/15772793200430.jpg"></p><h2 id="q-learning">Q-Learning</h2><p>value-base 方法，利用 critic 网络评价 actor 。通过状态价值函数 <span class="math inline">\(V^{\pi}(s)\)</span> 来衡量预期的期望。V 和 pi、s 相关。</p><p><img src="/file/15773668251569.jpg"></p><ol type="1"><li>Monte-Carlo MC: 训练网络使预测的 <span class="math inline">\(V^{\pi}(s_a)\)</span> 和实际完整游戏 reward <span class="math inline">\(G_a\)</span> 接近。</li><li>Temporal-difference TD: 训练网络尽量满足 <span class="math inline">\(V^{\pi}(s_t)=V^{\pi}(s_{t+1}) + r_t\)</span> 等式，两个状态之间的收益差。</li></ol><p>MC: 根据策虑 <span class="math inline">\(\pi\)</span> 进行游戏得到最后的 <span class="math inline">\(G(a)\)</span>，最终存在方差大的问题。<span class="math inline">\(\operatorname{Var}[k X]=k^{2} \operatorname{Var}[X]\)</span></p><p>TD: r 的方差比较小，<span class="math inline">\(V^{\pi}(s_{t+1})\)</span> 在采样不充分的情况下，可能不准确。</p><p><img src="/file/15773670631327.jpg"></p><h3 id="another-critic">Another Critic</h3><p>State-action value function <span class="math inline">\(Q^{\pi}(s, a)\)</span>：预测在 pi 策略下，pair(s, a) 的值。相当于假设 state 情况下强制采取 action a。</p><p><img src="/file/15773671301402.jpg"></p><p>对于非分类的方法：</p><p><img src="/file/15773671400847.jpg"></p><p>Q-Learning</p><ol type="1"><li>初始 actor <span class="math inline">\(\pi\)</span> 与环境互动</li><li>学习该 actor 对应的 Q function</li><li>找一个比 <span class="math inline">\(\pi\)</span> 好的策虑：<span class="math inline">\(\pi \prime\)</span>，满足 <span class="math inline">\(V^{\pi \prime}(s,a) \ge V^{\pi}(s,a)\)</span>, <span class="math inline">\(\pi^{\prime}(s)=\arg \max _{a} Q^{\pi}(s, a)\)</span></li></ol><p>在给定 state 下，分别代入 action，取函数值最大的 a，作为后面对该 state 时采取的 action。</p><p>证明新的策虑存在： <img src="/file/15773672909262.jpg"></p><h3 id="target-network">Target NetWork</h3><p>左右两边的网络相同，如果同时训练比较困难。简单的想法是固定右边的网络进行训练，一定次数后再拷贝左边的网络。</p><p><img src="/file/15773673947112.jpg"></p><h3 id="exploration">Exploration</h3><p>Q function 导致 actor 每次都会选择具有更大值的 action，无法准确估计某一些动作，对于收集数据而言是一个弊端。</p><ul><li>Epsilon Greedy<ul><li>小概率进行损失采样</li></ul></li><li>Boltzmann Exploration<ul><li>利用 softmax 计算选取动作的概率，然后进行采样</li></ul></li></ul><p><img src="/file/15773674189861.jpg"></p><h3 id="replay-buffer">Replay buffer</h3><p>采样之后的 <span class="math inline">\((s_t, a_t, r_t, s_{t+1})\)</span> 保存在一个 buffer 里面（可能是不同策虑下采样得到的)，每次训练从 buffer 中 sample 一个 batch。</p><p>结果：训练方法变成 off-policy。减少 RL 重复采样，充分利用数据。</p><p><img src="/file/15773675111439.jpg"></p><h3 id="typical-q-learning-algorithm">Typical Q-Learning Algorithm</h3><p>Q-Learning 流程：</p><p><img src="/file/15773677168275.jpg"></p><h3 id="double-dqn-ddqn">Double DQN DDQN</h3><ul><li>Q value 容易高估：目标值 <span class="math inline">\(r_t + maxQ(s_{t+1}, a)\)</span> 倾向于选择被高估的 action，导致 target 很大。</li><li>选动作的 Q' 和计算 value 的 Q(target network) 不同。Q 中高估 a，Q' 可能会准确估计 V 值。Q' 中高估 a ，可能不会被 Q 选中。</li></ul><p><img src="/file/15773677986325.jpg"></p><h3 id="dueling-dqn">Dueling DQN</h3><p>改 network 架构。V(s) 代表 s 所具有的价值，不同的 action 共享。 A(s,a) advantage function 代表在 s 下执行 a 的价值。最后 <span class="math inline">\(Q(s, a) = A(s, a) + V(s)\)</span>。</p><p>为了让网络倾向于使用 V（能训练这个网络），得到 A 后，要对 A 做 normalize。</p><p><img src="/file/15773680103445.jpg"></p><h3 id="prioritized-reply">Prioritized Reply</h3><p>在训练过程中，对于经验 buffer 里面的样本，TD error 比较大的样本有更大的概率被采样，即难训练的数据增大被采样的概率。</p><p><img src="/file/15773681449261.jpg"></p><h3 id="multi-step">Multi-step</h3><p>综合 MC 和 TD 的优点，训练样本按一定步长 N 进行采样。MC 准确方差大，TD 方差小，估计不准。 <img src="/file/15773682226911.jpg"></p><h3 id="noisy-net">Noisy Net</h3><p><img src="/file/15773682895872.jpg"></p><ul><li>Noise on Action：在相同状态下，可能会采取不同的动作。</li><li>Noise on Parameters：开始时加入噪声。同一个 episode 内，参数不会改变。相同状态下，动作相同。 更好探索环境。</li></ul><h3 id="distributional-q-function">Distributional Q-function</h3><p>Q 是累积收益的期望，实际上在 s 采取 a 时，最终所有得到的 reward 为一个分布 reward distribution。部分时候分布不同，可能期望相同，所以用期望来代替 reward 会损失一些信息。</p><p>Distributional Q-function 直接输出分布，均值相同时，采取方差小的方案。这种方法不会产生高估 q 值的情况。</p><p><img src="/file/15773684681347.jpg"></p><h3 id="rainbow">Rainbow</h3><p>rainbow 是各种策略的混合体。</p><p><img src="/file/15773684767629.jpg"></p><p>DDQN 影响不大。</p><p><img src="/file/15773684832549.jpg"></p><h3 id="continuous-actions">Continuous Actions</h3><p>action 是一个连续的向量，Q-learning 不是一个很好的方法。</p><p><span class="math display">\[a=\arg \max _{a} Q(s, a)\]</span></p><ol type="1"><li>从 a 中采样出一批动作，看哪个行动 Q 值最大。</li><li>使用 gradient ascent 解决最优化问题。</li><li>设计一个网络来化简过程。<ol type="1"><li><span class="math inline">\(\sum\)</span> 和 <span class="math inline">\(\mu\)</span> 是高斯分布的方差和均值，保证矩阵一定是正定。</li><li>最小化下面的函数，需要最小化 <span class="math inline">\(a - \mu\)</span>。</li></ol></li></ol><p><img src="/file/15773684903236.jpg"></p><h2 id="actor-critic">Actor Critic</h2><h2 id="sparse-reward">Sparse Reward</h2><h2 id="imitation-learning">Imitation Learning</h2><h2 id="reference">Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/41712025" target="_blank" rel="noopener">强化学习基础知识 - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;info&quot;&gt;Info&lt;/h2&gt;
&lt;p&gt;课件下载：&lt;a href=&quot;http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hung-yi Lee
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="algorithm" scheme="https://xiang578.com/tags/algorithm/"/>
    
      <category term="reinforcementlearning" scheme="https://xiang578.com/tags/reinforcementlearning/"/>
    
  </entry>
  
  <entry>
    <title>每月分享 201912</title>
    <link href="https://xiang578.com/post/monthly-issue-201912.html"/>
    <id>https://xiang578.com/post/monthly-issue-201912.html</id>
    <published>2019-12-16T08:55:11.000Z</published>
    <updated>2020-03-28T09:07:31.367Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。</p></blockquote><h2 id="x02.-org-mode-workflow">0x02. <a href="https://blog.jethro.dev/posts/org_mode_workflow_preview/" target="_blank" rel="noopener">Org-mode Workflow</a></h2><p>国外一名 CS 学生的 org mode workflow 教程，包括 GTD 和 Zettelkasten 两个主要的部分，分别对应时间管理和知识管理，是一份很好的参考资料。</p><h2 id="x01.-hhkb-更新">0x01. HHKB 更新</h2><blockquote class="twitter-tweet"><p lang="ja" dir="ltr">改めてとなりますが今回新しく登場したHHKBは3機種になります。それぞれの特徴をまとめたものがこちらになります。<a href="https://twitter.com/hashtag/HHKB%E3%83%9F%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener">#HHKBミートアップ</a> <a href="https://t.co/GVVxNI6H72" target="_blank" rel="noopener">pic.twitter.com/GVVxNI6H72</a></p>— HHKB OFFICIAL (<span class="citation" data-cites="PFU_HHKB">@PFU_HHKB</span>) <a href="https://twitter.com/PFU_HHKB/status/1204345452658741248?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">December 10, 2019</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>HHKB 好久之后终于更新了！不过价格也变得更贵……还是很喜欢自己 18 年买的 HHKB BT 版。不过在使用 Emacs 之后，出现没有方向键的烦恼。之前通过映射 <code>Ctrl + HJKL</code> 替代方向键，然后和 Emacs 的一些快捷键冲突……等买一个新的机械键盘。</p><h2 id="x00.-my-gtd-workflow-2019-ver.---yiming-chen-gtd">0x00. <a href="https://yiming.dev/blog/2019/05/22/my-gtd-workflow-2019-ver/" target="_blank" rel="noopener">My GTD Workflow (2019 ver.) - Yiming Chen</a> #gtd</h2><p>很少看到国人用英文写的 GTD 相关文章，年初自己也想按 Workflow 这种形式写一篇，不过一直拖到现在都没有完成。</p><ul><li>对任务设置优先级：A B C</li><li>如何设置任务优先级，对目标进行分解<ul><li>每年一月份设定年度目标</li><li>每月一号根据年度目标设定月度目标</li><li>每周日根据月度目标设定每周目标</li><li>每天早上设定当天目标</li></ul></li><li>任务安排优先级和截止日期后，可以使用四象限法则。</li><li>回顾技巧<ul><li>追求 100% 完成，可以接受 70%。</li><li>一个任务多次延迟之后，考虑是否还是重要。</li><li>如果任务还是重要，对任务进行拆分。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;x02.-org-mode-workflow&quot;&gt;0x02. &lt;a href=&quot;https://blog.
      
    
    </summary>
    
      <category term="月旦评" scheme="https://xiang578.com/categories/%E6%9C%88%E6%97%A6%E8%AF%84/"/>
    
    
      <category term="monthly-issue" scheme="https://xiang578.com/tags/monthly-issue/"/>
    
  </entry>
  
  <entry>
    <title>2019 年软硬件指北</title>
    <link href="https://xiang578.com/post/2019-consumer-report.html"/>
    <id>https://xiang578.com/post/2019-consumer-report.html</id>
    <published>2019-12-15T13:29:38.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<p>呼吸不止，折腾不停。记录在过去的一年，自己选择的软件和硬件。去年写指南并不能指南，所以今年直接写成指北。</p><h2 id="硬件更新">硬件更新</h2><h3 id="iphone-xr-和-apple-watch-series-4">iPhone XR 和 Apple Watch Series 4</h3><p>iPhone XR 刚出来的时候，一直被吐槽是大边框。不过随着在电商网站上不断降价，越来越被当成是无边框手机……在忍受不了使用多年 iPhone 6 的卡顿，以及很难脱离 iOS 生态的现实。终于在苏宁上下单 <span class="math inline">\((Product)^{read}\)</span> 版的 XR。经过半年多的使用，这部手机实用但是不出彩。</p><figure><img src="/file/15764169455007.jpg" alt="Apple Watch 圆环图"><figcaption>Apple Watch 圆环图</figcaption></figure><p>购买 Apple Watch Series 4(AW) 理由很简单：在去公司健身房锻炼的时候，希望有一个可以记录运动数据的设备。事实 AW 自带的运动软件很不错，可以满足我的运动记录需求。但是例如 Keep 之类的第三方 App 适配不好。另外，AW 很好扩展 iPhone 和 Mac 的使用，比如可以解锁 Mac、查看 iPhone 上的信息等等。到头来，AW 还只是一块需要一天一充的电子表。</p><h3 id="nintendo-switch">Nintendo Switch</h3><p>Nintendo Switch(NS) 是任天堂在 2017 年推出的，集掌机和主机于一体的游戏机。买 NS 的理由也很简单，Mac 上游戏太少，我需要一个设备玩游戏。更深层次的来说，感觉自己的反应太慢，想通过玩游戏来锻炼快速决策能力。</p><p>简单统计一下，我在 NS 上花费的时间大概有 200 多小时，购入游戏也花费上千元……反应能力不知道有没有上去，但是享受到游戏的快乐。</p><figure><img src="/file/15764170682610.jpg" alt="任天堂就是世界的主宰"><figcaption>任天堂就是世界的主宰</figcaption></figure><p>Nintendo Switch 目前可以选的有 Nintendo Switch、Nintendo Switch 续航版、Nintendo Switch Lite。</p><h3 id="kindle-oasis-2">Kindle Oasis 2</h3><p>大概是 5 月末，通过公司内部闲置群出了使用 5 年多的 Kindle PaperWhite 2 后，从淘宝上购入美版 Kindle Oasis 2 。可惜的是，1 个月不到的时间，亚马逊推出 Kindle Oasis 3 ……</p><figure><img src="/file/kindle_2_oasis.jpg" alt="kindle oasis 2 和 kindle 2 对比图"><figcaption>kindle oasis 2 和 kindle 2 对比图</figcaption></figure><p>和 KPW2 相比，KO2 主要带来一下几个方面的提升：</p><ol type="1"><li>7寸屏幕，更高的分辨率。看的更多，看的更清晰，更加逼近纸书的感觉。</li><li>不对称设计，电池集中在一边，握持比较舒服。</li><li>金属机身。前几代 kindle 都是亚马逊祖传的类肤质塑料机身，很容易沾上油脂，这一代采用金属机身，看起来更加富有科技感。毕竟当年小米用上金属边框的时候，都敢去吹一块钢板的艺术之旅。</li><li>两个翻页实体按键，按起来比较有安全感。</li></ol><p>上面说这么多，kindle 主要功能还是看书。这几年，很多 kindle 电子书分享站都由于版权问题陆续关闭，优质的资源比较难下载。不过，去年自己办信用卡时，领了一年的 Kinle U 会员（今年又领到一年的会员），在中亚上借阅很多本小说。从体验上来说，KU 会员不能实现全场自由借，而且大部分书籍都只是滥竽充数。一对比微信读书会员就是十分实惠，多期待微信读书可以出电子书阅读器吧。</p><h2 id="软件实践">软件实践</h2><p>18 年开始，着手准备构建自己的数字化系统。19 年在前面的基础上，进行了很多迁移。</p><h3 id="信息管理">信息管理</h3><p>11 月份看到一句话：input 做的越多，知识管理越差。这个很好形容我之前的状态，在印象笔记中囤积待看的剪藏、OF 里面有很多想写的主题、MWeb 遗留大量没有写完的文章。</p><p>年中的时候，想把自己写的一些笔记更好的管理起来。最初想到的是搭建 wiki ，实现知识的网状化连接。不过市面上常用的一些个人 wiki 方案都不是很满意。最终选择 hexo 搭配一个 wiki 主题 <a href="https://github.com/zthxxx/hexo-theme-Wikitten" target="_blank" rel="noopener">Wikitten</a>。另外，后来了解到有一种基于纯文本的知识管理方案：zettelkasten。感兴趣的可以去看一下。</p><p>写日记是这么多年以来坚持的一件小事情。之前一直是在笔记本上写，后来慢慢的尝试通过印象笔记来写。2月份，订阅 Day One ，开始尝试迁移到它上面去。作为一个专业的软件，体验真的比之前的方式不知道好多少。Day One 上也有很多数据统计，多少可以拿来得瑟用。另外，自己干的一件事情就是把印象笔记中的日记慢慢转移到 Day one 。写在笔记本上的日记，也被我拍成一张又一张的照片，只不过这个迁移起来比较麻烦。</p><figure><img src="/file/15764175403738.jpg" alt="Day one 按年统计图"><figcaption>Day one 按年统计图</figcaption></figure><h3 id="任务管理">任务管理</h3><p>这个问题一直是一个大坑，花费很多时间在多个软件中试来试去。在现在这个时间点，自己开始选择混合使用 OmniFocus 和 Org mode。具体怎么搭配使用，等再坚持几个月再出来分享。不过说回来，任务管理的关键不在于软件，而在于执行。</p><h3 id="其他实践">其他实践</h3><p>下面这一些今年自己做的选择，都有一个共同的特点：从商业软件到开源项目。很多人选着使用的开源项目的出发点在于害怕商业公司无休止的使用个人隐私数据，而吸引我的主要是自由软件自由开放的精神。</p><h4 id="从-moneywiz-到-beancount">从 MoneyWiz 到 Beancount</h4><p>MoneyWiz 是在少数派上了解到记账软件，Setapp 中可以免费使用。和国内那些整天搞社区和卖理财的记账软件相比，只是纯粹的一个记账软件。Beancount 是无意中从<a href="https://www.byvoid.com/zhs/blog/beyond-the-void" target="_blank" rel="noopener">BYvoid</a>文章中了解的一款纯文本记账软件。最大的优点是扩展性强。在使用过程中，搭配一些简单的脚本，可以实现每月底花一个小时就能把这个月的开销记录明白。</p><figure><img src="/file/15764177497946.jpg" alt="Beancount fava"><figcaption>Beancount fava</figcaption></figure><h4 id="从-1password-到-keepass">从 1Password 到 KeePass</h4><p>之前看过一个结论：密码破解的难度主要在于长度而不是复杂度。所以借助密码软件辅助记忆密码是不二之选。1Password 是在去年感恩节活动中获得的长达一年的免费体验。快要到期前，没有选择转向订阅（今年感恩节活动依然是新用户 长度一年的免费使用），反而是选择开源的 KeePass。KeePass 在不同的平台上有多个客户端可以选择，目前我主要用的是 MacPass 和奇密。KeePass 中所有的密码数据都保存在一个文件中，跨平台使用只需要简单同步这个文件。</p><h4 id="从搜狗输入法到鼠须管">从搜狗输入法到鼠须管</h4><p>网上关于搜狗输入法的声讨一直不绝于耳，我也长时间忍受搜狗动不动给你跳出来的斗图功能提示。在花费一番力气，配置鼠须管后，彻底删除搜狗，详见 <a href="https://xiang578.com/post/rime.html">「Rime 鼠须管」小鹤双拼配置指南 | 算法花园</a>。另外 Mac 上自带的输入法的体验也没有那么差。</p><p>博客上和这个主题相关的文章：</p><ul><li><a href="https://xiang578.com/post/best-of-iphone-2019.html">Best of iPhone 2019 软件清单 | 算法花园</a></li><li><a href="https://xiang578.com/post/2018-consumer-report.html">2018 年消费指南 | 算法花园</a></li><li><a href="https://xiang578.com/post/iphone5s.html">iPhone软件清单 | 算法花园</a></li><li><a href="https://xiang578.com/post/mac-software.html">Mac软件清单 | 算法花园</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;呼吸不止，折腾不停。记录在过去的一年，自己选择的软件和硬件。去年写指南并不能指南，所以今年直接写成指北。&lt;/p&gt;
&lt;h2 id=&quot;硬件更新&quot;&gt;硬件更新&lt;/h2&gt;
&lt;h3 id=&quot;iphone-xr-和-apple-watch-series-4&quot;&gt;iPhone XR 和 Ap
      
    
    </summary>
    
      <category term="生活志" scheme="https://xiang578.com/categories/%E7%94%9F%E6%B4%BB%E5%BF%97/"/>
    
    
      <category term="life" scheme="https://xiang578.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>Standford CS231n 2017 课程部分总结</title>
    <link href="https://xiang578.com/post/cs231n-summary.html"/>
    <id>https://xiang578.com/post/cs231n-summary.html</id>
    <published>2019-12-04T09:58:39.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>去年学习这门做的部分笔记，现在分享出来。 笔记格式有些问题，持续整理中。</p></blockquote><ul><li>大量内容参考 <a href="https://github.com/mbadry1/CS231n-2017-Summary/blob/master/README.md" target="_blank" rel="noopener">mbadry1/CS231n-2017-Summary</a></li></ul><h2 id="table-of-contents">Table of contents</h2><ul><li><a href="#standford-cs231n-2017-summary">Standford CS231n 2017 Summary</a><ul><li><a href="#table-of-contents">Table of contents</a></li><li><a href="#course-info">Course Info</a></li><li><a href="#01-introduction-to-cnn-for-visual-recognition">01. Introduction to CNN for visual recognition</a></li><li><a href="#02-image-classification">02. Image classification</a></li><li><a href="#03-loss-function-and-optimization">03. Loss function and optimization</a></li><li><a href="#04-introduction-to-neural-network">04. Introduction to Neural network</a></li><li><a href="#05-convolutional-neural-networks-cnns">05. Convolutional neural networks (CNNs)</a></li><li><a href="#06-training-neural-networks-i">06. Training neural networks I</a></li><li><a href="#07-training-neural-networks-ii">07. Training neural networks II</a></li><li><a href="#08-deep-learning-software">08. Deep learning software</a></li><li><a href="#09-cnn-architectures">09. CNN architectures</a></li></ul></li></ul><h2 id="course-info">Course Info</h2><ul><li>主页: http://cs231n.stanford.edu/</li><li>视频：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">斯坦福深度学习课程CS231N 2017中文字幕版+全部作业参考_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></li><li>大纲：<a href="http://cs231n.stanford.edu/2017/syllabus" target="_blank" rel="noopener">Syllabus | CS 231N</a></li><li>课件：<a href="http://cs231n.stanford.edu/slides/2017/" target="_blank" rel="noopener">Index of /slides/2017</a></li><li>笔记：<a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">贺完结！CS231n官方笔记授权翻译总集篇发布</a></li><li>作业仓库：<a href="https://github.com/xiang578/MachineLearning/tree/master/CS231n" target="_blank" rel="noopener">MachineLearning/CS231n at master · xiang578/MachineLearning</a></li><li>总课时: <strong>16</strong></li></ul><h2 id="introduction-to-cnn-for-visual-recognition">01. Introduction to CNN for visual recognition</h2><ul><li>视觉地出现促进了物种竞争。</li><li>ImageNet 是由李飞飞维护的一个大型图像数据集。</li><li>自从 2012 年 CNN 出现之后，图像分类的错误率大幅度下降。 神经网络的深度也从 7 层增加到 2015 年的 152 层。截止到目前，机器分类准确率已经超过人类，所以 ImageNet 也不再举办相关比赛。</li><li>CNN 在 1998 年就被提出，但是这几年才流行开来。主要原因有：1) 硬件发展，并行计算速度提到 2）大规模带标签的数据集。</li><li>Gola: Understand how to write from scratch, debug and train convolutional neural networks.</li></ul><h2 id="image-classification">02. Image classification</h2><ul><li>图像由一大堆没有规律的数字组成，无法直观的进行分类，所以存在语义鸿沟。分类的挑战有：视角变化、大小变化、形变、遮挡、光照条件、背景干扰、类内差异。<ul><li><img src="/file/2019-10-08-15387098703701.jpg"></li></ul></li><li>Data-Driven Approach<ul><li>Collect a dataset of images and labels</li><li>Use Machine Learning to train a classifier</li><li>Evaluate the classifier on new images</li></ul></li><li>图像分类流程：输入、学习、评估</li><li>图像分类数据集：<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10</a>，这个数据集包含了60000张32X32的小图像。每张图像都有10种分类标签中的一种。这60000张图像被分为包含50000张图像的训练集和包含10000张图像的测试集。</li><li>一种直观的图像分类算法：K-nearest neighbor(knn)<ul><li>为每一张需要预测的图片找到距离最近的 k 张训练集中的图片，然后选着在这 k 张图片中出现次数最多的标签做为预测图片的标签（多数表决）。</li><li>训练过程：记录所有的数据和标签 <span class="math inline">\({O(1)}\)</span></li><li>预测过程：预测给定图片的标签 <span class="math inline">\({O(n)}\)</span></li><li>Hyperparameters：k and the distance Metric</li><li>Distance Metric<ul><li>L1 distance(Manhattan Distance)</li><li>L2 distance(Euclidean Distance)</li></ul></li><li>knn 缺点<ul><li>Very slow at test time</li><li>Distance metrics on pixels are not informative</li></ul></li><li>反例：下面四张图片的 L2 距离相同<ul><li><img src="/file/2019-10-08-15387110626779.jpg" title="fig:" alt="-w622"></li></ul></li></ul></li><li>Hyperparameters: choices about the algorithm that we set ranther than learn</li><li>留一法 Setting Hyperparameters by Cross-validation:<ul><li>将数据划分为 f 个集合以及一个 test 集合，数据划分中药保证数据集的分布一致。</li><li>给定超参数，利用 f-1 个集合对算法进行训练，在剩下的一个集合中测试训练效果，重复这一个过程，直到所有的集合都当过测试集。</li><li>选择在训练集中平均表现最好的超参数。</li></ul></li><li>Linear classification: <code>Y = wX + b</code><ul><li>b 为 bias，调节模型对结果的偏好</li><li>通过最小化损失函数来，来确定 w 和 b 的值。</li></ul></li><li><strong>Linear SVM</strong>: classifier is an option for solving the image classification problem, but the curse of dimensions makes it stop improving at some point. <span class="citation" data-cites="todo">@todo</span></li><li><strong>Logistics Regression</strong>: 无法解决非线性的图像数据</li></ul><h2 id="loss-function-and-optimization">03. Loss function and optimization</h2><ul><li>通过 Loss function 评估参数质量<ul><li>比如 <span class="math display">\[L=\frac{1}{N}\sum_iL_i\left(f\left(x_i,W\right),y_i\right)\]</span></li></ul></li><li>Multiclass SVM loss 多分类支持向量机损失函数<ul><li><span class="math display">\[L_i=\sum_{j \neq y_j}\max\left(0,s_j-s_{y_i}+1\right)\]</span></li><li>这种损失函数被称为合页损失 Hinge loss</li><li>SVM 的损失函数要求正确类别的分类分数要比其他类别的高出一个边界值。</li><li>L2-SVM 中使用平方折叶损失函数<span class="math display">\[\max(0,-)^2\]</span>能更强烈地惩罚过界的边界值。但是选择使用哪一个损失函数需要通过实验结果来判断。</li><li>举例<ul><li><img src="/file/2019-10-08-15388343449162.jpg"></li><li>根据上面的公式计算：<span class="math display">\[L = \max(0,437.9-(-96.8)) + \max(0,61.95-(-96.8))=695.45\]</span></li><li>猫的分类得分在三个类别中不是最高得，所以我们需要继续优化。</li></ul></li></ul></li><li>Suppose that we found a W such that L = 0. Is this W unique?<ul><li>No! 2W is also has L = 0!</li></ul></li><li>Regularization: 正则化，向某一些特定的权值 W 添加惩罚，防止权值过大，减轻模型的复杂度，提高泛化能力，也避免在数据集中过拟合现象。<ul><li><span class="math display">\[L=\frac{1}{N}\sum_iL_i\left(f\left(x_i,W\right),y_i\right) + \lambda R(W)\]</span></li><li><code>R</code> 正则项 <span class="math display">\[\lambda\]</span> 正则化参数</li></ul></li><li>常用正则化方法<ul><li>L2<span class="math display">\[\begin{matrix} R(W)=\sum_{k}\sum_l W^2_{k,l} \end{matrix}\]</span></li><li>L1<span class="math display">\[\begin{matrix} R(W)=\sum_{k}\sum_l \left\vert W_{k,l} \right\vert \end{matrix}\]</span></li><li>Elastic net(L1 + L2): <span class="math display">\[\begin{matrix} R(W)=\sum_{k}\sum_l \beta W^2_{k,l} + \left\vert W_{k,l} \right\vert \end{matrix}\]</span></li><li>Dropout</li><li>Batch normalization</li><li>etc</li></ul></li><li>L2 惩罚倾向于更小更分散的权重向量，L1 倾向于稀疏项。</li><li>Softmax function：<ul><li><span class="math display">\[f_j(z)=\frac{e^{s_i}}{\sum e^{s_j}}\]</span></li><li>该分类器将输出向量 f 中的评分值解释为没有归一化的对数概率，通过归一化之后，所有概率之和为1。</li><li>Loss 也称交叉熵损失 cross-entropy loss <span class="math display">\[L_i = - \log\left(\frac{e^{s_i}}{\sum e^{s_j}}\right)\]</span></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure><ul><li>SVM 和 Softmax 比较<ol type="1"><li>评分，SVM 的损失函数鼓励正确的分类的分值比其他分类的分值高出一个边界值。</li><li>对数概率，Softmax 鼓励正确的分类归一化后的对数概率提高。</li><li>Softmax 永远不会满意，SVM 超过边界值就满意了。</li></ol></li><li>Optimization：最优化过程<ul><li>Follow the slope<ul><li><img src="/file/2019-10-08-15388374738605.jpg"></li></ul></li></ul></li><li>梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量，它是各个维度的斜率组成的向量。<ul><li>Numerical gradient: Approximate, slow, easy to write. (But its useful in debugging.)</li><li>Analytic gradient: Exact, Fast, Error-prone. (Always used in practice)</li><li>实际应用中使用分析梯度法，但可以用数值梯度法去检查分析梯度法的正确性。</li></ul></li><li>利用梯度优化参数的过程：<code>W = W - learning_rate * W_grad</code></li><li>learning_rate 被称为是学习率，是一个比较重要的超参数</li><li>Stochastic Gradient Descent SGD 随机梯度下降法<ul><li>每次使用一小部分的数据进行梯度计算，这样可以加快计算的速度。</li><li>每个批量中只有1个数据样本，则被称为随机梯度下降（在线梯度下降）</li></ul></li><li>图像分类任务中三大关键部分：<ol type="1"><li>评分函数</li><li>损失函数：量化某个具体参数 <span class="math inline">\({W}\)</span> 的质量</li><li>最优化：寻找能使得损失函数值最小化的参数 <span class="math inline">\({W}\)</span> 的过程</li></ol></li></ul><h2 id="introduction-to-neural-network">04. Introduction to Neural network</h2><ul><li>反向传播：在已知损失函数 <span class="math inline">\({L}\)</span> 的基础上，如何计算导数<span class="math inline">\({\nabla _WL}\)</span>？</li><li>计算图<ul><li>由于计算神经网络中某些函数的梯度很困难，所以引入计算图的概念简化运算。</li><li>在计算图中，对应函数所有的变量转换成为计算图的输入，运算符号变成图中的一个节点（门单元）。</li></ul></li><li>反向传播：从尾部开始，根据链式法则递归地向前计算梯度，一直到网络的输入端。<ul><li><img src="/file/2019-10-08-15390088806657.jpg" title="fig:" alt="-w1107"></li><li>绿色是正向传播，红色是反向传播。</li></ul></li><li>对于计算图中的每一个节点，我们需要计算这个节点上的局部梯度，之后根据链式法则反向传递梯度。</li><li>Sigmoid 函数：<span class="math inline">\({f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}}\)</span><ul><li><img src="/file/2019-10-08-15390092983570.jpg"></li><li>对于门单元 <span class="math inline">\({\frac{1}{x}}\)</span>，求导的结果是 <span class="math inline">\({-\frac{1}{x^2}}\)</span>，输入为 1.37，梯度返回值为 1.00，所以这一步中的梯度是 <span class="math inline">\({(\frac{-1}{1.37^2})*1.00=-0.53}\)</span>。</li><li>模块化思想：对 <span class="math inline">\({\sigma(x)=\frac{1}{1+e^{-x}}}\)</span> 求导的结果是 <span class="math inline">\({(1-\sigma(x))\sigma(x)}\)</span>。如果 sigmoid 表达式输入值为 1.0 时，则前向传播中的结果是 0.73。根据求导结果计算可得局部梯度是 <span class="math inline">\({(1-0.73)*0.73=0.2}\)</span>。</li></ul></li><li>Modularized implementation: forward/backwar API</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultuplyGate</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  x,y are scalars</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    z = x*y</span><br><span class="line">    self.x = x  <span class="comment"># Cache</span></span><br><span class="line">    self.y = y<span class="comment"># Cache</span></span><br><span class="line">    <span class="comment"># We cache x and y because we know that the derivatives contains them.</span></span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(dz)</span>:</span></span><br><span class="line">    dx = self.y * dz         <span class="comment">#self.y is dx</span></span><br><span class="line">    dy = self.x * dz</span><br><span class="line">    <span class="keyword">return</span> [dx, dy]</span><br></pre></td></tr></table></figure><ul><li>深度学习框架中会实现的门单元：Multiplication、Max、Plus、Minus、Sigmoid、Convolution</li><li>常用计算单元<ul><li><strong>加法门单元：</strong>把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。</li><li><strong>取最大值门单元：</strong>将梯度转给前向传播中值最大的那个输入，其余输入的值为0。</li><li><strong>乘法门单元：</strong>等值缩放。局部梯度就是输入值，但是需要相互交换，然后根据链式法则乘以输出值得梯度。</li></ul></li><li>Neural NetWorks<ul><li>(Before) Linear score function <span class="math display">\[f = Wx\]</span></li><li>(Now) 2-layer Neural NetWork <span class="math display">\[f=W_2\max(0,W_1x)\]</span></li><li>ReLU <span class="math display">\[\max(0,x)\]</span> 是激活函数，如果不使用激活函数，神经网络只是线性模型的组合，无法拟合非线性情况。</li><li>神经网络是更复杂的模型的基础组件</li></ul></li></ul><h2 id="convolutional-neural-networks-cnns">05. Convolutional neural networks (CNNs)</h2><ul><li>这一轮浪潮的开端：<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlxNet</a></li><li>卷积神经网络<ul><li>Fully Connected Layer 全连接层：这一层中所有的神经元链接在一起。</li><li>Convolution Layer：<ul><li>通过参数共享来控制参数的数量。Parameter sharing</li><li>Sparsity of connections</li></ul></li><li>卷积神经网络能学习到不同层次的输入信息</li><li>常见的神经网络结构：<code>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC</code></li><li>使用小的卷积核大小的优点：多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好地特征。而且使用的参数也会更少</li></ul></li><li>计算卷积层输出<ul><li>stride 是卷积核在移动时的步长</li><li>通用公式 (N-F)/stride + 1<ul><li>stride 1 =&gt; (7-3)/1 + 1 = 5</li><li>stride 2 =&gt; (7-3)/2 + 1 = 3</li><li>stride 3 =&gt; (7-3)/3 + 1 = 2.33</li></ul></li><li>Zero pad the border: 用零填充所有的边界，保证输入输出图像大小相同，保留图像边缘信息，提高算法性能<ul><li>步长为 1 时，需要填充的边界计算公式：(F-1)/2<ul><li>F = 3 =&gt; zero pad with 1</li><li>F = 5 =&gt; zero pad with 2</li><li>F = 7 =&gt; zero pad with 3</li></ul></li></ul></li><li>计算例子<ul><li>输入大小 <code>32*32*3</code> 卷积大小 10 5*5 stride 1 pad 2</li><li>output <code>32*32*10</code></li><li>每个 filter 的参数数量：<code>5*5*3+1 =76</code> bias</li><li>全部参数数量 76*10=760</li></ul></li></ul></li><li>卷积常用超参数设置<ul><li>卷积使用小尺寸滤波器</li><li>卷积核数量 K 一般为 2 的次方倍</li><li>卷积核的空间尺寸 F</li><li>步长 S</li><li>零填充数量 P</li></ul></li><li>Pooling layer<ul><li>降维，减少参数数量。在卷积层中不对数据做降采样</li><li>卷积特征往往对应某个局部的特征，通过池化聚合这些局部特征为全局特征</li></ul></li><li>Max pooling<ul><li>2*2 stride 2</li><li>避免区域重叠</li></ul></li><li>Average pooling</li></ul><h2 id="training-neural-networks-i">06. Training neural networks I</h2><ul><li><p>Activation functions 激活函数</p><ul><li>不使用激活函数，最后的输出会是输入的线性组合。利用激活函数对数据进行修正。</li><li><img src="/file/2019-10-08-15395012747510.jpg"></li><li>Sigmoid<ul><li>限制输出在 [0,1]区间内</li><li>firing rate</li><li>二分类输出层激活函数</li><li>Problem<ul><li>梯度消失：x很大或者很小时，梯度很小，接近于0（考虑图像中的斜率。无法得到梯度反馈。</li><li>输出不是 0 均值的数据，梯度更新效率低</li><li>exp is a bit compute expensive</li></ul></li></ul></li><li>tanh<ul><li>输出范围 [-1, 1]</li><li>0 均值</li><li>x 很大时，依然没有梯度</li><li><span class="math inline">\({f(x)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}}\)</span></li><li><span class="math inline">\({1-(tanh(x))^2}\)</span></li></ul></li><li>RELU rectified linear unit 线性修正单元<ul><li>一半空间梯度不会饱和，计算速度快，对结果又有精确的计算</li><li>不是 0 均值</li></ul></li><li>Leaky RELU<ul><li><code>leaky_RELU(x) = max(0.01x, x)</code></li><li>梯度不会消失</li><li>需要学习参数</li></ul></li><li>ELU<ul><li>比 ReLU 好用</li><li>反激活机制</li></ul></li><li>Maxout<ul><li>maxout(x) = max(w1.T<em>x + b1, w2.T</em>x + b2)</li><li>梯度不会消失</li><li>增大参数数量</li></ul></li><li>激活函数选取经验<ul><li>使用 ReLU ，但要仔细选取学习率</li><li>尝试使用 Leaky ReLU Maxout ELU</li><li>使用 tanh 时，不要抱有太大的期望</li><li>不要使用 sigmoid</li></ul></li></ul></li><li><p>数据预处理 Data Preprocessing</p><ul><li>均值减法：对数据中每个独立特征减去平均值，从几何上来看是将数据云的中心都迁移到原点。</li><li>归一化：将数据中的所有维度都归一化，使数值范围近似相等。但是在图像处理中，像素的数值范围几乎一致，所以不需要额外处理。</li></ul><p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line">X /= np.std(X, axis =<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><ul><li>图像归一化<ul><li>Subtract the mean image AlexNet<ul><li>mean image 32,32,3</li></ul></li><li>Subtract per-channel mean VGGNet<ul><li>mean along each channel = 3 numbers</li></ul></li><li>如果需要进行均值减法时，均值应该是从训练集中的图片平均值，然后训练集、验证集、测试集中的图像再减去这个平均值。</li></ul></li><li>Weight Initialization<ul><li>全零初始化<ul><li>网络中的每个神经元都计算出相同的输出，然后它们就会在反向传播中计算出相同的梯度。神经元之间会从源头上对称。</li></ul></li><li>Small random numbers<ul><li>初始化权值要非常接近 0 又不能等于 0。将权重初始化为很小的数值，以此来打破对称性</li><li>randn 函数是基于零均值和标准差的高斯分布的随机函数</li><li>W = 0.01 * np.random.rand(D,H)</li><li>问题：一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度。会减小反向传播中的“梯度信号”，在深度网络中就会出现问题。</li></ul></li><li>Xavier initialization<ul><li>W = np.random.rand(in, out) / np.sqrt(in)</li><li>校准方差，解决输入数据量增长，随机初始化的神经元输出数据的分布中的方差也增大问题。</li></ul></li><li>He initialization<ul><li>W = np.random.rand(in, out) / np.sqrt(in/2)</li></ul></li></ul></li><li>Batch normalization<ul><li>保证输入到神经网络中的数据服从标准的高斯分布</li><li>通过批量归一化可以加快训练的速度</li><li>步骤<ul><li>首先计算每个特征的平均值和平方差</li><li>通过减去平局值和除以方差对数据进行归一化</li><li><code>Result = gamma * normalizedX + beta</code><ul><li>对数据进行线性变换，相当于对数据分布进行一次移动，可以恢复数据之前的分布特征</li></ul></li></ul></li><li>BN 的好处<ul><li>加快训练速度</li><li>可以使用更快的而学习率</li><li>减少数据对初始化权值的敏感程度</li><li>相当于进行一次正则化</li></ul></li><li>BN 适用于卷积神经网络和常规的 DNN，在 RNN 和增强学习中表现不是很好</li></ul></li><li>Babysitting the Learning Provess</li><li>Hyperparameter Optimization<ul><li>Cross-validation 策略训练</li><li>小范围内随机搜索</li></ul></li></ul></li></ul><h2 id="training-neural-networks-ii">07. Training neural networks II</h2><ul><li>Optimization Algorithms:<ul><li>SGD 的问题<ul><li><code>x += - learning_rate * dx</code></li><li>梯度在某一个方向下降速度快，在其他方向下降缓慢</li><li>遇到局部最小值点，鞍点</li></ul></li><li>mini-batches GD<ul><li>Shuffling and Partitioning are the two steps required to build mini-batches</li><li>Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.</li></ul></li><li>SGD + Momentun<ul><li>动量更新：从物理学角度启发最优化问题</li><li><code>V[t+1] = rho * v[t] + dx; x[t+1] = x[t] - learningRate * V[t+1]</code></li><li>rho 被看做是动量，其物理意义与摩擦系数想类似，常取 0.9 或0.99</li><li>和 momentun 项更新方向相同的可以快速更新。</li><li>在 dx 中改变梯度方向后， rho 可以减少更新。momentun 能在相关方向加速 SGD，抑制震荡，加快收敛。</li></ul></li><li>Nestrov momentum<ul><li><img src="/file/2019-10-08-15499574039274.jpg"></li><li><code>v_prev = v; v = mu * v - learning_rate * dx; x += -mu * v_prev + (1 + mu) * v</code></li></ul></li><li>AdaGrad<ul><li><span class="math inline">\(n_t=n_{t-1}+g^2_t\)</span></li><li><span class="math inline">\(\Delta \theta _t = -\frac{\eta}{\sqrt{n_t+\epsilon}}\)</span></li><li>下面根号中会递推形成一个约束项。前期这一项比较大，能够放大梯度。后期这一项比较小，能约束梯度。</li><li>gt 的平方累积会使梯度趋向于 0</li></ul></li><li>RMSProp<ul><li>RMS 均方根</li><li>自适应学习率方法</li><li>求梯度的平方和平均数：<code>cache =  decay_rate * cache + (1 - decay_rate) * dx**2</code></li><li><code>x += - learning_rate * dx / (sqrt(cache) + eps)</code></li><li>依赖全局学习率</li></ul></li><li>Adam<ul><li>RMSProp + Momentum</li><li>It calculates an exponentially weighted average of past gradients, and stores it in variables <span class="math inline">\(v\)</span> (before bias correction) and <span class="math inline">\(v^{corrected}\)</span> (with bias correction).</li><li>It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables <span class="math inline">\(s\)</span> (before bias correction) and <span class="math inline">\(s^{corrected}\)</span> (with bias correction).</li><li>一阶到导数累积，二阶导数累积</li><li>It updates parameters in a direction based on combining information from "1" and "2".</li><li>The update rule is, for <span class="math inline">\(l = 1, ..., L\)</span>: <span class="math display">\[\begin{cases}  v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \\  v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\  s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\  s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_1)^t} \\  W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}}} + \varepsilon}  \end{cases}\]</span></li></ul>where:<ul><li>t counts the number of steps taken of Adam</li><li>L is the number of layers</li><li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters that control the two exponentially weighted averages.</li><li><span class="math inline">\(\alpha\)</span> is the learning rate</li><li><span class="math inline">\(\varepsilon\)</span> is a very small number to avoid dividing by zero</li></ul>特点：<ul><li>适用于大数据集和高维空间。</li><li>对不同的参数计算不同的自适应学习率。</li></ul></li><li>Learning decay<ul><li>学习率随着训练变化，比如每一轮在前一轮的基础上减少一半。</li><li>防止学习停止</li></ul></li><li>Second order optimization</li></ul></li><li>Regularization<ul><li>Dropout<ul><li>每一轮中随机使部分神经元失活，减少模型对神经元的依赖，增强模型的鲁棒性。</li></ul></li></ul></li><li>Transfer learning<ul><li>CNN 中的人脸识别，可以在大型的模型基础上利用少量的相关图像进行继续训练。</li></ul></li></ul><h2 id="cnn-architectures">09. CNN architectures</h2><ul><li>研究模型的方法：搞清楚每一层的输入和输出的大小关系。</li><li>LeNet - 5 [1998]<ul><li>60k 参数</li><li>深度加深，图片大小减少，通道数量增加</li><li>ac: Sigmod/tanh</li></ul></li><li>AlexNet [2012]<ul><li>(227,227,3) （原文错误）</li><li>60M 参数</li><li>LRN：局部响应归一化，之后很少使用</li></ul></li><li>VGG - 16 [2015]<ul><li>138 M</li><li>结构不复杂，相对一致，图像缩小比例和通道增加数量有规律</li></ul></li><li>ZFNet [2013]<ul><li>在 AlexNet 的基础上修改<ul><li><code>CONV1</code>: change from (11 x 11 stride 4) to (7 x 7 stride 2)</li><li><code>CONV3,4,5</code>: instead of 384, 384, 256 filters use 512, 1024, 512</li></ul></li></ul></li><li>VGG [2014]<ul><li>模型中只使用 3*3 conv：与 77 卷积有相同的感受野，而且可以将网络做得更深。比如每一层可以获取到原始图像的范围：第一层 33，第二层 55，第三层 77。</li><li>前面的卷积层参数量很少，模型中大部分参数属于底部的全连接层。</li></ul></li></ul><p><img src="/file/2019-10-08-15705415499052.jpg"></p><ul><li>GoogLeNet<ul><li>引入 <code>Inception module</code><ul><li>design a good local network topology (network within a network) and then stack these modules on top of each other</li><li>该模块可以并行计算</li><li>conv 和 pool 层进行 padding，最后将结果 concat 在一起</li></ul></li></ul></li></ul><figure><img src="/file/2019-10-08-15705420412305.jpg" alt="Reset"><figcaption>Reset</figcaption></figure><ul><li>ResNet<ul><li>目标：深层模型表现不应该差于浅层模型，解决随着网络加深，准确率下降的问题。</li><li><code>Y = (W2* RELU(W1x+b1) + b2) + X</code></li><li>如果网络已经达到最优，继续加深网络，residual mapping会被设置为 0，一直保存网络最优的情况。</li></ul></li></ul><h2 id="reference">Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam） - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;去年学习这门做的部分笔记，现在分享出来。 笔记格式有些问题，持续整理中。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;大量内容参考 &lt;a href=&quot;https://github.com/mbadry1/CS231n-2017-Summa
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ml, course" scheme="https://xiang578.com/tags/ml-course/"/>
    
  </entry>
  
  <entry>
    <title>算法花园风格清单</title>
    <link href="https://xiang578.com/post/blog-writing-checklist.html"/>
    <id>https://xiang578.com/post/blog-writing-checklist.html</id>
    <published>2019-11-03T10:03:23.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>李如一在 <a href="https://www.qdaily.com/articles/1397.html" target="_blank" rel="noopener">写作风格手册</a> 中提到写作风格的作用是 「保持机构和组织内部的文体统一，提高沟通效率。」</p><p>本清单会持续更新，如果有相关的建议，可以在留言中告诉我。</p></blockquote><p>算法花园定位为个人博客，也是我和这个世界沟通的窗口。为提高读者阅读体验，参考相关文章后，推出该清单统一网站文章的基础风格。</p><h2 id="写作">写作</h2><ol type="1"><li>减少形容词使用，尽可能删除 「的」和「了」。</li><li>给出引用图片及引文来源。</li><li>文章如果发布后大幅度修改，在末尾给出版本信息。</li><li>写完文章后，整体阅读一遍。</li></ol><h2 id="排版">排版</h2><ol type="1"><li>中文、英文、数字中间加空格，数字与单位之间无需增加空格，全角标点与其他字符之间不加空格。链接前后增加空格用以区分。</li><li>不重复使用标点符号。</li><li>体中文中使用直角引号 「」以及『 』。</li><li>使用全角中文标点，数字使用半角字符。中文中出现英文部分，仍然使用中文标点。</li><li>遇到完整的英文整句、特殊名词，其內容使用半角标点。</li><li>专有名词使用正确的大小写，使用公认的缩写。</li><li>todo 如何处理图片排版和命名。</li><li>使用英文命名文档，使用 <code>-</code> 来连接。为保证搜索引擎效果，尽量不要修改文档名称。</li><li>每篇文章开头添加简单介绍 <code>&lt;!--more--&gt;</code>。</li><li>发布后，在网页中确认格式是否符合预期、链接能否点击以及图片能否展示。</li></ol><h2 id="版本">版本</h2><blockquote><p>20191103: 第一版</p></blockquote><h2 id="参考">参考</h2><ul><li><a href="https://www.qdaily.com/articles/1397.html" target="_blank" rel="noopener">写作风格手册_设计词典_好奇心日报</a></li><li><a href="https://mazhuang.org/wiki/chinese-copywriting-guidelines/" target="_blank" rel="noopener">中文文案排版指北（简体中文版） — 码志</a></li><li><a href="https://zhuanlan.zhihu.com/p/49729668" target="_blank" rel="noopener">简体中文文本排版指南 - 知乎</a></li><li><a href="https://sspai.com/post/37815" target="_blank" rel="noopener">少数派写作排版指南 - 少数派</a></li><li><a href="https://www.jianshu.com/p/8ffc3e0d11e2" target="_blank" rel="noopener">城堡制作检查清单 0.1 版 - 简书</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;李如一在 &lt;a href=&quot;https://www.qdaily.com/articles/1397.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;写作风格手册&lt;/a&gt; 中提到写作风格的作用是 「保持机构和组织内部的文体
      
    
    </summary>
    
      <category term="站务" scheme="https://xiang578.com/categories/%E7%AB%99%E5%8A%A1/"/>
    
    
      <category term="blog" scheme="https://xiang578.com/tags/blog/"/>
    
      <category term="writing" scheme="https://xiang578.com/tags/writing/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统 CTR Paper 阅读小结</title>
    <link href="https://xiang578.com/post/ctr.html"/>
    <id>https://xiang578.com/post/ctr.html</id>
    <published>2019-10-24T13:13:35.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<p>去年看的 <a href="https://book.douban.com/subject/30416291/" target="_blank" rel="noopener">AI极简经济学</a> 中提到一个观点：AI 对企业的贡献是带来强大的推荐预测能力，比如打开手机淘宝，首屏大量的推荐商品。作者猜想未来的某一天，淘宝上的商家可以直接把预测的商品寄到你家，然后由你选择是否购买。支撑这个想法的关键是推荐系统中的点击率预测技术（CTR，Click-Through-Rate），大量的学术机构和商业公司发表相关的论文。目前所做的业务模型中，也参考了很多 CTR 中的方法。自己也看过一些这个领域的论文，并且做了一些笔记。没有太多将笔记单独整理发出来的意义，通过这一篇阅读小结，和大家分享我的学习过程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;去年看的 &lt;a href=&quot;https://book.douban.com/subject/30416291/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AI极简经济学&lt;/a&gt; 中提到一个观点：AI 对企业的贡献是带来强大的推荐预测能力，比如打开手机淘
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ctr" scheme="https://xiang578.com/tags/ctr/"/>
    
      <category term="fm" scheme="https://xiang578.com/tags/fm/"/>
    
      <category term="gbdt" scheme="https://xiang578.com/tags/gbdt/"/>
    
      <category term="dnn" scheme="https://xiang578.com/tags/dnn/"/>
    
  </entry>
  
  <entry>
    <title>(WDR) Learning to Estimate the Travel Time</title>
    <link href="https://xiang578.com/post/wdr.html"/>
    <id>https://xiang578.com/post/wdr.html</id>
    <published>2019-07-28T14:14:33.000Z</published>
    <updated>2020-03-28T09:07:31.367Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>严重申明：本篇文章所有信息从论文、网络等公开渠道中获得，不会透露滴滴地图 ETA 任何实现方法。</p></blockquote><p>这篇论文是滴滴时空数据 2018 年在 KDD 上发表的关于在 ETA 领域应用深度学习的文章，里面提到的深度学习方法大家都耳熟能详，主要是属于工业界的创新。说点题外话，<a href="https://www.zhihu.com/question/22385673/answer/522580778" target="_blank" rel="noopener">你为什么从滴滴出行离职？ - 知乎</a> 中提到一点：</p><blockquote><p>8.同年大跃进，在滴滴中高层的眼里，没有BAT。滴滴单量超淘宝指日可待，GAFA才是滴滴要赶超的对象。百度系，LinkedIn系，学院派，uber帮，联想系，MBB就算了，据说连藤校都混成了一个小圈子。。一个项目A team ，B team。一个ETA，投入了多少人力自相残杀？MAPE做到0%又如何？用户体验就爆表了吗？长期留存就高枕无忧了吗？风流总被雨打风吹去，滴滴是二龙山，三虫聚首？是不是正确的事情不知道，反正跟着公司大势所趋，升D10保平安。</p></blockquote><p>简单介绍一下背景：ETA 是 Estimate Travel Time 的缩写，中文大概能翻译成到达时间估计。这个问题描述是：在某一个时刻，估计从 A 点到 B 点需要的时间。对于滴滴，关注的是司机开车把乘客从起点送到终点需要的时间。抽象出来 ETA 就是一个时间空间信息相关的回归问题。CTR 中常用的方法都可以在这里面尝试。</p><p>对于这个问题：文章中提到一个最通用的方法 Route ETA：即在获得 A 点到 B 点路线的情况下，计算路线中每一段路的行驶时间，并且预估路口的等待时间。最终 ETA 由全部时间相加得到。这种方法实现起来很简单，也能拿到一些收益。但是仔细思考一下，没有考虑未来道路的同行状态变化情况以及路线的拓扑关系。针对这些问题，文章中提到滴滴内部也有利用 GBDT 或者 FM 的方法解决 ETA 问题，不过没有仔细写实现的方法，我也不好继续分析下去。</p><h2 id="评价指标">评价指标</h2><p>对于 ETA 问题来说，工业界和学术界常用的指标是 MAPE(mean absolute percentage error)，<span class="math inline">\({y_i}\)</span> 是司机实际从 A 点到 B 点花费的时间，<span class="math inline">\({f(x_i)}\)</span> 是 ETA 模型估计出来的时间。得到计算公式如下：</p><p><span class="math display">\[{min_f \sum_{i=1}^{N}\frac{|y_i - f(x_i)|}{y_i}}\]</span></p><p>多说一句，如果使用 GBDT 模型实现 ETA 时，这个损失函数的推导有点困难，全网也没有看见几个人推导过。</p><p>这个公式主要考虑预估时间偏差大小对用户感知体验的影响，目前我们更加关心极端 badcase 对用户的影响。</p><h2 id="特征">特征</h2><ul><li>特征：<ul><li>空间特征：路线序列、道路等级、POI等</li><li>时间特征：月份、星期、时间片等</li><li>路况特征：道路的通行速度、拥堵程度</li><li>个性化信息：司机特征、乘客特征、车辆特征</li><li>附近特征：天气、交通管制</li></ul></li></ul><h2 id="模型">模型</h2><p>WDR 模型，包含 3 个部分： - Wide Learning Models：利用交叉积学习信息，泛化能力。 - Deep Neural networks：对 sparse feature 做一次 Embedding，使用 3 层 MLP 和 ReLU 的网络。 - Long-Short Term Memory：解决 Wide &amp; Deep 没用使用路线的顺序特征，利用 LSTM 学习 link 信息以及序列信息，最后一个单元的隐藏状态作为输出。 - Regressor： 将 3 个模型的输出综合起来，作为最后的 ETA 预估。MAPE 作为损失函数，利用 BP 训练模型。</p><figure><img src="/file/15643233780326.jpg" alt="-w962"><figcaption>-w962</figcaption></figure><p>上面模型中使用的特征分类： - Dense feature：行程级别的实数特征，比如起终点球面距离、起终点 GPS 坐标等。 - Sparse feature：行程级别的离散特征，比如时间片编号、星期几、天气类型等。 - Sequential feature：link 级别的特征，实数特征直接输入模型，而离散特征先做 embedding 再输入模型。注意，这里不再是每个行程一个特征向量，而是行程中每条 link 都有一个特征向量。比如，link 的长度、车道数、功能等级、实时通行速度等。</p><h2 id="评估">评估</h2><p>包括两部分：离线评估和在线评估。</p><p>离线评估中取滴滴 2017 年北京前6个月的订单数据，分成两类 pickup （平台给司机分单后，司机开车去接乘客的过程）和 trip （司机接到乘客并前往目的地的过程）。具体数据集划分如下。</p><p><img src="/file/15643234056004.jpg"></p><p>离线使用 MAPE 来评价模型。在线评估时，为了更好的与用户体验挂钩，采用多个指标来衡量 ETA 的效果。包括： - APE20: absolute percentage error 小于 20% 的订单占比。（越大越好） - Badcase率：APE 大于 50% 或者 AE 大于 180s 的订单占比，定义为对用户造成巨大影响的情况。（越小越好） - 低估率：低估订单的比例。（越小越好）</p><p>离线结果如下图所示，说来汗颜 PTTE 和 TEMP 是什么算法我都不知道…… WD-MLP 指的是将 WDR 中的 R 部分换成 MLP 。最终 WDR 较 route-ETA 有巨大提升，而且 LSTM 引入的序列信息也在 pikcup 上提升了 0.75%。文章的最后还提出来，LSTM 也可以换成是 Attention，这样替换有什么优点和缺点留给大家思考。</p><p><img src="/file/15643234140115.jpg"></p><p>在线实验结果如下图所示，滴滴 ETA MAPE 明显小于 com1、com2、com3 ，这三家地图公司具体是哪三家，大家也能猜到吧。</p><p><img src="/file/15643234258049.jpg"></p><h2 id="eta-服务工程架构">ETA 服务工程架构：</h2><figure><img src="/file/15643234352132.jpg" alt="-w486"><figcaption>-w486</figcaption></figure><p>从上面的图中可以看出 ETA 服务工程架构主要包括三个部分： - Data Aggregation：包括利用 Map Matching 将司机上传到平台的 GPS 对应到滴滴的 Map Info 中得到司机真实行驶过的路线信息，Order Context 指的是订单相关的信息，augmented Data 额外数据比如上文说的交通情况相关信息。 - Offline Training：利用上一步得到的历史数据训练模型。这里可以值得一提的是，ETA 模型是和时间强相关的（节假日和工作日的数据分布明显不同），所以在文章中作者指出将拿出最新的一部分数据用来 fine-tune 训练出来的 WDR 模型。 - Online Service：这里需要一个完整的模型服务系统，其他公司也有很多分享，所以原文没有多提。</p><h2 id="总结">总结</h2><p>从上面简单的介绍来看，ETA 可以使用 CTR 和 NLP 领域的很多技术，大有可为。最后，滴滴 ETA 团队持续招人中（社招、校招、日常实习等），感兴趣者快快和我联系。</p><h2 id="参考">参考</h2><blockquote><ul><li><a href="https://www.leiphone.com/news/201808/EmRne91YDwwNCl4A.html" target="_blank" rel="noopener">KDD 2018：滴滴提出WDR模型显著提升ETA预测精度 | 雷锋网</a></li><li><a href="http://www.semocean.com/lbs%e5%b7%a5%e4%b8%9a%e7%95%8ceta%e5%ba%94%e7%94%a8%e5%8f%8a%e6%bb%b4%e6%bb%b4wdr%e6%8a%80%e6%9c%af/" target="_blank" rel="noopener">LBS工业界ETA应用及滴滴WDR技术 – Semocean</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;严重申明：本篇文章所有信息从论文、网络等公开渠道中获得，不会透露滴滴地图 ETA 任何实现方法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这篇论文是滴滴时空数据 2018 年在 KDD 上发表的关于在 ETA 领域应用深度学习的文章，里面提到的深
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="lstm" scheme="https://xiang578.com/tags/lstm/"/>
    
      <category term="widedeep" scheme="https://xiang578.com/tags/widedeep/"/>
    
      <category term="didi" scheme="https://xiang578.com/tags/didi/"/>
    
  </entry>
  
  <entry>
    <title>(FM) Factorization Machines</title>
    <link href="https://xiang578.com/post/fm.html"/>
    <id>https://xiang578.com/post/fm.html</id>
    <published>2019-07-28T10:18:52.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<p>Factorization Machines(FM) 由日本 Osaka University 的 Steffen Rendle [1] 在 2010 年提出,是一种常用的因子机模型。</p><h2 id="fm">FM</h2><p>假设现在有一个电影评分的任务，给定如下如所示的特征向量 x（包括用户名、当前在看的电影、已经打分的电影、时间特征、之前看的电影），预测用户对当前观看电影的评分。</p><figure><img src="/file/15643023365227.jpg" alt="电影评分"><figcaption>电影评分</figcaption></figure><p>作者在线性回归模型的基础上，添加交叉项部分，用来自动组合二阶特征。 <span class="math display">\[\hat y(x):= w_0 + \sum_{i=1}^{n} w_ix_i + \sum_{i=1}^n \sum_{j=i+1}^n \left \langle v_i,v_j \right \rangle x_iy_i\]</span></p><p>其中交叉特征的权重由两个向量的点积得到，可以解决没有在模型中出现的特征组合权重问题，以及减少参数数量。</p><p><span class="math display">\[W_{i,j}=\left \langle v_i,v_j \right \rangle = \sum_{f=1}^kv_{i,f} \cdot v_{j,f}\]</span></p><p>通过下面的方法来化简交叉项权重计算，算法复杂度降到线性。</p><p><span class="math display">\[\sum_{i=1}^n \sum_{j=i+1}^n \left \langle v_i,v_j \right \rangle x_iy_i = \frac{1}{2}\sum^k_{f=1} \left( \left(\sum_{i=1}^nv_{i,f}x_i \right)^2 - \sum^n_{i=1} v^2_{i,f} x_i^2 \right)\]</span></p><p>对交叉项部分的求导：</p><p><span class="math display">\[\frac{\partial}{\partial \theta} \hat y \left( x \right) =\begin{cases}1, &amp; \text{ if $\theta$ is $w_0$} \\x_i, &amp; \text{ if $\theta$ is ${w_i}$} \\x_i\sum^n_{j=1} v_{j,f}x_j - v_{i,f}x_i^2, &amp;\text{if $\theta$ is ${v_{i,f}}$}  \end{cases}\]</span></p><p>其中 <span class="math inline">\({\sum^n_{j=1} v_{j,f}x_j}\)</span> 与 <span class="math inline">\({x_i}\)</span> 无关，可以在计算导数前预处理出来。</p><h3 id="fm-vs-svm">FM vs SVM</h3><p>对于经典的特征组合问题，不难想到使用 SVM 求解。Steffen 在论文中也多次将 FM 和 SVM 做对比。</p><p>在考虑 SVM 的 Polynomial kernel 为 <span class="math inline">\({K(\mathbf{x}, \mathbf{z}) :=(\langle\mathbf{x}, \mathbf{z}\rangle+ 1)^{2}}\)</span>，映射 <span class="math display">\[\begin{array}{l}{\phi(\mathbf{x}) :=\left(1, \sqrt{2} x_{1}, \ldots, \sqrt{2} x_{n}, x_{1}^{2}, \ldots, x_{n}^{2}\right.} {\sqrt{2} x_{1} x_{2}, \ldots, \sqrt{2} x_{1} x_{n}, \sqrt{2} x_{2} x_{3}, \ldots, \sqrt{2} x_{n-1} x_{n} )}\end{array}\]</span></p><p>SVM 的公式可以转化为：</p><p><span class="math display">\[\begin{aligned} \hat{y}(\mathbf{x})=w_{0}+\sqrt{2} \sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} w_{i, i}^{(2)} x_{i}^{2} &amp;+\sqrt{2} \sum_{i=1}^{n} \sum_{j=i+1}^{n} w_{i, j}^{(2)} x_{i} x_{j} \end{aligned}\]</span></p><p>论文中提到一句上面的公式中 <span class="math inline">\({w_{i}}\)</span> 和 <span class="math inline">\({w_{i,i}}\)</span> 表达能力类似，我猜这也是为什么 FM 中没有自身交叉项的原因吧。</p><p>FM 相比于 SVM 有下面三个特点： 1. SVM 中虽然也有特征交叉项，但是只能在样本中含有相对应的特征交叉数据时才能学习。但是 FM 能在数据稀疏的时候学习到交叉项的参数。 2. SVM 问题无法直接求解，常用的方法是根据拉格朗日对偶性将原始问题转化为对偶问题。 3. 在使用模型预测时，SVM 依赖部分训练数据（支持向量），FM 模型则没有这种依赖。</p><h3 id="rank">Rank</h3><p>FM 用来做回归和分类都很好理解，简单写一下如何应用到排序任务中。以 pairwise 为例。假设排序结果有两个文档 <span class="math inline">\({x_i}\)</span> 和 <span class="math inline">\({x_j}\)</span>，显然用户点击文档有先后顺序，如果先点击 <span class="math inline">\({x_i}\)</span>，记 label <span class="math inline">\({y_{ij}=1}\)</span>，反之点击 <span class="math inline">\({x_j}\)</span>，label <span class="math inline">\({y_{ij}=0}\)</span>。模型需要去预测 <span class="math inline">\({\hat y_{ij} = sigmoid(\hat y_i - \hat y_j)}\)</span>。</p><p>参考逻辑回归，用最大似然对参数进行估计，得到损失函数为 <span class="math inline">\({L=\log(1+\exp(-(\hat y(x_i)-\hat y(x_j))}\)</span>。优化过程和前面提到类似。</p><h2 id="nfm">NFM</h2><p>NFM 和 AFM 两篇论文是同一个作者写的，所以文章的结构很相近。</p><p>FM 模型由于复杂度问题，一般只使用特征二阶交叉的形式，缺少对 higher-order 以及 non-liner 特征的交叉能力。NFM 尝试通过引入 NN 来解决这个问题。</p><p>NFM 的结构如下：第一项和第二项是线性回归，第三项是神经网络。神经网络中利用 FM 模型的二阶特征交叉结果做为输入，学习数据之间的高阶特征。与直接使用高阶 FM 模型相比，可以降低模型的训练复杂度，加快训练速度。</p><p><span class="math display">\[\hat{y}_{N F M}(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+f(\mathbf{x})\]</span></p><p>NFM 的神经网络部分包含 4 层，分别是 Embedding Layer、Bi-Interaction Layer、Hidden Layers、Prediction Score。</p><figure><img src="/file/15643037118475.jpg" alt="NFM"><figcaption>NFM</figcaption></figure><ul><li>Embedding Layer 层对输入的稀疏数据进行 Embedding 操作。最常见的 Embedding 操作是在一张权值表中进行 lookup ，论文中作者强调他们这一步会将 Input Feture Vector 中的值与 Embedding 向量相乘。</li><li>Bi-Interaction Layer 层是这篇论文的创新，对 embedding 之后的特征两两之间做 element-wise product，并将结果相加得到一个 k 维（Embeding 大小）向量。这一步相当于对特征的二阶交叉，与 FM 类似，这个公式也能进行化简：</li></ul><p><span class="math display">\[f_{B I}\left(\mathcal{V}_{x}\right)=\sum_{i=1}^{n} \sum_{j=i+1}^{n} x_{i} \mathbf{v}_{i} \odot x_{j} \mathbf{v}_{j} =\frac{1}{2}\left[\left(\sum_{i=1}^{n} x_{i} \mathbf{v}_{i}\right)^{2}-\sum_{i=1}^{n}\left(x_{i} \mathbf{v}_{i}\right)^{2}\right]\]</span></p><ul><li>Hidden Layers 层利用常规的 DNN 学习高阶特征交叉</li><li>Prdiction Layer 层输出最终的结果： <span class="math display">\[\begin{aligned} \hat{y}_{N F M}(\mathbf{x}) &amp;=w_{0}+\sum_{i=1}^{n} w_{i} x_{i} +\mathbf{h}^{T} \sigma_{L}\left(\mathbf{W}_{L}\left(\ldots \sigma_{1}\left(\mathbf{W}_{1} f_{B I}\left(\mathcal{V}_{x}\right)+\mathbf{b}_{1}\right) \ldots\right)+\mathbf{b}_{L}\right) \end{aligned}\]</span></li></ul><p>实验结果：</p><p><img src="/file/15643059963915.jpg"></p><h2 id="afm">AFM</h2><p>AFM(Attentional Factorization Machine), 在 FM 的基础上将 Attention 机制引入到交叉项部分，用来区分不同特征组合的权重。</p><p><span class="math display">\[\hat{y}_{A F M}(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\mathbf{p}^{T} \sum_{i=1}^{n} \sum_{j=i+1}^{n} a_{i j}\left(\mathbf{v}_{i} \odot \mathbf{v}_{j}\right) x_{i} x_{j}\]</span></p><p>单独看上面公式中的第三项结构：</p><p><img src="/file/15643076111641.jpg"></p><ul><li>Embedding Layer 与 NFM 里面的作用一样，转化特征。</li><li>Pair-wise Interaction Layer 是将特征两两交叉，如果对这一步的结果求和就是 FM 中的交叉项。</li><li>Attention 机制在 Attention-based Pooling 层引入。将 Pair-wise Interaction Layer 中的结果输入到 Attention Net 中，得到特征组合的 score <span class="math inline">\({a_{i j}^{\prime} }\)</span>，然后利用 softmax 得到权重矩阵 <span class="math inline">\({a_{ij}}\)</span>。 <span class="math display">\[\begin{aligned} a_{i j}^{\prime} &amp;=\mathbf{h}^{T} \operatorname{Re} L U\left(\mathbf{W}\left(\mathbf{v}_{i} \odot \mathbf{v}_{j}\right) x_{i} x_{j}+\mathbf{b}\right) \\ a_{i j} &amp;=\frac{\exp \left(a_{i j}^{\prime}\right)}{\sum_{(i, j) \in \mathcal{R}_{x}} \exp \left(a_{i j}^{\prime}\right)} \end{aligned}\]</span></li><li>最后将 Pair-wise Interaction Layer 中的二阶交叉结果和权重矩阵对应相乘求和得到 AFM 的交叉项。</li></ul><p><img src="/file/15643086043204.jpg"></p><p>和前一节的实验结果对比，AFM 效果比 NFM 要差一些。这大概就能说明为什么论文中提到 NFM，但是最后没有把 NFM 的结果贴出来，实在是机智。又回到，发论文是需要方法有创新，还是一味追求 state-of-the-art。</p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="http://kubicode.me/2018/02/23/Deep%20Learning/Deep-in-out-Factorization-Machines-Series/#NFM" target="_blank" rel="noopener">深入浅出Factorization Machines系列 | Kubi Code'Blog</a></li><li><a href="https://zhuanlan.zhihu.com/p/34666996" target="_blank" rel="noopener">FM模型在LTR类问题中的应用 - 知乎</a></li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Factorization Machines(FM) 由日本 Osaka University 的 Steffen Rendle [1] 在 2010 年提出,是一种常用的因子机模型。&lt;/p&gt;
&lt;h2 id=&quot;fm&quot;&gt;FM&lt;/h2&gt;
&lt;p&gt;假设现在有一个电影评分的任务，给定
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="fm" scheme="https://xiang578.com/tags/fm/"/>
    
  </entry>
  
  <entry>
    <title>(Wide&amp;Deep) Wide &amp; Deep Learning for Recommender Systems</title>
    <link href="https://xiang578.com/post/wide-and-deep.html"/>
    <id>https://xiang578.com/post/wide-and-deep.html</id>
    <published>2019-07-02T12:56:43.000Z</published>
    <updated>2020-03-28T09:07:31.367Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>这是一篇推荐系统相关的论文，场景是谷歌 Play Store 的 App 推荐。文章开头，作者点明推荐系统需要解决的两个能力： memorization 和 generalization。</p><p><strong>memorization</strong> 指的是学习数据中出现过的组合特征能力。最常使用的算法是 Logistic Regression，简单、粗暴、可解释性强，而且会人工对特征进行交叉，从而提升效果。但是，对于在训练数据中没有出现过的特征就无能为力。</p><p><strong>generalization</strong> 指的是通过泛化出现过特征从解释新出现特征的能力。常用的是将高维稀疏的特征转换为低维稠密 embedding 向量，然后使用 fm 或 dnn 等算法。与 LR 相比，减少特征工程的投入，而且对没有出现过的组合有较强的解释能力。但是当遇到的用户有非常小众独特的爱好时（对应输入的数据非常稀疏和高秩），模型会过度推荐。</p><p>综合前文 ，作者提出一种新的模型 Wide &amp; Deep。</p><h2 id="模型">模型</h2><p>从文章题目中顾名思义，Wide &amp; Deep 是融合 Wide Models 和 Deep Models 得到，下图形象地展示出来。</p><figure><img src="/file/15610337156969.jpg" alt="Wide &amp; Deep Models"><figcaption>Wide &amp; Deep Models</figcaption></figure><p><strong>Wide Component</strong> 是由一个常见的广义线性模型：<span class="math inline">\({y=w^Tx+b}\)</span>。其中输入的特征向量 <span class="math inline">\({x}\)</span> 包括两种类型：原始输入特征（raw input features）和组合特征（transformed features）。</p><p>常用的组合特征公式如下： <span class="math display">\[{\phi_k(x)=\prod_{i=1}^dx_i^{c_{ki}},c_{ki}\in\{0,1\}}\]</span> <span class="math inline">\({c_{ki}}\)</span> 代表对于第k个组合特征是否包含第i个特征。<span class="math inline">\({x_i}\)</span>是布尔变量，代表第i个特征是否出现。例如对于组合特征 <code>AND(gender=female, language=en)</code> 当且仅当 x 满足<code>(“gender=female” and “language=en”)</code>时，<span class="math inline">\({\phi_k(x)=1}\)</span>。</p><p><strong>Deep Component</strong> 是一个标准的前馈神经网络，每一个层的形式诸如：<span class="math inline">\({a^{(l+1)}=f(W^{(l)}a^{(l)} + b^{(l)})}\)</span>。对于输入中的 categorical feature 需要先转化成低维稠密的 embedding 向量，再和其他特征一起喂到神经网络中。</p><p>对于这种由基础模型组合得到的新模型，常用的训练形式有两种：joint training 和 ensemble。ensemble 指的是，不同的模型单独训练，且不共享信息（比如梯度）。只有在预测时根据不同模型的结果，得到最终的结果。相反，joint training 将不同的模型结果放在同一个损失函数中进行优化。因此，ensmble 要且模型独立预测时就有有些的表现，一般而言模型会比较大。由于 joint training 训练方式的限制，每个模型需要由不同的侧重。对于 Wide&amp;Deep 模型来说，wide 部分只需要处理 Deep 在低阶组合特征学习的不足，所以可以使用简单的结果，最终完美使用 joint traing。</p><p>预测时，会将 Wide 和 Deep 的输出加权得到结果。在训练时，使用 logistic loss function 做为损失函数。模型优化时，利用 mini-batch stochastic optimization 将梯度信息传到 Wide 和 Deep 部分。然后，Wide 部分通过 FTRL + L1 优化，Deep 部分通过 AdaGrad 优化。</p><h2 id="实验">实验</h2><p>本篇论文选择的实验场景是谷歌 app 商店的应用推荐，根据用户相关的历史信息，推荐最有可能会下载的 App。</p><p>使用的模型如下： <img src="/file/15611212418726.jpg" alt="Wide &amp; Deep model structure for apps recommendation."></p><p>一些细节： - 对于出现超过一定次数的 categorical feature，ID 化后放入到模型中。 - Continuous real-valued features 通过 cumulative distribution function 归一化到 [0, 1] 区间。 - categorical feature 由 32 维 embedding 向量组成，最终的输入到 Deep 部分的向量大概在 1200 维。 - 每天在前一天 embedding 和模型的基础上进行增量更新。</p><p>实验结果：</p><figure><img src="/file/15610335523493.jpg" alt="实验结果"><figcaption>实验结果</figcaption></figure><p>Wide &amp; Deep 模型相对于其他两个模型毫无疑问有提升。但结果中也一个反常的现象：单独使用 Deep 模型离线 AUC 指标比单独使用 Wide 模型差，但是线上对比实验时却有较大的提升。论文中作者用了一句：线下实验中的特征是固定的，线上实验会遇到很多没有出现过的特征组合，Deep 相对于 Wide 有更好的模型泛化能力，所以会有反常现象。由于笔者工作中不关注 AUC，也没有办法继续分析。</p><h2 id="总结">总结</h2><p>作者从推荐系统的的 memorization 和 generalization 入手，设计出新的算法框架。通过线上和线下实验实验，证明 Deep 和 Wide 联合是必须的且有效的。最终也在自己的业务场景带来提升。</p><h2 id="reference">Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/37733208" target="_blank" rel="noopener">Wide &amp; Deep Learning for Recommender Systems - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/53361519" target="_blank" rel="noopener">详解 Wide &amp; Deep 结构背后的动机 - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;这是一篇推荐系统相关的论文，场景是谷歌 Play Store 的 App 推荐。文章开头，作者点明推荐系统需要解决的两个能力： memorization 和 generalization。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;memoriz
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ctr" scheme="https://xiang578.com/tags/ctr/"/>
    
      <category term="dnn" scheme="https://xiang578.com/tags/dnn/"/>
    
      <category term="lr" scheme="https://xiang578.com/tags/lr/"/>
    
  </entry>
  
  <entry>
    <title>Best of iPhone 2019 软件清单</title>
    <link href="https://xiang578.com/post/best-of-iphone-2019.html"/>
    <id>https://xiang578.com/post/best-of-iphone-2019.html</id>
    <published>2019-06-22T13:32:22.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<p>一直想做一个推荐软件的系列文章，不过完成 2017 年的<a href="https://xiang578.com/post/iphone5s.html">iPhone软件清单</a> 后就没有动力……很多时候在思考，自己为什么一定要使用 iPhone？iOS 中的软件正是最好的答案，让每一个人享受科技带来的快乐。</p><p>这一篇文章和 2017 年的形式一样，删除常用的软件，推荐一些我认为有趣的软件。自从 iOS 12 中引入屏幕时间，今年的推荐顺序就按照屏幕时间中的排序。</p><p>目前使用设备：iPhone XR &amp; Apple Watch Series 4</p><ul><li>Inoreader：RSS 订阅服务商以及 RSS 阅读器。随着这两年对推荐算法相关的新闻阅读器批判，RSS 大有一股复新之势。可惜的是在去年年底左右，inoreader 中每个免费帐号只能关注 150 个订阅源。目前，我通过使用多个帐号来临时解决。</li><li>脉脉：职场社交软件。有匿名区以及公司圈，定期查看公司内部的吐槽和爆料。用 Leader 的话来说，整天操 M6 的心。</li><li>Keep/Nike Running/健身记录：锻炼相关 App 集合。Keep 和 Apple Watch 的配合太差，很多数据不能同步。自带的健身记录展示数据不够详细。Nike Running 不太用。</li><li>Overcast：免费且功能强大的播客软件，良心到只有少量的播客相关广告。喜欢智能播放和人声增强功能。最近，也支持章节播放功能。</li><li>AutoSleep：配合 Apple Watch 实现睡眠监控功能。原来大致为读取 Apple Watch 写入健康中的数据判断是否入睡以及睡眠质量。由于硬件的限制，很多时候还是不太准确。不过我也和网上说的一样，每天起来需要看这软件中的评分，判断昨天睡的如何……</li><li>Kindle：管它微信阅读如何火爆，不忘初心依旧选择 Kindle。配合 Kindle Unlimted，可以阅读很多好书。</li><li>Google 相册：免费强大的照片管理软件，允许谷歌优化照片质量的后，可以无限存储。</li><li>Quantumult：梯子软件，美区下载，比小火箭好用。</li><li>OmniFoucs：GTD 软件，居然使用的正版……</li><li>Anki：多年之后，我的摘抄终于有了存放之地。</li><li>Day One：日记软件，喜欢里面的去年今天功能，陆续把日记导入中。</li><li>Pocket：稍后读。</li><li>1 Password：密码软件，全平台通用。</li><li>熊猫吃短信：垃圾短信过滤，可以自己配置规则把公司的报警短信给屏蔽了。</li><li>Ulysses：Markdown 软件，iOS 版也包含在 SetApp 订阅。</li></ul><p>在与 2017 版相比，有两个趋势：1. 减少很多与学习强相关的软件。 2. 越来越多的付费或者订阅制软件。期待明年。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一直想做一个推荐软件的系列文章，不过完成 2017 年的&lt;a href=&quot;https://xiang578.com/post/iphone5s.html&quot;&gt;iPhone软件清单&lt;/a&gt; 后就没有动力……很多时候在思考，自己为什么一定要使用 iPhone？iOS 中的软件正是
      
    
    </summary>
    
      <category term="生活志" scheme="https://xiang578.com/categories/%E7%94%9F%E6%B4%BB%E5%BF%97/"/>
    
    
      <category term="iOS" scheme="https://xiang578.com/tags/iOS/"/>
    
      <category term="app" scheme="https://xiang578.com/tags/app/"/>
    
      <category term="best of" scheme="https://xiang578.com/tags/best-of/"/>
    
  </entry>
  
  <entry>
    <title>Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr)</title>
    <link href="https://xiang578.com/post/gbdt_lr.html"/>
    <id>https://xiang578.com/post/gbdt_lr.html</id>
    <published>2019-06-16T12:56:43.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<p><strong>主题：</strong>Facebook 2014 年发表的广告点击预测文章。最主要是提出经典 GBDT+LR 模型，可以自动实现特征工程，效果好比于人肉搜索。另外，文章中还给出一个 online learning 的工程框架。</p><p><strong>问题：</strong> - GBDT 如何处理大量 id 类特征 - 广告类对于 user id 的处理：利用出现的频率以及转化率来代替 - id 特征放在 lr 中处理。 - GBDT+LR 和 RF+LR 的区别 - 选出能明显区分正负样本的特征的变换方式，转换成 one hot 有意义 - RF + LR 可以并行训练，但是 RF 中得到的区分度不高</p><p><strong>收获：</strong></p><ul><li>数据支撑去做决策，收获和实验数量成正比。</li><li>CTR click through rate，点击率</li><li>评价指标：<ul><li>Normalized Entropy：越小模型越好</li><li>Calibration：预测点击数除以真实点击数</li><li>AUC 正样本出现在负样本前面的概率。</li></ul></li><li>数据新鲜度：模型天级训练比周级训练在 NE 下降 1%。</li><li>GBDT 和 LR 模型采用不同的更新频率，解决训练耗时不同。但是 GBDT 重新训练之后，LR 必须要重新训练。</li></ul><h2 id="网络">网络：</h2><h3 id="gbdt-lr">GBDT + LR</h3><p>利用 GBDT 模型进行自动特征组合和筛选，然后根据样本落在哪棵树哪个叶子生成一个 feature vector 输入到 LR 模型中。这种方法的有点在于两个模型在训练过程从是独立，不需要进行联合训练。</p><p>GBDT 由多棵 CART 树组成，每一个节点按贪心分裂。最终生成的树包含多层，相当于一个特征组合的过程。根据规则，样本一定会落在一个叶子节点上，将这个叶子节点记为1，其他节点设为0，得到一个向量。比如下图中有两棵树，第一棵树有三个叶子节点，第二棵树有两个叶子节点。如果一个样本落在第一棵树的第二个叶子，将它编码成 [0, 1, 0]。在第二棵树落到第一个叶子，编码成 [1, 0]。所以，输入到 LR 模型中的向量就是 [0, 1, 0, 1, 0]</p><p><img src="/file/15529885839484.jpg"></p><h3 id="online-learning">Online Learning</h3><p>文章中提到的 Online Learning 包括三个部分： - Joiner 将每次广告展示结果（特征）是否用户点击（标签） join 在一起形成一个完成的训练数据； - Trainer 定时根据一个 small batch 的数据训练一个模型； - Ranker 利用上一个模块得到模型预测用户点击。</p><p><img src="/file/15529904431696.jpg"></p><p>注意的点： - waiting window time：给用户展示广告之后，我们只能知道用户点击的广告，也就是模型中的正样本。负样本需要设置一个等待时间来判断，即超过某一个时间没有观测到用户点击某一个广告，就认为这是一个负样本。另外设置这个时间也是一个技术活，时间过短导致click没有及时join到样本上，时间太长数据实时性差以及有存储的压力。最后，无论如何都会有一些数据缺失，为了避免累积误差，需要定期重新训练整个模型。 - request ID：人家的模型是分布式架构的，需要使用 request ID 来匹配每次展示给用户的结果以及click。为了实现快速匹配，使用 HashQueue 来保存结果。 - 监控：避免发生意向不到的结果，导致业务损失。我们的实时模型也在上线前空跑了好久。</p><h2 id="实验">实验：</h2><h4 id="有无-gbdt-特征对比">有无 GBDT 特征对比</h4><p>训练两个 LR 模型，一个模型输入样本经过 GBDT 得到的特征，另外一个不输入。混合模型比单独 LR 或 Tree <img src="/file/15529901354294.jpg"></p><h4 id="学习率选择">学习率选择</h4><p>5 种学习率，前三个每一个特征设置一个学习率，最后两种全局学习率。 <img src="/file/15541928146842.jpg"></p><p>结果：应该给每一个特征设置一个不同的学习率，而且学习率应该随着轮次缓慢衰减。 <img src="/file/15541931302704.jpg"></p><h4 id="gbdt-参数相关实验">GBDT 参数相关实验</h4><ul><li>前面的树会带来大量的收益，但是树越多训练越慢。</li><li>特征重要程度，累加不同树上某个特征的得分减少贡献。</li><li>两种特征：<ul><li>上下文，冷启动的时候比较重要，与数据新鲜度有关。</li><li>历史史特征，权重比较大，关键在于长时间积累。</li></ul></li></ul><h4 id="采样">采样</h4><p>训练数据大多，需要进行采样。</p><ul><li><p>uniform subsampling ：无差别采样。使用 10 % 的样本，NE 减少 1 % <img src="/file/15542640937514.jpg"></p></li><li><p>negative down subsampling ：对负样本进行下采样。但不是负采样率越低越好，比如下面的图中0.0250就可能是解决了正负样本不平衡问题。最后的CTR指标结果需要重新进行一次映射。 <img src="/file/15542641884458.jpg"></p></li></ul><h3 id="reference">Reference</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/57987311" target="_blank" rel="noopener">回顾Facebook经典CTR预估模型 - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;主题：&lt;/strong&gt;Facebook 2014 年发表的广告点击预测文章。最主要是提出经典 GBDT+LR 模型，可以自动实现特征工程，效果好比于人肉搜索。另外，文章中还给出一个 online learning 的工程框架。&lt;/p&gt;
&lt;p&gt;&lt;strong
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="gbdt" scheme="https://xiang578.com/tags/gbdt/"/>
    
      <category term="machine learing" scheme="https://xiang578.com/tags/machine-learing/"/>
    
      <category term="lr" scheme="https://xiang578.com/tags/lr/"/>
    
  </entry>
  
  <entry>
    <title>「Rime 鼠须管」小鹤双拼配置指南</title>
    <link href="https://xiang578.com/post/rime.html"/>
    <id>https://xiang578.com/post/rime.html</id>
    <published>2019-06-15T13:21:14.000Z</published>
    <updated>2020-03-28T09:07:31.367Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/file/15599183665365.jpg"></p><h2 id="引言">引言</h2><p>如何将汉字输入到计算机中是一个编码有关的问题，目前市面上主流的方案包括音码、形码、音形码。和大多数人一样，之前我一直使用全拼，而且得益于 NLP 技术发展，使用搜狗输入法搭配云词库，输入效率可以媲美五笔输入法。</p><p>但是今天要和大家分享，是从年初开始使用的全新音码输入方案——小鹤双拼。最初关于双拼的概念来自李笑来《把时间当作朋友》：</p><blockquote><p>在很长的一段时间里，我常言之凿凿地对同学们说“练习打字完全是浪费时间。”我当时的逻辑是这样的。首先，我认为王码五笔字型输入法是给打字员用的。为什么要学它？难道你将来想要当个打字员？我总觉得五笔字型知识一种抄写输入法，因为用他输入时只能边看边打。而对真正创造内容的人来说，先用纸和笔写出来再录入电脑，还有比这更荒诞的事情吗？学习拆字学法已经很累人了，还要练什么指法，见鬼。更不用说这种所谓的输入法对思考的干扰——不仅要把字拆开再输入，还要按照莫名其妙的方法拆字。其次，盲打。我现在不是盲打，只是两根手指输入速度就已经很快了（至少比手写快）。</p><p>这样看来，我还有必要学习什么五笔字型和盲打吗？</p><p>在我有了这些定见很久之后，发生了一件事情。</p><p>那是在1997年，我25岁。当时互联网除了聊天室和论坛，几乎没有什么实际的应用。适逢windows捆绑了哈尔滨工业大学开发的“微软拼音输入法1.0”，某天下午，当我在网上和一位永远都不会知道是谁的女生放肆地聊了两个小时之后，突然发现自己竟已无师自通地学会了所谓的“盲打”了！在这之后的一段时间里，我身边甚至很多人羡慕我打字的速度。为了让自己的打字速度再快一点，我索性花了差不多20分钟，把原本默认的“全拼输入”改成了“”双拼输入“。而这还远远不够。后来，我增设了”慢放模糊音“（不区分z/zh、c/ch、s⇧），又把打字速度提高了一些。这时我第一次意识到‘有些认识，哪怕是简单的常识，也需要亲身经历后才能真正体会”。只有拥有无与伦比的打字速度，才会体会打字速度快的好处。</p><p>打字速度提升后，我发现自己不再讨厌在读书的时候做笔记了，因为在键盘上敲字相对于笔写字来说轻松太多。我开始大段地纪录感悟，有时甚至干脆整篇摘抄原文！</p></blockquote><p>李笑来思考的问题正是如何利用最小的代价快速的输入文字。这里代价包括两个方面，输入方案的学习成本以及输入文字的速度。简单分析一下，主流输入法需要用户在键盘上敲击一些字符，然后映射到汉字。就是说输入文字的速度和两个因数有关：敲击键盘的次数（对应计算机中的码长）以及重码的字符数量（对于拼音输入法来说，这里指的是每个拼音对应多少的字）。</p><p>从这两个指标来说，五笔应该是接近输入法的极限，五次敲击键盘肯定能选定你需要的汉字。但是学习五笔需要记忆大量的说是有序其实没有太多规律的字根，学习曲线不是一般的陡峭。几年前，自己尝试跟着网上的视频教程学习，最后还是太复杂而放弃。</p><p>这时候要介绍一下双拼输入法，它是一种基于全拼的激进改良版拼音输入法。简单来说，它将一个汉字的拼音分成声母和韵母两个部分，输入一个拼音只需要按两个键（减少键盘的敲击次数但是没有减少重码率）。比如对于“拼音”两个字来说，全拼需要输入 <code>pinyin</code> 六个字符，换成是双拼，只需要输入 4 个字符 <code>pbyb</code>。具体介绍可以看 <a href="https://sspai.com/post/32809" target="_blank" rel="noopener">做少数派中的少数派：双拼输入快速入门</a>。</p><p>由于双拼有声母和韵母，需要先从键盘上进行一次映射，所以有很多的输入方案。我自己入门使用的是小鹤双拼，也推荐大家使用这个方案。主要原因有两点：声母和韵母全部放在字母键上（微软双拼中要用到 <code>;</code> 键，以及 iOS12 和 MacOS 中自带的输入法都支持这个方案（国内那几个流氓输入法更不用提）。</p><p>下图是一张小鹤双拼的输入法键位图，其中黑色的字符代表是键盘上的什么键，棕色的代表这个键表示声母时是什么，蓝色的代表这个键表示韵母时是什么。学习双拼的过程也很简单，拼音本身就会，无非是熟悉键位。从我自己的角度来说，每天抽出几分看一下键位图，再在 <a href="https://api.ihint.me/shuang/" target="_blank" rel="noopener">双拼练习 @ BlueSky</a> （相关的介绍可以看：<a href="https://sspai.com/post/40185" target="_blank" rel="noopener">快速上手双拼，可以尝试这个练习平台 - 少数派</a>）网站上练习 5 分钟，一周后可以完全脱离键位图来打字，之后就是孰能生巧的过程。</p><p><img src="/file/15600908556759.jpg"></p><p>说回来当你学习了这么强大的内功之后，自然需要神兵来辅助你输入。在饱受国内的流氓输入法侵害之后（比如输入到一半给你跳一个什么斗图功能提示），我遇到今天的主角 <a href="https://rime.im/" target="_blank" rel="noopener">RIME | 中州韻輸入法引擎</a>，它是由佛振开发的一种开源输入框架，业内人士称之为「神级输入法」。上一个有类似拉风的称号的软件还是「神级编辑器」—— Vim。Rime 有趣的一点是在不同的平台上有不同的名字，包括 Linux 上的「中州韵」，Win 上的「小狼毫」以及 Mac 上的「鼠须管」。稍微有文化的人可以反应过来，后面两个正是两种不同的毛笔名字。</p><p>简单总结一下为什么要使用鼠须管：一是安全，不会出现什么输入法读取你个人信息更甚者是密码发送到服务器，也不知道他们用来机器学习什么；二是配置全平台同步，解决多台设备的输入法配置问题；三是快，不会出现输入法跟不上我的打字速度而导致思路中断的情况。</p><h2 id="安装">安装</h2><p>在 <a href="https://rime.im/download/" target="_blank" rel="noopener">下載及安裝 | RIME | 中州韻輸入法引擎</a> 主页，你可以找到对应不同平台的安装方法。</p><p>对于 MacOS 来说，可以在终端中使用 <code>curl -fsSL https://git.io/rime-install | bash -s -- :preset double-pinyin</code> 安装软件本体。<code>preset double-pinyin</code> 指定下载的时候默认包括双拼输入方案。</p><h2 id="配置">配置</h2><p>防止配置时候出现各种意想不到的情况，首先推荐阅读官方文档 <a href="https://github.com/rime/home/wiki/CustomizationGuide" target="_blank" rel="noopener">CustomizationGuide · rime/home Wiki</a>。</p><p>Rime 的配置文件默认放在 <code>~/Library/Rime</code>，而且是一种扩展 yaml 文件。默认的文件名为 <code>.schema.yaml</code>，比如小鹤双拼相关的默认配置在 <code>double_pinyin_flpy.schema.yaml</code> 中。如果我们自己想添加一些设置，推荐写在以<code>.custom.yaml</code> 结尾的新文件中，比如 <code>double_pinyin_flypy.custom.yaml</code></p><h3 id="default.custom.yaml">default.custom.yaml</h3><p>这个文件写一些全局的配置。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">patch:</span></span><br><span class="line">  <span class="attr">switcher:</span></span><br><span class="line">    <span class="attr">caption:</span> <span class="string">〔方案选单〕</span></span><br><span class="line">    <span class="attr">hotkeys:</span> <span class="string">Control+grave</span></span><br><span class="line">  <span class="comment"># 候选词数量</span></span><br><span class="line">  <span class="attr">menu:</span></span><br><span class="line">    <span class="attr">page_size:</span> <span class="number">9</span></span><br><span class="line">  <span class="comment"># 使用的输入方案</span></span><br><span class="line">  <span class="attr">schema_list:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">schema:</span> <span class="string">luna_pinyin_simp</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">schema:</span> <span class="string">luna_pinyin</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">schema:</span> <span class="string">double_pinyin_flypy</span></span><br><span class="line">  <span class="comment"># 输入法中英文状态快捷键</span></span><br><span class="line">  <span class="attr">ascii_composer/switch_key:</span></span><br><span class="line">    <span class="attr">Caps_Lock:</span> <span class="string">commit_code</span></span><br><span class="line">    <span class="attr">Control_L:</span> <span class="string">noop</span></span><br><span class="line">    <span class="attr">Control_R:</span> <span class="string">noop</span></span><br><span class="line">    <span class="comment"># 按下左 shift 英文字符直接上屏，不需要再次回车，输入法保持英文状态</span></span><br><span class="line">    <span class="attr">Shift_L:</span> <span class="string">noop</span></span><br><span class="line">    <span class="attr">Shift_R:</span> <span class="string">noop</span></span><br><span class="line">  <span class="comment"># 在一些软件中默认使用英文输入状态</span></span><br><span class="line">  <span class="attr">app_options:</span></span><br><span class="line">    <span class="attr">com.apple.finder:</span> <span class="meta">&amp;a</span></span><br><span class="line">      <span class="attr">ascii_mode:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">no_inline:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">com.googlecode.iterm2:</span> <span class="meta">*a</span></span><br><span class="line">    <span class="attr">com.alfredapp.Alfred:</span> <span class="meta">*a</span></span><br><span class="line">    <span class="attr">com.runningwithcrayons.Alfred-2:</span> <span class="meta">*a</span></span><br><span class="line">    <span class="attr">org.vim.MacVim:</span> <span class="meta">*a</span></span><br><span class="line">    <span class="attr">com.apple.Terminal:</span> <span class="meta">*a</span></span><br></pre></td></tr></table></figure><p>最后在修改配置时，可以查阅 <a href="https://github.com/LEOYoon-Tsaw/Rime_collections/blob/master/Rime_description.md" target="_blank" rel="noopener">Rime_collections/Rime_description.md at master · LEOYoon-Tsaw/Rime_collections</a> 寻找相关信息。</p><h3 id="installation.yaml">installation.yaml</h3><p>配置文件多平台同步相关文件，<code>sync_dir</code> 指定同步文件夹的位置，配合例如坚果云之类的软件实现备份同步。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">distribution_code_name:</span> <span class="string">Squirrel</span></span><br><span class="line"><span class="attr">distribution_name:</span> <span class="string">"鼠鬚管"</span></span><br><span class="line"><span class="attr">distribution_version:</span> <span class="number">0.11</span><span class="number">.0</span></span><br><span class="line"><span class="attr">install_time:</span> <span class="string">"Sun Dec 23 23:42:01 2018"</span></span><br><span class="line"><span class="attr">installation_id:</span> <span class="string">"mac_didi"</span></span><br><span class="line"><span class="attr">sync_dir:</span> <span class="string">"/Users/didi/Documents/rime_sync"</span></span><br><span class="line"><span class="attr">rime_version:</span> <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="attr">update_time:</span> <span class="string">"Mon Jun  3 07:18:30 2019"</span></span><br></pre></td></tr></table></figure><h3 id="double_pinyin_flypy.custom.yaml">double_pinyin_flypy.custom.yaml</h3><p>小鹤双拼相关的自用配置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">patch:</span></span><br><span class="line">  <span class="comment"># 引用 `symbols.custom` 文件里面的符号</span></span><br><span class="line">  <span class="comment"># 'punctuator/import_preset': symbols.custom</span></span><br><span class="line">  <span class="attr">'recognizer/patterns/punct':</span> <span class="string">"^/([a-z]+|[0-9])$"</span></span><br><span class="line">  <span class="comment"># 載入朙月拼音擴充詞庫</span></span><br><span class="line">  <span class="attr">"translator/dictionary":</span> <span class="string">ryen</span></span><br><span class="line">  <span class="comment"># 更改‘西文’为‘英文’，‘增广’为‘扩展集’</span></span><br><span class="line">  <span class="attr">punctuator:</span></span><br><span class="line">    <span class="attr">import_preset:</span> <span class="string">symbols.custom</span></span><br><span class="line">    <span class="attr">half_shape:</span></span><br><span class="line">      <span class="string">"#"</span><span class="string">:</span> <span class="string">"#"</span></span><br><span class="line">      <span class="string">"`"</span><span class="string">:</span> <span class="string">"`"</span></span><br><span class="line">      <span class="string">"~"</span><span class="string">:</span> <span class="string">"~"</span></span><br><span class="line">      <span class="string">"@"</span><span class="string">:</span> <span class="string">"@"</span></span><br><span class="line">      <span class="string">"="</span><span class="string">:</span> <span class="string">"="</span></span><br><span class="line">      <span class="string">"/"</span><span class="string">:</span> <span class="string">["/",</span> <span class="string">"÷"</span><span class="string">]</span></span><br><span class="line">      <span class="string">'\': ["、", '</span><span class="string">\']</span></span><br><span class="line">      <span class="string">"'"</span><span class="string">:</span> <span class="string">&#123;pair:</span> <span class="string">["「",</span> <span class="string">"」"</span><span class="string">]&#125;</span></span><br><span class="line">      <span class="string">"["</span><span class="string">:</span> <span class="string">["【",</span> <span class="string">"["</span><span class="string">]</span></span><br><span class="line">      <span class="string">"]"</span><span class="string">:</span> <span class="string">["】",</span> <span class="string">"]"</span><span class="string">]</span></span><br><span class="line">      <span class="string">"$"</span><span class="string">:</span> <span class="string">["¥",</span> <span class="string">"$"</span><span class="string">,</span> <span class="string">"€"</span><span class="string">,</span> <span class="string">"£"</span><span class="string">,</span> <span class="string">"¢"</span><span class="string">,</span> <span class="string">"¤"</span><span class="string">]</span></span><br><span class="line">      <span class="string">"&lt;"</span><span class="string">:</span> <span class="string">["《",</span> <span class="string">"〈"</span><span class="string">,</span> <span class="string">"«"</span><span class="string">,</span> <span class="string">"&lt;"</span><span class="string">]</span></span><br><span class="line">      <span class="string">"&gt;"</span><span class="string">:</span> <span class="string">["》",</span> <span class="string">"〉"</span><span class="string">,</span> <span class="string">"»"</span><span class="string">,</span> <span class="string">"&gt;"</span><span class="string">]</span></span><br><span class="line">  <span class="attr">switches:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ascii_mode</span></span><br><span class="line">      <span class="attr">reset:</span> <span class="number">0</span></span><br><span class="line">      <span class="attr">states:</span> <span class="string">["中文",</span> <span class="string">"英文"</span><span class="string">]</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">full_shape</span></span><br><span class="line">      <span class="attr">states:</span> <span class="string">["半角",</span> <span class="string">"全角"</span><span class="string">]</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">zh_simp</span></span><br><span class="line">      <span class="attr">reset:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">states:</span> <span class="string">["漢字","汉字"]</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ascii_punct</span></span><br><span class="line">      <span class="attr">states:</span> <span class="string">["，。",</span> <span class="string">"，．"</span><span class="string">]</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">extended_charset</span> <span class="comment">#生僻字开关</span></span><br><span class="line">      <span class="attr">states:</span> <span class="string">["通用",</span> <span class="string">"扩展集"</span><span class="string">]</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">show_emoji</span> <span class="comment"># 该项为表情输入，具体内容可见下文中 [关于表情输入] 部分</span></span><br><span class="line">      <span class="attr">reset:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">states:</span> <span class="string">[</span> <span class="string">"🈚️️\uFE0E"</span><span class="string">,</span> <span class="string">"🈶️️\uFE0F"</span> <span class="string">]</span></span><br><span class="line">  <span class="comment"># 输入双拼码的时候不转化为全拼码</span></span><br><span class="line">  <span class="attr">translator/preedit_format:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">  <span class="attr">simplifier:</span></span><br><span class="line">    <span class="attr">option_name:</span> <span class="string">zh_simp</span></span><br><span class="line">  <span class="comment"># 分号上屏二候选词；引号上屏三候选词</span></span><br><span class="line">  <span class="attr">"key_binder/bindings":</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#123;</span> <span class="attr">when:</span> <span class="string">has_menu,</span> <span class="attr">accept:</span> <span class="string">semicolon,</span> <span class="attr">send:</span> <span class="number">2</span> <span class="string">&#125;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#123;</span> <span class="attr">when:</span> <span class="string">has_menu,</span> <span class="attr">accept:</span> <span class="string">apostrophe,</span> <span class="attr">send:</span> <span class="number">3</span> <span class="string">&#125;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#123;</span> <span class="attr">when:</span> <span class="string">paging,</span> <span class="attr">accept:</span> <span class="string">bracketleft,</span> <span class="attr">send:</span> <span class="string">Page_Up</span> <span class="string">&#125;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#123;</span> <span class="attr">when:</span> <span class="string">has_menu,</span> <span class="attr">accept:</span> <span class="string">bracketright,</span> <span class="attr">send:</span> <span class="string">Page_Down</span> <span class="string">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="squirrel.custom.yaml">squirrel.custom.yaml</h3><p>自定义皮肤相关文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">patch:</span></span><br><span class="line">  <span class="attr">style:</span></span><br><span class="line">    <span class="attr">color_scheme:</span> <span class="string">psionics</span></span><br><span class="line">    <span class="attr">horizontal:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">inline_preedit:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">candidate_format:</span> <span class="string">"%c\u2005%@ \u2005"</span>   <span class="comment"># 用 1/6 em 空格 U+2005 来控制编号 %c 和候选词 %@ 前后的空间。</span></span><br><span class="line">    <span class="attr">font_point:</span> <span class="number">16</span>                          <span class="comment"># 候选文字大小</span></span><br><span class="line">    <span class="attr">label_font_point:</span> <span class="number">14</span>                    <span class="comment"># 候选编号大小</span></span><br><span class="line">    <span class="attr">corner_radius:</span> <span class="number">5</span>                        <span class="comment"># 候选条圆角</span></span><br><span class="line">    <span class="attr">border_height:</span> <span class="number">0</span>                        <span class="comment"># 窗口边界高度，大于圆角半径才生效</span></span><br><span class="line">    <span class="attr">border_width:</span> <span class="number">0</span>                         <span class="comment"># 窗口边界宽度，大于圆角半径才生效</span></span><br></pre></td></tr></table></figure><p>最后，我的配置在 <a href="https://github.com/xiang578/rime" target="_blank" rel="noopener">xiang578/rime</a> 同步。</p><h2 id="debug">DEBUG</h2><p>Debug 是折腾 rime 不得不面对的一步，主要通过查看 rime 部署的時候 log 文件实现。对于鼠鬚管， log 文件保存在 <code>$TMPDIR/rime.squirrel.*</code> 中。首先在命令行中输入 <code>echo $TMPDIR</code> 获得路径，然后将地址输入到 「访达-前往-前往文件夹...」跳转。点击 <code>rime.squirrel.WARNING</code> 选择显示原身，利用文本编辑器打开。</p><p>文件格式类似于下面：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Log file created at:</span> <span class="number">2019</span><span class="string">/06/07</span> <span class="number">23</span><span class="string">:31:54</span></span><br><span class="line"><span class="attr">Running on machine:</span> <span class="string">didideMiBook-Pro.local</span></span><br><span class="line"><span class="attr">Log line format:</span> <span class="string">[IWEF]mmdd</span> <span class="string">hh:mm:ss.uuuuuu</span> <span class="string">threadid</span> <span class="string">file:line]</span> <span class="string">msg</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:31:54.549346</span> <span class="number">162086912</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">punctuation.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:31:54.554167</span> <span class="number">162086912</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">key_bindings.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:31:54.560144</span> <span class="number">162086912</span> <span class="string">deployment_tasks.cc:179]</span> <span class="string">schema</span> <span class="string">list</span> <span class="string">not</span> <span class="string">defined.</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.133517</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">punctuation.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.135843</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">key_bindings.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.149406</span> <span class="number">161013760</span> <span class="string">config_data.cc:62]</span> <span class="string">nonexistent</span> <span class="string">config</span> <span class="string">file</span> <span class="string">'/Users/didi/Library/Rime/luna_pinyin_simp.custom.yaml'</span><span class="string">.</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.154920</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">punctuation.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.156643</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">key_bindings.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.331344</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">key_bindings.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.333066</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">punctuation.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.668072</span> <span class="number">161013760</span> <span class="string">config_data.cc:62]</span> <span class="string">nonexistent</span> <span class="string">config</span> <span class="string">file</span> <span class="string">'/Users/didi/Library/Rime/stroke.custom.yaml'</span><span class="string">.</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.670761</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">key_bindings.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:13.672724</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">punctuation.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:14.281919</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">punctuation.custom:/patch</span></span><br><span class="line"><span class="string">W0607</span> <span class="number">23</span><span class="string">:36:14.283246</span> <span class="number">161013760</span> <span class="string">config_compiler.cc:391]</span> <span class="attr">inaccessible node:</span> <span class="string">key_bindings.custom:/patch</span></span><br></pre></td></tr></table></figure><h2 id="按右-shift-切换输入法">按右 shift 切换输入法</h2><p>之前使用搜狗输入法时，特别喜欢的一个功能：按右 shift 切换输入法的输入状态，实现暂时切换到英文状态。Rime 作者在 <a href="https://gist.github.com/lotem/2981316" target="_blank" rel="noopener">使用 Control 鍵切換中西文，上屏已輸入的編碼；令 Caps Lock 改變字母的大小寫</a> 中提到一种方案。例如下面：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">patch:</span></span><br><span class="line">  <span class="attr">ascii_composer/good_old_caps_lock:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">ascii_composer/switch_key:</span></span><br><span class="line">    <span class="attr">Caps_Lock:</span> <span class="string">commit_code</span></span><br><span class="line">    <span class="attr">Shift_L:</span> <span class="string">noop</span></span><br><span class="line">    <span class="attr">Shift_R:</span> <span class="string">commit_code</span></span><br><span class="line">    <span class="attr">Control_L:</span> <span class="string">commit_code</span></span><br><span class="line">    <span class="attr">Control_R:</span> <span class="string">commit_code</span></span><br></pre></td></tr></table></figure><p>然而这样修改完成之后，不论按哪个 Shift 键，都会切换到英文输入状态。看前面那个网页下面作者与其他人的讨论中发现，鼠须管无法区分 Shift 键。</p><p>网上查了一下，简单实现的方法是通过 karabiner 软件来改键。详细步骤可以参考 <a href="https://github.com/rime/squirrel/wiki/%E7%A6%81%E7%94%A8-Squirrel-%E8%8B%B1%E6%96%87%E6%A8%A1%E5%BC%8F%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%B7%A6%E4%BE%A7-Shift-%E5%88%87%E6%8D%A2%E4%B8%AD%E8%8B%B1" target="_blank" rel="noopener">禁用 Squirrel 英文模式，使用左侧 Shift 切换中英 · rime/squirrel Wiki</a>。</p><h2 id="reference">Reference</h2><ul><li><a href="https://github.com/xiang578/xiang578.github.io/issues/37" target="_blank" rel="noopener">双拼 · Issue #37 · xiang578/xiang578.github.io</a></li><li><a href="http://zhizhi.betahouse.us/2018/10/17/rime-setup/" target="_blank" rel="noopener">基于Rime的鼠须管输入法配置记录</a></li><li><a href="https://withdewhua.space/2019/01/30/rime-configuration" target="_blank" rel="noopener">Rime 输入法配置记录</a></li><li><a href="https://mritd.me/2019/03/23/oh-my-rime/" target="_blank" rel="noopener">Mac 下调校 Rime - 漠然的博客 | mritd Blog</a></li><li><a href="https://jdhao.github.io/2019/02/18/rime_configuration_intro/" target="_blank" rel="noopener">最新版 Rime 输入法使用 - jdhao's blog</a></li><li><a href="https://scomper.me/gtd/-shu-xu-guan-de-diao-jiao-bi-ji" target="_blank" rel="noopener">「鼠须管」的调教笔记</a></li><li><a href="https://scomper.me/gtd/-shu-xu-guan-de-an-zhuang-he-bu-shu" target="_blank" rel="noopener">「鼠须管」输入方案的添加</a></li><li><a href="https://whyhow.tk/2016/04/29/rime0429.html" target="_blank" rel="noopener">RIME输入法在Finder中自动切换成英文 @ Why &amp; How</a></li><li><a href="https://placeless.net/blog/rime-squirrel-customization-2019#article" target="_blank" rel="noopener">鼠须管配置 2019</a></li><li><a href="https://github.com/rime/squirrel/wiki/%E7%A6%81%E7%94%A8-Squirrel-%E8%8B%B1%E6%96%87%E6%A8%A1%E5%BC%8F%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%B7%A6%E4%BE%A7-Shift-%E5%88%87%E6%8D%A2%E4%B8%AD%E8%8B%B1" target="_blank" rel="noopener">禁用 Squirrel 英文模式，使用左侧 Shift 切换中英 · rime/squirrel Wiki</a></li><li><a href="https://github.com/rime-aca/dictionaries" target="_blank" rel="noopener">rime-aca/dictionaries: Rime詞庫</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/file/15599183665365.jpg&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;如何将汉字输入到计算机中是一个编码有关的问题，目前市面上主流的方案包括音码、形码、音形码。和大多数人一样，之前我一直使用全拼，而且得益于 NLP 
      
    
    </summary>
    
    
      <category term="rime" scheme="https://xiang578.com/tags/rime/"/>
    
  </entry>
  
  <entry>
    <title>博客维护日志 beta 0.1</title>
    <link href="https://xiang578.com/post/blog-log.html"/>
    <id>https://xiang578.com/post/blog-log.html</id>
    <published>2019-06-15T05:39:49.000Z</published>
    <updated>2020-03-28T09:07:31.363Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记录博客修修补补的故事</p></blockquote><h2 id="beta-0.1">beta 0.1</h2><ul><li>开始使用版本号来记录博客更新。从 beta 0.1 开始，预计完成博客发布会的计划后（参见：<a href="https://github.com/xiang578/xiang578.github.io/issues/23" target="_blank" rel="noopener">博客发布会 · Issue #23</a>，切换到正式版本。</li><li>本次更新带来最大的变化是在博客中加入豆瓣个人<a href="https://xiang578.com/books">阅读</a>和<a href="https://xiang578.com/movies">观影</a>记录。感谢 <a href="https://github.com/mythsman/hexo-douban" target="_blank" rel="noopener">hexo-douban: A simple plugin for hexo that helps us generate pages for douban books ,movies and games.</a> 提供技术支持。</li></ul><h2 id="beta-0">beta 0</h2><p>早期对博客进行的相关修改有：</p><ul><li><a href="https://xiang578.com/post/how-to-build-a-hexo-blog.html">从零开始利用 hexo + Github/Coding 搭建个人博客 | 算法花园</a></li><li><a href="https://xiang578.com/post/add-return-button-to-blog.html">为博客添加返回顶部按钮 | 算法花园</a></li><li><a href="https://xiang578.com/post/use-travis-ci-to-auto-update.html">博客折腾记：使用 Travis CI 自动部署 | 算法花园</a></li><li><a href="https://xiang578.com/post/use-travis-ci-to-auto-build-blog.html">博客折腾记：使用 Travis CI 自动部署博客 | 算法花园</a></li><li><a href="https://xiang578.com/post/fix-qiniu-test-url-error.html">博客折腾记：修复七牛云测试域名失效问题 | 算法花园</a></li><li><a href="https://xiang578.com/post/use-cos-to-store-blog.html">博客折腾记：主题更新、迁移博客到腾讯云COS以及解决百度收录 | 算法花园</a></li><li><a href="https://xiang578.com/post/meet-leancloud-counter-security-problem.html">博客折腾记：hexo-leancloud-counter-security 与标题中的引号冲突 | 算法花园</a></li></ul><h2 id="博客专栏">博客专栏</h2><ul><li>【每周分享】：每周六更新，记录过去一周，我看到值得分享的内容</li><li>【月读】：每月更新，推荐本月我阅读的一本书</li><li>【数字生活】：我的数字生活实践</li><li>【博客公告】：分享与这个博客维护相关的内容</li></ul><h2 id="博客记录">博客记录</h2><ul><li>本博客采用<a href="https://hexo.io/" target="_blank" rel="noopener">hexo</a>搭建，使用<a href="https://github.com/ahonn/hexo-theme-even" target="_blank" rel="noopener">Even</a>主题</li><li>托管：Coding Pages</li><li>域名：腾讯云</li><li>评论：Disqus</li><li>统计：百度统计</li><li>图床：七牛</li><li>20151006：恢复订阅功能</li><li>20160303：开启分类和请我喝一杯咖啡</li><li>20160623：重新开启评论和百度统计，与自己和解</li><li>20160624：更换主题大道至简Maupassant，增加favicon和apple-touch-icon</li><li>20170104：放弃github+hexo,投入vps+wordpress怀抱,依旧使用maupassant主题</li><li>20170722：主机系统升级失败,从备份中恢复博客,并采用 Twenty Twelve 主题</li><li>20171005：重新投入hexo怀抱，并托管于Coding Pages</li><li>20180101：开启功德箱</li><li>20180501：PV: 657 | UV: 242</li><li>20180528：使用 Travis CI 自动部署</li></ul><h2 id="除虫记录">除虫记录</h2><h3 id="error-cannot-find-module-node-sass-magic-importer">Error: Cannot find module 'node-sass-magic-importer'</h3><p><a href="https://blog.csdn.net/Nalaluky/article/details/82598300" target="_blank" rel="noopener">ERROR in Cannot find module 'node-sass'（已解决） - line - CSDN博客</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cnpm install node-sass@latest</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;记录博客修修补补的故事&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;beta-0.1&quot;&gt;beta 0.1&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;开始使用版本号来记录博客更新。从 beta 0.1 开始，预计完成博客发布会的计划后（参见：&lt;a href
      
    
    </summary>
    
    
      <category term="blog" scheme="https://xiang578.com/tags/blog/"/>
    
      <category term="hexo" scheme="https://xiang578.com/tags/hexo/"/>
    
      <category term="vps" scheme="https://xiang578.com/tags/vps/"/>
    
  </entry>
  
  <entry>
    <title>博客折腾记：hexo-leancloud-counter-security 与标题中的引号冲突</title>
    <link href="https://xiang578.com/post/meet-leancloud-counter-security-problem.html"/>
    <id>https://xiang578.com/post/meet-leancloud-counter-security-problem.html</id>
    <published>2019-05-25T13:21:14.000Z</published>
    <updated>2020-03-28T09:07:31.367Z</updated>
    
    <content type="html"><![CDATA[<p>昨天按照 <a href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/LEANCLOUD-COUNTER-SECURITY.md" target="_blank" rel="noopener">hexo-theme-next/LEANCLOUD-COUNTER-SECURITY.md at master · theme-next/hexo-theme-next</a> 这个文档配置博客阅读次数时，遇到 <code>hexo-leancloud-counter-security</code> 插件的一个冲突。</p><p>完成配置使用 <code>hexo -d</code> 时，终端中出现下面的错误提示：</p><p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> ATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html</span><br><span class="line">SyntaxError: Unexpected token h in JSON at position 30</span><br><span class="line">    at JSON.parse (&lt;anonymous&gt;)</span><br><span class="line">    at /Users/didi/Documents/personal/xiang578.github.io/node_modules/hexo-leancloud-counter-security/index.js:92:42</span><br><span class="line">    at arrayEach (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_lodash@4.17.11@lodash/lodash.js:516:11)</span><br><span class="line">    at Function.forEach (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_lodash@4.17.11@lodash/lodash.js:9344:14)</span><br><span class="line">    at Hexo._callee$ (/Users/didi/Documents/personal/xiang578.github.io/node_modules/hexo-leancloud-counter-security/index.js:83:27)</span><br><span class="line">    at tryCatch (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:62:40)</span><br><span class="line">    at Generator.invoke [as _invoke] (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:296:22)</span><br><span class="line">    at Generator.prototype.(anonymous function) [as next] (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:114:21)</span><br><span class="line">    at step (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_babel-runtime@6.26.0@babel-runtime/helpers/asyncToGenerator.js:17:30)</span><br><span class="line">    at /Users/didi/Documents/personal/xiang578.github.io/node_modules/_babel-runtime@6.26.0@babel-runtime/helpers/asyncToGenerator.js:28:13</span><br><span class="line">    at process._tickCallback (internal/process/next_tick.js:68:7)</span><br></pre></td></tr></table></figure></p><p>看提示貌似是利用 Json 解析字符串的时候出现问题。打开 <code>node_modules/hexo-leancloud-counter-security/index.js:92</code>，对应出现一个解析 JSON的：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = <span class="built_in">JSON</span>.parse(memoData[memoIdx].substring(<span class="number">0</span>, memoData[memoIdx].length - <span class="number">1</span>));</span><br></pre></td></tr></table></figure></p><p>js 没有怎么接触过，不知道能不能单步调试之类的，只好祭出输出调试大法，加上两个输出：</p><p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">console</span>.log(memoIdx)</span><br><span class="line"><span class="built_in">console</span>.log(memoData[memoIdx])</span><br><span class="line">y = <span class="built_in">JSON</span>.parse(memoData[memoIdx].substring(<span class="number">0</span>,memoData[memoIdx].length - <span class="number">1</span>));</span><br></pre></td></tr></table></figure></p><p>然后再执行 <code>hexo -d</code> 命令，命令行输出为： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">28</span><br><span class="line">&#123;"title":"System.out.println("hello world!");","url":"/post/hello-world.html"&#125;,</span><br><span class="line">FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html</span><br><span class="line">SyntaxError: Unexpected token h in JSON at position 30</span><br><span class="line">    at JSON.parse (&lt;anonymous&gt;)</span><br><span class="line">    at /Users/didi/Documents/personal/xiang578.github.io/node_modules/hexo-leancloud-counter-security/index.js:92:42</span><br><span class="line">    at arrayEach (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_lodash@4.17.11@lodash/lodash.js:516:11)</span><br><span class="line">    at Function.forEach (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_lodash@4.17.11@lodash/lodash.js:9344:14)</span><br><span class="line">    at Hexo._callee$ (/Users/didi/Documents/personal/xiang578.github.io/node_modules/hexo-leancloud-counter-security/index.js:83:27)</span><br><span class="line">    at tryCatch (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:62:40)</span><br><span class="line">    at Generator.invoke [as _invoke] (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:296:22)</span><br><span class="line">    at Generator.prototype.(anonymous function) [as next] (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:114:21)</span><br><span class="line">    at step (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_babel-runtime@6.26.0@babel-runtime/helpers/asyncToGenerator.js:17:30)</span><br><span class="line">    at /Users/didi/Documents/personal/xiang578.github.io/node_modules/_babel-runtime@6.26.0@babel-runtime/helpers/asyncToGenerator.js:28:13</span><br><span class="line">    at process._tickCallback (internal/process/next_tick.js:68:7)</span><br></pre></td></tr></table></figure></p><p>JSON 在解析字符串<code>{"title":"System.out.println("hello world!");","url":"/post/hello-world.html"}</code> 时出现错误。对应的正是之前写的一篇名为 <code>System.out.println("hello world!");</code> 的文章，由于 JSON 格式中字符串是需要用<code>""</code> 修饰，导致JSON 中出现了一个 <code>"title":"System.out.println("hello world!");"</code> key-value 组合。然而实际上 JSON 只会将 <code>"System.out.println("h</code> 解析成 value，之后出现的 <code>h</code> 被当成非法字符报错。</p><p>定位问题之后，暂时修改文章的标题为 <a href="https://xiang578.com/post/hello-world.html">hello world! | 算法花园</a>，绕过部署失败。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;昨天按照 &lt;a href=&quot;https://github.com/theme-next/hexo-theme-next/blob/master/docs/LEANCLOUD-COUNTER-SECURITY.md&quot; target=&quot;_blank&quot; rel=&quot;noopener
      
    
    </summary>
    
      <category term="站务" scheme="https://xiang578.com/categories/%E7%AB%99%E5%8A%A1/"/>
    
    
      <category term="blog" scheme="https://xiang578.com/tags/blog/"/>
    
      <category term="hexo" scheme="https://xiang578.com/tags/hexo/"/>
    
      <category term="hack" scheme="https://xiang578.com/tags/hack/"/>
    
  </entry>
  
</feed>
