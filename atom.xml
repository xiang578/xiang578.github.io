<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>算法花园</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xiang578.com/"/>
  <updated>2021-01-09T02:59:54.286Z</updated>
  <id>https://xiang578.com/</id>
  
  <author>
    <name>Ryen Xiang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深入浅出 BERT 源代码之 BertModel 类</title>
    <link href="https://xiang578.com/post/all-about-bert-code.html"/>
    <id>https://xiang578.com/post/all-about-bert-code.html</id>
    <published>2020-10-03T12:56:43.000Z</published>
    <updated>2021-01-09T02:59:54.286Z</updated>
    
    <content type="html"><![CDATA[<p>国庆节前突然对如何计算 BERT 的参数量感兴趣，不过一直看不明白网上的计算过程，索性下载 BERT 源代码阅读一番。这篇文章记录阅读 BertModel 类（核心代码实现）时写的一些笔记，反正我也是纸上谈兵，所以不需要太关注数据处理和 Finetune 相关部分，最后附上计算 BERT 参数量的过程仅供参考。</p><a id="more"></a><p>代码地址：<a href="https://github.com/google-research/bert/blob/master/modeling.py" target="_blank" rel="noopener">bert/modeling.py at master · google-research/bert</a></p><h2 id="bertconfig">BertConfig</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Configuration for `BertModel`."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_act=<span class="string">"gelu"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               max_position_embeddings=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               type_vocab_size=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               initializer_range=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    self.vocab_size = vocab_size</span><br><span class="line">    self.hidden_size = hidden_size</span><br><span class="line">    self.num_hidden_layers = num_hidden_layers</span><br><span class="line">    self.num_attention_heads = num_attention_heads</span><br><span class="line">    self.hidden_act = hidden_act</span><br><span class="line">    self.intermediate_size = intermediate_size</span><br><span class="line">    self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">    self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">    self.max_position_embeddings = max_position_embeddings</span><br><span class="line">    self.type_vocab_size = type_vocab_size</span><br><span class="line">    self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_dict</span><span class="params">(cls, json_object)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a `BertConfig` from a Python dictionary of parameters."""</span></span><br><span class="line">    config = BertConfig(vocab_size=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> (key, value) <span class="keyword">in</span> six.iteritems(json_object):</span><br><span class="line">      config.__dict__[key] = value</span><br><span class="line">    <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_json_file</span><span class="params">(cls, json_file)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a `BertConfig` from a json file of parameters."""</span></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(json_file, <span class="string">"r"</span>) <span class="keyword">as</span> reader:</span><br><span class="line">      text = reader.read()</span><br><span class="line">    <span class="keyword">return</span> cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Serializes this instance to a Python dictionary."""</span></span><br><span class="line">    output = copy.deepcopy(self.__dict__)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_json_string</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Serializes this instance to a JSON string."""</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(self.to_dict(), indent=<span class="number">2</span>, sort_keys=<span class="literal">True</span>) + <span class="string">"\n"</span></span><br></pre></td></tr></table></figure><p><code>BertConfig</code> 类包含模型参数、几个读取和存储参数的方法。</p><p><code>@classmethod</code> 代表类方法，不需要实例化就可以调用类中的方法。参考其他的文件可以发现它的使用是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)</span><br></pre></td></tr></table></figure><p>主要参数有：</p><ul><li><code>vocab_size</code>: 词表大小</li><li><code>hidden_size</code>: Size of the encoder layers and the pooler layer. 词向量 embedding 大小</li><li><code>num_hidden_layers</code>: Number of hidden layers in the Transformer encoder. 层数</li><li><code>num_attention_heads</code>: Number of attention heads for each attention layer in the Transformer encoder. 多头数量</li><li><code>intermediate_size</code>: The size of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder. FFN 中间层的大小</li><li><code>hidden_act</code>: The non-linear activation function (function or string) in the encoder and pooler. 激活函数</li><li><code>hidden_dropout_prob</code>: The dropout probability for all fully connected layers in the embeddings, encoder, and pooler. dropout 参数</li><li><code>attention_probs_dropout_prob</code>: The dropout ratio for the attention probabilities.</li><li><code>max_position_embeddings</code>: position embedding 的最大值 (e.g., 512 or 1024 or 2048).</li><li><code>type_vocab_size</code>: next sentence prediction 中的 Segment A 和 Segment B，默认大小是 2</li><li><code>initializer_range</code>: The stdev of the truncated_normal_initializer for initializing all weight matrices. """</li></ul><h2 id="embedding_lookup">embedding_lookup</h2><p>根据 <code>input_ids</code> 生成词向量 <code>embedding table</code> 以及对应的 <code>input_id_embeddings</code>。简单一点理解就是向量从 <code>[batch_size, seq_size]</code> 到 <code>[batch_size, seq_size，embedding_size]</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                     vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     word_embedding_name=<span class="string">"word_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings=False)</span>:</span></span><br><span class="line">  <span class="string">"""Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span></span><br><span class="line"><span class="string">      ids.</span></span><br><span class="line"><span class="string">    vocab_size: int. Size of the embedding vocabulary.</span></span><br><span class="line"><span class="string">    embedding_size: int. Width of the word embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Embedding initialization range.</span></span><br><span class="line"><span class="string">    word_embedding_name: string. Name of the embedding table.</span></span><br><span class="line"><span class="string">    use_one_hot_embeddings: bool. If True, use one-hot method for word</span></span><br><span class="line"><span class="string">      embeddings. If False, use `tf.gather()`.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">  <span class="comment"># num_inputs].</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">  <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">  <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [<span class="number">-1</span>])</span><br><span class="line">  <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[<span class="number">0</span>:<span class="number">-1</span>] + [input_shape[<span class="number">-1</span>] * embedding_size])</span><br><span class="line">  <span class="keyword">return</span> (output, embedding_table)</span><br></pre></td></tr></table></figure><p>从 embedding_table 取 input_ids 对应的 embedding 有两种方法：</p><ul><li>矩阵乘法：先通过 input_ids 构造出 one_hot 矩阵，然后和 embedding_table 相乘得到结果。</li><li><code>tf.gather</code> 根据 input_ids 取 embedding_table 对应行的结果。和 <code>tf.nn.embedding_lookup</code> 方法类似。具体原理可以参考 <a href="https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do" target="_blank" rel="noopener">python - What does tf.nn.embedding_lookup function do? - Stack Overflow</a></li></ul><p>看网上的解释，定义两种方法主要是不同设备（CPU、GPU、TPU）运算速度导致的。</p><h2 id="embedding_postprocessor">embedding_postprocessor</h2><p>embedding_postprocessor 将 <code>token embeddings</code> <code>segmentation embeddings</code> <code>position embeddings</code> 三个向量相加得到最终的输入向量。</p><p><img src="https://media.xiang578.com/bert-input-embeddings.png"></p><ul><li><code>token embeddings</code> 对应单词 embedding</li><li><code>segmentation embeddings</code> 代表单词来自哪个句子，在 Next Sentence Prediction 任务中使用。</li><li><code>position embeddings</code> 位置 embedding。在「Attention is all your need」论文中，Google 生成 position embedding 的方法是一个花里胡哨 cos/sin 公式，这一次换成训练 position embedding。猜测在之前的论文中，输入的 seq len 可能长短不一，导致部分 position embedding 训练不充分。BERT 中强行定死 seq len。</li><li>最后直接将三个 embedding 相加，可能对新人来说也有点迷惑。我自己的理解是，物理中多个不同波长的波叠加，是可以通过方法区分的。所以三个 embedding 相加，模型也能学到差异。<ul><li>知乎这个问题<a href="https://www.zhihu.com/question/374835153" target="_blank" rel="noopener">为什么 Bert 的三个 Embedding 可以进行相加</a>可以提供更加严谨的理由。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_postprocessor</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_token_type=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_vocab_size=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_position_embeddings=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                            position_embedding_name=<span class="string">"position_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            max_position_embeddings=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            dropout_prob=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Performs various post-processing on a word embedding tensor.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      embedding_size].</span></span><br><span class="line"><span class="string">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      Must be specified if `use_token_type` is True.</span></span><br><span class="line"><span class="string">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for token type ids.</span></span><br><span class="line"><span class="string">    use_position_embeddings: bool. Whether to add position embeddings for the</span></span><br><span class="line"><span class="string">      position of each token in the sequence.</span></span><br><span class="line"><span class="string">    position_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for positional embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initialization.</span></span><br><span class="line"><span class="string">    max_position_embeddings: int. Maximum sequence length that might ever be</span></span><br><span class="line"><span class="string">      used with this model. This can be longer than the sequence length of</span></span><br><span class="line"><span class="string">      input_tensor, but cannot be shorter.</span></span><br><span class="line"><span class="string">    dropout_prob: float. Dropout probability applied to the final output tensor.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float tensor with same shape as `input_tensor`.</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: One of the tensor shapes or input values is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 是否使用有 segmentation embeddings</span></span><br><span class="line">  <span class="keyword">if</span> use_token_type:</span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"`token_type_ids` must be specified if"</span></span><br><span class="line">                       <span class="string">"`use_token_type` is True."</span>)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    <span class="comment"># segmentation vocab 大小一般是 2，所以使用 one-hot 速度比较快</span></span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [<span class="number">-1</span>])</span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      <span class="comment"># Since the position embedding table is a learned variable, we create it</span></span><br><span class="line">      <span class="comment"># using a (long) sequence length `max_position_embeddings`. The actual</span></span><br><span class="line">      <span class="comment"># sequence length might be shorter than this, for faster training of</span></span><br><span class="line">      <span class="comment"># tasks that do not have long sequences.</span></span><br><span class="line">      <span class="comment">#</span></span><br><span class="line">      <span class="comment"># So `full_position_embeddings` is effectively an embedding table</span></span><br><span class="line">      <span class="comment"># for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span></span><br><span class="line">      <span class="comment"># sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span></span><br><span class="line">      <span class="comment"># perform a slice.</span></span><br><span class="line">      <span class="comment"># position embedding 可以通过学习得到，然后可能输入句子的长度没有到达 512。使用 tf.slice 取对应的向量速度比较快。大小是[seq_length, width]</span></span><br><span class="line">      position_embeddings = tf.slice(full_position_embeddings, [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                     [seq_length, <span class="number">-1</span>])</span><br><span class="line">      num_dims = len(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Only the last two dimensions are relevant (`seq_length` and `width`), so</span></span><br><span class="line">      <span class="comment"># we broadcast among the first dimensions, which is typically just</span></span><br><span class="line">      <span class="comment"># the batch size.</span></span><br><span class="line">      <span class="comment"># word embedding 的大小是 [batch_size, seq_length, width]，上一步取出的 position embedding 大小是 [seq_length, width]，需要对后面一个矩阵进行广播。</span></span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_dims - <span class="number">2</span>):</span><br><span class="line">        position_broadcast_shape.append(<span class="number">1</span>)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width]) <span class="comment"># 大小为 [1, seq_length, width] </span></span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      <span class="comment"># 通过 broadcast 相加                          </span></span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>三个 embedding 向量相加后，还会过一个 <code>layer_norm_and_dropout</code> 层，都是标准的，没有什么特殊。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(input_tensor, dropout_prob)</span>:</span></span><br><span class="line">  <span class="string">"""Perform dropout.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor.</span></span><br><span class="line"><span class="string">    dropout_prob: Python float. The probability of dropping out a value (NOT of</span></span><br><span class="line"><span class="string">      *keeping* a dimension as in `tf.nn.dropout`).</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A version of `input_tensor` with dropout applied.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">if</span> dropout_prob <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> dropout_prob == <span class="number">0.0</span>:</span><br><span class="line">    <span class="keyword">return</span> input_tensor</span><br><span class="line"></span><br><span class="line">  output = tf.nn.dropout(input_tensor, <span class="number">1.0</span> - dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_norm</span><span class="params">(input_tensor, name=None)</span>:</span></span><br><span class="line">  <span class="string">"""Run layer normalization on the last dimension of the tensor."""</span></span><br><span class="line">  <span class="keyword">return</span> tf.contrib.layers.layer_norm(</span><br><span class="line">      inputs=input_tensor, begin_norm_axis=<span class="number">-1</span>, begin_params_axis=<span class="number">-1</span>, scope=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_norm_and_dropout</span><span class="params">(input_tensor, dropout_prob, name=None)</span>:</span></span><br><span class="line">  <span class="string">"""Runs layer normalization followed by dropout."""</span></span><br><span class="line">  output_tensor = layer_norm(input_tensor, name)</span><br><span class="line">  output_tensor = dropout(output_tensor, dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output_tensor</span><br></pre></td></tr></table></figure><h2 id="create_attention_mask_from_input_mask">create_attention_mask_from_input_mask</h2><p>create_attention_mask_from_input_mask 用来构造 attention 时的 mask 矩阵（padding 的单词不参与计算 attention socre）。输入向量 <code>[batch_size, from_seq_length, ...]</code> 和 <code>[batch_size, to_seq_length]</code> 输出向量 <code>[batch_size, from_seq_length, to_seq_length]</code>。</p><p>偷个例子来举：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from_tensor = tf.constant([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">1</span>]]) <span class="comment"># 中间的 0 代表 padding 的结果</span></span><br><span class="line">to_mask = tf.constant([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]) <span class="comment"># 和 from_tensor 对应。如果 1 代表对应位置有词，如果 0 代表对应位置是 padding 的。</span></span><br><span class="line"></span><br><span class="line">to_mask = tf.cast(tf.reshape(to_mask, [<span class="number">2</span>, <span class="number">1</span>, <span class="number">5</span>]), tf.float32)</span><br><span class="line"><span class="comment"># print(to_mask_2)</span></span><br><span class="line">broadcast_ones = tf.ones(</span><br><span class="line">      shape=[<span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span>], dtype=tf.float32)</span><br><span class="line">mask = broadcast_ones * to_mask</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># print(sess.run(to_mask))</span></span><br><span class="line">    print(sess.run(mask))</span><br></pre></td></tr></table></figure><p>最后的结果是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[[1. 1. 1. 0. 0.] #第一个词可以和前三个词计算 attention</span><br><span class="line">  [1. 1. 1. 0. 0.]</span><br><span class="line">  [1. 1. 1. 0. 0.]</span><br><span class="line">  [1. 1. 1. 0. 0.]</span><br><span class="line">  [1. 1. 1. 0. 0.]]</span><br><span class="line"></span><br><span class="line"> [[1. 1. 1. 1. 1.]</span><br><span class="line">  [1. 1. 1. 1. 1.]</span><br><span class="line">  [1. 1. 1. 1. 1.]</span><br><span class="line">  [1. 1. 1. 1. 1.]</span><br><span class="line">  [1. 1. 1. 1. 1.]]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_attention_mask_from_input_mask</span><span class="params">(from_tensor, to_mask)</span>:</span></span><br><span class="line">  <span class="string">"""Create 3D attention mask from a 2D tensor mask.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].</span></span><br><span class="line"><span class="string">    to_mask: int32 Tensor of shape [batch_size, to_seq_length].</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length, to_seq_length].</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">  from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  to_shape = get_shape_list(to_mask, expected_rank=<span class="number">2</span>)</span><br><span class="line">  to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  to_mask = tf.cast(</span><br><span class="line">      tf.reshape(to_mask, [batch_size, <span class="number">1</span>, to_seq_length]), tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We don't assume that `from_tensor` is a mask (although it could be). We</span></span><br><span class="line">  <span class="comment"># don't actually care if we attend *from* padding tokens (only *to* padding)</span></span><br><span class="line">  <span class="comment"># tokens so we create a tensor of all ones.</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># `broadcast_ones` = [batch_size, from_seq_length, 1]</span></span><br><span class="line">  broadcast_ones = tf.ones(</span><br><span class="line">      shape=[batch_size, from_seq_length, <span class="number">1</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Here we broadcast along two dimensions to create the mask.</span></span><br><span class="line">  <span class="comment"># 广播得到最后的 mask 矩阵 [batch_size, from_seq_length, to_seq_length]</span></span><br><span class="line">  mask = broadcast_ones * to_mask</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><h2 id="transformer_model">transformer_model</h2><p>顾名思议 BERT 最核心的 Multi-headed, multi-layer Transformer 实现过程。Attention is all you need 中的实现在 <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py" target="_blank" rel="noopener">链接</a></p><p>一个 Transformer 的示意图：</p><p><img src="https://media.xiang578.com/transformer-encoder.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformer_model</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_act_fn=gelu,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      do_return_all_layers=False)</span>:</span></span><br><span class="line">  <span class="string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".</span></span><br><span class="line"><span class="string">  This is almost an exact implementation of the original Transformer encoder.</span></span><br><span class="line"><span class="string">  See the original paper:</span></span><br><span class="line"><span class="string">  https://arxiv.org/abs/1706.03762</span></span><br><span class="line"><span class="string">  Also see:</span></span><br><span class="line"><span class="string">  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      seq_length], with 1 for positions that can be attended to and 0 in</span></span><br><span class="line"><span class="string">      positions that should not be. 就是前面 create_attention_mask_from_input_mask 产出的结果</span></span><br><span class="line"><span class="string">    hidden_size: int. Hidden size of the Transformer.</span></span><br><span class="line"><span class="string">    num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads in the Transformer.</span></span><br><span class="line"><span class="string">    intermediate_size: int. The size of the "intermediate" (a.k.a., feed</span></span><br><span class="line"><span class="string">      forward) layer.</span></span><br><span class="line"><span class="string">    intermediate_act_fn: function. The non-linear activation function to apply</span></span><br><span class="line"><span class="string">      to the output of the intermediate/feed-forward layer.</span></span><br><span class="line"><span class="string">    hidden_dropout_prob: float. Dropout probability for the hidden layers.</span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: float. Dropout probability of the attention</span></span><br><span class="line"><span class="string">      probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the initializer (stddev of truncated</span></span><br><span class="line"><span class="string">      normal).</span></span><br><span class="line"><span class="string">    do_return_all_layers: Whether to also return all layers or just the final</span></span><br><span class="line"><span class="string">      layer.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span></span><br><span class="line"><span class="string">    hidden layer of the Transformer.</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: A Tensor shape or parameter is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># 最终输出的 hidden_size 能被 num_attention_heads 整除</span></span><br><span class="line">  <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">        <span class="string">"heads (%d)"</span> % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 定义 attention 每个输出的头的大小</span></span><br><span class="line">  <span class="comment"># 最后结果 concat 之后和原始输入大小相同。</span></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The Transformer performs sum residuals on all layers so the input needs</span></span><br><span class="line">  <span class="comment"># to be the same as the hidden size.</span></span><br><span class="line">  <span class="comment"># Transformer 中有残差连接，所以输入和输出 embedding size 要相同</span></span><br><span class="line">  <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"The width of the input tensor (%d) != hidden size (%d)"</span> %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">  <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">  <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">  <span class="comment"># help the optimizer.</span></span><br><span class="line">  <span class="comment"># TPU 不擅长 reshape 操作，所以把所有的 3D tensor 变成 2D tensor</span></span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="comment"># 遍历多层</span></span><br><span class="line">  <span class="keyword">for</span> layer_idx <span class="keyword">in</span> range(num_hidden_layers):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer_%d"</span> % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"attention"</span>):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"self"</span>):</span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> len(attention_heads) == <span class="number">1</span>:</span><br><span class="line">          attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">          <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">          <span class="comment"># concat 多头的结果</span></span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">        <span class="comment"># with `layer_input`. 加上残差</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          <span class="comment"># dropout 和 layer_norm</span></span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># 全连接层</span></span><br><span class="line">      <span class="comment"># The activation is only applied to the "intermediate" hidden layer.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"intermediate"</span>):</span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">    </span><br><span class="line">      <span class="comment"># 变回原来的大小，才能加上残差</span></span><br><span class="line">      <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 是不是要输出中间结果</span></span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure><h2 id="attention_layer">attention_layer</h2><p><code>attention_layer</code> 中实现 self-attention 和 multi-head，细节在 「Attention is all your need」里面有。query_layer 由 from_tensor 得到，key_layer 和 value_layer 由 to_tensor 得到。由于是 self-attention-encoder，from_tensor 和 to_tensor 相同。</p><p>示意图：</p><p><img src="https://media.xiang578.com/self-attention-and-multi-head.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_layer</span><span class="params">(from_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    num_attention_heads=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    size_per_head=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    query_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    key_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    value_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    do_return_2d_tensor=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    from_seq_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_seq_length=None)</span>:</span></span><br><span class="line">  <span class="string">"""Performs multi-headed attention from `from_tensor` to `to_tensor`.</span></span><br><span class="line"><span class="string">  This is an implementation of multi-headed attention based on "Attention</span></span><br><span class="line"><span class="string">  is all you Need". If `from_tensor` and `to_tensor` are the same, then</span></span><br><span class="line"><span class="string">  this is self-attention. Each timestep in `from_tensor` attends to the</span></span><br><span class="line"><span class="string">  corresponding sequence in `to_tensor`, and returns a fixed-with vector.</span></span><br><span class="line"><span class="string">  This function first projects `from_tensor` into a "query" tensor and</span></span><br><span class="line"><span class="string">  `to_tensor` into "key" and "value" tensors. These are (effectively) a list</span></span><br><span class="line"><span class="string">  of tensors of length `num_attention_heads`, where each tensor is of shape</span></span><br><span class="line"><span class="string">  [batch_size, seq_length, size_per_head].</span></span><br><span class="line"><span class="string">  Then, the query and key tensors are dot-producted and scaled. These are</span></span><br><span class="line"><span class="string">  softmaxed to obtain attention probabilities. The value tensors are then</span></span><br><span class="line"><span class="string">  interpolated by these probabilities, then concatenated back to a single</span></span><br><span class="line"><span class="string">  tensor and returned.</span></span><br><span class="line"><span class="string">  In practice, the multi-headed attention are done with transposes and</span></span><br><span class="line"><span class="string">  reshapes rather than actual separate tensors.</span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    from_tensor: float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      from_width].</span></span><br><span class="line"><span class="string">    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].</span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size,</span></span><br><span class="line"><span class="string">      from_seq_length, to_seq_length]. The values should be 1 or 0. The</span></span><br><span class="line"><span class="string">      attention scores will effectively be set to -infinity for any positions in</span></span><br><span class="line"><span class="string">      the mask that are 0, and will be unchanged for positions that are 1.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads.</span></span><br><span class="line"><span class="string">    size_per_head: int. Size of each attention head.</span></span><br><span class="line"><span class="string">    query_act: (optional) Activation function for the query transform.</span></span><br><span class="line"><span class="string">    key_act: (optional) Activation function for the key transform.</span></span><br><span class="line"><span class="string">    value_act: (optional) Activation function for the value transform.</span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: (optional) float. Dropout probability of the</span></span><br><span class="line"><span class="string">      attention probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initializer.</span></span><br><span class="line"><span class="string">    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size</span></span><br><span class="line"><span class="string">      * from_seq_length, num_attention_heads * size_per_head]. If False, the</span></span><br><span class="line"><span class="string">      output will be of shape [batch_size, from_seq_length, num_attention_heads</span></span><br><span class="line"><span class="string">      * size_per_head].</span></span><br><span class="line"><span class="string">    batch_size: (Optional) int. If the input is 2D, this might be the batch size</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor` and `to_tensor`.</span></span><br><span class="line"><span class="string">    from_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor`.</span></span><br><span class="line"><span class="string">    to_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `to_tensor`.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span></span><br><span class="line"><span class="string">      true, this will be of shape [batch_size * from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]).</span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: Any of the arguments or tensor shapes are invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(input_tensor, batch_size, num_attention_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                           seq_length, width)</span>:</span></span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]) <span class="comment">#[batch_size,  num_attention_heads, seq_length, width]</span></span><br><span class="line">    <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) != len(to_shape):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The rank of `from_tensor` must match the rank of `to_tensor`."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) == <span class="number">3</span>:</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">elif</span> len(from_shape) == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(</span><br><span class="line">          <span class="string">"When passing in rank 2 tensors to attention_layer, the values "</span></span><br><span class="line">          <span class="string">"for `batch_size`, `from_seq_length`, and `to_seq_length` "</span></span><br><span class="line">          <span class="string">"must all be specified."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">  <span class="comment">#   B = batch size (number of sequences) </span></span><br><span class="line">  <span class="comment">#   F = `from_tensor` sequence length 输入单词长度</span></span><br><span class="line">  <span class="comment">#   T = `to_tensor` sequence length 输出单词长度</span></span><br><span class="line">  <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">  <span class="comment">#   H = `size_per_head`</span></span><br><span class="line"></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor) <span class="comment"># [B*F，hidden_size=N*H]</span></span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor) <span class="comment"># [B*T，head_size=N*H]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B*F, N*H] 从 from_tensor 得到 query_layer</span></span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=<span class="string">"query"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B*T, N*H]</span></span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=<span class="string">"key"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B*T, N*H]</span></span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=<span class="string">"value"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 计算多头调整 tensor shape，都是为了方便计算.变成 [batch_size,  num_attention_heads, seq_length, width]</span></span><br><span class="line">  <span class="comment"># `query_layer` = [B, N, F, H]</span></span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B, N, T, H]</span></span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Take the dot product between "query" and "key" to get the raw</span></span><br><span class="line">  <span class="comment"># attention scores.</span></span><br><span class="line">  <span class="comment"># `attention_scores` = [B, N, F, T] =&gt; [F, H] * [H, T] = [F, T]</span></span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 <span class="number">1.0</span> / math.sqrt(float(size_per_head))) <span class="comment"># 经典缩小 scroe 值，防止落到 softmask 梯度饱和区</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 处理 padding 部分的 score 值， padding 为 0 的在对应的位置上加上 -10000.0， 这样求 exp 之后就是一个接近于 0 的值</span></span><br><span class="line">  <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># `attention_mask` = [B, 1, F, T]</span></span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span></span><br><span class="line">    <span class="comment"># masked positions, this operation will create a tensor which is 0.0 for</span></span><br><span class="line">    <span class="comment"># positions we want to attend and -10000.0 for masked positions.</span></span><br><span class="line">    adder = (<span class="number">1.0</span> - tf.cast(attention_mask, tf.float32)) * <span class="number">-10000.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">    <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">  <span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">  <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, N, T, H]</span></span><br><span class="line">  value_layer = tf.transpose(value_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># attention 之后的结果</span></span><br><span class="line">  <span class="comment"># `context_layer` = [B, N, F, H]</span></span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, F, N, H]</span></span><br><span class="line">  context_layer = tf.transpose(context_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_2d_tensor:</span><br><span class="line">    <span class="comment"># `context_layer` = [B*F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># `context_layer` = [B, F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> context_layer</span><br></pre></td></tr></table></figure><h2 id="bertmodel-构造类">BertModel 构造类</h2><p>init 方法就是将上面的内容串联起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               config,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               use_one_hot_embeddings=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Constructor for BertModel.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      config: `BertConfig` instance.</span></span><br><span class="line"><span class="string">      is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">        whether dropout will be applied.</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">        embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">      scope: (optional) variable scope. Defaults to "bert".</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">      ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">        is invalid.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    config = copy.deepcopy(config)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">      config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">      config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line">    <span class="comment"># 处理 embedding</span></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">"bert"</span>):</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"embeddings"</span>):</span><br><span class="line">        <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">        (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            vocab_size=config.vocab_size,</span><br><span class="line">            embedding_size=config.hidden_size,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            word_embedding_name=<span class="string">"word_embeddings"</span>,</span><br><span class="line">            use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">        <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">        self.embedding_output = embedding_postprocessor(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            use_token_type=<span class="literal">True</span>,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span><br><span class="line">            use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">            position_embedding_name=<span class="string">"position_embeddings"</span>,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">            dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line">        <span class="comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span></span><br><span class="line">        <span class="comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span></span><br><span class="line">        <span class="comment"># for the attention scores. 获得 attention_mask</span></span><br><span class="line">        attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">            input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the stacked transformer. 计算 transformer 的结果</span></span><br><span class="line">        <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line">        self.all_encoder_layers = transformer_model(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            hidden_size=config.hidden_size,</span><br><span class="line">            num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">            num_attention_heads=config.num_attention_heads,</span><br><span class="line">            intermediate_size=config.intermediate_size,</span><br><span class="line">            intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">            hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            do_return_all_layers=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.sequence_output = self.all_encoder_layers[<span class="number">-1</span>]</span><br><span class="line">      <span class="comment"># The "pooler" converts the encoded sequence tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">      <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">      <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">      <span class="comment"># 分类任务取第一个 [CLS] 对应的 embedding 值</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"pooler"</span>):</span><br><span class="line">        <span class="comment"># We "pool" the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line">        first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">        self.pooled_output = tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation=tf.tanh,</span><br><span class="line">            kernel_initializer=create_initializer(config.initializer_range))</span><br></pre></td></tr></table></figure><h2 id="模型使用">模型使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Already been converted into WordPiece token ids</span></span><br><span class="line">input_ids = tf.constant([[<span class="number">31</span>, <span class="number">51</span>, <span class="number">99</span>], [<span class="number">15</span>, <span class="number">5</span>, <span class="number">0</span>]])</span><br><span class="line">input_mask = tf.constant([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">token_type_ids = tf.constant([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">config = modeling.BertConfig(vocab_size=<span class="number">32000</span>, hidden_size=<span class="number">512</span>,</span><br><span class="line">  num_hidden_layers=<span class="number">8</span>, num_attention_heads=<span class="number">6</span>, intermediate_size=<span class="number">1024</span>)</span><br><span class="line"><span class="comment"># 调用模型</span></span><br><span class="line">model = modeling.BertModel(config=config, is_training=<span class="literal">True</span>,</span><br><span class="line">  input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span><br><span class="line">label_embeddings = tf.get_variable(...)</span><br><span class="line">pooled_output = model.get_pooled_output()</span><br><span class="line">logits = tf.matmul(pooled_output, label_embeddings)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="bert-参数量计算">Bert 参数量计算</h2><p>回到写这篇文章的起点，最后通过计算 <span class="math inline">\(BERT_{BASE}\)</span> 的参数量，加深对模型的理解。论文介绍 Layer = 12，Hidden Size = 768，multi head = 12，参数量是 110M 左右。</p><p>总的计算公式为 <code>(30522 + 512 + 2)*768 + 768*2 + (3*768*64*12 + 3*64*12 + 64*768*12 + 768 + 768 + 768 + 768*3072 + 3072 + 3072*768 + 768 + 768 + 768) * 12 = 108891648</code></p><ul><li>embedding 部分 <code>(30522 + 512 + 2)*768 + 768*2</code><ul><li>embedding size = 768</li><li>单词数仅有 30522，比起 CTR 几千万的物品还是少很多。</li><li>position size = 512</li><li>sentence size = 2</li><li>三个 embedding 相加后 Norm 的参数 2</li></ul></li><li>multi attention 部分 <code>(3*768*64*12 + 3*64*12 + 64*768*12 + 768 + 768 + 768 + 768*3072 + 3072 + 3072*768 + 768 + 768 + 768) * 12</code><ul><li>一共是 12 层，对应 12 个 Transformer</li><li><code>3*768*64*12 + 3*64*12</code> 12 个 multi-head 对应的 Q K V 参数</li><li><code>64*768*12 + 768 + 768 + 768</code> multi-head 结果 concat 之后接的全连接层参数以及后面的 norm</li><li><code>768*3072 + 3072 + 3072*768 + 768 + 768 + 768</code> FFN 以及 norm 的参数</li></ul></li></ul><h2 id="ref">Ref</h2><ul><li><a href="http://fancyerii.github.io/2019/03/09/bert-codes/" target="_blank" rel="noopener">BERT代码阅读 - 李理的博客</a></li><li><a href="https://lsc417.com/2020/06/19/bert_parameter/" target="_blank" rel="noopener">BERT encoder参数量计算 | 417's blog</a></li><li><a href="https://zhuanlan.zhihu.com/p/69106080" target="_blank" rel="noopener">BERT源码分析PART I - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;国庆节前突然对如何计算 BERT 的参数量感兴趣，不过一直看不明白网上的计算过程，索性下载 BERT 源代码阅读一番。这篇文章记录阅读 BertModel 类（核心代码实现）时写的一些笔记，反正我也是纸上谈兵，所以不需要太关注数据处理和 Finetune 相关部分，最后附上计算 BERT 参数量的过程仅供参考。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="nlp" scheme="https://xiang578.com/tags/nlp/"/>
    
      <category term="google" scheme="https://xiang578.com/tags/google/"/>
    
      <category term="bert" scheme="https://xiang578.com/tags/bert/"/>
    
      <category term="code" scheme="https://xiang578.com/tags/code/"/>
    
      <category term="transformer" scheme="https://xiang578.com/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Never-Reading 202008 选择记忆</title>
    <link href="https://xiang578.com/post/Never-Reading-202008.html"/>
    <id>https://xiang578.com/post/Never-Reading-202008.html</id>
    <published>2020-09-13T15:55:11.000Z</published>
    <updated>2021-01-09T02:59:54.286Z</updated>
    
    <content type="html"><![CDATA[<p>某一刻突然意识到可以选择自己的记忆，我顿悟了。</p><a id="more"></a><h2 id="roam-cn-聚会">Roam CN 聚会</h2><p>8 月 29 日参与由 <a href="https://twitter.com/JESSCATE93" target="_blank" rel="noopener">Jessie@FG</a>发起一次北京 RoamCN 微信群聚会，其他到场的还有 <a href="https://twitter.com/pimgeek" target="_blank" rel="noopener">pimgeek</a>、<a href="http://flynngao.github.io/about" target="_blank" rel="noopener">Flynn</a> 等 7 位群友。聚会中大家讨论和知识管理、Roam Research 相关的内容。记录一些给我灵感的内容：</p><h3 id="anki">Anki</h3><p>做为一个知识管理爱好者，很难没有听说过 Anki。这一次线下的相关内容讨论，给我留下的最大感受就是本期标题的由来「选择记忆」。Flynn 介绍在 Michael Nielsen 的 <a href="http://augmentingcognition.com/ltm.html" target="_blank" rel="noopener">Augmenting Long-term Memory</a> ，一直坚持使用 Anki，他举的例子是「可以记住全部的正则匹配」。和绝大部分人一样，我记忆内容的方式就是看一遍，然后等到要用的时候，再去查相关的内容。Anki 可以自己制作需要记忆内容的卡片，这个过程不就是选择自己的记忆吗？其实几年前也看过 Michael Nielsen 的文章，可能是当时境界太低，没有理解选择记忆对自己的重要性。现在觉得有几点很重要：</p><ol type="1"><li>自己制作卡片，网上下载的卡片没有灵魂。</li><li>卡片尽量原子化，使用 QA 形式。原子化保证没一张可以快速过去，QA 形式费曼技巧，防止假懂。</li><li>坚持使用。</li></ol><p>等到自制卡片能到达 1k 张，再来具体分享自己的体验。</p><p>Michael Nielsen 的文章实在是太长了，先分享第一部分的摘录：</p><ul><li>之前看过中文翻译但是没有实践 <a href="https://zhuanlan.zhihu.com/p/65131722" target="_blank" rel="noopener">【三万字长文】量子物理学家是如何使用 Anki 的？ - 知乎</a></li><li>Solomon Shereshevsky 以超级记忆力闻名</li><li>memex 外部记忆机器，汇总全部个人资料<ul><li>[[Douglas Engelbart]] augmentation of human intelligence</li><li>[[Ted Nelson]] [[Project Xanadu]]</li><li>Tim Berners-Lee world wide web</li></ul></li><li>Anki makes memory a <strong>choice</strong>, rather than a haphazard event, to be left to chance.</li><li>But, as we shall see, there are already powerful ideas about personal memory systems based solely on the structuring and presentation of information. 从信息组织和展示的角度入手。</li><li>Anki 卡片之间的复习间隔时间<ul><li>一张卡片在 20 年间需要花费 4-7 分钟去记忆</li></ul></li><li>制作卡片的标准，Anki make memory a choice：你可以选择自己记忆的内容<ul><li>if memorizing a fact seems worth 10 minutes of my time in the future, then I do it</li><li>superseding the first, if a fact seems striking then into Anki it goes, regardless of whether it seems worth 10 minutes of my future time or not.</li></ul></li><li>尝试使用 Anki 很不容易，通过学习 Unix 命令掌握需要的技巧。可以把之前 [[Vim 实用技巧]] 拆解到 Anki 中。</li><li>用 QA 的形式来使用 Anki</li><li>Using Anki to thoroughly read a research paper in an unfamiliar field<ul><li>[[AlphaGo]]还记得多少？</li><li>举例的问题<ul><li>“What's the size of a Go board?”;</li><li>“Who plays first in Go?”;</li><li>“How many human game positions did AlphaGo learn from?”;</li><li>“Where did AlphaGo get its training data?”;</li><li>“What were the names of the two main types of neural network AlphaGo used?”</li></ul></li><li>多轮记录问题</li><li>通过一年之后阅读 [[AlphaGo Zero]] 检验记忆效果</li><li>I find Anki works much better when used in service to some personal creative project.</li><li>为了一个明确目标去设置 Anki 的问题，没有温度。<ul><li>when I'm reading in support of some creative project, I ask much better Anki questions.</li></ul></li></ul></li><li>Using Anki to do shallow reads of papers <a href="10%20to%2060%20minutes%20Ankifying%20a%20paper">[how to read a paper]</a><ul><li>基于主题去阅读论文，读重要的论文。</li><li>选择性阅读，记录关键。I'll add to Anki questions about the core claims, core questions, and core ideas of the paper.</li><li>一篇论文 5 到 20 个问题。</li><li>避免 Anki 化有误导性的条目，仔细选择提问的方法。<ul><li>“What does Jones 2011 claim is the average age at which physics Nobelists made their prizewinning discovery, over 1980-2011?” (Answer: 48).</li><li>“Which paper claimed that physics Nobelists made their prizewinning discovery at average age 48, over the period 1980-2011?” (Answer: Jones 2011).</li></ul></li><li>Ankifying figures：知道某张图存在，然后参考。<ul><li>曲线大概走向？</li><li>图表细节</li></ul></li></ul></li><li>Syntopic reading using Anki<ul><li>从 key paper 出发，best 5-10 paper，普通的论文也有助于认识整个领域。</li><li>Anki 从设计上不是为了创造性工作而生。</li><li>Anki 创造理解新领域的机会。</li></ul></li><li>More patterns of Anki use<ul><li><a href="https://www.supermemo.com/en/archives1990-2015/articles/20rules" target="_blank" rel="noopener">Effective learning: Twenty rules of formulating knowledge</a></li><li>原子化 <strong>Make most Anki questions and answers as atomic as possible</strong><ul><li>How to create a soft link from linkname to filename?<ul><li><code>ln -s filename linkname</code></li><li>根据答案分解成两部分。</li></ul></li><li>回答错原子化问题，能更清楚在哪一方面不足。</li></ul></li><li>追求大师级使用 <strong>Anki use is best thought of as a virtuoso skill, to be developed:</strong><ul><li>软件简单，但是功能强大。</li><li>你就是创造者。</li></ul></li><li>一个卡组 <strong>Use one big deck</strong><ul><li>不同知识混合在一起，会产生意想不到的结果。</li></ul></li><li>避免临时感兴趣的话题 <strong>Avoid orphan questions</strong></li><li><strong>95% of Anki's value comes from 5% of the features</strong></li><li><strong>Using Anki for APIs, books, videos, seminars, conversations, the web, events, and places</strong><ul><li>研讨会记录 3 个左右高质量的问题。</li><li>有选择记忆 unmindfully Ankifying everything in sight is a bad habit</li><li>Ankify things that serve your long-term goals</li><li>Anki 之前读过的书或者论文</li></ul></li><li>避免判断题</li><li>创造性工作需要内化理解部分知识。<ul><li>知识的流畅度 Fluency matters in thinking。</li></ul></li><li>为什么 Anki 不火？<strong>If personal memory systems are so great, why aren't they more widely used?</strong><ul><li>人们更喜欢临时报佛脚。In experimental research on memory, people consistently underestimate the gains that come from distributing their study in a manner similar to Anki. Instead, they prefer last-minute cramming, and believe it produces better results, though many studies show it does not.</li><li>理想难度原理 The psychologist Robert Bjork has suggested the “principle of desirable difficulty”, the idea that memories are maximally strengthened if tested when we're on the verge of forgetting them. This suggests that an efficient memory system will intrinsically be somewhat difficult to use. Human beings have a complex relationship to difficult activities, and often dislike performing them, unless strongly motivated (in which case they may become pleasurable).</li><li>用好很难。Systems such as Anki are challenging to use well, and easy to use poorly.</li></ul></li></ul></li></ul><p>最后，Michael Nielsen 在文章中提到 AlphaGo，分享一个这个相关的例子：</p><p><a href="https://twitter.com/xiang578/status/1300068256045129728" target="_blank" rel="noopener">RyenX 算法花园 在 Twitter: "几周前和大老板开会，他突然问我们一个问题，总结一下就是知识层次。 第一层：alpha go 和 alpha zero 的策略网络是如何训练的？（go 学习获胜的下法，zero 学习分布） 第二层：为什么会产出这样的区别？ 第三层：那么这和我们的业务中什么类似？" / Twitter</a> ### Zettelkasten</p><p>最近中文圈比较火的概念，我也多次在 「Never Reading」分享相关的内容。Flynn 在聚会中也分享自己之前写过的一篇文章 <a href="http://flynngao.github.io/2020/07/18/zettelkasten-1" target="_blank" rel="noopener">拆解Zettelkasten | 卡片盒知识管理体系实践反思 - Flynn</a>，摘录相关的内容：</p><ul><li>工作流和笔记方式</li><li>Zettelkasten 认可思考的非线性特征，给出了一种脱离现有顺序框架的方式。</li><li><strong>本来就没有一个完整的知识管理工作流。</strong> 如何判断<ul><li>是否在各种情景下都可以使用有效方式捕捉素材？</li><li>是否对待读材料有合适的管理方式？是否能够不定期清空待读材料？</li><li>是否建立了自己的关注领域清单，并且建立对应的项目？</li><li>是否知道如何为任何项目快速建立下一步行动？</li><li>是否有种合理的方式检验自己对知识的理解？</li><li>是否确认现有正在执行系统/习惯能可以长期执行？</li><li>是否确定现有系统在遭遇中断甚至奔溃之后可以很快恢复？</li></ul></li><li>[[Evergreen Note]] 比 Permanent Notes 更合适<ul><li>以能够直接公布为目的逻辑完备的小文章，并且需要不停的更新他们之间的可能存在的逻辑关系，共同点，冲突点。<ul><li>笔记可以直接公开，比如 [[Andy Matuschak]]</li></ul></li><li>写作当做最重要的事情</li><li>引用 [[Simon Eskildsen]] <a href="https://superorganizers.substack.com/p/how-to-build-a-learning-machine" target="_blank" rel="noopener">How to Make Yourself Into a Learning Machine - Superorganizers</a> 以及 <a href="https://notes.andymatuschak.org/z7kEFe6NfUSgtaDuUjST1oczKKzQQeQWk4Dbc" target="_blank" rel="noopener">notes.andymatuschak.org</a><ul><li><img src="https://media.xiang578.com/flynn-andy.png"></li><li><img src="https://media.xiang578.com/flynn-zk.png"></li></ul></li></ul></li><li>从他的理解来看，[[Obsidian]] 比 [[Roam]] 更合适，以及在 RoamCN 聚会时提到的，需要对文字数量进行限制。[[Progressive Summarization]] 和 [[Evergreen Note]] 的生长概念类似。</li></ul><h3 id="有限与无限的游戏">有限与无限的游戏</h3><blockquote><p>有本书对我蛮有影响的——叫做《有限与无限的游戏》。有限游戏在边界内玩，无限游戏却是在和边界，也就是和“规则”玩，探索改变边界本身。实际上只有一个无限游戏，那就是你的人生，死亡是不可逾越的边界。与之相比，其他的边界并不是那么重要了。 人人网、美团网、饭否网创始人王兴</p></blockquote><p>有限游戏有明确的游戏结束目标，无限游戏的目标是让游戏一直进行下去。生活中的很多方面更多接近于无限游戏。比如工作之后的学习，没有明确的目标引导，更多是去保持这个习惯。</p><p>本期中脱不花的四个面试题中的驱动力也和这两个游戏有关。文章中介绍到谷歌之前从编程比赛中寻找程序员（回想到自己大学参加 ACM 经历），这些人习惯与在边界内做事情（外部刺激和目标）。而谷歌希望的人更多能根据自己的兴趣不断地拓展边界，打破旧有规则。结合到自己的工作中，很多任务没有之前编程时那么明确的目标，更多时候是依赖自己去寻找问题和解决问题。</p><p>另外 <a href="https://mp.weixin.qq.com/s/0tUYITGjZjFb7JLU4F-MUw" target="_blank" rel="noopener">谁在驱动滴滴发动机？</a> 中也提到张博最近在看 《有限与无限的游戏》，互联网下半场可能各个大佬更希望自己的企业基业长青。</p><h3 id="其他">其他</h3><p>还有部分提及的内容还没有时间看，记录一下关键字和大家分享：active recall、spaced repetition、learn how to learn、nstigation habit、Atomic Habits、Mini Habits。</p><h2 id="阅读">阅读</h2><h3 id="个人成长知乎上最神奇的专业领域">「个人成长」，知乎上最神奇的「专业领域」</h3><ul><li>厉害的标准<ul><li>在一个职级体系中通过竞争的手段达到了高位。</li><li>留下传世的精品</li><li>让足够多的人改善自己的生活</li></ul></li></ul><h3 id="产品沉思录-product-thinkingvol.20200816把自己作为方法">[[产品沉思录]] Product Thinking】Vol.20200816：[[把自己作为方法]]</h3><ul><li>他用「<strong>蜂鸟般悬浮</strong>」来描述当下中国人焦虑的现状</li><li>反而对这种「温州乡绅式」的做事方式</li><li>他还提到了一种温柔而坚定地力量 —— <strong>认命不认输</strong><ul><li>所以关键是要把自己所在的社会位置想透。在这个现实下如何去<strong>超脱自己的角色，和强大的社会和历史力量持续的较劲，不认输的较劲。</strong></li></ul></li><li>Temporary Social Media<ul><li>事实证明我还是想的肤浅了、Snap 这两篇文章，是从现有社交网络的设计模式出发，从中寻找被忽略掉的边缘行为，然后反思为何社交网络会的「默认值」会被设计为「记录一切」，而这背后会带来什么样的代价。</li><li>铭记每一刻，但也带来各种挖坟的效果。#idea 删除全部 QQ 空间的说说？</li><li>所有的事情都能被记录的时候，往往意味着什么事情都不重要。#idea 你写日记的时候有没有这个问题？</li><li>所以 Snap 看似用有限的时间和形式，反而促进你能更好的记住某个故事和感觉，而不是像档案馆一样收集你所有的，几乎不会回溯的过往。</li></ul></li><li>温故知新：如何巧妙地达到你沟通的目的？<ul><li>文中关于沟通的定义值得着重记录 —— Ronald B. Adler与Russell F. Proctor将沟通定义为：<strong>“（沟通是）一种交流的过程，参与者经由交换信息而建立关系。”</strong></li><li>不过另一个有趣的地方是关于沟通深度的，即不是两个人在一起聊得越多越好，而是聊得是否是<strong>对自己意义重大的事件，或者是极为隐私的事件</strong>。</li></ul></li></ul><h3 id="回顾我学心理学的这8年经历---discovery---hipda-hipda-fatdragoncat"><a href="https://www.hi-pda.com/forum/viewthread.php?tid=2760589&amp;extra=page%3D1" target="_blank" rel="noopener">回顾我学心理学的这8年经历 - Discovery - Hi!PDA Hi!PDA</a> #[[fatdragoncat]]</h3><ul><li>沙盘游戏展示个人心理状态</li><li>为了改变这个现状，我用了一个最简单粗暴的办法，就是你不理我，那我就去跟随你，参加你参加的一切活动</li><li>我一改过去记者生涯养成的晚上到处约饭的习惯，下班准点回家，周末绝不加班，而且开始主动参加老婆的各种活动，有时是到公园一群妈妈带娃的聚会，有时是做蛋糕，有时是去不同妈妈家聚餐，我本身是极端内向的人，而且前面也说过，我其实是有社交恐惧症的，所以参加这些活动每次都会迅速让我极端疲劳</li><li>移情(transference)来访者对分析者产生的一种强烈的情感，将自己过去对生活中某些重要人物的情感会太多投射到分析者身上的过程。</li><li>反移情(counter-transference)移情是咨询师把对生活中某个重要人物的情感、态度和属性转移到了来访者身上。</li><li>从那时起我就知道了，自己真正渴望的就是一个温暖的家，我想服务的客户，也是渴望家的温暖的人，而不是只想疗愈自己，而不顾伴侣死活的人。</li><li>我不想再听一帮只是花钱找人诉苦，却不肯承担家庭责任的人说废话了</li><li>我的心理咨询工作终结了，我的婚姻也over了，我以为往后的岁月，就是漂泊江湖，四海为家了。</li><li>于是我开始研究，怎么才能背一个双肩背，就能一边工作，一边生活，然后就撞上了知识付费时代的来临。</li><li>心理咨询教练很难，但是保持自己的心理健康可能很容易。</li></ul><h3 id="观点得到ceo脱不花面试一个人你只问ta这四个问题就够了"><a href="https://mp.weixin.qq.com/s/ks6JKet-GR81QZbqxHgOCw" target="_blank" rel="noopener">【观点】得到CEO脱不花：面试一个人，你只问ta这四个问题就够了 ！</a></h3><ul><li>[[面试]] 后来 HR 和 Mentor 告诉我日常要，多积累一些面试相关的题目，这样进去之前就知道，通过什么问题考核什么方面。从此收集一些面试题就成了习惯，而在这个过程中关注点也从对专业内容的考核，变成了对人本身的考核，也变成了对自己的向内思索：这些问题，自己会如何回答呢？</li><li>[[少楠]] 的面试题<ul><li>有哪些事情是别人做起来觉得很难很无趣，但是你自己却乐此不彼的坚持了很久。</li><li>最近半年有什么观点改变了你的认知或者行为方式，为什么？</li><li>你迄今为止做过的，最让你有成就感的一件事是什么？</li></ul></li><li>[[脱不花]] 面试题<ul><li><strong>驱动力：</strong>如果你突然有半个月的带薪休假，只有一个条件，就是必须研究一个事儿，你会研究什么?<ul><li>主要考察的是内驱力，擅长比赛的要靠外部刺激和目标(原文中举例谷歌从编程比赛中寻找程序员)，在边界内做事。<ul><li>#idea 所以我对工作的不适应，除了不了解机器学习，另外一方面来自于大学靠的是外部刺激和目标。现在需要尝试的是突破业务理解？</li></ul></li><li>而内驱力更能适应变化，他们能根据自己的兴趣不断地拓展边界，打破旧有规则</li><li>和最终在谷歌成功概率相关的问题：你几岁开始拥有自己的电脑？</li><li>谷歌的这两条经验,本质上都指向一个问题:一个人的驱动力是从哪里来的?是来自他自己的兴趣或者对自己的要求,还是来自外界什么人给他设定的标准？</li><li>瞬间反应和回答问题的思路。研究的标的、怎么使用这些时间、达到什么目标？</li><li>因为了解个人的内驱力高低,基本上就能判断出来这个人未来应对变化的能力。当环境对他提出新的要求时,他的抗压性强不强,能不能主动适应变化,就能从这个问题中反映出来了。</li></ul></li><li><strong>期望值：</strong>你正在做的事，行业里最顶尖的人或公司是谁，他们是怎么做的?<ul><li>主要是看眼界如何，视野开阔与否。能不能把内驱力转化为行动力</li><li>对自己领域的理解</li><li>对高手的定义决定了他的认知，定义的过程能看到研究的过程及人脉情况</li><li>关键是如何定义和标杆的差异，以及差异如何形成及缩短方式。</li></ul></li><li><strong>人际</strong>：你在此之前的人生经历中，做过什么重要的取舍?<ul><li>如果你做重要取舍都是一个人，是不是验证你是一个孤独的人？</li><li>核心是看决策机制的形成，了解进退感和分寸，是否有清晰的边界意识。</li><li>为什么做出这种选择？出发点是什么？为什么这个时候做？你能不能清晰地界定选择的代价是什么？在你做出这些选择的前后都发生了什么，分别怎么解决的？</li><li>重大决策都会有一两个关键人的影响，关注是否有其他相关人被提及，以及这些人在决策中的角色，来判断他的关系网络</li><li>啰嗦的人无法带大队伍，因为边界不清晰</li></ul></li><li><strong>反思：</strong>针对你刚才提到过的这件事，如果你有机会能重新做一遍，会有哪些地方不一样?<ul><li>反思能力，自己经历过的事情是否有清醒的觉察，评估他对待机会的敏感度。</li><li>对过往是否有总结和复盘，以及对机会的敏感度。颗粒度越细，反思越深。</li><li>面试中很难靠一面之词分清哪些是团队的水平，哪些是个人的贡献。通过反思，根据提到的颗粒度，能够判断，他在项目中究竟起到了多大的作用。</li></ul></li><li>对一个人的内驱力、关系建立能力、目标感和反思能力都有充分的了解能更好帮助做出准确的判断。</li><li>很幽默以及不温和</li><li>最后推荐了 [[奈飞文化手册]]</li></ul></li></ul><h2 id="商业">商业</h2><ul><li><a href="https://mp.weixin.qq.com/s/0tUYITGjZjFb7JLU4F-MUw" target="_blank" rel="noopener">谁在驱动滴滴发动机？</a><ul><li>平台治理</li><li>[<a href="#有限与无限的游戏">有限与无限的游戏</a>]</li><li>叶杰平来滴滴的两道面试题：<ul><li>“一道题目，跟出行里抢单、派单相关，问我能不能抽象成数学问题。”</li><li>能不能把抢单到派单问题，具体建立成一套算法模型？</li><li>纪念一下人已经走了。</li></ul></li></ul></li><li>失去字节技术中台支持的 TikTok，还会是曾经那个 TikTok 吗？ [[The Information]]<ul><li>美国政府：威胁国家安全和违反数据隐私。</li><li>国内以什么理由封杀部分软件？美国是实行对等的权力吗？中国和美国之间的战争一部分，没有一个人是无辜的。另外，没有看到国内政府在这一件事情上发表任何的声明？</li><li>联系一下，这件事情对滴滴的国际化会产生什么样的影响？滴滴会不会被更加宽容的处理？还是不要抱有幻想？</li><li>从监管层到潜在买家，试图改写 TikTok 未来的角色变多了[[The Information]]</li></ul></li></ul><h2 id="算法">算法</h2><p>这个月看起来，看得论文和文章比较少。ai-labs 单机 93w QPS 的模型承保我好几点的笑点。</p><h3 id="滴滴kdd2020论文六-滴滴公开eta新系统线上推理速度进入微秒时代-compacteta-eta"><a href="https://mp.weixin.qq.com/s/quJq0bvTs0qpwYUglQrUfw" target="_blank" rel="noopener">滴滴KDD2020论文(六) | 滴滴公开ETA新系统，线上推理速度进入微秒时代</a> [[CompactETA]] #ETA</h3><ul><li>之前写过他们原来的 ETA 模型文章： <a href="https://xiang578.com/post/wdr.html">(WDR) Learning to Estimate the Travel Time | 算法花园</a>。</li><li>数据稀疏<ul><li>空间稀疏：link 历史数据少<ul><li>基于路况分布来度量不同 link 的相似性，利用 metric learning 进行训练</li></ul></li><li>时间稀疏：相邻时段的 embedding 设置共享参数，使得相邻时段的 embedding 更加相似</li></ul></li><li>如何解决线上预测耗时？<ul><li>[[GAN]] 替代 [[LSTM]]，link 之间的依赖关系通过学习路网的拓扑结构来建立</li><li>位置编码：保持序列信息</li><li>查表 + g 然后过 MLP</li><li><img src="https://media.xiang578.com/compacteta.png"></li></ul></li></ul><h3 id="算法工程师技术路线图---知乎"><a href="https://zhuanlan.zhihu.com/p/192633890" target="_blank" rel="noopener">算法工程师技术路线图 - 知乎</a></h3><ul><li>[[Python]]：[[Learn Python the Hard Way]] [[流畅的 Python]]<ul><li>能读懂 panads、sklearn 等包的源代码</li></ul></li><li>[[Scala]]<ul><li>Spark快速大数据分析</li><li>[[Scala函数式编程]]</li><li>[[冒号课堂]]</li></ul></li><li>[[cpp]] 能够读懂[[LightGBM]]里对于tweedie loss的相关定义代码。</li><li><a href="https://refactoringguru.cn/design-patterns" target="_blank" rel="noopener">常用设计模式有哪些？</a></li></ul><h2 id="embedding-技术的非端到端学习方法---知乎"><a href="https://zhuanlan.zhihu.com/p/188569580" target="_blank" rel="noopener">Embedding 技术的非端到端学习方法 - 知乎</a></h2><ul><li>下载记录变成一个 session</li><li>随机游走扩充数据</li><li>#Airbnb 全局 context [[Real-time Personalization using Embeddings for Search Ranking at Airbnb]]</li><li>同一个类别的随机负样本，分类成本不高。</li><li><img src="https://media.xiang578.com//tencent-app-store-embedding.png"></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;某一刻突然意识到可以选择自己的记忆，我顿悟了。&lt;/p&gt;
    
    </summary>
    
      <category term="Never-Reading" scheme="https://xiang578.com/categories/Never-Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 Imitation Learning</title>
    <link href="https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html"/>
    <id>https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html</id>
    <published>2020-09-06T15:14:47.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>我的笔记汇总：</p><ul><li><a href="https://xiang578.com/post/reinforce-learnning-basic.html">Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html">Actor Critic</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html">Sparse Reward</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html">Imitation Learning</a></li></ul><p>apprenticeship learning</p><ol type="1"><li>无法从环境中获得 reward。</li><li>某些任务中很难定义 reward。</li><li>人为设计的奖励可能导致意外的行为。</li></ol><p>学习专家的行为。</p><h2 id="behavior-cloning">Behavior Cloning</h2><p>监督学习，但是样本有限。</p><p>Dataset Aggregation</p><ol type="1"><li>通过行为克隆得到 actor <span class="math inline">\(\pi_1\)</span></li><li>利用 <span class="math inline">\(\pi_1\)</span> 和环境交互得到一些新的样本</li><li>由专家对上一步采样得到的样本进行标注</li><li>利用新得到的样本训练 <span class="math inline">\(\pi_2\)</span></li></ol><p>如果机器的学习能力有限，可能复制专家多余无用的动作。监督学习无法区分哪些是需要学习、哪些是需要忽视的行为。</p><h3 id="miss-match">Miss match</h3><p>监督学习中，我们假设训练数据和测试数据有相同的分布。Behavior Cloning 中可能分布不同。</p><p><img src="https://media.xiang578.com/15733541795048.jpg"></p><h2 id="inverse-reinfofcement-learning">Inverse Reinfofcement Learning</h2><p>反向强化学习 没有 reward 函数，通过专家和环境互动学到一个 reward function，然后再训练 actor。 <img src="https://media.xiang578.com/15733545279563.jpg"></p><p>类似于 GAN 的训练方法（actor 换成 generator，reward function 换成 discriminator）。 学到 actor 的 pi 后，调整 reward function，保证专家的行为得分大于学到的行为。</p><p><img src="https://media.xiang578.com/15733546273431.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我的笔记汇总：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://xiang578.com/post/reinforce-learnning-basic.html&quot;&gt;Policy Gradient、PPO: Proximal Policy Optimization
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="algorithm" scheme="https://xiang578.com/tags/algorithm/"/>
    
      <category term="reinforcement-learning" scheme="https://xiang578.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 Sparse Reward</title>
    <link href="https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html"/>
    <id>https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html</id>
    <published>2020-09-06T14:14:47.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>我的笔记汇总：</p><ul><li><a href="https://xiang578.com/post/reinforce-learnning-basic.html">Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html">Actor Critic</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html">Sparse Reward</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html">Imitation Learning</a></li></ul><h2 id="reward-shaping">Reward Shaping</h2><p>如果 reward 分布非常稀疏的时候，actor 会很难学习，所以刻意设计 reward 引导模型学习。</p><h3 id="curiosity-intrinsic-curiosity-module-icm">Curiosity Intrinsic Curiosity module (ICM)</h3><p>在原来 Reward 函数的基础上，引入 ICM 函数。ICM 鼓励模型去探索新的动作。最后 ICM 和 Reward 和越大越好。</p><p><img src="https://media.xiang578.com/15732008339393.jpg"></p><p>鼓励探索新动作之后，会导致系统风险变大。对比预测的下一个状态和真正的状态的差异程度进行抑制。</p><p><img src="https://media.xiang578.com/15732012078318.jpg"></p><p>Feature Ext 对状态进行抽取，过滤没有意义的内容。 Network 1 预测下一个状态，然后再和真实状态计算 diff 程度。 Network 2 预测 action，和真实的 action 进行对比。如果两个 action 接近，说明 f 可以进行特征提取。重要程度计算。</p><p><img src="https://media.xiang578.com/15732013249053.jpg"></p><h3 id="curriculum-learning">Curriculum Learning</h3><p>规划学习路线，从简单任务学习。</p><p>Reverse Curriculum Generation</p><p><img src="https://media.xiang578.com/15732019855111.jpg"></p><p><img src="https://media.xiang578.com/15732020592680.jpg"></p><h3 id="hierarchical-reinforcement-learning">Hierarchical Reinforcement Learning</h3><p>对 agent 分层，高层负责定目标，分配给底层 agent 执行。如果低一层的agent没法达到目标，那么高一层的agent会受到惩罚（高层agent将自己的愿景传达给底层agent）。</p><p>如果一个agent到了一个错误的目标，那就假设最初的目标本来就是一个错误的目标（保证已经实现的成果不被浪费）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我的笔记汇总：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://xiang578.com/post/reinforce-learnning-basic.html&quot;&gt;Policy Gradient、PPO: Proximal Policy Optimization
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="algorithm" scheme="https://xiang578.com/tags/algorithm/"/>
    
      <category term="reinforcement-learning" scheme="https://xiang578.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 Actor Critic</title>
    <link href="https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html"/>
    <id>https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html</id>
    <published>2020-09-06T13:14:47.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>我的笔记汇总：</p><ul><li><a href="https://xiang578.com/post/reinforce-learnning-basic.html">Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html">Actor Critic</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html">Sparse Reward</a></li><li><a href="https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html">Imitation Learning</a></li></ul><h2 id="actor-critic">Actor Critic</h2><h3 id="policy-gradient">policy gradient</h3><ul><li>给定在某个 state 采取某个 action 的概率。</li><li>baseline b 的作用是保证 reward 大的样本有更大的概率被采样到。</li><li>从当前时间点累加 reward，并且当前 action 对后面的 reward 影响很小，添加折扣系数。</li><li>PG 效果受到采样数量和质量影响。</li></ul><p><img src="https://media.xiang578.com/15731332476541.jpg"></p><h3 id="q-learning">Q-learning</h3><p>状态价值函数 <span class="math inline">\(V^{\pi}(s)\)</span> 状态行动价值函数 <span class="math inline">\(Q^{\pi}(s,a)\)</span></p><p><img src="https://media.xiang578.com/15731335120798.jpg"></p><h3 id="actor-critic-1">Actor-Critic</h3><p>用 V 和 Q 替换 PG 中的累积 reward 和 baseline。新的模型需要训练两个网络，比较困难。</p><p><img src="https://media.xiang578.com/15731338724158.jpg"></p><h3 id="advantage-actor-critic">Advantage Actor-Critic</h3><p>用 V 去替代 Q，能降低模型整体方差（MC 到 TD)。最下面两个公式转化是由实验得到。</p><p><img src="https://media.xiang578.com/15731340046945.jpg"></p><p>训练过程：</p><p><img src="https://media.xiang578.com/15731341766172.jpg"></p><p>tip:</p><ol type="1"><li>actor 和 critic 具有相同的输入 s，可以共享部分网络结构。</li><li>output entropy 作为 pi 的正则项，entropy 越大采样效果越好。</li></ol><p><img src="https://media.xiang578.com/15731342994690.jpg"></p><h3 id="asynchronous-advantage-acotr-critic-a3c">Asynchronous Advantage Acotr-Critic A3C</h3><ol type="1"><li>利用多个 worker 去训练。</li><li>每个 worker 复制主模型的参数。</li><li>每个模型单独采样，并且计算梯度。</li><li>更新全局参数。</li></ol><h3 id="pathwise-derivative-policy-gradient">Pathwise derivative policy gradient</h3><p>该网络不仅仅告诉 actor 某一个 action 的好坏，还告诉 actor 应该返回哪一个 action。</p><p><img src="https://media.xiang578.com/15731346701840.jpg"></p><p>将这个 actor 返回的 action 和 state 一起输入到一个固定的 Q，利用梯度上升更新 actor。</p><p><img src="https://media.xiang578.com/15731348191409.jpg"></p><p>完整的训练过程和 conditional GAN 类似， actor 是 generator，Q 是 discriminator。</p><p><img src="https://media.xiang578.com/15731350424130.jpg"></p><p>算法：</p><ol type="1"><li>action 由训练的 actor 决定</li><li>利用 s 和 a 更新 Q <img src="https://media.xiang578.com/15731351315860.jpg"></li></ol><h3 id="gan-和-ac-方法对比">GAN 和 AC 方法对比</h3><p><img src="https://media.xiang578.com/15731353632450.jpg"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我的笔记汇总：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://xiang578.com/post/reinforce-learnning-basic.html&quot;&gt;Policy Gradient、PPO: Proximal Policy Optimization
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="algorithm" scheme="https://xiang578.com/tags/algorithm/"/>
    
      <category term="reinforcement-learning" scheme="https://xiang578.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Never-Reading 202007 互联网商业模式</title>
    <link href="https://xiang578.com/post/Never-Reading-202007.html"/>
    <id>https://xiang578.com/post/Never-Reading-202007.html</id>
    <published>2020-08-08T15:55:11.000Z</published>
    <updated>2021-01-09T02:59:54.286Z</updated>
    
    <content type="html"><![CDATA[<p>不知不觉中每月分享已经进行半年，不过前 6 期都没有想到取什么名字。上期的标题「Never Reading」来自稍后读列表名称，仔细一想不正好成为每月分享的名字吗？而且还有致敬「Λ-Reading」的成分。</p><h2 id="互联网商业模式">互联网商业模式</h2><p>在 <a href="http://xiang578.com/post/monthly-issue-202006.html">202006 Never Reading</a> 中摘录过「即刻半月刊」的一段内容：</p><ul><li>所谓“商业模式”其实指的是这家公司的“价值创造模式”，即用什么样的模型创造了更多价值。<ul><li>世界上现存所有的商业模式无非三种，一是[[边际效应]]（规模效应/协同效应），二是[[双边效应]]，三是梅特卡夫[[网络效应]]。不同的价值创造模型，带来不同的增长动力，继而带来不同的货币化方法。</li><li>滴滴是什么模型？</li></ul></li></ul><p>当时没有找到上面这一段内容的解释，只是觉得有点神奇就记录下来。这个月收听「三五环」中刘飞和少楠关于交易平台两期内容（14、17），其中提到许小年教授的一本书「商业的本质和互联网」。在书中详细的介绍商业平台的效应，感兴趣的可以找来详细的阅读。</p><ul><li>[[规模效应]]：[[边际成本]]越低和边际收益越高</li><li>[[协同效应]]：依赖于品种增加带来的 1+1 &gt; 2<ul><li>百货公司拥有协同效应</li><li>#problem 协同效应失败的例子？</li></ul></li><li>[[双边市场效应]]<ul><li>双边供需的进入，都会有正外部性。公式：V=k<em>m</em>n</li><li>电商平台的效应弱：需要平台来管控质量，即变成了单边的；滴滴的效应强：司机和乘客的增加，都会带来正向效应（但边际收益未必持续提升）。</li></ul></li><li>[[梅特卡夫效应]]<ul><li>一个网络的价值与用户量的平方成正比。与常见网络效应的概念基本相同。任何用户的进入，都会有正外部性。</li><li>社交网络</li><li>曾李青定律：V=k*n²/r²（r 受 T、S、I、C 影响）。</li></ul></li></ul><p>读完之后，滴滴是什么模型这个问题就迎刃而解。这个月 有一个比较火视频 <a href="https://www.bilibili.com/video/BV1UT4y1j7Ht" target="_blank" rel="noopener">复盘出行大战：被BAT选中的滴滴，如何完成垄断霸业_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a>，介绍滴滴创业的故事。做为一名内部人士，也被里面的内容给震撼到。之前沈南鹏在吴海波的采访中说过一句话「未来十年看滴滴」。记录自己看到的三个细节：</p><ul><li>滴滴接受腾讯投资后，程维和王刚彻底关上和阿里的联系。</li><li>和快的补贴大战中：马化腾建议每次补贴金额在 12-20 中间的一个随机数</li><li>和 uber 大战中：腾讯封禁 uber 在微信上的微信号</li></ul><p>有一个梦想是能看到程维的传记，之前特意查过他的花名 —— 常遇春。知乎热榜这个月出现过一个奇怪的问题，里面有一段引用：</p><blockquote><p>王保保这个“奇男子”的称号，是朱元璋给的，评价较帝国双壁之一常遇春更高，含金量十足：朱元璋曾大会诸将，问道：“天下奇男子谁也？”诸将都说：“[[常遇春]]将不过万人，横行无敌，真奇男子。”明太祖笑曰：“遇春虽人杰，吾得而臣之。吾不能臣王保保，其人奇男子也。”（《明史》-《扩廓帖木儿传》）</p></blockquote><h2 id="商业">商业</h2><ul><li>阿里很强调价值观，但[[阿里巴巴]]首先是一家商业组织。</li><li>上学时，[[黄峥]]就意识到了机会可贵：“我在上学时就意识到几个事：一是寒门出贵子是小概率事件，大部分富二代，尤其是官二代非常优秀。二是田忌赛马，在整体资源劣势的情况下可以创造出局部优势，进而有机会获得整个战役的胜利。基于此，平凡人可以成就不凡事。第三是钱是工具，不是目的。”</li><li>趣头条本质上是一款游戏产品，只不过披着内容信息流的外衣。<ul><li>不过这一些信息流产品只有学习用户兴趣，真正的影响用户行为还是要靠基于强化学习开发的游戏。</li></ul></li><li>字节系广告变现涉及4个角色：内容消费者，内容生产者，平台，广告主；阿里系广告变现涉及3个角色：消费者，商家，平台；所以阿里的广告变现链条更短，一定程度上也是效率更高的原因。多一个角色，多一份成本，最终效果要多乘一个参数。</li><li><a href="https://medium.learningbyshipping.com/apples-relentless-strategy-and-execution-7544a76aa26" target="_blank" rel="noopener">Apple’s Relentless Strategy, Execution, and Point of View | by Steven Sinofsky | Jun, 2020 | Learning By Shipping</a> Steven Sinofsky 是前微软 Windows 业务部门的总裁评价苹果架构迁移。<ul><li>其实没有仔细看这一篇文章。震惊到我的是，这位兄弟管理过 20000 人的工程团队？不知道现在国内有哪一些公司的 CTO 能管理这么多的人。</li></ul></li><li><a href="https://sspai.com/post/61174" target="_blank" rel="noopener">从苹果的系统更新，理解设计中的「控制」与「自由」 - 少数派</a><ul><li>雪城大学建筑学教授理查德·洛萨（Rhichard Rosa）认为<strong>“设计的本质即是在于控制与自由。”</strong>这句话非常简洁但直接地介入了设计的核心——设计行为为混乱与无序赋予秩序，使之得以承载可控的人类行为；但设计也一定不是绝对的控制，它一定要为使用者预留一些自由。</li><li>设计师对美学的自信会落脚在对细节的强制：贝聿铭设计京都东郊的美秀美术馆，下雨天需要领取相应颜色的雨伞才能进入。</li><li><strong>有些人说：“消费者想要什么就给他们什么。”</strong> <strong>人们不知道想要什么，直到你把它摆在他们面前</strong></li><li><strong>设计一定需要控制，但一定不是绝对的控制。平台的搭建者需要预留一些自由给开发者以及用户，对秩序的全盘接手最终会导致自下而上民意的反弹。</strong></li><li>[[设计的哲学]]，设计的背后是不是需要有哲学？一味满足用户需求的软件是不是会变得十分的复杂？比如 Emacs 之类自由度高的软件？</li></ul></li><li><a href="https://www.jianshu.com/p/03080891e4ba" target="_blank" rel="noopener">顶级PM的产品观：王慧文看行业-合集 - 简书</a><ul><li>互联网 AB 面：A类是供给和履约在线上，B类是供给和履约在线下。</li><li>B类又可以分为：以SKU为中心的供给B1和以Location为中心的服务B2。</li><li>A 类能力体现在产品设计领域，体现在用户理解上，体现在对于通讯、社交以及内容把握上。</li><li>B1里面，主要体现在对于品类的理解，对于供应链的理解，对于定价的理解。</li><li>B2里面，如果你们去盘点一下B2的公司，他们总体来说有一个比较共有的特征，大规模的线下团队。</li><li>是否有大规模的线下团队是B1和B2一个很大的差别。</li><li>在LBS的方向上，中国和美国的企业这样的差距是怎么样发生的？大概有四个因素决定：人力成本、人口密度、人口规模、代际竞争。</li></ul></li></ul><figure><img src="https://media.xiang578.com/a-b1-b2.png" alt="互联网 AB 面"><figcaption>互联网 AB 面</figcaption></figure><h2 id="阅读">阅读</h2><ul><li>理解世界的一个有效方法是，在人生的某个阶段，把任何之前视为理所当然的事情全都重新研究与思考一遍，并弄清楚它们运行的真正起源与机理。在这个过程中，自问的问题越基础、越显得不需要去质疑，收获往往就会越多——人为什么要每天吃三顿饭、买东西为什么要花钱、书籍和文章为什么会存在——真理通常就藏在这些大多数人想都不会想的事情里。[[张潇雨]]<ul><li>「银河系漫游指」里面有一句：任何在我出生时已经有的科技都是稀松平常的世界本来秩序的一部分。</li></ul></li><li>什么是第一原理？<ul><li>[[亚里士多德]] 在[[形而上学]]中提出这个哲学概念，指「公理：无法再分、无法证明且不证自明的命题。」</li><li>“第一原理”本身并不是什么原理，它只是个简称。准确地说，应该叫做“从第一原理推理”（reasoning by first principle），是<strong>分析问题，找出其不可继续拆分的根本原因，即第一原理，再从第一原理反推出解决方案的思考方式。</strong></li><li>第一原理是先验</li></ul></li></ul><h2 id="算法">算法</h2><ul><li><a href="https://www.tanglei.name/blog/shuffle-algorithm.html" target="_blank" rel="noopener">面试官：会玩牌吧？给我讲讲洗牌算法和应用场景吧！ | 唐磊的个人博客</a> [[洗牌算法]]<ul><li>之前的分享中写房租分配 <a href="https://xiang578.com/post/week-issue-9.html">每周分享第 9 期：拼多多 | 算法花园</a></li><li>保证每次的概率是相同</li><li>#[[problem]]实现以下算法：一组数，每次不放回抽样，得到一个随机序列。白板编程。分析时间空间复杂度。<ul><li>follow up：能否时间O(n)完成，能否空间O(1)完成。</li></ul></li></ul></li><li><a href="https://www.youtube.com/watch?v=733m6qBH-jI" target="_blank" rel="noopener">Stanford CS230: Deep Learning | Autumn 2018 | Lecture 8 - Career Advice / Reading Research Papers - YouTube</a>：Ag 在课程中介绍如何阅读论文，又一次感受到大佬的真诚。<ul><li>主题阅读<ul><li>收集资料</li><li>列出一个 list ，标注阅读进度。挑选有价值的论文阅读。</li><li>5-20 初步了解</li><li>50-100 很好理解前沿工作</li></ul></li><li>如何阅读论文<ul><li>多遍阅读</li><li>第一遍：标题，摘要，图片</li><li>第二遍：简介、结论、图片相关材料</li><li>第三遍：进入论文主体部分，但是可以跳过数学，明白每个参数的含义。</li><li>第四遍：阅读整篇文章，跳过没意义的部分(内容过时，没有火起来过)。</li></ul></li><li>阅读时思考的问题<ul><li>作者试图解决什么问题？</li><li>研究方法的关键是什么？（最具有开创性）</li><li>哪些东西可以为你所用？</li><li>有哪些参考文献可以继续跟进？</li></ul></li><li>最后发现一篇相关实践文章： <a href="https://towardsdatascience.com/how-you-should-read-research-papers-according-to-andrew-ng-stanford-deep-learning-lectures-98ecbd3ccfb3" target="_blank" rel="noopener">How You Should Read Research Papers According To Andrew Ng (Stanford Deep Learning Lectures) | by Richmond Alake | Jul, 2020 | Towards Data Science</a><br></li></ul></li><li>另外一篇和论文阅读有关的文章：<a href="https://zhuanlan.zhihu.com/p/163227375" target="_blank" rel="noopener">沈向洋、华刚：读科研论文的三个层次、四个阶段与十个问题 - 知乎</a><ul><li>三个层次：速度、精读与研读</li><li>四个阶段：Passive Reading、Active Reading、Critical Reading、Creative Reading</li></ul></li></ul><p>这就是本期的 「Never-Reading」，我们下个月再见。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;不知不觉中每月分享已经进行半年，不过前 6 期都没有想到取什么名字。上期的标题「Never Reading」来自稍后读列表名称，仔细一想不正好成为每月分享的名字吗？而且还有致敬「Λ-Reading」的成分。&lt;/p&gt;
&lt;h2 id=&quot;互联网商业模式&quot;&gt;互联网商业模式&lt;/h2&gt;
      
    
    </summary>
    
      <category term="Never-Reading" scheme="https://xiang578.com/categories/Never-Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>202006 Never Reading</title>
    <link href="https://xiang578.com/post/monthly-issue-202006.html"/>
    <id>https://xiang578.com/post/monthly-issue-202006.html</id>
    <published>2020-07-14T15:55:11.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>这一份 6 月的阅读总结来的有一点晚。前几年一直断断续续在实践 GTD，人的拖延症超乎想象，学到的一个经验是 「Now or Never」。所以，这个月将自己的阅读列表取名为「Never Reading」。</p><h2 id="笔记方法精进">笔记方法精进</h2><p>Roam Research 引起现在这一波 Backlink 笔记软件浪潮，本月依然阅读一些和笔记方法相关的文章。</p><h3 id="我的-zettelkasten-卡片盒笔记法实践-吕立青的博客">我的 Zettelkasten 卡片盒笔记法实践 | 吕立青的博客</h3><p><a href="https://blog.jimmylv.info/2020-06-03-zettelkasten-in-action/" target="_blank" rel="noopener">文章链接</a></p><p>上个月介绍过，自己已经从 Roam Research 迁移到 Obsidian，然后就看到吕立青这一篇基于 Obsidian 的 Zettelkasten 实践。文章中将卡片盒笔记分拆分成四步：</p><blockquote><p>1️⃣ 第一步：必须用自己的话写笔记卡片，以确保你将来能够理解。</p><p>2️⃣ 第二步：无论何时添加新笔记，主动查找可链接到的已有笔记。</p><p>3️⃣ 第三步：通过添加新记录并联系起来，延续这一系列的连续思考。</p><p>4️⃣ 第四步：使用 Anki 间隔重复加深记忆，主动由大脑触发远程联想。</p></blockquote><p>搞笑地是，在他写出这篇文章没有多久之后，已经开始尝试往 Roam Research 上迁移……另外，为了更好在国内推广，他参与发起 <a href="roam-cn.github.io">roam/cn</a> 组织，从英文世界翻译一些推特、视频以及分享一些个人的案例。</p><h3 id="zettelkasten-note-taking-in-10-minutes">Zettelkasten note-taking in 10 minutes</h3><p>两条原则：</p><ul><li>Don’t try to get this method perfect from the get go.</li><li>The advanced practices are useful only when you’ve got close to 1000 notes</li></ul><h3 id="my-productivity-app-for-the-past-12-years-has-been-a-single-.txt-file">My productivity app for the past 12 years has been a single .txt file</h3><p>12 年间使用一个 txt 进行任务管理方法分享。作者提到 to do list 变成 what done list 的过程，每天晚上将日历中第二天的代办事项整理到 txt 中，第二天顺手记录任务相关的信息（比如讨论出的结论，或者得到的信息）。结合自己使用经历，OmniFocus 是一个 to do list，基于纯文本的任务管理方式（org-mode 或 taskpaper）更容易成为 what done list。</p><h2 id="商业">商业</h2><ul><li><a href="https://36kr.com/p/736767189897093" target="_blank" rel="noopener">滴滴重踩油门_详细解读_最新资讯_热点事件_36氪</a> 不开玩笑地说，看完这里面的分析，我才理解公司很多的战略。</li><li><a href="https://www.notion.so/c074ab234e2b4dbb9143a48faceec031" target="_blank" rel="noopener">淘宝宣战拼多多的前夜：吕晋杰、陈琪、徐易容和葛永昌的至暗时刻</a> 和很多同学聊天，17 年找工作的时候低估了拼多多的潜力。<ul><li>淘宝 PC 转移动互联网时，流量入口从网页短 300 个减少到手淘 app 48 个。是不是能解释淘宝这几年涌现出那么多的推荐系统相关的问题。</li><li>电视广告投放策略以及如何理解流量暴涨，拼多多上一直很火爆的「百亿补贴」</li><li>任何人都可以成为英雄，哪怕是做了一件不起眼的事情。</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/146363597" target="_blank" rel="noopener">快手的普通，抖音的美好，算法的价值观 - 知乎</a><ul><li>算法没有价值观，算法只会实现设计者的意图。</li><li>主动降低算法效率，是为了实现某些短期无法衡量的业务目标。</li><li>抖音将流量集中在头部。</li><li>快手整体的点击率被牺牲，普通人的流量被保证。</li><li>抖音说记录美好生活，快手说记录世界记录你。</li></ul></li></ul><h2 id="阅读">阅读</h2><h3 id="人生总有一刻我们会开始思考死亡">人生总有一刻，我们会开始思考死亡</h3><p>茨威格在《人类的群星闪耀时》所说的那种幸运是什么 —— 「最大的幸运，莫过于在年富力强的时候，发现了自己的使命」</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/24640592" target="_blank" rel="noopener">人生总有一刻，我们会开始思考死亡 - 知乎</a> [[张潇雨]] 于是我发现，面对死亡最终可能只有两种方法。 - 一种是将自己与一些更宏大的东西联系起来：一个数学定理、一本文学著作、一件艺术作品或一种恒久的信仰。马尔克斯与康德靠《百年孤独》与《纯粹理性批判》遗世独立，米开朗基罗把《创世纪》和《最后的审判》印刻在西斯廷大教堂里，供千万后朝拜——他们肉身虽灭，但精神不朽——反正建筑是永远戳在那儿的。 - 还有一种就是，生活在当下的每个瞬间里，不烦扰过去、不担忧将来。</p></blockquote><h3 id="即刻半月刊">即刻半月刊</h3><p>6.18？即刻重新开放，之前对这个社区没有太多印象，尝试关注一些人之后，信息流的质量也不错。「即刻半月刊」是某些爱好者挑选的一些即友发言集合。摘录一些我觉得有意思的内容。</p><ul><li>老罗带货首卖小米的中性笔：数量太少。之前看到过一个分析，抖音自己做电商最大的困难点在于没有用户的物流地址。简单想一下，用户可能在填写地址这一步流失。</li><li><span class="citation" data-cites="Rey_L">@Rey_L</span> 我在判断哪家公司的产品会换灰色皮肤的时候，基本都猜对了hhh，滴滴头条一定会做，京东会做，淘宝也许会做，拼多多肯定不做。</li><li>微信小程序减少 App Store 在国内的下载量。不过大部分巨头的小程序还是想要从微信中引流的。</li><li>底下有回复cite了一个working paper，大意说的是名校（elite education，这里用的是211高校与非211高校的断点回归）能够给学生带来的起薪上 30-45% 的提升，其原因在于名校当中的social network，elite的peer普遍家庭条件更好，成绩也更好。上网课带来不了这种social connection...所以说不单只是知识改变命运，环境也很重要</li><li>所谓“商业模式”其实指的是这家公司的“价值创造模式”，即用什么样的模型创造了更多价值。<ul><li>世界上现存所有的商业模式无非三种，一是[[边际效应]]（规模效应/协同效应），二是[[双边效应]]，三是梅特卡夫[[网络效应]]。不同的价值创造模型，带来不同的增长动力，继而带来不同的货币化方法。</li><li>滴滴是什么模型？</li></ul></li><li>A/B test只能做模型的小优化，但找不到大的绝对的增长点，所以除了推荐系统里的参数，即刻的很多产品决策希望尽量远离A/B test</li><li>张小龙的饭否，张一鸣的微博，黄峥的微信公众号，很可惜这几个都成了过去形态。现在只剩下王兴的饭否了。</li><li>一个反向小思考：兴趣社交刚需归刚需，社交app的本质还是信息分发。RSS订阅20周年，分发的有效性也不是什么相似推荐，^<sup>而是真正提供专注力的阅读</sup>^。因为个性推荐的本质就是专注力。信息廉价甩卖的今天，打开微博广场/抖音主页/朋友圈（广告），用户最需要的不是新鲜事而是减少噪音的阅读器，人永远看到自己想看到的，产品顺水推舟不也是更讨巧，所以需要设计者克制。沟通永远是双向的，当用户感到一个舒适被接纳的舆论场所，也会更加有表达欲。</li><li>芒果系这么多年都在做一件事，就是把握这个时代的情绪。[[乘风破浪的姐姐们]]</li></ul><h2 id="机器学习">机器学习</h2><p>本月 KDD 2020 的文章应该已经放出，推荐阅读 Airbnb 深度模型实践相关的文章 Managing Diversity in Airbnb Search 以及 Improving Deep Learning For Airbnb Search。 7 月份希望能写一篇博客分享自己的阅读笔记。</p><ul><li><a href="https://www.zhihu.com/question/389912594" target="_blank" rel="noopener">神经网络中对需要concat的特征进行线性变换然后相加是否好于直接concat? - 知乎</a> [<a href="#机器学习">机器学习</a>]<ul><li>concat 没有信息融合，也没有信息缺失</li><li>add 不同向量之间的权重相同，会导致信息缺失。待融合的特征具有相同分布，或特征属于同一类，直接相加才有可能提升模型性能。</li><li>concat + MLP 能够自动学习不同 channel 的权重，MLP 能引入非线性。利于通道间的信息融合，计算量大。</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/24851814" target="_blank" rel="noopener">【机器学习】Bootstrap详解 - 知乎</a>：随机森林里面用到的数据重采样方法，老板要求训练两个有差异模型时给我推荐的方法。</li><li><a href="https://zhuanlan.zhihu.com/p/148729018" target="_blank" rel="noopener">BERT 可解释性-从"头"说起 - 知乎</a>：蛮有意思的对 BERT 组件效果分析，这位作者举的例子有些蹭热点。</li><li><a href="https://zhuanlan.zhihu.com/p/63982470" target="_blank" rel="noopener">都9102年了，别再用Adam + L2 regularization了 - 知乎</a>：说明为什么要用 AdamW。另外推荐一下，之前一位同事写的 AdamW 实现：<a href="https://zhuanlan.zhihu.com/p/40814046" target="_blank" rel="noopener">L2正则=Weight Decay？并不是这样 - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这一份 6 月的阅读总结来的有一点晚。前几年一直断断续续在实践 GTD，人的拖延症超乎想象，学到的一个经验是 「Now or Never」。所以，这个月将自己的阅读列表取名为「Never Reading」。&lt;/p&gt;
&lt;h2 id=&quot;笔记方法精进&quot;&gt;笔记方法精进&lt;/h2&gt;
&lt;
      
    
    </summary>
    
      <category term="每月分享" scheme="https://xiang578.com/categories/%E6%AF%8F%E6%9C%88%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>每月分享 202005 Newsletter</title>
    <link href="https://xiang578.com/post/monthly-issue-202005.html"/>
    <id>https://xiang578.com/post/monthly-issue-202005.html</id>
    <published>2020-06-07T15:55:11.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<h2 id="newsletter">Newsletter</h2><p>从去年开始给我一种 RSS 复兴的感觉，这个月尝试使用 Newsletter。对于创作者来说，RSS 不仅无法统计数据，也很难开展会员模式。Newsletter 通过邮箱订阅的的手段，完美解决这两个问题，国外开始有一站式的解决方案，可能几个月之后也会在国内火起来。推荐自己订阅的一些邮件组给大家。</p><ul><li><a href="https://www.notion.so/PRODUCT-THINKING-a601a12335044f349a22caf57f274c27" target="_blank" rel="noopener">PRODUCT THINKING · 产品沉思录精选</a>：第一个付费订阅的邮件周刊，目前的价格是 199 元/年。根据少楠自己写的介绍，内容包括但不限于产品设计，服务设计，数据分析，互联网技术，经济学，心理学，社会学，决策学，自然科学，城市规划，零售，团队管理等内容。每周会推荐几篇网上比较好的文章，偶尔也翻译一些英语文章。挑选几篇我觉得不错的公开内容：<ul><li><a href="https://www.notion.so/Zettelkasten-25627d7ce99344c487f4e42d861f9e0a" target="_blank" rel="noopener">原子笔记法：Zettelkasten</a></li><li><a href="https://www.notion.so/P-A-R-A-Notion-19909e5aac3049d887197dcfb1e97fd5" target="_blank" rel="noopener">如何管理信息：P.A.R.A. 是什么及在 Notion中的应用</a></li></ul></li><li><a href="https://rizime.substack.com/" target="_blank" rel="noopener">Λ-Reading</a> 阅读相关分享，作者的读书笔记网站也值得一看 <a href="https://rizi.me/" target="_blank" rel="noopener">— Read the Word,Read the World.</a>。推荐内容：<ul><li><a href="https://rizime.substack.com/p/f08" target="_blank" rel="noopener">路径依赖和困扰计算机的简单问题 - Λ-Reading</a></li><li><a href="https://rizime.substack.com/p/d28" target="_blank" rel="noopener">号外：知识管理工具 - Λ-Reading</a> 中文为数不多关于 TiddlyWiki 的介绍。</li></ul></li><li><a href="https://clearbox.substack.com/" target="_blank" rel="noopener">透明盒子计划</a> 深度阅读分享，盒子对应 Zettelkasten。<ul><li><a href="https://clearbox.substack.com/p/coming-soon" target="_blank" rel="noopener">透明盒子计划 - 透明盒子计划</a></li></ul></li><li><a href="https://superorganizers.substack.com/" target="_blank" rel="noopener">Superorganizers</a> 对国外人士的采访，有关于效率、数字生活等。目前只看他的免费内容……<ul><li><a href="https://superorganizers.substack.com/p/how-to-build-a-learning-machine" target="_blank" rel="noopener">How to Make Yourself Into a Learning Machine - Superorganizers</a>：对一名高中辍学的小哥的采访，介绍来一些自我教育的方法。</li></ul></li></ul><h2 id="阅读">阅读</h2><ul><li><a href="https://sspai.com/post/60466" target="_blank" rel="noopener">How to take smart notes，方法及工具 - 少数派</a>：Zettelkasten 这种做笔记方法慢慢开始要在国内流行起来，自己已经关注差不多超过半年的时间，接下来也在计划写一篇相关的博客文章。</li><li><a href="https://beepb00p.xyz/hpi.html" target="_blank" rel="noopener">Human Programming Interface</a> 简单看来一下，利用 py 包和 Emacs 管理所有相关的个人数据，挺疯狂的。</li><li>上古论坛差不多十年前的帖子， <a href="https://www.hi-pda.com/forum/viewthread.php?tid=819978&amp;extra=&amp;authorid=1956&amp;page=1" target="_blank" rel="noopener">我的千书阅读计划 - 意欲蔓延 - Hi!PDA Hi!PDA</a> fatdragoncat 通过阅读成为一名自由职业者。帖子中介绍大量篇幅介绍如何高效阅读、锻炼、自我管理等等。在印象笔记中找到几年前自己写的笔记，现在重新整理一下相关的内容，并分享给大家。</li><li><a href="https://www.twitch.tv/videos/611050187" target="_blank" rel="noopener">AndyMatuschak - Making sense of Design Unbound vs. prior theories of collaborative design work - Twitch</a> [[Evergreen notes]]的创始人公开展示写作的过程。通过这个视频可以发现他使用的笔记软件是 [[Bear]]，看起来 Reference 和 Backlink 都是手动输入的，不过这样也符合 [[Zettelkasten]] 的原则。只是 [[Roam Research]] 这样的软件让我们变懒。</li><li>莫言获得诺贝尔文学奖发表的演讲中有一个故事：到了荒滩上，我把牛羊放开，让它们自己吃草。蓝天如海，草地一望无际，周围看不到一个人影，没有人的声音，只有鸟儿在天上鸣叫。我感到很孤独，很寂寞，心里空空荡荡。有时候，我躺在草地上，望着天上懒洋洋地飘动着的白云，脑海里便浮现出许多莫名其妙的幻象。我们那地方流传着许多狐狸变成美女的故事，我幻想着能有一个狐狸变成美女与我来作伴放牛，但她始终没有出现。但有一次，一只火红色的狐狸从我面前的草丛中跳出来时，我被吓得一屁股蹲在地上。狐狸跑没了踪影，我还在那里颤抖。有时候我会蹲在牛的身旁，看着湛蓝的牛眼和牛眼中的我的倒影。有时候我会模仿着鸟儿的叫声试图与天上的鸟儿对话，有时候我会对一棵树诉说心声。但鸟儿不理我，树也不理我。许多年后，当我成为一个小说家，当年的许多幻想，都被我写进了小说。很多人夸我想象力丰富，有一些文学爱好者，希望我能告诉他们培养想象力的秘诀，对此，我只能报以苦笑。</li></ul><h2 id="机器学习">机器学习</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/138136777" target="_blank" rel="noopener">谈谈推荐系统中的用户行为序列建模 - 知乎</a> 一篇关于用户行为序列建模的文章，基本上常用的方法都介绍了。<ul><li>和上一次 "<a href="https://zhuanlan.zhihu.com/p/125507748" target="_blank" rel="noopener">从谷歌到阿里，谈谈工业界推荐系统多目标预估的两种范式 - 知乎</a>[[机器学习实践]][[MTL]]" 属于同一个作者</li><li>目前主流推荐系统框架 [[Deep Neural Networks for YouTube Recommendations]] 中的 Matching 和 Ranking。另外可能还有规则模块。</li><li>pooling-based architecture 范式，用户行为是无序集合，使用 sum/max pooling 或各种 attention<ul><li>[[Deep Neural Networks for YouTube Recommendations]] 中将用户观看过的视频序列取到 embedding 后，做一个 mean pooling 作为用户历史兴趣的表达</li><li>Ranking 阶段：[[DIN]] target item 和行为序列的 item 做一个 attention，得到一个 weight，然后加权求和。</li><li>结合 [[Transformer]] 做 self-attention 并行的建模长序列依赖，除去用户行为序列中的噪声：[[Behavior Sequence Transformer for E-commerce Recommendation in Alibaba]]</li></ul></li><li>sequential-modeling architecture 范式，用户行为当成一个具有时间属性的序列，使用 RNN、LSTM、GRU 等<ul><li>[[Perceive Your Users in Depth: Learning Universal User Representations from Multiple E-commerce Tasks]] Property Gated LSTM</li><li><a href="https://zhuanlan.zhihu.com/p/30720579" target="_blank" rel="noopener">推荐中的序列化建模：Session-based neural recommendation - 知乎</a></li></ul></li><li>上面两种方法都是将用户行为经过 pooling/attention/rnn 的处理，聚合成用户行为序列的 embedding，再和其他的特征 concat 在一起，经过 mlp 后接 sigmod/softmax</li><li>抽取聚类出用户多峰兴趣，Capsule<ul><li>阿里 [[MIND]] 胶囊网络</li></ul></li><li>辅助损失函数<ul><li>[[DIEN]] 兴趣提取和兴趣演化，以最后一个 hidden state 做为用户兴趣的表达。兴趣提取模块，使用隐状态和下一件商品预测做二分类。不加入辅助loss，GRU 的隐变量完全受限于最终点击的 label，加入后能约束 GRU 每个隐状态表示其本身的兴趣。</li></ul></li><li>提升用户序列长度，可以带来可观的 auc 提升。[[MIMN]]</li></ul></li><li>Applying Deep Learning To Airbnb Search：一篇关于从 GBDT 模型迁移到深度模型的工业实践记录 paper。对于我这种没有经历过这种技术迭代的人来说，工业级的深度模型上线比想象中的要困难。作者们针对自己遇到的比如 listing embedding 训练不充分、如何判断 feature 的重要性等问题设计实验去验证以及给出解释。严谨的精神值得吾辈学习。</li></ul><h2 id="其他">其他</h2><p>出于对 Roam Research 开发者的不放心，已将全部文档迁移到 Obsidian。目前还在探索新的工作流，5 月分享不可避免产生拖延。另外还在寻找一种建立 Digital Garden 的方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;newsletter&quot;&gt;Newsletter&lt;/h2&gt;
&lt;p&gt;从去年开始给我一种 RSS 复兴的感觉，这个月尝试使用 Newsletter。对于创作者来说，RSS 不仅无法统计数据，也很难开展会员模式。Newsletter 通过邮箱订阅的的手段，完美解决这两个问题
      
    
    </summary>
    
      <category term="Never-Reading" scheme="https://xiang578.com/categories/Never-Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>每月分享 202004 新的尝试</title>
    <link href="https://xiang578.com/post/monthly-issue-202004.html"/>
    <id>https://xiang578.com/post/monthly-issue-202004.html</id>
    <published>2020-05-01T08:55:11.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>不知不觉又到更新每月分享的时间。</p><p>想写一下我为什么做这件事情？分享自己平时看到有意思的内容，现实世界认识的人，很少对我关注的内容感兴趣，所幸能借助博客超越时间和空间限制的分享。</p><p>另外一点，我希望自己能将这个系列当成一个产品去迭代，每一期都有形式和内容上的进步。这件事看起来很简单，但却需要耗费很大的精力。其实在网上看到很多人通过这种形式分享，到头来还在坚持的大概也没有多少人（比如阮一峰的科技爱好者周刊）。</p><h2 id="zettelkasten-以及-roam-research">Zettelkasten 以及 Roam Research</h2><p>Zettlekasten 是一个德语单词，意思是卡片盒。现在主要指一种记笔记的思路。Roam Research 是目前国外很火的一个笔记软件，最大的特点是实现不同笔记之间的双向链接。好几个月前就开始尝试 Zettlekasten 的方法，4 月开始才使用 Roam Research。这里先分享一些我看过的文章。</p><ul><li><a href="https://twitter.com/Tisoga/status/1244856639439515649" target="_blank" rel="noopener">Roam Research 入门指南（thread）</a></li><li><a href="https://medium.com/@dongyangvic/%E7%94%A8-roam-research-%E6%9D%A5%E6%89%93%E8%8D%89%E7%A8%BF-a3b1d3873aa4" target="_blank" rel="noopener">用 Roam Research 来打草稿 - Dongyang Vic - Medium</a></li><li>Roam Fu<ul><li><span class="citation" data-cites="RoamResearch">[Part I: My plan for using @RoamResearch for a thesis]</span>(https://twitter.com/kcorazo/status/1247260599760736256)</li><li><a href="https://twitter.com/kcorazo/status/1252669427125895169" target="_blank" rel="noopener">Part II: Using Twitter as an inter-brain zettelkasten</a></li><li><a href="https://twitter.com/kcorazo/status/1255296825566920705" target="_blank" rel="noopener">Part III: Beyond the Empire</a></li></ul></li></ul><h2 id="阅读">阅读</h2><ul><li><a href="https://www.gcores.com/articles/121924" target="_blank" rel="noopener">中文互联网中“讨论”的消亡 | 机核 GCORES</a>：从产品设计角度分析国内主流网站如何限制用户讨论，互联网缩短人与人之间的距离，但我们用来互喷。</li><li><a href="https://www.huxiu.com/article/350854.html" target="_blank" rel="noopener">互联网是人类历史的一段弯路吗？-虎嗅网</a>：又一篇深度长文，很多观点可以背下来出去装B。</li><li><a href="https://zhuanlan.zhihu.com/p/135281778" target="_blank" rel="noopener">ByteDance程序员生存指南 - 知乎</a><ul><li>有些年轻人，在结束一整天的工作后，拖着疲惫不堪的肉体回到出租屋，这时候只想躺着啥都不想干，躺在好几周没有换过的床单上点开了抖音，不一会儿就刷到了凌晨，第二天再拖着疲惫不堪的肉体上班。到了周末也没有任何心力出去玩，只能睡到中午然后随便吃点东西看看剧。没有生活，没有朋友，一晃就单身了四五年，长此以往不仅仅是肉体在亚健康和崩溃的边缘，心理健康更是会出现问题。 孤独、焦虑、易怒等等情绪时刻伴随着一个人。</li><li>每个月仅仅只有收到工资短信时可以高兴几分钟。但这笔钱并不敢轻易花出去，因为都是血汗积攒而来，付出的多自然不敢随意花出去。</li></ul></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzM0MjUyMQ==&amp;mid=2652149604&amp;idx=1&amp;sn=3c96ebfe992694e5c57affe9ef5ba33f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">构建优雅的知识创造系统</a> [[阳志平]] [[卡片写作法]]<ul><li>利用电子卡片后，如何量化每天的产出？是不是可以换算成 github 更新多少字？</li><li>卡片是自己看的，不需要分享：<strong>在卡片层级最大的误区是：分享</strong>。<strong>不少人误将卡片、文件和项目三个层级混为一谈，喜欢在卡片层级搞分享</strong>。这样每次撰写卡片时就增加了一个选择项：<strong>这张卡片我是该分享还是该存着自己看？增加的认知操作加大了认知负荷，从简单反应时变为选择反应时</strong>。所以尽可能在卡片层级少做分享。</li><li>一周、一月、一年、十年与数十年，进行自己的实践。</li></ul></li><li>所有人问俞军<ul><li>[[金字塔原理]] 和 [[学会提问]]</li><li>优秀的思维方式以及对人性和世界的底层理解</li><li>我不是说人工智能不好，而是我自己只关心我能想明白一两年内能给创造什么用户价值的产品(或技术等)，如果我想不明白这东西马上能给用户创造什么价值，我就毫无兴趣。</li><li>看《搜索研究院》页首页尾的那句话，“我们若能更妥善地搜寻资料，实在已经改变世界。”</li></ul></li><li>前百度首席科学家吴恩达谈学习<ul><li>如果你学习，两天后的周一，你不会很快的就在工作中出彩，你的老板也不会知道你花了整天的时间学习，更不会夸奖你什么。你几乎找不到任何东西可以证明你在努力学习。</li></ul></li><li><a href="https://blog.jimmylv.info/2016-07-12-pkm-again-to-innovate-my-note-system/" target="_blank" rel="noopener">再谈个人知识管理：革新我的笔记系统 | 吕立青的博客</a> 好久没有看到立青写文章<ul><li>不同软件之间，单条笔记的迁移相当于一次对知识进行提炼的过程。</li></ul></li><li><a href="https://tundrazone.com/guanyudongwuzhisendesanze/" target="_blank" rel="noopener">关于动物之森的三则 – 苔原带</a><ul><li>以前玩家渴望在游戏里杀死巨龙，飞向人马座，击败外星侵入者。而现在我们只需要在游戏里“正常”的生活就乐呵呵了。和朋友一起野餐，请朋友到家里来玩，一个人傻乎乎做开心的事情在现实世界中已经有这么高的门槛了么？又或者只是因为在游戏里做这些事情的成本足够低，多巴胺回报足够快？</li></ul></li></ul><h2 id="机器学习">机器学习</h2><ul><li>内部开始尝试 MCTS 相关的项目。正好接这个机会看一下 DeeepMind 前几年的论文 <a href>Mastering the game of Go with deep neural networks and tree search</a> 以及 <a href>Mastering the game of Go without human knowledge</a>。推荐去看一下田渊栋在知乎上 <a href="https://zhuanlan.zhihu.com/p/20607684" target="_blank" rel="noopener">AlphaGo的分析</a> ，当时他在 Facebook 参与类似围棋相关的项目。另外就是木遥的 <a href="https://36kr.com/p/5042969" target="_blank" rel="noopener">关于 AlphaGo 论文的阅读笔记</a> 有更多关于现实的思考。最后推荐 <a href="https://movie.douban.com/subject/27012433/" target="_blank" rel="noopener">阿尔法围棋</a>，记录从 DeepMind 开发 AlphaGo 到战胜李世石的全过程。有一个疑问第四局之后，他们有没有增加使用的 GPU 和 CPU？</li><li><a href="https://www.zhihu.com/question/32218407" target="_blank" rel="noopener">在你做推荐系统的过程中都遇到过什么坑？ - 知乎</a> [[CTR]]<ul><li>没有明确的指标：CTR，staytime，read/unread</li><li>精准推荐以及兴趣探索</li><li>线下auc涨，线上 ctr 跌</li></ul></li><li><a href="https://zhuanlan.zhihu.com/p/125507748" target="_blank" rel="noopener">从谷歌到阿里，谈谈工业界推荐系统多目标预估的两种范式 - 知乎</a>[[机器学习实践]][[多任务学习]]<ul><li>范式一：[[MMOE]] 替换 hard parameter sharing<ul><li>[[Recommending what video to watch next: a multitask ranking system]]</li></ul></li><li>范式二：任务序列依赖关系建模<ul><li>[[ESMM]]</li></ul></li><li>模型负采样，存在 CTR 漂移问题 U5Jvma3de</li></ul></li></ul><h2 id="放弃的事情emacs">放弃的事情：Emacs</h2><p>写作软件中积累一些文章的草稿，不过由于我的兴趣变化太快，很多文章还没有完成就已经被我放弃。借这个机会，展示一些有意思的东西。</p><p>程序员圈子中编辑器战争一直是一个绕不过去的话题。自己日常的工作中会使用多种编辑器：</p><ul><li>Sublime Text 简单处理文本</li><li>IDEA 处理 Scala Spark 相关的代码</li><li>PyCharm 连接 GPU 服务器处理 python 相关的代码</li><li>VS Code 本机上写 python、shell、cpp、sql 等脚本</li><li>Vim 服务器上修改文件</li></ul><p>不论选择什么编辑器，都推荐大家去看陈斌的<a href="https://github.com/redguardtoo/mastering-emacs-in-one-year-guide/blob/master/guide-zh.org" target="_blank" rel="noopener">一年成为Emacs高手 (像神一样使用编辑器)</a>。</p><p>去年底的时候，由于想尝试 <code>org-mode</code> 做任务管理（下个月再分享相关的内容），开始尝试使用 Emacs。Emacs 最大的有点是基于 Elisp 开发，软件中的每一个功能都对应一个函数，一个快捷键对应一个按键和函数的 map。修改功能和配置非常的方法。比如有人完全将 Vim 在文本操作上的功能迁移过来做成 evil 这个插件（号称所有和 Vim 中表现不同的情况都是 bug）。</p><p>Emacs 需要大量时间调教才能用起来舒心，对于初学者推荐去网上找一些成熟的配置直接使用。目前比较流行的有 Spacemacs 和 Doom emacs，这些配置维护以及使用的人很多，方便解决你遇到的各种问题。另外那些某些大佬个人分享的配置，如果你和大佬的技术栈不同，没有必要强行 clone。把它当成是一个学习素材，更好的理解 Emacs 背后的哲学。再这些基础上，成为高效的程序员的第一步，就是打造属于你自己的专门的配置文件。</p><p>使用好的编辑器是为了更快的工作。那如何更快的工作？</p><ol type="1"><li>在加快敲击键盘的速度</li><li>减少敲击键盘的次数</li><li>减少鼠标和键盘之间的切换</li></ol><p>关于 2，我在之前的文章中提到过一点，改变中文的输入方式（从全拼切换到小鹤双拼）。另外一点就是多使用快捷功能，比如 vim 里面的行号跳转。大部分软件的快捷键都是开发商配置好的，不过每一人主要使用的功能其实是完全不一样的。Emacs 中所有的快捷键可以查到定义的文件，从而进行修改。想象一种情况，为了减少我们按快捷键的次数以及难度。我们统计一段时间内使用 Emacs 各个功能的次数（插件 keyfreq），然后重新定义对应的快捷键。</p><p>由于我自己之前主要使用的是 vim，所以也给 vim 用户一个相对于合理的替换过程：</p><ol type="1"><li>当成普通的 vim 使用</li><li>逐步接触 org-mode 相关的功能</li><li>使用 emacs 其他的特性</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;不知不觉又到更新每月分享的时间。&lt;/p&gt;
&lt;p&gt;想写一下我为什么做这件事情？分享自己平时看到有意思的内容，现实世界认识的人，很少对我关注的内容感兴趣，所幸能借助博客超越时间和空间限制的分享。&lt;/p&gt;
&lt;p&gt;另外一点，我希望自己能将这个系列当成一个产品去迭代，每一期都有形式和
      
    
    </summary>
    
      <category term="Never-Reading" scheme="https://xiang578.com/categories/Never-Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>每月分享 202003</title>
    <link href="https://xiang578.com/post/monthly-issue-202003.html"/>
    <id>https://xiang578.com/post/monthly-issue-202003.html</id>
    <published>2020-03-28T08:55:11.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<h2 id="读书">读书</h2><ul><li><a href="https://book.douban.com/subject/34672176/" target="_blank" rel="noopener">呼吸 (豆瓣)</a>：这是一本由 Byte.Coffee 主播 MilkShake 🐑 推荐的一本科幻小说集（前几天看其他东西的时候学会科幻小说的英文 sci-fi）。之前看到过，小说的价值在于作者用一个故事告诉你一个道理。最喜欢的是《商人和炼金术士之门》这篇：在传统的穿越小说无法改变未来和过去的基础上，论证穿越能更深刻理解生活。书中其他探讨的几个问题也很有价值，值得一读。</li></ul><h2 id="我看">我看</h2><ul><li><a href="https://space.bilibili.com/390461123/" target="_blank" rel="noopener">徐大sao的个人空间 - 哔哩哔哩 ( ゜- ゜)つロ 乾杯~ Bilibili</a>：最近挑着看完大 sao 做饭视频，被他展示的生活激情所吸引。</li></ul><h2 id="文章">文章</h2><ul><li><a href="https://writings.stephenwolfram.com/2019/02/seeking-the-productive-life-some-details-of-my-personal-infrastructure" target="_blank" rel="noopener">Seeking the Productive Life: Some Details of My Personal Infrastructure—Stephen Wolfram Writings</a>：这篇超长的文章是 Stephen Wolfram (Mathematica CEO) 介绍自己几十年在家办公的经验，包括如何搭建一个适合自己的工作环境、如何管理文件和个人数据等等。不过这次疫情期间在家办公，大部分同事反应最大的问题是沟通效率降低。另外想想，或许是我们在这方面的思考不够。总之，互联网行业居家办公才是光明的未来。</li><li><a href="https://fs.blog/knowledge-project/naval-ravikant/" target="_blank" rel="noopener">Naval Ravikant: The Angel Philosopher</a>：AngelList 的 CEO Naval Ravikant 的播客访谈，Naval 的公司投资 Uber Twitter 等 100 多家科技公司。主要介绍 Naval 的一些哲学，文字版在 <a href="https://fs.blog/wp-content/uploads/2017/02/Naval-Ravikant-TKP.pdf" target="_blank" rel="noopener">Naval-Ravikant-TKP.pdf</a><ul><li>利用 Kindle 阅读，遇到喜欢的书，购买实体书收藏。对比书的价格，从书中学到可以改变自己人生的内容更重要。</li><li>发现一个新的博客后，会在 Archived 页面挑选几篇仔细阅读。读书时也可以使用这个技巧。</li></ul></li><li><a href="https://superorganizers.substack.com/p/how-to-build-a-learning-machine" target="_blank" rel="noopener">How to Make Yourself Into a Learning Machine - Superorganizers</a>：一位高中毕业后离开丹麦来到加拿大创业公司工作人的自我学习之路。<ul><li>自我定位 T 型人才。</li><li>兴趣面广，主题阅读。</li><li>高亮关键内容，使用 anki 记忆，相关的想法用 zettelkasten 记录。</li><li>zettelkasten 使用纯文本 + 脚本实现。（如果只记录英文 vim 和 emacs 真的是很强大的软件）</li><li>利用谷歌检索的数量判断单词的重要性</li></ul></li><li><a href="https://fortelabs.co/blog/para/" target="_blank" rel="noopener">The PARA Method: A Universal System for Organizing Digital Information - Forte Labs</a>：介绍 PARA 这种数字信息整理方法。</li><li><a href="https://tim.blog/2020/01/08/reading-recommendations/" target="_blank" rel="noopener">The Best Books and Articles I Read in 2019 – The Blog of Author Tim Ferriss</a>：一篇 2019 年阅读总结文章。对作者介绍的阅读流程比较感兴趣：<ul><li>Evernote 搭配 web clipper 收集文章。 利用 <code>***</code> 以及高亮在文章中做笔记，方便之后进行快速回顾。</li><li>阅读 Kindle 格式的电子书，定期从亚马逊官网导出高亮笔记（国内不支持）。</li><li>利用 Readwise 回顾之前提到的高亮。</li><li>阅读实体书时，写简单的索引卡片，然后将卡片拍照导入 Evernote 中。</li></ul></li></ul><h2 id="机器学习">机器学习</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/111842425" target="_blank" rel="noopener">为什么有些深度学习网络要加入Product层？ - 知乎</a>：解释为什么 MLP 只包含特征累加而有学习特征交叉的能力，后面展开讲了一些提高模型特征交叉能力的方法。</li><li><a href="https://blog.csdn.net/u011508640/article/details/72815981" target="_blank" rel="noopener">详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解_网络_nebulaf91的博客-CSDN博客</a>：看过讲 MLE 和 MAP 比较清晰的文章。刚看开头的时候，想到自己大学上过《概率论和统计》居然没有考虑过概率和统计有什么区别……</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;读书&quot;&gt;读书&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/34672176/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;呼吸 (豆瓣)&lt;/a&gt;：这是一本由 Byte.Coffe
      
    
    </summary>
    
      <category term="Never-Reading" scheme="https://xiang578.com/categories/Never-Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>每月分享 202002 「山川异域，风月同天」</title>
    <link href="https://xiang578.com/post/monthly-issue-202002.html"/>
    <id>https://xiang578.com/post/monthly-issue-202002.html</id>
    <published>2020-03-07T07:45:15.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>现在看来，又有多少人预测到这一次超级黑天鹅事件。</p><a id="more"></a><blockquote><p>这里记录过去一个月，我看到、想到值得分享的东西。</p></blockquote><h2 id="x03-how-instapaper-changed-my-kindle-life-for-the-better">0x03 <a href="https://www.guidingtech.com/29107/instapaper-kindle-merits/" target="_blank" rel="noopener">How Instapaper Changed My Kindle Life For the Better</a></h2><p>利用 Instapaper 定时将稍后读文章发送到 kindle 上。</p><h2 id="x02-这是我为武汉雷神山火神山医院设计的品牌形象标志logo---步行街主干道---虎扑社区">0x02 <a href="https://bbs.hupu.com/32137599.html" target="_blank" rel="noopener">这是我为武汉雷神山、火神山医院设计的品牌形象标志logo - 步行街主干道 - 虎扑社区</a></h2><p><img src="https://media.xiang578.com/15807855350677.jpg"></p><h2 id="x01-山川异域风月同天">0x01 <a href="https://weibo.com/1218287234/Is1lGejd6?type=comment#_rnd1580519347991" target="_blank" rel="noopener">“山川异域，风月同天”</a></h2><blockquote><p><span class="citation" data-cites="扎宝">@扎宝</span>：日本汉语水平考试HSK事务所捐赠给武湖北的物资，20000个口罩和一批红外体温计。 标签上写着“山川异域，风月同天”，感动[泪][泪] 求一个英文译文！ p.s. 据记载鉴真事迹的历史典籍《东征传》记载：日本长屋亲王在赠送大唐的千件袈裟上绣“山川异域，风月同天，寄诸佛子，共结来缘”偈。鉴真大师被此偈打动，决心东渡弘法。</p></blockquote><blockquote><p><span class="citation" data-cites="文冤阁大学士">@文冤阁大学士</span>：We are created to share Nature and love. 扫了下原博评转，翻得都差我好几座唐招提寺。嘻嘻。</p></blockquote><p><img src="https://media.xiang578.com/15807341522123.jpg"></p><h2 id="x00-xgboost">0x00 <a href="https://arxiv.org/abs/1603.02754" target="_blank" rel="noopener">XGBoost</a></h2><p>春节在家，重新把这些经典的内容再拿出来多读几遍。网上写的那些总结感觉都不是很好，还是要回去看论文。说句实话，纸上谈兵这么久，居然没有跑过 xgb 的包……</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现在看来，又有多少人预测到这一次超级黑天鹅事件。&lt;/p&gt;
    
    </summary>
    
      <category term="Never-Reading" scheme="https://xiang578.com/categories/Never-Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>2019 起步</title>
    <link href="https://xiang578.com/post/2019.html"/>
    <id>https://xiang578.com/post/2019.html</id>
    <published>2020-02-04T13:44:09.000Z</published>
    <updated>2021-01-09T02:59:54.286Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>受 <a href="http://freemind.pluskid.org/" target="_blank" rel="noopener">Free Mind</a> 的影响按这种形式写年度总结</p></blockquote><p>年初的时候看到一句话：「 2019 是过去十年中最差的一年，也是未来十年中最好的一年」。和其他人一样，我害怕不确定性，不过生活除了鼓起勇气前进，还有什么其他选择。</p><h2 id="工作">工作</h2><p>完整在滴滴工作一年，自己没有太多变化，可是环境却变了很多。从年初内部会议上 Will 优化员工开始，很多同事陆续离开，从而我都快要成为团队元老……</p><p>做为食物链低端的算法工程师，工作中杂七杂八的事情干了很多。洗数据、跑模型、改工程代码、测试、上线、实验各个方面都干过。</p><p>说回来，算法还是自己的主要工具。今年用的最多的是 FM 和 GBDT，这些都是几年前的技术，但是架不住效果好，性能要求小。自己也写了一些相关的文章，可以供大家参考。</p><blockquote><p><a href="https://xiang578.com/post/fm.html">(FM) Factorization Machines | 算法花园</a></p><p><a href="https://xiang578.com/post/ftrl.html">(FTRL) Follow The Regularized Leader | 算法花园</a></p><p><a href="https://xiang578.com/post/gbdt.html">All About GBDT (1) | 算法花园</a></p><p><a href="https://xiang578.com/post/gbdt_lr.html">Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr) | 算法花园</a></p></blockquote><p>关于深度学习，在我入职前模型就基本迭代完成，今年主要探索个性化场景的解决以及模型性能优化。很遗憾，这两方面的工作目前还没有什么可以写成博客分享的。最后，自己没有参与到组内强化学习的项目中，不过还是通过李宏毅老师的相关课程了解初步的概念，争取 20 年内做一些相关的事情。</p><p>9月份开始，leader突然让我准备一些编程题目，开始去面试实习生。通过牛客网以及北邮人论坛大概收到简历60多份，我面试10多个候选人，最终通过的大概五六人，不过过来实习的也就 2 个。印证自己之前的想法，一家已经不是快速发展的公司，很难招到即懂机器学习又会做编程题的实习生。</p><p>另外想写的一点是是匿名交流。内部论坛之前有一个匿名区，后来由于一件比较有名的事情，匿名喷得太厉害，被某位海归高管以提高交流效率减少戾气所关闭（目前这位已经离职，有人开玩笑期待干掉他新公司的匿名论坛）。所幸脉脉还有职言（匿名）以及公司圈。在上面混了一年之后，越来越理解匿名交流的必要，说事。比如今年发生的延迟发年终奖，快手可以直接在内部匿名区引起宿华回复。我们的公司圈一堆人才自嗨。本质是国内环境下很难公开交流一些话题。</p><p>之前看过[一篇文章]中介绍 Google 的 TGIF：</p><blockquote><p>TGIF是Larry和Sergey在公司早期就创立的，一个全公司范围的周会。在这个周会上高管们会透露公司新项目的进展，也安排有答疑环节，员工可以询问两位创始人任何问题。TGIF毫无疑问是为了提高公司内部的透明度，但它在增强员工凝聚力的同时也对公司文化提出了挑战，最直接的就是保密问题。比如Chrome项目在公司内部的公开就是在一次TGIF上发生的，那时离Chrome的正式对外宣布早了一年多的时间。</p></blockquote><h2 id="阅读">阅读</h2><p>今年读过 <a href="https://book.douban.com/people/xiang578/collect?sort=time&amp;tags_sort=count&amp;filter=all&amp;tag=2019&amp;mode=grid" target="_blank" rel="noopener">33</a> 本书，阅读量和前几年基本持平。年底发现自己的一个坏习惯：很多书读了一半就放在那里，导致开的坑很多。应对方法也很简单：一段时间内只读一本书。而且为了提高阅读的质量，将自己读完一本书的定义从读到最后一页改成完成对这本书内容的整理。</p><p>阅读的主要工具是 kindle 和微信阅读（iPhone）。kindle 是这么多年一直使用的阅读介质，从前几年的找破解图书到现在的完全中亚购买（以目前看书的频率还不至于承受不住），长时间看电子水墨屏能减轻一些疲劳。微信阅读的特点是白领无限卡后就能全场免费读，实在是太香了。理想状态下用这两个工具读不类型的材料，微信阅读读小说以及人物传记，kindle 看需要大量抄记的书。对于需要反复阅读的内容，实体书则是最佳的选择。</p><p>分享读完觉得不错的几本书：</p><ol type="1"><li>经济学通识/薛兆丰经济学讲义：薛兆丰目前看起来风评不是很好，这两本也不是什么严肃的经济学读物。前一本书是作者专栏文章的合集，后一本是得到专栏的文字版，两本书大量的内容是重复的。书中通过现实中的例子来讲解背后的经济学原理，很适合看完之后做为饭后谈资。反正我运用书中的一些原理，给同事分析好久公司的停车场应该怎么分配车位。</li><li>银河帝国：这一套书有很多本，只看完前三本。概括起来，这本小说是以太空为背景讲政治故事，以谢顿的预言为主线，讲述基地对抗各种危机的挑战。另外书中提到看起来有多少分像统计学的心理史学，谢顿一直用这种方法预测未来，而且信徒们一直强调，预测结果不会因为个人而改变。第三本书，围绕寻找第二基地展开，把所有读者能猜的地方都写了出来，选择了一种情理之中，意料之外的结局。</li><li>人类简史/未来简史：尤瓦尔·赫拉利是前几年很火的一个历史学家。人类简史主要是按他的框架回顾从原始人类到现代人类的文明发展历程。对于我这种没有系统接受过历史学教育的人，完全是一种震撼。未来简史讨论的是人类未来的发展方向，成神。</li><li>房思琪的初戀樂園：讲述一个小女孩被文明所不齿的方式杀死的故事，最让人痛心的这女孩就是作者本人……引用最近很流行的一句话：地狱空荡荡，魔鬼在人间。</li><li>基督山伯爵：看完《了不起的盖茨比》后，老板强力推荐的爽文小说。快意恩仇，永远不要丧失对生活的期望。</li><li>临高启明：工科党神书，死于历史空无主义，最后放上来缅怀一下。</li></ol><p>2020 年开始使用 Notion 记录读书过程，点击 <a href="https://www.notion.so/ryanx/4666b7440155430880b9c9787adde5ab?v=39111f7ebd5e4be6a28d7ef712c4aebb" target="_blank" rel="noopener">看书也就图一乐</a> 查看。</p><h2 id="观影">观影</h2><p>和去年一样，看电影比起看书来更加容易，豆瓣上轻松标记 <a href="https://movie.douban.com/people/xiang578/collect?sort=time&amp;tags_sort=count&amp;filter=all&amp;tag=2018&amp;mode=grid" target="_blank" rel="noopener">60</a> 部。想想原因，打开一个视频放在那里，不用怎么理它就能结束。按类别推荐一下自己觉得不错的影视：</p><ol type="1"><li>小丑/蝙蝠侠三部曲：去年在观影中大力推荐漫威宇宙，今年看完 DC 这 4 部电影，刷新对超级英雄片的认知。蝙蝠侠：黑暗骑士在是在超级英雄的框架下对人性进行探讨。小丑展示出社会如何逼一个人成为恶魔。</li><li>黑客帝国三部曲：经典的电影，赛博朋克风格。之前的神话描述神创造了世界，在这部片子里面，这个神就变成了机器人。多少算是人工智能行业的从业者，强人工智能离我们看起来还是很远。</li><li>人生七年9：这应该是拍摄时间最慢长的纪录片，也给我们机会在几十个小时时间里面见证这些主人公 60 多年岁月。很大一个感受，除了 Nick 之外，其他人不过是重复父辈的道路，阶级跃迁又是谈何容易。</li><li>哪吒之魔童降世：即大鱼海棠之后，第二部在电影院看的动画电影。之前想过一个问题：为什么一些小说要隔一段时间就翻拍一次？简单的认为要赋予时代主题。这部片子中最喜欢的一个设定：龙族也是妖怪，镇守龙宫，其实也是镇住自己。</li><li>邪不压正：电影看到一半的时候，我就觉得自己看不懂。说回来，看这部电影有一种酣畅淋漓的感觉，节奏很快，比《一步之遥》和《让子弹飞》更快。半夜在知乎上看了很多回答之后渐渐地懂得其中的情节，蓝先生的爱国情怀，李的复仇梦想。在历史的框架下演绎，始终无法逃离历史的结果，日本人还会按照发展进入北京城。一句“异父异母的亲兄弟”就值得一看。</li></ol><h2 id="游戏">游戏</h2><p>今年新增的一个板块，自从购入 Switch 之后，开始重新接触一些游戏。</p><ol type="1"><li>隐形守护者：抗战背景下面，一个特工面对选择的游戏。所有的场景都是真实拍摄出来的，比绝大部分国内的抗战剧精美。游戏有很多个结局，当然只有符合社会主义核心价值观的才算善终。最印象深刻是第二号突然的一句：什么都是马尔可夫链。</li><li>有氧拳击/健身环大冒险：NS 上的铁人三项之二，充分发挥体感的优势，晚上下班之后健身用。不过从目前的使用频率来看，大概率和买健身卡一个性质。</li><li>塞尔达传说：旷野之息：决定买 Switch 很大程度上源于少数派中一篇关于这个游戏的介绍，大意是没有传统的等级增长，只有你真正掌握一个技巧，林克才能使用出来。这款游戏的给我带来看似无限大的空间，但也有一点遗憾，有时候遇到下雨不好攀岩时，我想让林克坐下等雨停，然后发现没有坐下的选项……</li><li>超级马力欧创作家2：大学的时候，经常看超级小桀玩这个游戏。对于我这种连普通的马里奥都要靠无敌才能通关的来说，大部分自制的地图还是有点难的。说回来，买这个游戏就是买一个青春。只可惜物是人非。</li><li>暗黑破坏神3：永恒之战版 ：Switch 上的冷饭，自己瞎玩了很久，看完所有的剧情。最终在咸鱼上买了很多强力的装备后，速通 150 层大秘境后索然无味。所以玩游戏还是不要作弊。</li></ol><h2 id="未来">未来</h2><p>世界变化太快，未来可期。</p><p>于浙江临海</p><p><a href="https://xiang578.com/post/2017.html">2017 迷茫</a> &gt;&gt; <a href="https://xiang578.com/post/2018.html">2018 探索</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;受 &lt;a href=&quot;http://freemind.pluskid.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Free Mind&lt;/a&gt; 的影响按这种形式写年度总结&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;年初的
      
    
    </summary>
    
    
      <category term="life" scheme="https://xiang578.com/tags/life/"/>
    
      <category term="book" scheme="https://xiang578.com/tags/book/"/>
    
      <category term="game" scheme="https://xiang578.com/tags/game/"/>
    
      <category term="movie" scheme="https://xiang578.com/tags/movie/"/>
    
  </entry>
  
  <entry>
    <title>All About GBDT (1)</title>
    <link href="https://xiang578.com/post/gbdt.html"/>
    <id>https://xiang578.com/post/gbdt.html</id>
    <published>2020-01-26T06:15:43.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>GBDT(Gradient Boosting Decision Tree) 从名字上理解包含三个部分：提升、梯度和树。它最早由 Freidman 在 <code>greedy function approximation ：a gradient boosting machine</code> 中提出。很多公司线上模型是基于 GBDT+FM 开发的，我们 Leader 甚至认为 GBDT 是传统的机器学习集大成者。断断续续使用 GBDT 一年多后，大胆写一篇有关的文章和大家分享。</p><h2 id="朴素的想法">朴素的想法</h2><p>假设有一个游戏：给定数据集 <span class="math inline">\({(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}\)</span>，寻找一个模型<span class="math inline">\({\hat y=F(x_i)}\)</span>，使得平方损失函数 <span class="math inline">\({\sum \frac{1}{2}(\hat y_i - y_i)^2}\)</span> 最小。</p><p>如果你的朋友提供一个可以使用但是不完美的模型，比如 <span class="math display">\[F(x_1)=0.8,y_1=0.9\]</span> <span class="math display">\[F(x_2)=1.4,y_2=1.3\]</span> 在如何不修改这个模型的参数情况下，提高模型效果？</p><p>一个简单的思路是：重新训练一个模型实现 <span class="math display">\[F(x_1)+h(x_1)=y_1\]</span> <span class="math display">\[F(x_2)+h(x_2)=y_2\]</span> <span class="math display">\[...\]</span> <span class="math display">\[F(x_n)+h(x_n)=y_n\]</span></p><p>换一个角度是用模型学习数据 <span class="math inline">\({(x_1,y_1-F(x_1)),(x_2,y_2-F(x_2)),...,(x_n,y_n-F(x_n))}\)</span>。得到新的模型 <span class="math inline">\({\hat y=F(x_i)+h(x_i)}\)</span>。</p><p>其中 <span class="math inline">\({y_i-F(x_i)}\)</span> 的部分被我们称之为残差，即之前的模型没有学习到的部分。重新训练模型 <span class="math inline">\({h(x)}\)</span>正是学习残差。如果多次执行上面的步骤，可以将流程描述成：</p><p><span class="math display">\[{F_0(x)}\]</span> <span class="math display">\[{F_1(x)=F_0(x)+h_1(x)}\]</span> <span class="math display">\[{F_2(x)=F_1(x)+h_2(x)}\]</span> <span class="math display">\[{...}\]</span> <span class="math display">\[{F_t(x)=F_t-1(x)+h_t(x)}\]</span></p><p>即 <span class="math inline">\({F(x;w)=\sum^T_{t=1}h_t(x;w)}\)</span>，这也就是 GBDT 。</p><h2 id="如何理解-gradient-boosting-decision-tree">如何理解 Gradient Boosting Decision Tree ?</h2><p>Gradient Boosting Decision Tree 简称 GBDT，最早由 Friedman 在论文《Greedy function approximation: a gradient boosting machine》中提出。简单从题目中理解包含三个部分内容：Gradient Descent、Boosting、Decision Tree。</p><p>Decision Tree 即决策树，利用超平面对特征空间划分来预测和分类，根据处理的任务不同分成两种：分类树和回归树。在 GBDT 算法中，用到的是 CART 即分类回归树。用数学语言来描述为 <span class="math inline">\({F=\{f(x)=w_{q(x)}\}}\)</span>，完成样本 <span class="math inline">\({x}\)</span> 到决策树叶子节点 <span class="math inline">\({q(x)}\)</span> 的映射，并将该叶子节点的权重 <span class="math inline">\({w_{q(x)}}\)</span> 赋给样本。CART 中每次通过计算 gain 值贪心来进行二分裂。</p><p>Boosting 是一种常用的集成学习方法（另外一种是 Bagging）。利用弱学习算法，反复学习，得到一系列弱分类器（留一个问题，为什么不用线性回归做为弱分类器）。然后组合这些弱分类器，构成一个强分类器。上面提到的模型 <span class="math inline">\({F(x;w)=\sum^T_{t=1}h_t(x)}\)</span> 即是一种 boosting 思路，依次训练多个 CART 树 <span class="math inline">\({h_i}\)</span>，并通过累加这些树得到一个强分类器 <span class="math inline">\({F(x;w)}\)</span>。</p><h2 id="为什么-gbdt-可行">为什么 GBDT 可行？</h2><p>在 2 中我提到 GBDT 包括三个部分并且讲述了 Boosting 和 Decison Tree。唯独没有提到 Gradient Descent，GBDT 的理论依据却恰恰和它相关。</p><p>回忆一下，Gradient Descent 是一种常用的最小化损失函数 <span class="math inline">\({L(\theta)}\)</span> 的迭代方法。</p><ul><li>给定初始值 <span class="math inline">\({\theta_0}\)</span></li><li>迭代公式：<span class="math inline">\({\theta ^t = \theta ^{t-1} + \Delta \theta}\)</span></li><li>将 <span class="math inline">\({L(\theta ^t)}\)</span> 在 <span class="math inline">\({\theta ^{t-1}}\)</span> 处进行一阶泰勒展开：<span class="math inline">\({L(\theta ^t)=L(\theta ^{t-1} + \Delta \theta) \approx L(\theta ^{t-1}) + L^\prime(\theta ^{t-1})\Delta \theta}\)</span></li><li>要使 <span class="math inline">\({L(\theta ^t) &lt; L(\theta ^{t-1}) }\)</span>，取 <span class="math inline">\({\Delta \theta = -\alpha L^\prime(\theta ^{t-1})}\)</span></li><li>其中 <span class="math inline">\({\alpha}\)</span> 是步长，可以通过 line search 确定，但一般直接赋一个很小的数。</li></ul><p>在 1 中提到的问题中，损失函数是 MSE <span class="math inline">\({L(y, F(x))=\frac{1}{2}(y_i - f(x_i))^2}\)</span>。</p><p>我们的任务是通过调整 <span class="math inline">\({F(x_1), F(x_2), ..., F(x_n)}\)</span> 最小化 <span class="math inline">\({J=\sum_i L(y_i, F(x_i))}\)</span>。</p><p>如果将 <span class="math inline">\({F(x_i)}\)</span> 当成是参数，并对损失函数求导得到 <span class="math inline">\({ \frac{\partial J}{\partial F(x_i)} = \frac{\partial \sum_i L(y_i, F(x_i))}{\partial F(x_i)} = \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} = F(x_i)-y_i}\)</span>。</p><p>可以发现，在 1 中提到的模型 <span class="math inline">\({h(x)}\)</span> 学习的残差 <span class="math inline">\({y_i-F(x_i)}\)</span>正好等于负梯度，即 <span class="math inline">\({y_i-F(x_i)=-\frac{\partial J}{\partial F(x_i)}}\)</span>。</p><p>所以，参数的梯度下降和函数的梯度下降原理上是一致的：</p><ul><li><span class="math inline">\({F_{t+1}(x_i)=F_t(x_i)+h(x_i)=F(x_i)+y_i-F(x_i)=F_t(x_i)-1\frac{\partial J}{\partial F(x_i)}}\)</span></li><li><span class="math inline">\({\theta ^t = \theta ^{t-1} + \alpha L^\prime(\theta ^{t-1})}\)</span></li></ul><h2 id="gbdt-算法流程">GBDT 算法流程</h2><p>模型 F 定义为加法模型：</p><p><span class="math display">\[{F(x;w)=\sum^{M}_{m=1} \alpha_m h_m(x;w_m) = \sum^{M}_{m=1}f_t(x;w_t)}\]</span> 其中，x 为输入样本，h 为分类回归树，w 是分类回归树的参数，<span class="math inline">\({\alpha}\)</span> 是每棵树的权重。</p><p>通过最小化损失函数求解最优模型：<span class="math inline">\({F^* = argmin_F \sum^N_{i=1}L(y_i, F(x_i))}\)</span></p><p>输入: <span class="math inline">\({(x_i,y_i),T,L}\)</span></p><ol type="1"><li>初始化：<span class="math inline">\({f_0(x)}\)</span></li><li>对于 <span class="math inline">\({t = 1 to T}\)</span> ：<ol type="1"><li>计算负梯度（伪残差）： <span class="math inline">\({ \tilde{y_i} = -[\frac{\partial L(y_i, F(x_i))}{\partial F(x)}]_{F(x)=F_{m-1}(x)} ,i=1,2,...,N}\)</span></li><li>根据 <span class="math inline">\({\tilde{y_i}}\)</span> 学习第 m 棵树： <span class="math inline">\({w^*=argmin_{w} \sum_{i=1}^N(\tilde{y_i} - h_t(x_i;w))^2}\)</span></li><li>line searcher 找步长：<span class="math inline">\({\rho^* = argmin_\rho \sum_{i=1}^{N}L(y_i, F_{t-1}(x_i)+\rho h_t(x_i;w^*))}\)</span></li><li>令 <span class="math inline">\({f_t=\rho^*h_t(x;w*)}\)</span>，更新模型：<span class="math inline">\({F_t=F_{t-1}+f_t}\)</span></li></ol></li><li>输出 <span class="math inline">\({F_T}\)</span></li></ol><p>说明：</p><ol type="1"><li>初始化 <span class="math inline">\({f_0}\)</span> 方法<ol type="1"><li>求解损失函数最小</li><li>随机初始化</li><li>训练样本的充分统计量</li></ol></li><li>每一轮拟合负梯度，而不是拟合残差，是为方便之后扩展到其他损失函数。</li><li>最小化问题中，如果有解析解，直接带入。否则，利用泰勒二阶展开，Newton Step 得到近似解。</li></ol><p>这一篇就先到这里，之后还会分享 GBDT 常用损失函数推导以及 XGboost 相关内容。如果有任何想法，都可以在留言区和我交流。</p><h2 id="reference">Reference</h2><ol type="1"><li>李航, 《统计学习方法》8.4 提升树</li><li>Freidman，greedy function approximation ：a gradient boosting machine</li><li><a href="https://zhuanlan.zhihu.com/p/73381835" target="_blank" rel="noopener">【19年ML思考笔记】GBDT碎碎念（1）谈回归树的分裂准则 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/29765582" target="_blank" rel="noopener">机器学习-一文理解GBDT的原理-20171001 - 知乎</a></li><li><a href="https://louisscorpio.github.io/2017/12/13/GBDT%E5%85%A5%E9%97%A8%E8%AF%A6%E8%A7%A3/#" target="_blank" rel="noopener">GBDT入门详解 - Scorpio.Lu|Blog</a></li><li><a href="https://stackoverflow.com/questions/45409110/why-gradient-boosting-not-working-in-linear-regression" target="_blank" rel="noopener">python - Why Gradient Boosting not working in Linear Regression? - Stack Overflow</a></li><li><a href="https://blog.csdn.net/qq_24519677/article/details/82020863" target="_blank" rel="noopener">GBDT基本原理及算法描述 - Y学习使我快乐V的博客 - CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/30711812" target="_blank" rel="noopener">GBDT的那些事儿 - 知乎</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;GBDT(Gradient Boosting Decision Tree) 从名字上理解包含三个部分：提升、梯度和树。它最早由 Freidman 在 &lt;code&gt;greedy function approximation ：a gradient boosting machi
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ml" scheme="https://xiang578.com/tags/ml/"/>
    
      <category term="gdbt" scheme="https://xiang578.com/tags/gdbt/"/>
    
  </entry>
  
  <entry>
    <title>(FTRL) Follow The Regularized Leader</title>
    <link href="https://xiang578.com/post/ftrl.html"/>
    <id>https://xiang578.com/post/ftrl.html</id>
    <published>2020-01-03T13:26:06.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>FTRL 是 Google 提出的一种优化算法。常规的优化方法例如梯度下降、牛顿法等属于批处理算法，每次更新需要对 batch 内的训练样本重新训练一遍。在线学习场景下，我们希望模型迭代速度越快越好。例如用户发生一次点击行为后，模型就能快速进行调整。FTRL 在这个场景中能求解出稀疏化的模型。</p><h2 id="基础知识">基础知识</h2><ul><li>L1 正则比 L2 正则可以产生更稀疏的解。</li><li>次梯度：对于 L1 正则在 <span class="math inline">\(x=0\)</span> 处不可导的情况，使用次梯度下降来解决。次梯度对应一个集合 <span class="math inline">\(\{v: v(x-x_t) \le f(x)-f(x_t)\}\)</span>，集合中的任意一个元素都能被当成次梯度。以 L1 正则为例，非零处梯度是 1 或 -1，所以 <span class="math inline">\(x=0\)</span> 处的次梯度可以取 <span class="math inline">\([-1, 1]\)</span> 之内任意一个值。</li></ul><h2 id="ftl">FTL</h2><p>FTL(Follow The Leader) 算法：每次找到让之前所有损失函数之和最小的参数。</p><p><span class="math display">\[W=argmin_W \sum^t_{i=1}F_i(W)\]</span></p><p>FTRL 中的 R 是 Regularized，可以很容易猜出来在 FTL 的基础上加正则项。</p><p><span class="math display">\[W=argmin_W \sum^t_{i=1}F_i(W) + R(W)\]</span></p><h2 id="代理函数">代理函数</h2><p>FTRL 的损失函数直接很难求解，一般需要引入一个代理损失函数 <span class="math inline">\(h(w)\)</span>。代理损失函数常选择比较容易求解析解以及求出来的解和优化原函数得到的解差距不能太大。</p><p>我们通过两个解之间的距离 Regret 来衡量效果：</p><p><span class="math display">\[\begin{array}{c}{w_{t}=\operatorname{argmin}_{w} h_{t-1}(w)} \\ {\text {Regret}_{t}=\sum_{t=1}^{T} f_{t}\left(w_{t}\right)-\sum_{t=1}^{T} f_{t}\left(w^{*}\right)}\end{array}\]</span></p><p>其中 <span class="math inline">\(w^{*}\)</span> 是直接优化 FTRL 算法得到的参数。当距离满足 <span class="math inline">\(\lim _{t \rightarrow \infty} \frac{\text {Regret}_{t}}{t}=0\)</span>，损失函数认为是有效的。其物理意义是，随着训练样本的增加，两个优化目标优化出来的参数效果越接近。</p><h2 id="推导过程">推导过程</h2><p>参数 <span class="math inline">\(w_{t+1}\)</span> 的迭代公式：</p><p><span class="math display">\[{w_{t+1}=argmin_w\{ g_{(1:t)}w + \frac{1}{2} \sum_{s=1}^t \sigma_s \lVert w - w_s \rVert ^2   + \lambda_1 \lVert W \rVert_1 + \frac{1}{2} \lambda_2 \lVert W \rVert^2 \}}\]</span></p><p>其中 <span class="math inline">\(g_{(1:t)}=\sum^{t}_{s=1}g_s\)</span>，<span class="math inline">\(g_s\)</span> 为 <span class="math inline">\(f(w_s)\)</span> 的次梯度。参数 <span class="math inline">\(\sum^t_{s=1}\sigma_s=\frac{1}{\eta _t}\)</span>，学习率 <span class="math inline">\(\eta _t = \frac{1}{\sqrt{t}}\)</span>，随着迭代轮数增加而减少。</p><p>展开迭代公式</p><p><span class="math display">\[{F(w)=  g_{(1:t)}w + \frac{1}{2} \sum_{s=1}^t \sigma_s \lVert w - w_s \rVert ^2   + \lambda_1 \lVert W \rVert_1 + \frac{1}{2} \lambda_2 \lVert W \rVert^2 }\]</span></p><p><span class="math display">\[{F(w)=  g_{(1:t)}w + \frac{1}{2} \sum_{s=1}^t \sigma_s ( w^Tw - 2w^Tw_s + w_s^Tw_s)   + \lambda_1 \lVert W \rVert_1 + \frac{1}{2} \lambda_2 \lVert W \rVert^2 }\]</span></p><p><span class="math display">\[{F(w)=  (g_{(1:t)} - \sum_{s=1}^t \sigma_s w_s)w + \frac{1}{2} (\sum_{s=1}^t \sigma_s + \lambda_2) w^Tw   + \lambda_1 \lVert W \rVert_1 + const }\]</span></p><p><span class="math display">\[{F(w)=  z_t^Tw + \frac{1}{2} (\frac{1}{\eta _t} + \lambda_2) w^Tw   + \lambda_1 \lVert W \rVert_1 + const }\]</span></p><p>其中 <span class="math inline">\({z_{t-1}=g^{(1:t-1)} - \sum_{s=1}^{t-1} \sigma_s w_s}\)</span>。</p><p>对 <span class="math inline">\(F(w)\)</span> 求偏导得到：</p><p><span class="math display">\[{z_t + (\frac{1}{\eta _t} + \lambda_2) w + \lambda_1 \partial \lvert W \rvert = 0}\]</span></p><p><span class="math inline">\(w\)</span> 和 <span class="math inline">\(z\)</span> 异号时，等式成立。</p><p>根据基础知识里面提到的对于 L1 正则利用偏导数代替无法求解的情况，得到：</p><p><span class="math display">\[\partial|W|=\left\{\begin{array}{ll}{0,} &amp; {\text { if }-1&lt;w&lt;1} \\ {1,} &amp; {\text { if } w&gt;1} \\ {-1,} &amp; {\text { if } w&lt;-1}\end{array}\right.\]</span></p><ol type="1"><li>当 <span class="math inline">\({ z_t &gt; \lambda_1}\)</span> 时，<span class="math inline">\({w_i &lt; 0}\)</span> , <span class="math inline">\({w_i = \frac{- z_t + \lambda_1 }{\frac{1}{\eta _t} + \lambda_2 }}\)</span></li><li>当 <span class="math inline">\({ z_t &lt; - \lambda_1}\)</span> 时，<span class="math inline">\({w_i &gt; 0}\)</span> , <span class="math inline">\({w_i = \frac{- z_t - \lambda_1 }{\frac{1}{\eta _t} + \lambda_2 }}\)</span></li><li>当 <span class="math inline">\({ \lvert z_t \rvert &lt; \lambda_1}\)</span> 时，当且仅当 <span class="math inline">\({w_i=0}\)</span> 成立</li></ol><p>因此可得：</p><p><span class="math display">\[w_{i}=\left\{\begin{array}{ll}{0,} &amp; {\text { if }\left|z_{i}\right| \leq \lambda_{1}} \\ {\frac{-\left(z_{i}-\text sgn(z_i) \lambda_{1}\right)}{\eta_{t}+\lambda_{2}},} &amp; {\text { if others }}\end{array}\right.\]</span></p><h2 id="ftrl-和-sgd-的关系">FTRL 和 SGD 的关系</h2><p>将 SGD 的迭代公式写成：<span class="math inline">\({W^{t+1}=W^t - \eta _tg_t}\)</span></p><p>FTRL 迭代公式为：<span class="math inline">\({W^{t+1}=argmin_w\{ G^{(1:t)}W + \lambda_1 \lVert W \rVert_1 +\lambda_2 \frac{1}{2} \lVert W \rVert \}}\)</span></p><p>代入 <span class="math inline">\({\sum^t_{s=1}\sigma _s= \frac{1}{\eta _t}}\)</span> 到上面的公式中，得到 <span class="math inline">\({W^{t+1}=argmin_w\{ \sum_t^{s=1}g_sW + \frac{1}{2} \sum^t_{s=1}\sigma _s\lVert W - W_s \rVert_2^2 \}}\)</span></p><p>求偏导得到 <span class="math inline">\({\frac{\partial f(w)}{\partial w} = \sum^t_{s=1}g_s + \sum^t_{s=1}\sigma _s( W - W_s )}\)</span></p><p>令偏导等于 0 ：<span class="math inline">\({\sum^t_{s=1}g_s + \sum^t_{s=1}\sigma _s( W^{t+1} - W_s ) = 0}\)</span></p><p>化简得到：<span class="math inline">\({(\sum^t_{s=1}\sigma _s) W^{t+1} = \sum^t_{s=1}\sigma _s W^{s} - \sum^t_{s=1}g_s}\)</span></p><p>代入 <span class="math inline">\(\sigma\)</span>：<span class="math inline">\({\frac{1}{\eta _t} W^{t+1} = \sum^t_{s=1}\sigma _s W^{s} - \sum^t_{s=1}g_s}\)</span></p><p>根据上一个公式得出上一轮的迭代公式：<span class="math inline">\({\frac{1}{\eta _{t-1}} W^{t} = \sum^{t-1}_{s=1}\sigma _s W^{s} - \sum^{t-1}_{s=1}g_s}\)</span></p><p>两式相减：<span class="math inline">\({\frac{1}{\eta _t} W^{t+1} - \frac{1}{\eta _{t-1}} W^{t} = (\frac{1}{\eta _t} - \frac{1}{\eta _{t-1}}) W_t - g_t}\)</span></p><p>最终化简得到和 SGD 迭代公式相同的公式：<span class="math inline">\({W_{t+1} = W_t - \eta_t g_t}\)</span></p><h2 id="ftrl-工程化伪代码">FTRL 工程化伪代码</h2><p>引用自论文 <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41159.pdf" target="_blank" rel="noopener">Ad Click Prediction: a View from the Trenches</a></p><p>下面的伪代码中学习率和前面公式推导时使用的一些不一样： <span class="math inline">\(\eta_{t_{i}}=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^{t} g_{s_{i}}^{2}}}\)</span>。Facebook 在 GBDT + LR 的论文中研究过不同的学习率影响，具体可以参看博文 <a href="https://xiang578.com//post/media/gbdt_lr.html#%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%89%E6%8B%A9">Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr) | 算法花园</a>。</p><figure><img src="https://media.xiang578.com/15780559628822.jpg" alt="FTRL"><figcaption>FTRL</figcaption></figure><h2 id="例fm-使用-ftrl-优化">例：FM 使用 FTRL 优化</h2><p>FM 是工业界常用的机器学习算法，在之前博文 <a href="https://xiang578.com/post/fm.html">(FM)Factorization Machines</a> 中有简单的介绍。内部的 FTRL+FM 代码没有开源，所以也不好分析。从 <a href="https://zhuanlan.zhihu.com/p/58508137" target="_blank" rel="noopener">FM+FTRL算法原理以及工程化实现 - 知乎</a> 中找了一张 FTRL+FM 的伪代码图片。</p><p><img src="https://media.xiang578.com/15780576261639.jpg"></p><h2 id="reference">Reference</h2><ul><li><a href="https://tech.meituan.com/2016/04/21/online-learning.html" target="_blank" rel="noopener">Online Learning算法理论与实践 - 美团技术团队</a></li><li><a href="https://zhuanlan.zhihu.com/p/32694097" target="_blank" rel="noopener">FTRL公式推导 - 知乎</a></li><li><a href="https://blog.csdn.net/fangqingan_java/article/details/51020653" target="_blank" rel="noopener">每周一文】Ad Click Prediction: a View from the Trenches(2013)_机器学习,CTR,online_fangqingan_java的专栏-CSDN博客</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;FTRL 是 Google 提出的一种优化算法。常规的优化方法例如梯度下降、牛顿法等属于批处理算法，每次更新需要对 batch 内的训练样本重新训练一遍。在线学习场景下，我们希望模型迭代速度越快越好。例如用户发生一次点击行为后，模型就能快速进行调整。FTRL 在这个场景中能
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="fm" scheme="https://xiang578.com/tags/fm/"/>
    
      <category term="google" scheme="https://xiang578.com/tags/google/"/>
    
      <category term="ml" scheme="https://xiang578.com/tags/ml/"/>
    
      <category term="ftrl" scheme="https://xiang578.com/tags/ftrl/"/>
    
  </entry>
  
  <entry>
    <title>每月分享 202001 Fine-Tune Your Days</title>
    <link href="https://xiang578.com/post/monthly-issue-202001.html"/>
    <id>https://xiang578.com/post/monthly-issue-202001.html</id>
    <published>2020-01-01T08:55:11.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。</p></blockquote><h2 id="x04-你见过哪些让你目瞪口呆脑洞大开的骗局---sme情报员的回答---知乎">0x04 <a href="https://www.zhihu.com/question/41182911/answer/966701311" target="_blank" rel="noopener">你见过哪些让你目瞪口呆、脑洞大开的骗局？ - SME情报员的回答 - 知乎</a></h2><p>推荐其中的吉普赛读心术，当成脑筋急转弯来看。</p><blockquote><p>首先任选一个两位数，在心里默默记住，然后用这个两位数再依次减去它的十位和个位，最后用得数查表，找到对应的怪符号。 比如67,相应的计算就是67-6-7=54， 现在，在表中找到你心中数字经过计算后所对应的符号。</p></blockquote><p><img src="https://media.xiang578.com/15792508672842.jpg"></p><p>最后的答案都会是</p><p><img src="https://media.xiang578.com/15792508879083.jpg"></p><h2 id="x03-deep-neural-networks-for-youtube-recommendations-paper-ml">0x03 Deep Neural Networks for YouTube Recommendations #paper #ml</h2><p>Youtube 几年前的论文，最近拿过来看一下。工业界的论文最大的价值是提到的一些 tick，比如这篇论文中分析到用户对新视频的偏好，引入 example age 代表视频的上传到预测时的时间。再比如，给用户推荐视频时，考虑用户看过这个视频相关频道次数以及这个视频在用户实现中出现的次数。所以，做算法实现需要深入理解自己所处的场景。</p><p>推荐知乎上一些关于这篇论文的解读：</p><blockquote><ul><li><a href="https://zhuanlan.zhihu.com/p/25343518" target="_blank" rel="noopener">Deep Neural Network for YouTube Recommendation论文精读 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/52169807" target="_blank" rel="noopener">重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/52504407" target="_blank" rel="noopener">YouTube深度学习推荐系统的十大工程问题 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/61827629" target="_blank" rel="noopener">揭开YouTube深度推荐系统模型Serving之谜 - 知乎</a></li></ul></blockquote><h2 id="x02-fine-tune-your-days-with-the-scientific-method">0x02 Fine-Tune Your Days with the Scientific Method</h2><p>这个小标题出自 <a href="https://book.douban.com/subject/30327043/" target="_blank" rel="noopener">Make Time</a>，翻译成中文是利用科学的方法每天微调你的习惯。</p><h2 id="x01-2020-阅读看板">0x01 2020 阅读看板</h2><p>参考部分网友 Notion 的用法，搭建一个自己的阅读看板 <a href="https://www.notion.so/ryanx/4666b7440155430880b9c9787adde5ab?v=39111f7ebd5e4be6a28d7ef712c4aebb" target="_blank" rel="noopener">看书也就图一乐</a>。目前挑选出来的书远远超过前两年的阅读量，加油一起读书。</p><p>Notion 这种一个数据库 + 可选的 View 很接近我心目中任务管理软件的极限。</p><figure><img src="https://media.xiang578.com/15782399749595.jpg" alt="reading board"><figcaption>reading board</figcaption></figure><h2 id="x00-大佬们的年度总结">0x00 大佬们的年度总结</h2><p>新的一年开始时，最期待翻看大佬们的年度总结，罗列一些我觉得有总结。</p><ul><li><a href="https://yiming.dev/blog/2019/12/31/growing-a-result-driven-mindset/" target="_blank" rel="noopener">Growing a Result-Driven Mindset - Yiming Chen</a>：英文总结，Yiming 的博客给我带来学习英语并且用之来表达的动力。</li><li><a href="https://wdxmzy.com/pastfuture/year2019/2019/12/31/" target="_blank" rel="noopener">2019 总结与 2020 计划 | 小土刀 2.0</a>：从不同角度回顾自己的 2019 年。</li><li><a href="http://freemind.pluskid.org/misc/2019-summary/" target="_blank" rel="noopener">2019 时光小偷</a>：这位博主每年总结的标题都是一首歌，也是几年前看他的总结才开始尝试写自己的总结。</li><li><a href="https://zhuanlan.zhihu.com/p/100357148" target="_blank" rel="noopener">致敬时间的价值：一品十年 - 知乎</a>：和这个主题没有太大的关系，看一下其他人十年的总结，也能很好的指导自己的生活。</li><li><a href="http://zhengruonan.com/2019/12/29/2019-12-29-farewell-2019/" target="_blank" rel="noopener">2019年：下个十年路口，Farewell | Crossairplane的博客</a>：读这篇文章的时候突然想到一点，之后再看年终总结时，留言一句「新年快乐」。</li><li><a href="http://www.ztleespace.com/2020/01/01/create-vs-consume/" target="_blank" rel="noopener">Create vs. Consume - ends 2019 then starts 2020 - Ziting Li</a>：真诚的思考。</li><li><a href="https://www.hi-pda.com/forum/viewthread.php?tid=819978&amp;extra=page%3D1" target="_blank" rel="noopener">我的千书阅读计划 - 意欲蔓延 - Hi!PDA Hi!PDA</a>：fatdragoncat 13 年在 Hi!PDA 上立下愿望，这么多年过去，不知道数量上有没有达到，但是读书的收获已经改变他的生活。难得可贵这篇帖子展示他的变化过程。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;x04-你见过哪些让你目瞪口呆脑洞大开的骗局---sme情报员的回答---知乎&quot;&gt;0x04 &lt;a hre
      
    
    </summary>
    
      <category term="Never-Reading" scheme="https://xiang578.com/categories/Never-Reading/"/>
    
    
  </entry>
  
  <entry>
    <title>李宏毅强化学习课程笔记 PG PPO Q-Learing</title>
    <link href="https://xiang578.com/post/reinforce-learnning-basic.html"/>
    <id>https://xiang578.com/post/reinforce-learnning-basic.html</id>
    <published>2019-12-26T13:14:47.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<h2 id="info">Info</h2><p>课件下载：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">Hung-yi Lee - Deep Reinforcement Learning</a></p><p>课程视频：<a href="https://www.youtube.com/watch?v=z95ZYgPgXOY&amp;list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_" target="_blank" rel="noopener">DRL Lecture 1: Policy Gradient (Review) - YouTube</a></p><ul><li>Change Log<ul><li>20191226: 整理 PPO 相关资料</li><li>20191227: 整理 Q-Learning 相关资料</li><li>20200906: 拖延半年多没有整理笔记，将剩下的内容整理到单独的笔记中。</li></ul></li></ul><p>我的笔记汇总： - <a href="https://xiang578.com/post/reinforce-learnning-basic.html">Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning</a> - <a href="https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html">Actor Critic</a> - <a href="https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html">Sparse Reward</a> - <a href="https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html">Imitation Learning</a></p><h2 id="rl-基础">RL 基础</h2><p>强化学习基本定义：</p><ul><li>Actor：可以感知环境中的状态，通过执行不同的动作得到反馈的奖励，在此基础上进行学习优化。</li><li>Environment：指除 Actor 之外的所有事务，受 Actor 动作影响而改变其状态，并给 Actor 对应的奖励。</li><li>on-policy 和 off-policy 的区别在于 Actor 和 Environment 交互的策略和它自身在学习的策略是否是同一个。</li></ul><p>一些符号：</p><ul><li>State s 是对环境的描述，其状态空间是 S。</li><li>Action a 是 Actore 的行为描述，其动作空间是 A。</li><li>Policy <span class="math inline">\(\pi(a|s)=P[A_t=a|S_t=s]\)</span> 代表在给定环境状态 s 下 动作 a 的分布。</li><li>Reward <span class="math inline">\({r(s,a,s^{\prime})}\)</span> 在状态 s 下执行动作 a 后，Env 给出的打分。</li></ul><h2 id="policy-gradient">Policy Gradient</h2><p>Policy Network 最后输出的是概率。</p><p>目标：调整 actor 中神经网络 policy <span class="math inline">\(\pi(\theta)\)</span>，得到 <span class="math inline">\(a=\pi(s, \theta)\)</span>，最大化 reward。</p><p>trajectory <span class="math inline">\(\tau\)</span> 由一系列的状态和动作组成，出现这种组合的概率是 <span class="math inline">\(p_{\theta}(\tau)\)</span> 。</p><p><span class="math display">\[\begin{array}{l}{p_{\theta}(\tau)} \\ {\quad=p\left(s_{1}\right) p_{\theta}\left(a_{1} | s_{1}\right) p\left(s_{2} | s_{1}, a_{1}\right) p_{\theta}\left(a_{2} | s_{2}\right) p\left(s_{3} | s_{2}, a_{2}\right) \cdots} \\ {\quad=p\left(s_{1}\right) \prod_{l=1}^{T} p_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)}\end{array}\]</span></p><p>reward ：根据 s 和 a 计算得分 r，求和得到 R。在围棋等部分任务中，无法获得中间的 r（下完完整的一盘棋后能得到输赢的结果）。</p><p>需要计算 R 的期望 <span class="math inline">\(\bar{R}_{\theta}\)</span>，形式和 GAN 类似。如果一个动作得到 reward 多，那么就增大这个动作出现的概率。最终达到 agent 所做 policy 的 reward 一直都比较高。</p><p><span class="math display">\[\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)\]</span></p><p>强化学习中，没有 label。需要从环境中采样得到 <span class="math inline">\(\tau\)</span> 和 R，根据下面的公式去优化 agent。相当于去求一个 likelihood。</p><p><span class="math inline">\(\nabla f(x) = f(x) \frac{\nabla f(x)}{f(x)}= f(x) \nabla \log f(x)\)</span> ，这一步中用到对 log 函数进行链式求导。</p><p><span class="math display">\[\nabla \bar{R}_{\theta}=\sum_{\tau} R(\tau) \nabla p_{\theta}(\tau)\]</span></p><p><span class="math display">\[\begin{array}{l}{=E_{\left.\tau \sim p_{\theta}(\tau)[R(\tau)] \log p_{\theta}(\tau)\right]} \approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(\tau^{n}\right)} \\ {=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)}\end{array}\]</span></p><p>参数更新方法：</p><ol type="1"><li>在环境中进行采样，得到一系列的轨迹和回报。</li><li>利用状态求梯度，更新模型。如果 R 为正，增大概率 <span class="math inline">\(p_{\theta}(a_t|s_t)\)</span>, 否则减少概率。</li><li>重复上面的流程。</li></ol><p><img src="https://media.xiang578.com/15726799935625.jpg"></p><h3 id="pg-的例子">PG 的例子</h3><p>训练 actor 的过程看成是分类任务：输入 state ，输出 action。</p><p>最下面公式分别是反向传播梯度计算和 PG 的反向梯度计算，PG 中要乘以整个轨迹的 R。</p><figure><img src="https://media.xiang578.com/15752505145081.jpg" alt="PG"><figcaption>PG</figcaption></figure><p>tip 1： add a baseline</p><p>强化学习的优化和样本质量有关，避免采样不充分。Reawrd 函数变成 R-b，代表回报小于 b 的都被我们当成负样本，这样模型能去学习得分更高的动作。b 一般可以使用 R 的均值。</p><p>tip 2: assign suitable credit</p><p>一场游戏中，不论动作好坏，总会乘上相同的权重 R，这种方法是不合理的，希望每个 action 的权重不同。</p><ol type="1"><li>引入一个 discount rate，对 t 之后的动作 r 进行降权重。</li><li>利用 Advantage Function 评价状态 s 下动作 a 的好坏 critic。</li></ol><figure><img src="https://media.xiang578.com/15752505547153.jpg" alt="Assign Suitable Credit"><figcaption>Assign Suitable Credit</figcaption></figure><h2 id="ppo-proximal-policy-optimization">PPO: Proximal Policy Optimization</h2><h3 id="importance-sampling">importance sampling</h3><p>假设需要估计期望 <span class="math inline">\(E_{x~p[f(x)]}\)</span>，x 符合 p 分布，将期望写成积分的形式。由于在 P 分布下面很难采样，把问题转化到已知 q 分布上，得到在 p 分布下计算期望公式。</p><p><img src="https://media.xiang578.com/15771920437580.jpg"></p><p>上面方法得到 p 和 q 期望接近，但是方差可能相差很大，且和 <span class="math inline">\(\frac{p(x)}{q(x)}\)</span> 有关。</p><p>原分布的方差： <span class="math display">\[\operatorname{Var}_{x-p}[f(x)]=E_{x-p}\left[f(x)^{2}\right]-\left(E_{x-q}[f(x)]\right)^{2}\]</span></p><p>新分布的方差： <span class="math display">\[\begin{array}{l}{\operatorname{Var}_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}} \\ {\operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2}} \\ {=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}}\end{array}\]</span></p><p>在 p 和 q 分布不一致时，且采样不充分时，可能会带来比较大的误差。</p><figure><img src="https://media.xiang578.com/15771931618906.jpg" alt="Issue of Importance Sampling"><figcaption>Issue of Importance Sampling</figcaption></figure><h3 id="从-on-policy-到-off-policy">从 On-policy 到 Off-policy</h3><p>on-policy 时，PG 每次参数更新完成后，actor 就改变了，不能使用之前的数据，必须和环境重新互动收集数据。引入 <span class="math inline">\(p_{\theta \prime}\)</span> 进行采样，就能将 PG 转为 off-ploicy。</p><p><img src="https://media.xiang578.com/15771932374489.jpg"></p><p>和之前相比，相当于引入重要性采样，所以也有前一节中提到的重要性采样不足问题。</p><p><span class="math display">\[J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]\]</span></p><h3 id="ppotrpo">PPO/TRPO</h3><p>为了克服采样的分布与原分布差距过大的不足，PPO 引入 KL 散度进行约束。KL 散度用来衡量两个分布的接近程度。</p><p><span class="math display">\[J_{P P O}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta K L\left(\theta, \theta^{\prime}\right)\]</span></p><p>TRPO(Trust Region Policy Optimization)，要求 <span class="math inline">\(K L\left(\theta, \theta^{\prime}\right)&lt;\delta\)</span>。</p><p><span class="math display">\[J_{T R P O}^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]\]</span></p><p>KL 散度可能比较难计算，在实际中常使用 PPO2。</p><ul><li>A&gt;0，代表当前策虑表现好。需要增大 <span class="math inline">\(\pi( \theta )\)</span>，通过 clip 增加一个上限，防止 <span class="math inline">\(\pi( \theta )\)</span> 和旧分布变化太大。</li><li>A&lt;0，代表当前策虑表现差，不限制新旧分布的差异程度，只需要大幅度改变 <span class="math inline">\(\pi( \theta )\)</span>。</li></ul><p>参考 <a href="https://zhuanlan.zhihu.com/p/43114711" target="_blank" rel="noopener">【点滴】策略梯度之PPO - 知乎</a></p><p><img src="https://media.xiang578.com/15772793086357.jpg"></p><h3 id="ppo-algorithm">PPO algorithm</h3><p>系数 <span class="math inline">\(\beta\)</span> 在迭代的过程中需要进行动态调整。引入 <span class="math inline">\(KL_{max} KL_{min}\)</span>，KL &gt; KLmax，说明 penalty 没有发挥作用，增大 <span class="math inline">\(\beta\)</span>。</p><p><img src="https://media.xiang578.com/15772793200430.jpg"></p><h2 id="q-learning">Q-Learning</h2><p>value-base 方法，利用 critic 网络评价 actor 。通过状态价值函数 <span class="math inline">\(V^{\pi}(s)\)</span> 来衡量预期的期望。V 和 pi、s 相关。</p><p><img src="https://media.xiang578.com/15773668251569.jpg"></p><ol type="1"><li>Monte-Carlo MC: 训练网络使预测的 <span class="math inline">\(V^{\pi}(s_a)\)</span> 和实际完整游戏 reward <span class="math inline">\(G_a\)</span> 接近。</li><li>Temporal-difference TD: 训练网络尽量满足 <span class="math inline">\(V^{\pi}(s_t)=V^{\pi}(s_{t+1}) + r_t\)</span> 等式，两个状态之间的收益差。</li></ol><p>MC: 根据策虑 <span class="math inline">\(\pi\)</span> 进行游戏得到最后的 <span class="math inline">\(G(a)\)</span>，最终存在方差大的问题。<span class="math inline">\(\operatorname{Var}[k X]=k^{2} \operatorname{Var}[X]\)</span></p><p>TD: r 的方差比较小，<span class="math inline">\(V^{\pi}(s_{t+1})\)</span> 在采样不充分的情况下，可能不准确。</p><p><img src="https://media.xiang578.com/15773670631327.jpg"></p><h3 id="another-critic">Another Critic</h3><p>State-action value function <span class="math inline">\(Q^{\pi}(s, a)\)</span>：预测在 pi 策略下，pair(s, a) 的值。相当于假设 state 情况下强制采取 action a。</p><p><img src="https://media.xiang578.com/15773671301402.jpg"></p><p>对于非分类的方法：</p><p><img src="https://media.xiang578.com/15773671400847.jpg"></p><p>Q-Learning</p><ol type="1"><li>初始 actor <span class="math inline">\(\pi\)</span> 与环境互动</li><li>学习该 actor 对应的 Q function</li><li>找一个比 <span class="math inline">\(\pi\)</span> 好的策虑：<span class="math inline">\(\pi \prime\)</span>，满足 <span class="math inline">\(V^{\pi \prime}(s,a) \ge V^{\pi}(s,a)\)</span>, <span class="math inline">\(\pi^{\prime}(s)=\arg \max _{a} Q^{\pi}(s, a)\)</span></li></ol><p>在给定 state 下，分别代入 action，取函数值最大的 a，作为后面对该 state 时采取的 action。</p><p>证明新的策虑存在： <img src="https://media.xiang578.com/15773672909262.jpg"></p><h3 id="target-network">Target NetWork</h3><p>左右两边的网络相同，如果同时训练比较困难。简单的想法是固定右边的网络进行训练，一定次数后再拷贝左边的网络。</p><p><img src="https://media.xiang578.com/15773673947112.jpg"></p><h3 id="exploration">Exploration</h3><p>Q function 导致 actor 每次都会选择具有更大值的 action，无法准确估计某一些动作，对于收集数据而言是一个弊端。</p><ul><li>Epsilon Greedy<ul><li>小概率进行损失采样</li></ul></li><li>Boltzmann Exploration<ul><li>利用 softmax 计算选取动作的概率，然后进行采样</li></ul></li></ul><p><img src="https://media.xiang578.com/15773674189861.jpg"></p><h3 id="replay-buffer">Replay buffer</h3><p>采样之后的 <span class="math inline">\((s_t, a_t, r_t, s_{t+1})\)</span> 保存在一个 buffer 里面（可能是不同策虑下采样得到的)，每次训练从 buffer 中 sample 一个 batch。</p><p>结果：训练方法变成 off-policy。减少 RL 重复采样，充分利用数据。</p><p><img src="https://media.xiang578.com/15773675111439.jpg"></p><h3 id="typical-q-learning-algorithm">Typical Q-Learning Algorithm</h3><p>Q-Learning 流程：</p><p><img src="https://media.xiang578.com/15773677168275.jpg"></p><h3 id="double-dqn-ddqn">Double DQN DDQN</h3><ul><li>Q value 容易高估：目标值 <span class="math inline">\(r_t + maxQ(s_{t+1}, a)\)</span> 倾向于选择被高估的 action，导致 target 很大。</li><li>选动作的 Q' 和计算 value 的 Q(target network) 不同。Q 中高估 a，Q' 可能会准确估计 V 值。Q' 中高估 a ，可能不会被 Q 选中。</li></ul><p><img src="https://media.xiang578.com/15773677986325.jpg"></p><h3 id="dueling-dqn">Dueling DQN</h3><p>改 network 架构。V(s) 代表 s 所具有的价值，不同的 action 共享。 A(s,a) advantage function 代表在 s 下执行 a 的价值。最后 <span class="math inline">\(Q(s, a) = A(s, a) + V(s)\)</span>。</p><p>为了让网络倾向于使用 V（能训练这个网络），得到 A 后，要对 A 做 normalize。</p><p><img src="https://media.xiang578.com/15773680103445.jpg"></p><h3 id="prioritized-reply">Prioritized Reply</h3><p>在训练过程中，对于经验 buffer 里面的样本，TD error 比较大的样本有更大的概率被采样，即难训练的数据增大被采样的概率。</p><p><img src="https://media.xiang578.com/15773681449261.jpg"></p><h3 id="multi-step">Multi-step</h3><p>综合 MC 和 TD 的优点，训练样本按一定步长 N 进行采样。MC 准确方差大，TD 方差小，估计不准。 <img src="https://media.xiang578.com/15773682226911.jpg"></p><h3 id="noisy-net">Noisy Net</h3><p><img src="https://media.xiang578.com/15773682895872.jpg"></p><ul><li>Noise on Action：在相同状态下，可能会采取不同的动作。</li><li>Noise on Parameters：开始时加入噪声。同一个 episode 内，参数不会改变。相同状态下，动作相同。 更好探索环境。</li></ul><h3 id="distributional-q-function">Distributional Q-function</h3><p>Q 是累积收益的期望，实际上在 s 采取 a 时，最终所有得到的 reward 为一个分布 reward distribution。部分时候分布不同，可能期望相同，所以用期望来代替 reward 会损失一些信息。</p><p>Distributional Q-function 直接输出分布，均值相同时，采取方差小的方案。这种方法不会产生高估 q 值的情况。</p><p><img src="https://media.xiang578.com/15773684681347.jpg"></p><h3 id="rainbow">Rainbow</h3><p>rainbow 是各种策略的混合体。</p><p><img src="https://media.xiang578.com/15773684767629.jpg"></p><p>DDQN 影响不大。</p><p><img src="https://media.xiang578.com/15773684832549.jpg"></p><h3 id="continuous-actions">Continuous Actions</h3><p>action 是一个连续的向量，Q-learning 不是一个很好的方法。</p><p><span class="math display">\[a=\arg \max _{a} Q(s, a)\]</span></p><ol type="1"><li>从 a 中采样出一批动作，看哪个行动 Q 值最大。</li><li>使用 gradient ascent 解决最优化问题。</li><li>设计一个网络来化简过程。<ol type="1"><li><span class="math inline">\(\sum\)</span> 和 <span class="math inline">\(\mu\)</span> 是高斯分布的方差和均值，保证矩阵一定是正定。</li><li>最小化下面的函数，需要最小化 <span class="math inline">\(a - \mu\)</span>。</li></ol></li></ol><p><img src="https://media.xiang578.com/15773684903236.jpg"></p><h2 id="reference">Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/41712025" target="_blank" rel="noopener">强化学习基础知识 - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;info&quot;&gt;Info&lt;/h2&gt;
&lt;p&gt;课件下载：&lt;a href=&quot;http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hung-yi Lee
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="algorithm" scheme="https://xiang578.com/tags/algorithm/"/>
    
      <category term="reinforcementlearning" scheme="https://xiang578.com/tags/reinforcementlearning/"/>
    
  </entry>
  
  <entry>
    <title>每月分享 201912</title>
    <link href="https://xiang578.com/post/monthly-issue-201912.html"/>
    <id>https://xiang578.com/post/monthly-issue-201912.html</id>
    <published>2019-12-16T08:55:11.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。</p></blockquote><h2 id="x02.-org-mode-workflow">0x02. <a href="https://blog.jethro.dev/posts/org_mode_workflow_preview/" target="_blank" rel="noopener">Org-mode Workflow</a></h2><p>国外一名 CS 学生的 org mode workflow 教程，包括 GTD 和 Zettelkasten 两个主要的部分，分别对应时间管理和知识管理，是一份很好的参考资料。</p><h2 id="x01.-hhkb-更新">0x01. HHKB 更新</h2><blockquote class="twitter-tweet"><p lang="ja" dir="ltr">改めてとなりますが今回新しく登場したHHKBは3機種になります。それぞれの特徴をまとめたものがこちらになります。<a href="https://twitter.com/hashtag/HHKB%E3%83%9F%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener">#HHKBミートアップ</a> <a href="https://t.co/GVVxNI6H72" target="_blank" rel="noopener">pic.twitter.com/GVVxNI6H72</a></p>— HHKB OFFICIAL (<span class="citation" data-cites="PFU_HHKB">@PFU_HHKB</span>) <a href="https://twitter.com/PFU_HHKB/status/1204345452658741248?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">December 10, 2019</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>HHKB 好久之后终于更新了！不过价格也变得更贵……还是很喜欢自己 18 年买的 HHKB BT 版。不过在使用 Emacs 之后，出现没有方向键的烦恼。之前通过映射 <code>Ctrl + HJKL</code> 替代方向键，然后和 Emacs 的一些快捷键冲突……等买一个新的机械键盘。</p><h2 id="x00.-my-gtd-workflow-2019-ver.---yiming-chen-gtd">0x00. <a href="https://yiming.dev/blog/2019/05/22/my-gtd-workflow-2019-ver/" target="_blank" rel="noopener">My GTD Workflow (2019 ver.) - Yiming Chen</a> #gtd</h2><p>很少看到国人用英文写的 GTD 相关文章，年初自己也想按 Workflow 这种形式写一篇，不过一直拖到现在都没有完成。</p><ul><li>对任务设置优先级：A B C</li><li>如何设置任务优先级，对目标进行分解<ul><li>每年一月份设定年度目标</li><li>每月一号根据年度目标设定月度目标</li><li>每周日根据月度目标设定每周目标</li><li>每天早上设定当天目标</li></ul></li><li>任务安排优先级和截止日期后，可以使用四象限法则。</li><li>回顾技巧<ul><li>追求 100% 完成，可以接受 70%。</li><li>一个任务多次延迟之后，考虑是否还是重要。</li><li>如果任务还是重要，对任务进行拆分。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;x02.-org-mode-workflow&quot;&gt;0x02. &lt;a href=&quot;https://blog.
      
    
    </summary>
    
      <category term="Never-Reading" scheme="https://xiang578.com/categories/Never-Reading/"/>
    
    
      <category term="monthly-issue" scheme="https://xiang578.com/tags/monthly-issue/"/>
    
  </entry>
  
  <entry>
    <title>2019 年软硬件指北</title>
    <link href="https://xiang578.com/post/2019-consumer-report.html"/>
    <id>https://xiang578.com/post/2019-consumer-report.html</id>
    <published>2019-12-15T13:29:38.000Z</published>
    <updated>2021-01-09T02:59:54.286Z</updated>
    
    <content type="html"><![CDATA[<p>呼吸不止，折腾不停。记录在过去的一年，自己选择的软件和硬件。去年写指南并不能指南，所以今年直接写成指北。</p><h2 id="硬件更新">硬件更新</h2><h3 id="iphone-xr-和-apple-watch-series-4">iPhone XR 和 Apple Watch Series 4</h3><p>iPhone XR 刚出来的时候，一直被吐槽是大边框。不过随着在电商网站上不断降价，越来越被当成是无边框手机……在忍受不了使用多年 iPhone 6 的卡顿，以及很难脱离 iOS 生态的现实。终于在苏宁上下单 <span class="math inline">\((Product)^{read}\)</span> 版的 XR。经过半年多的使用，这部手机实用但是不出彩。</p><figure><img src="https://media.xiang578.com/15764169455007.jpg" alt="Apple Watch 圆环图"><figcaption>Apple Watch 圆环图</figcaption></figure><p>购买 Apple Watch Series 4(AW) 理由很简单：在去公司健身房锻炼的时候，希望有一个可以记录运动数据的设备。事实 AW 自带的运动软件很不错，可以满足我的运动记录需求。但是例如 Keep 之类的第三方 App 适配不好。另外，AW 很好扩展 iPhone 和 Mac 的使用，比如可以解锁 Mac、查看 iPhone 上的信息等等。到头来，AW 还只是一块需要一天一充的电子表。</p><h3 id="nintendo-switch">Nintendo Switch</h3><p>Nintendo Switch(NS) 是任天堂在 2017 年推出的，集掌机和主机于一体的游戏机。买 NS 的理由也很简单，Mac 上游戏太少，我需要一个设备玩游戏。更深层次的来说，感觉自己的反应太慢，想通过玩游戏来锻炼快速决策能力。</p><p>简单统计一下，我在 NS 上花费的时间大概有 200 多小时，购入游戏也花费上千元……反应能力不知道有没有上去，但是享受到游戏的快乐。</p><figure><img src="https://media.xiang578.com/15764170682610.jpg" alt="任天堂就是世界的主宰"><figcaption>任天堂就是世界的主宰</figcaption></figure><p>Nintendo Switch 目前可以选的有 Nintendo Switch、Nintendo Switch 续航版、Nintendo Switch Lite。</p><h3 id="kindle-oasis-2">Kindle Oasis 2</h3><p>大概是 5 月末，通过公司内部闲置群出了使用 5 年多的 Kindle PaperWhite 2 后，从淘宝上购入美版 Kindle Oasis 2 。可惜的是，1 个月不到的时间，亚马逊推出 Kindle Oasis 3 ……</p><figure><img src="https://media.xiang578.com/kindle_2_oasis.jpg" alt="kindle oasis 2 和 kindle 2 对比图"><figcaption>kindle oasis 2 和 kindle 2 对比图</figcaption></figure><p>和 KPW2 相比，KO2 主要带来一下几个方面的提升：</p><ol type="1"><li>7寸屏幕，更高的分辨率。看的更多，看的更清晰，更加逼近纸书的感觉。</li><li>不对称设计，电池集中在一边，握持比较舒服。</li><li>金属机身。前几代 kindle 都是亚马逊祖传的类肤质塑料机身，很容易沾上油脂，这一代采用金属机身，看起来更加富有科技感。毕竟当年小米用上金属边框的时候，都敢去吹一块钢板的艺术之旅。</li><li>两个翻页实体按键，按起来比较有安全感。</li></ol><p>上面说这么多，kindle 主要功能还是看书。这几年，很多 kindle 电子书分享站都由于版权问题陆续关闭，优质的资源比较难下载。不过，去年自己办信用卡时，领了一年的 Kinle U 会员（今年又领到一年的会员），在中亚上借阅很多本小说。从体验上来说，KU 会员不能实现全场自由借，而且大部分书籍都只是滥竽充数。一对比微信读书会员就是十分实惠，多期待微信读书可以出电子书阅读器吧。</p><h2 id="软件实践">软件实践</h2><p>18 年开始，着手准备构建自己的数字化系统。19 年在前面的基础上，进行了很多迁移。</p><h3 id="信息管理">信息管理</h3><p>11 月份看到一句话：input 做的越多，知识管理越差。这个很好形容我之前的状态，在印象笔记中囤积待看的剪藏、OF 里面有很多想写的主题、MWeb 遗留大量没有写完的文章。</p><p>年中的时候，想把自己写的一些笔记更好的管理起来。最初想到的是搭建 wiki ，实现知识的网状化连接。不过市面上常用的一些个人 wiki 方案都不是很满意。最终选择 hexo 搭配一个 wiki 主题 <a href="https://github.com/zthxxx/hexo-theme-Wikitten" target="_blank" rel="noopener">Wikitten</a>。另外，后来了解到有一种基于纯文本的知识管理方案：zettelkasten。感兴趣的可以去看一下。</p><p>写日记是这么多年以来坚持的一件小事情。之前一直是在笔记本上写，后来慢慢的尝试通过印象笔记来写。2月份，订阅 Day One ，开始尝试迁移到它上面去。作为一个专业的软件，体验真的比之前的方式不知道好多少。Day One 上也有很多数据统计，多少可以拿来得瑟用。另外，自己干的一件事情就是把印象笔记中的日记慢慢转移到 Day one 。写在笔记本上的日记，也被我拍成一张又一张的照片，只不过这个迁移起来比较麻烦。</p><figure><img src="https://media.xiang578.com/15764175403738.jpg" alt="Day one 按年统计图"><figcaption>Day one 按年统计图</figcaption></figure><h3 id="任务管理">任务管理</h3><p>这个问题一直是一个大坑，花费很多时间在多个软件中试来试去。在现在这个时间点，自己开始选择混合使用 OmniFocus 和 Org mode。具体怎么搭配使用，等再坚持几个月再出来分享。不过说回来，任务管理的关键不在于软件，而在于执行。</p><h3 id="其他实践">其他实践</h3><p>下面这一些今年自己做的选择，都有一个共同的特点：从商业软件到开源项目。很多人选着使用的开源项目的出发点在于害怕商业公司无休止的使用个人隐私数据，而吸引我的主要是自由软件自由开放的精神。</p><h4 id="从-moneywiz-到-beancount">从 MoneyWiz 到 Beancount</h4><p>MoneyWiz 是在少数派上了解到记账软件，Setapp 中可以免费使用。和国内那些整天搞社区和卖理财的记账软件相比，只是纯粹的一个记账软件。Beancount 是无意中从<a href="https://www.byvoid.com/zhs/blog/beyond-the-void" target="_blank" rel="noopener">BYvoid</a>文章中了解的一款纯文本记账软件。最大的优点是扩展性强。在使用过程中，搭配一些简单的脚本，可以实现每月底花一个小时就能把这个月的开销记录明白。</p><figure><img src="https://media.xiang578.com/15764177497946.jpg" alt="Beancount fava"><figcaption>Beancount fava</figcaption></figure><h4 id="从-1password-到-keepass">从 1Password 到 KeePass</h4><p>之前看过一个结论：密码破解的难度主要在于长度而不是复杂度。所以借助密码软件辅助记忆密码是不二之选。1Password 是在去年感恩节活动中获得的长达一年的免费体验。快要到期前，没有选择转向订阅（今年感恩节活动依然是新用户 长度一年的免费使用），反而是选择开源的 KeePass。KeePass 在不同的平台上有多个客户端可以选择，目前我主要用的是 MacPass 和奇密。KeePass 中所有的密码数据都保存在一个文件中，跨平台使用只需要简单同步这个文件。</p><h4 id="从搜狗输入法到鼠须管">从搜狗输入法到鼠须管</h4><p>网上关于搜狗输入法的声讨一直不绝于耳，我也长时间忍受搜狗动不动给你跳出来的斗图功能提示。在花费一番力气，配置鼠须管后，彻底删除搜狗，详见 <a href="https://xiang578.com/post/rime.html">「Rime 鼠须管」小鹤双拼配置指南 | 算法花园</a>。另外 Mac 上自带的输入法的体验也没有那么差。</p><p>博客上和这个主题相关的文章：</p><ul><li><a href="https://xiang578.com/post/best-of-iphone-2019.html">Best of iPhone 2019 软件清单 | 算法花园</a></li><li><a href="https://xiang578.com/post/2018-consumer-report.html">2018 年消费指南 | 算法花园</a></li><li><a href="https://xiang578.com/post/iphone5s.html">iPhone软件清单 | 算法花园</a></li><li><a href="https://xiang578.com/post/mac-software.html">Mac软件清单 | 算法花园</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;呼吸不止，折腾不停。记录在过去的一年，自己选择的软件和硬件。去年写指南并不能指南，所以今年直接写成指北。&lt;/p&gt;
&lt;h2 id=&quot;硬件更新&quot;&gt;硬件更新&lt;/h2&gt;
&lt;h3 id=&quot;iphone-xr-和-apple-watch-series-4&quot;&gt;iPhone XR 和 Ap
      
    
    </summary>
    
      <category term="生活志" scheme="https://xiang578.com/categories/%E7%94%9F%E6%B4%BB%E5%BF%97/"/>
    
    
      <category term="life" scheme="https://xiang578.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>Standford CS231n 2017 课程部分总结</title>
    <link href="https://xiang578.com/post/cs231n-summary.html"/>
    <id>https://xiang578.com/post/cs231n-summary.html</id>
    <published>2019-12-04T09:58:39.000Z</published>
    <updated>2021-01-09T02:59:54.290Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>去年学习这门做的部分笔记，现在分享出来。 笔记格式有些问题，持续整理中。</p></blockquote><ul><li>大量内容参考 <a href="https://github.com/mbadry1/CS231n-2017-Summary/blob/master/README.md" target="_blank" rel="noopener">mbadry1/CS231n-2017-Summary</a></li></ul><h2 id="table-of-contents">Table of contents</h2><ul><li><a href="#standford-cs231n-2017-summary">Standford CS231n 2017 Summary</a><ul><li><a href="#table-of-contents">Table of contents</a></li><li><a href="#course-info">Course Info</a></li><li><a href="#01-introduction-to-cnn-for-visual-recognition">01. Introduction to CNN for visual recognition</a></li><li><a href="#02-image-classification">02. Image classification</a></li><li><a href="#03-loss-function-and-optimization">03. Loss function and optimization</a></li><li><a href="#04-introduction-to-neural-network">04. Introduction to Neural network</a></li><li><a href="#05-convolutional-neural-networks-cnns">05. Convolutional neural networks (CNNs)</a></li><li><a href="#06-training-neural-networks-i">06. Training neural networks I</a></li><li><a href="#07-training-neural-networks-ii">07. Training neural networks II</a></li><li><a href="#08-deep-learning-software">08. Deep learning software</a></li><li><a href="#09-cnn-architectures">09. CNN architectures</a></li></ul></li></ul><h2 id="course-info">Course Info</h2><ul><li>主页: http://cs231n.stanford.edu/</li><li>视频：<a href="https://www.bilibili.com/video/av17204303" target="_blank" rel="noopener">斯坦福深度学习课程CS231N 2017中文字幕版+全部作业参考_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></li><li>大纲：<a href="http://cs231n.stanford.edu/2017/syllabus" target="_blank" rel="noopener">Syllabus | CS 231N</a></li><li>课件：<a href="http://cs231n.stanford.edu/slides/2017/" target="_blank" rel="noopener">Index of /slides/2017</a></li><li>笔记：<a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">贺完结！CS231n官方笔记授权翻译总集篇发布</a></li><li>作业仓库：<a href="https://github.com/xiang578/MachineLearning/tree/master/CS231n" target="_blank" rel="noopener">MachineLearning/CS231n at master · xiang578/MachineLearning</a></li><li>总课时: <strong>16</strong></li></ul><h2 id="introduction-to-cnn-for-visual-recognition">01. Introduction to CNN for visual recognition</h2><ul><li>视觉地出现促进了物种竞争。</li><li>ImageNet 是由李飞飞维护的一个大型图像数据集。</li><li>自从 2012 年 CNN 出现之后，图像分类的错误率大幅度下降。 神经网络的深度也从 7 层增加到 2015 年的 152 层。截止到目前，机器分类准确率已经超过人类，所以 ImageNet 也不再举办相关比赛。</li><li>CNN 在 1998 年就被提出，但是这几年才流行开来。主要原因有：1) 硬件发展，并行计算速度提到 2）大规模带标签的数据集。</li><li>Gola: Understand how to write from scratch, debug and train convolutional neural networks.</li></ul><h2 id="image-classification">02. Image classification</h2><ul><li>图像由一大堆没有规律的数字组成，无法直观的进行分类，所以存在语义鸿沟。分类的挑战有：视角变化、大小变化、形变、遮挡、光照条件、背景干扰、类内差异。<ul><li><img src="https://media.xiang578.com/2019-10-08-15387098703701.jpg"></li></ul></li><li>Data-Driven Approach<ul><li>Collect a dataset of images and labels</li><li>Use Machine Learning to train a classifier</li><li>Evaluate the classifier on new images</li></ul></li><li>图像分类流程：输入、学习、评估</li><li>图像分类数据集：<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">CIFAR-10</a>，这个数据集包含了60000张32X32的小图像。每张图像都有10种分类标签中的一种。这60000张图像被分为包含50000张图像的训练集和包含10000张图像的测试集。</li><li>一种直观的图像分类算法：K-nearest neighbor(knn)<ul><li>为每一张需要预测的图片找到距离最近的 k 张训练集中的图片，然后选着在这 k 张图片中出现次数最多的标签做为预测图片的标签（多数表决）。</li><li>训练过程：记录所有的数据和标签 <span class="math inline">\({O(1)}\)</span></li><li>预测过程：预测给定图片的标签 <span class="math inline">\({O(n)}\)</span></li><li>Hyperparameters：k and the distance Metric</li><li>Distance Metric<ul><li>L1 distance(Manhattan Distance)</li><li>L2 distance(Euclidean Distance)</li></ul></li><li>knn 缺点<ul><li>Very slow at test time</li><li>Distance metrics on pixels are not informative</li></ul></li><li>反例：下面四张图片的 L2 距离相同<ul><li><img src="https://media.xiang578.com/2019-10-08-15387110626779.jpg" title="fig:" alt="-w622"></li></ul></li></ul></li><li>Hyperparameters: choices about the algorithm that we set ranther than learn</li><li>留一法 Setting Hyperparameters by Cross-validation:<ul><li>将数据划分为 f 个集合以及一个 test 集合，数据划分中药保证数据集的分布一致。</li><li>给定超参数，利用 f-1 个集合对算法进行训练，在剩下的一个集合中测试训练效果，重复这一个过程，直到所有的集合都当过测试集。</li><li>选择在训练集中平均表现最好的超参数。</li></ul></li><li>Linear classification: <code>Y = wX + b</code><ul><li>b 为 bias，调节模型对结果的偏好</li><li>通过最小化损失函数来，来确定 w 和 b 的值。</li></ul></li><li><strong>Linear SVM</strong>: classifier is an option for solving the image classification problem, but the curse of dimensions makes it stop improving at some point. <span class="citation" data-cites="todo">@todo</span></li><li><strong>Logistics Regression</strong>: 无法解决非线性的图像数据</li></ul><h2 id="loss-function-and-optimization">03. Loss function and optimization</h2><ul><li>通过 Loss function 评估参数质量<ul><li>比如 <span class="math display">\[L=\frac{1}{N}\sum_iL_i\left(f\left(x_i,W\right),y_i\right)\]</span></li></ul></li><li>Multiclass SVM loss 多分类支持向量机损失函数<ul><li><span class="math display">\[L_i=\sum_{j \neq y_j}\max\left(0,s_j-s_{y_i}+1\right)\]</span></li><li>这种损失函数被称为合页损失 Hinge loss</li><li>SVM 的损失函数要求正确类别的分类分数要比其他类别的高出一个边界值。</li><li>L2-SVM 中使用平方折叶损失函数<span class="math display">\[\max(0,-)^2\]</span>能更强烈地惩罚过界的边界值。但是选择使用哪一个损失函数需要通过实验结果来判断。</li><li>举例<ul><li><img src="https://media.xiang578.com/2019-10-08-15388343449162.jpg"></li><li>根据上面的公式计算：<span class="math display">\[L = \max(0,437.9-(-96.8)) + \max(0,61.95-(-96.8))=695.45\]</span></li><li>猫的分类得分在三个类别中不是最高得，所以我们需要继续优化。</li></ul></li></ul></li><li>Suppose that we found a W such that L = 0. Is this W unique?<ul><li>No! 2W is also has L = 0!</li></ul></li><li>Regularization: 正则化，向某一些特定的权值 W 添加惩罚，防止权值过大，减轻模型的复杂度，提高泛化能力，也避免在数据集中过拟合现象。<ul><li><span class="math display">\[L=\frac{1}{N}\sum_iL_i\left(f\left(x_i,W\right),y_i\right) + \lambda R(W)\]</span></li><li><code>R</code> 正则项 <span class="math display">\[\lambda\]</span> 正则化参数</li></ul></li><li>常用正则化方法<ul><li>L2<span class="math display">\[\begin{matrix} R(W)=\sum_{k}\sum_l W^2_{k,l} \end{matrix}\]</span></li><li>L1<span class="math display">\[\begin{matrix} R(W)=\sum_{k}\sum_l \left\vert W_{k,l} \right\vert \end{matrix}\]</span></li><li>Elastic net(L1 + L2): <span class="math display">\[\begin{matrix} R(W)=\sum_{k}\sum_l \beta W^2_{k,l} + \left\vert W_{k,l} \right\vert \end{matrix}\]</span></li><li>Dropout</li><li>Batch normalization</li><li>etc</li></ul></li><li>L2 惩罚倾向于更小更分散的权重向量，L1 倾向于稀疏项。</li><li>Softmax function：<ul><li><span class="math display">\[f_j(z)=\frac{e^{s_i}}{\sum e^{s_j}}\]</span></li><li>该分类器将输出向量 f 中的评分值解释为没有归一化的对数概率，通过归一化之后，所有概率之和为1。</li><li>Loss 也称交叉熵损失 cross-entropy loss <span class="math display">\[L_i = - \log\left(\frac{e^{s_i}}{\sum e^{s_j}}\right)\]</span></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure><ul><li>SVM 和 Softmax 比较<ol type="1"><li>评分，SVM 的损失函数鼓励正确的分类的分值比其他分类的分值高出一个边界值。</li><li>对数概率，Softmax 鼓励正确的分类归一化后的对数概率提高。</li><li>Softmax 永远不会满意，SVM 超过边界值就满意了。</li></ol></li><li>Optimization：最优化过程<ul><li>Follow the slope<ul><li><img src="https://media.xiang578.com/2019-10-08-15388374738605.jpg"></li></ul></li></ul></li><li>梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量，它是各个维度的斜率组成的向量。<ul><li>Numerical gradient: Approximate, slow, easy to write. (But its useful in debugging.)</li><li>Analytic gradient: Exact, Fast, Error-prone. (Always used in practice)</li><li>实际应用中使用分析梯度法，但可以用数值梯度法去检查分析梯度法的正确性。</li></ul></li><li>利用梯度优化参数的过程：<code>W = W - learning_rate * W_grad</code></li><li>learning_rate 被称为是学习率，是一个比较重要的超参数</li><li>Stochastic Gradient Descent SGD 随机梯度下降法<ul><li>每次使用一小部分的数据进行梯度计算，这样可以加快计算的速度。</li><li>每个批量中只有1个数据样本，则被称为随机梯度下降（在线梯度下降）</li></ul></li><li>图像分类任务中三大关键部分：<ol type="1"><li>评分函数</li><li>损失函数：量化某个具体参数 <span class="math inline">\({W}\)</span> 的质量</li><li>最优化：寻找能使得损失函数值最小化的参数 <span class="math inline">\({W}\)</span> 的过程</li></ol></li></ul><h2 id="introduction-to-neural-network">04. Introduction to Neural network</h2><ul><li>反向传播：在已知损失函数 <span class="math inline">\({L}\)</span> 的基础上，如何计算导数<span class="math inline">\({\nabla _WL}\)</span>？</li><li>计算图<ul><li>由于计算神经网络中某些函数的梯度很困难，所以引入计算图的概念简化运算。</li><li>在计算图中，对应函数所有的变量转换成为计算图的输入，运算符号变成图中的一个节点（门单元）。</li></ul></li><li>反向传播：从尾部开始，根据链式法则递归地向前计算梯度，一直到网络的输入端。<ul><li><img src="https://media.xiang578.com/2019-10-08-15390088806657.jpg" title="fig:" alt="-w1107"></li><li>绿色是正向传播，红色是反向传播。</li></ul></li><li>对于计算图中的每一个节点，我们需要计算这个节点上的局部梯度，之后根据链式法则反向传递梯度。</li><li>Sigmoid 函数：<span class="math inline">\({f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}}\)</span><ul><li><img src="https://media.xiang578.com/2019-10-08-15390092983570.jpg"></li><li>对于门单元 <span class="math inline">\({\frac{1}{x}}\)</span>，求导的结果是 <span class="math inline">\({-\frac{1}{x^2}}\)</span>，输入为 1.37，梯度返回值为 1.00，所以这一步中的梯度是 <span class="math inline">\({(\frac{-1}{1.37^2})*1.00=-0.53}\)</span>。</li><li>模块化思想：对 <span class="math inline">\({\sigma(x)=\frac{1}{1+e^{-x}}}\)</span> 求导的结果是 <span class="math inline">\({(1-\sigma(x))\sigma(x)}\)</span>。如果 sigmoid 表达式输入值为 1.0 时，则前向传播中的结果是 0.73。根据求导结果计算可得局部梯度是 <span class="math inline">\({(1-0.73)*0.73=0.2}\)</span>。</li></ul></li><li>Modularized implementation: forward/backwar API</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultuplyGate</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  x,y are scalars</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    z = x*y</span><br><span class="line">    self.x = x  <span class="comment"># Cache</span></span><br><span class="line">    self.y = y<span class="comment"># Cache</span></span><br><span class="line">    <span class="comment"># We cache x and y because we know that the derivatives contains them.</span></span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(dz)</span>:</span></span><br><span class="line">    dx = self.y * dz         <span class="comment">#self.y is dx</span></span><br><span class="line">    dy = self.x * dz</span><br><span class="line">    <span class="keyword">return</span> [dx, dy]</span><br></pre></td></tr></table></figure><ul><li>深度学习框架中会实现的门单元：Multiplication、Max、Plus、Minus、Sigmoid、Convolution</li><li>常用计算单元<ul><li><strong>加法门单元：</strong>把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。</li><li><strong>取最大值门单元：</strong>将梯度转给前向传播中值最大的那个输入，其余输入的值为0。</li><li><strong>乘法门单元：</strong>等值缩放。局部梯度就是输入值，但是需要相互交换，然后根据链式法则乘以输出值得梯度。</li></ul></li><li>Neural NetWorks<ul><li>(Before) Linear score function <span class="math display">\[f = Wx\]</span></li><li>(Now) 2-layer Neural NetWork <span class="math display">\[f=W_2\max(0,W_1x)\]</span></li><li>ReLU <span class="math display">\[\max(0,x)\]</span> 是激活函数，如果不使用激活函数，神经网络只是线性模型的组合，无法拟合非线性情况。</li><li>神经网络是更复杂的模型的基础组件</li></ul></li></ul><h2 id="convolutional-neural-networks-cnns">05. Convolutional neural networks (CNNs)</h2><ul><li>这一轮浪潮的开端：<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlxNet</a></li><li>卷积神经网络<ul><li>Fully Connected Layer 全连接层：这一层中所有的神经元链接在一起。</li><li>Convolution Layer：<ul><li>通过参数共享来控制参数的数量。Parameter sharing</li><li>Sparsity of connections</li></ul></li><li>卷积神经网络能学习到不同层次的输入信息</li><li>常见的神经网络结构：<code>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC</code></li><li>使用小的卷积核大小的优点：多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好地特征。而且使用的参数也会更少</li></ul></li><li>计算卷积层输出<ul><li>stride 是卷积核在移动时的步长</li><li>通用公式 (N-F)/stride + 1<ul><li>stride 1 =&gt; (7-3)/1 + 1 = 5</li><li>stride 2 =&gt; (7-3)/2 + 1 = 3</li><li>stride 3 =&gt; (7-3)/3 + 1 = 2.33</li></ul></li><li>Zero pad the border: 用零填充所有的边界，保证输入输出图像大小相同，保留图像边缘信息，提高算法性能<ul><li>步长为 1 时，需要填充的边界计算公式：(F-1)/2<ul><li>F = 3 =&gt; zero pad with 1</li><li>F = 5 =&gt; zero pad with 2</li><li>F = 7 =&gt; zero pad with 3</li></ul></li></ul></li><li>计算例子<ul><li>输入大小 <code>32*32*3</code> 卷积大小 10 5*5 stride 1 pad 2</li><li>output <code>32*32*10</code></li><li>每个 filter 的参数数量：<code>5*5*3+1 =76</code> bias</li><li>全部参数数量 76*10=760</li></ul></li></ul></li><li>卷积常用超参数设置<ul><li>卷积使用小尺寸滤波器</li><li>卷积核数量 K 一般为 2 的次方倍</li><li>卷积核的空间尺寸 F</li><li>步长 S</li><li>零填充数量 P</li></ul></li><li>Pooling layer<ul><li>降维，减少参数数量。在卷积层中不对数据做降采样</li><li>卷积特征往往对应某个局部的特征，通过池化聚合这些局部特征为全局特征</li></ul></li><li>Max pooling<ul><li>2*2 stride 2</li><li>避免区域重叠</li></ul></li><li>Average pooling</li></ul><h2 id="training-neural-networks-i">06. Training neural networks I</h2><ul><li><p>Activation functions 激活函数</p><ul><li>不使用激活函数，最后的输出会是输入的线性组合。利用激活函数对数据进行修正。</li><li><img src="https://media.xiang578.com/2019-10-08-15395012747510.jpg"></li><li>Sigmoid<ul><li>限制输出在 [0,1]区间内</li><li>firing rate</li><li>二分类输出层激活函数</li><li>Problem<ul><li>梯度消失：x很大或者很小时，梯度很小，接近于0（考虑图像中的斜率。无法得到梯度反馈。</li><li>输出不是 0 均值的数据，梯度更新效率低</li><li>exp is a bit compute expensive</li></ul></li></ul></li><li>tanh<ul><li>输出范围 [-1, 1]</li><li>0 均值</li><li>x 很大时，依然没有梯度</li><li><span class="math inline">\({f(x)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}}\)</span></li><li><span class="math inline">\({1-(tanh(x))^2}\)</span></li></ul></li><li>RELU rectified linear unit 线性修正单元<ul><li>一半空间梯度不会饱和，计算速度快，对结果又有精确的计算</li><li>不是 0 均值</li></ul></li><li>Leaky RELU<ul><li><code>leaky_RELU(x) = max(0.01x, x)</code></li><li>梯度不会消失</li><li>需要学习参数</li></ul></li><li>ELU<ul><li>比 ReLU 好用</li><li>反激活机制</li></ul></li><li>Maxout<ul><li>maxout(x) = max(w1.T<em>x + b1, w2.T</em>x + b2)</li><li>梯度不会消失</li><li>增大参数数量</li></ul></li><li>激活函数选取经验<ul><li>使用 ReLU ，但要仔细选取学习率</li><li>尝试使用 Leaky ReLU Maxout ELU</li><li>使用 tanh 时，不要抱有太大的期望</li><li>不要使用 sigmoid</li></ul></li></ul></li><li><p>数据预处理 Data Preprocessing</p><ul><li>均值减法：对数据中每个独立特征减去平均值，从几何上来看是将数据云的中心都迁移到原点。</li><li>归一化：将数据中的所有维度都归一化，使数值范围近似相等。但是在图像处理中，像素的数值范围几乎一致，所以不需要额外处理。</li></ul><p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line">X /= np.std(X, axis =<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><ul><li>图像归一化<ul><li>Subtract the mean image AlexNet<ul><li>mean image 32,32,3</li></ul></li><li>Subtract per-channel mean VGGNet<ul><li>mean along each channel = 3 numbers</li></ul></li><li>如果需要进行均值减法时，均值应该是从训练集中的图片平均值，然后训练集、验证集、测试集中的图像再减去这个平均值。</li></ul></li><li>Weight Initialization<ul><li>全零初始化<ul><li>网络中的每个神经元都计算出相同的输出，然后它们就会在反向传播中计算出相同的梯度。神经元之间会从源头上对称。</li></ul></li><li>Small random numbers<ul><li>初始化权值要非常接近 0 又不能等于 0。将权重初始化为很小的数值，以此来打破对称性</li><li>randn 函数是基于零均值和标准差的高斯分布的随机函数</li><li>W = 0.01 * np.random.rand(D,H)</li><li>问题：一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度。会减小反向传播中的“梯度信号”，在深度网络中就会出现问题。</li></ul></li><li>Xavier initialization<ul><li>W = np.random.rand(in, out) / np.sqrt(in)</li><li>校准方差，解决输入数据量增长，随机初始化的神经元输出数据的分布中的方差也增大问题。</li></ul></li><li>He initialization<ul><li>W = np.random.rand(in, out) / np.sqrt(in/2)</li></ul></li></ul></li><li>Batch normalization<ul><li>保证输入到神经网络中的数据服从标准的高斯分布</li><li>通过批量归一化可以加快训练的速度</li><li>步骤<ul><li>首先计算每个特征的平均值和平方差</li><li>通过减去平局值和除以方差对数据进行归一化</li><li><code>Result = gamma * normalizedX + beta</code><ul><li>对数据进行线性变换，相当于对数据分布进行一次移动，可以恢复数据之前的分布特征</li></ul></li></ul></li><li>BN 的好处<ul><li>加快训练速度</li><li>可以使用更快的而学习率</li><li>减少数据对初始化权值的敏感程度</li><li>相当于进行一次正则化</li></ul></li><li>BN 适用于卷积神经网络和常规的 DNN，在 RNN 和增强学习中表现不是很好</li></ul></li><li>Babysitting the Learning Provess</li><li>Hyperparameter Optimization<ul><li>Cross-validation 策略训练</li><li>小范围内随机搜索</li></ul></li></ul></li></ul><h2 id="training-neural-networks-ii">07. Training neural networks II</h2><ul><li>Optimization Algorithms:<ul><li>SGD 的问题<ul><li><code>x += - learning_rate * dx</code></li><li>梯度在某一个方向下降速度快，在其他方向下降缓慢</li><li>遇到局部最小值点，鞍点</li></ul></li><li>mini-batches GD<ul><li>Shuffling and Partitioning are the two steps required to build mini-batches</li><li>Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.</li></ul></li><li>SGD + Momentun<ul><li>动量更新：从物理学角度启发最优化问题</li><li><code>V[t+1] = rho * v[t] + dx; x[t+1] = x[t] - learningRate * V[t+1]</code></li><li>rho 被看做是动量，其物理意义与摩擦系数想类似，常取 0.9 或0.99</li><li>和 momentun 项更新方向相同的可以快速更新。</li><li>在 dx 中改变梯度方向后， rho 可以减少更新。momentun 能在相关方向加速 SGD，抑制震荡，加快收敛。</li></ul></li><li>Nestrov momentum<ul><li><img src="https://media.xiang578.com/2019-10-08-15499574039274.jpg"></li><li><code>v_prev = v; v = mu * v - learning_rate * dx; x += -mu * v_prev + (1 + mu) * v</code></li></ul></li><li>AdaGrad<ul><li><span class="math inline">\(n_t=n_{t-1}+g^2_t\)</span></li><li><span class="math inline">\(\Delta \theta _t = -\frac{\eta}{\sqrt{n_t+\epsilon}}\)</span></li><li>下面根号中会递推形成一个约束项。前期这一项比较大，能够放大梯度。后期这一项比较小，能约束梯度。</li><li>gt 的平方累积会使梯度趋向于 0</li></ul></li><li>RMSProp<ul><li>RMS 均方根</li><li>自适应学习率方法</li><li>求梯度的平方和平均数：<code>cache =  decay_rate * cache + (1 - decay_rate) * dx**2</code></li><li><code>x += - learning_rate * dx / (sqrt(cache) + eps)</code></li><li>依赖全局学习率</li></ul></li><li>Adam<ul><li>RMSProp + Momentum</li><li>It calculates an exponentially weighted average of past gradients, and stores it in variables <span class="math inline">\(v\)</span> (before bias correction) and <span class="math inline">\(v^{corrected}\)</span> (with bias correction).</li><li>It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables <span class="math inline">\(s\)</span> (before bias correction) and <span class="math inline">\(s^{corrected}\)</span> (with bias correction).</li><li>一阶到导数累积，二阶导数累积</li><li>It updates parameters in a direction based on combining information from "1" and "2".</li><li>The update rule is, for <span class="math inline">\(l = 1, ..., L\)</span>: <span class="math display">\[\begin{cases}  v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \\  v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\  s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\  s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_1)^t} \\  W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}}} + \varepsilon}  \end{cases}\]</span></li></ul>where:<ul><li>t counts the number of steps taken of Adam</li><li>L is the number of layers</li><li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters that control the two exponentially weighted averages.</li><li><span class="math inline">\(\alpha\)</span> is the learning rate</li><li><span class="math inline">\(\varepsilon\)</span> is a very small number to avoid dividing by zero</li></ul>特点：<ul><li>适用于大数据集和高维空间。</li><li>对不同的参数计算不同的自适应学习率。</li></ul></li><li>Learning decay<ul><li>学习率随着训练变化，比如每一轮在前一轮的基础上减少一半。</li><li>防止学习停止</li></ul></li><li>Second order optimization</li></ul></li><li>Regularization<ul><li>Dropout<ul><li>每一轮中随机使部分神经元失活，减少模型对神经元的依赖，增强模型的鲁棒性。</li></ul></li></ul></li><li>Transfer learning<ul><li>CNN 中的人脸识别，可以在大型的模型基础上利用少量的相关图像进行继续训练。</li></ul></li></ul><h2 id="cnn-architectures">09. CNN architectures</h2><ul><li>研究模型的方法：搞清楚每一层的输入和输出的大小关系。</li><li>LeNet - 5 [1998]<ul><li>60k 参数</li><li>深度加深，图片大小减少，通道数量增加</li><li>ac: Sigmod/tanh</li></ul></li><li>AlexNet [2012]<ul><li>(227,227,3) （原文错误）</li><li>60M 参数</li><li>LRN：局部响应归一化，之后很少使用</li></ul></li><li>VGG - 16 [2015]<ul><li>138 M</li><li>结构不复杂，相对一致，图像缩小比例和通道增加数量有规律</li></ul></li><li>ZFNet [2013]<ul><li>在 AlexNet 的基础上修改<ul><li><code>CONV1</code>: change from (11 x 11 stride 4) to (7 x 7 stride 2)</li><li><code>CONV3,4,5</code>: instead of 384, 384, 256 filters use 512, 1024, 512</li></ul></li></ul></li><li>VGG [2014]<ul><li>模型中只使用 3*3 conv：与 77 卷积有相同的感受野，而且可以将网络做得更深。比如每一层可以获取到原始图像的范围：第一层 33，第二层 55，第三层 77。</li><li>前面的卷积层参数量很少，模型中大部分参数属于底部的全连接层。</li></ul></li></ul><p><img src="https://media.xiang578.com/2019-10-08-15705415499052.jpg"></p><ul><li>GoogLeNet<ul><li>引入 <code>Inception module</code><ul><li>design a good local network topology (network within a network) and then stack these modules on top of each other</li><li>该模块可以并行计算</li><li>conv 和 pool 层进行 padding，最后将结果 concat 在一起</li></ul></li></ul></li></ul><figure><img src="https://media.xiang578.com/2019-10-08-15705420412305.jpg" alt="Reset"><figcaption>Reset</figcaption></figure><ul><li>ResNet<ul><li>目标：深层模型表现不应该差于浅层模型，解决随着网络加深，准确率下降的问题。</li><li><code>Y = (W2* RELU(W1x+b1) + b2) + X</code></li><li>如果网络已经达到最优，继续加深网络，residual mapping会被设置为 0，一直保存网络最优的情况。</li></ul></li></ul><h2 id="reference">Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam） - 知乎</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;去年学习这门做的部分笔记，现在分享出来。 笔记格式有些问题，持续整理中。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;大量内容参考 &lt;a href=&quot;https://github.com/mbadry1/CS231n-2017-Summa
      
    
    </summary>
    
      <category term="机器学习" scheme="https://xiang578.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ml, course" scheme="https://xiang578.com/tags/ml-course/"/>
    
  </entry>
  
  <entry>
    <title>算法花园写作风格清单</title>
    <link href="https://xiang578.com/post/blog-writing-checklist.html"/>
    <id>https://xiang578.com/post/blog-writing-checklist.html</id>
    <published>2019-11-03T10:03:23.000Z</published>
    <updated>2021-01-09T02:59:54.286Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>李如一在 <a href="https://www.qdaily.com/articles/1397.html" target="_blank" rel="noopener">写作风格手册</a> 中提到写作风格的作用是 「保持机构和组织内部的文体统一，提高沟通效率。」</p><p>本清单会持续更新，如果有相关的建议，可以在留言中告诉我。</p></blockquote><p>算法花园定位为个人博客，也是我和这个世界沟通的窗口。为提高读者阅读体验，参考相关文章后，推出该清单统一网站文章的基础风格。</p><h2 id="写作">写作</h2><ol type="1"><li>减少形容词使用，尽可能删除 「的」和「了」。</li><li>给出引用图片及引文来源。</li><li>文章如果发布后大幅度修改，在末尾给出版本信息。</li><li>写完文章后，整体阅读一遍。</li></ol><h2 id="排版">排版</h2><ol type="1"><li>中文、英文、数字中间加空格，数字与单位之间无需增加空格，全角标点与其他字符之间不加空格。链接前后增加空格用以区分。</li><li>不重复使用标点符号。</li><li>中文使用直角引号 「」以及『 』。</li><li>使用全角中文标点，数字使用半角字符。中文中出现英文部分，仍然使用中文标点。</li><li>遇到完整的英文整句、特殊名词，其內容使用半角标点。</li><li>专有名词使用正确的大小写，使用公认的缩写。</li><li>todo 如何处理图片排版和命名。</li><li>使用英文命名文档，使用 <code>-</code> 来连接。为保证搜索引擎效果，尽量不要修改文档名称。</li><li><del>每篇文章开头添加简单介绍 <code>&lt;!--more--&gt;</code>。</del></li><li>发布后，在网页中确认格式是否符合预期、链接能否点击以及图片能否展示。</li></ol><h2 id="changelog">ChangeLog</h2><blockquote><p>20191103: 第一版</p></blockquote><h2 id="参考">参考</h2><ul><li><a href="https://www.qdaily.com/articles/1397.html" target="_blank" rel="noopener">写作风格手册_设计词典_好奇心日报</a></li><li><a href="https://mazhuang.org/wiki/chinese-copywriting-guidelines/" target="_blank" rel="noopener">中文文案排版指北（简体中文版） — 码志</a></li><li><a href="https://zhuanlan.zhihu.com/p/49729668" target="_blank" rel="noopener">简体中文文本排版指南 - 知乎</a></li><li><a href="https://sspai.com/post/37815" target="_blank" rel="noopener">少数派写作排版指南 - 少数派</a></li><li><a href="https://www.jianshu.com/p/8ffc3e0d11e2" target="_blank" rel="noopener">城堡制作检查清单 0.1 版 - 简书</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;李如一在 &lt;a href=&quot;https://www.qdaily.com/articles/1397.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;写作风格手册&lt;/a&gt; 中提到写作风格的作用是 「保持机构和组织内部的文体
      
    
    </summary>
    
      <category term="站务" scheme="https://xiang578.com/categories/%E7%AB%99%E5%8A%A1/"/>
    
    
      <category term="blog" scheme="https://xiang578.com/tags/blog/"/>
    
      <category term="writing" scheme="https://xiang578.com/tags/writing/"/>
    
  </entry>
  
</feed>
