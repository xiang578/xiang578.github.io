{"meta":{"title":"算法花园","subtitle":null,"description":null,"author":"Ryen Xiang","url":"https://xiang578.com","root":"/"},"pages":[{"title":"About","date":"2015-08-15T02:59:25.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"about/index.html","permalink":"https://xiang578.com/about/index.html","excerpt":"","text":"About Me 网上冲浪常用 ID：Ryenxx、xiang578 2014-2018：国内某不知名大学，计算机专业业余学生 多次参加 ACM 比赛，实属打铁水平，侥幸抱上大腿获得过金、银、铜奖。 2018-Now：国内某出行公司地图部门，不会机器学习的算法工程师 加入我们一起建模现实世界，详见【滴滴出行-地图路线引擎】算法实习生 兴趣： 机器学习、深度学习 Zettelkasten 阅读 数字生活：这里将成为我的数字坟墓 ErgoDox EZ：键位设置 macOS 软件清单 Chrome 扩展程序清单 ## About Blog 一万人来一百次的岛 写一些可能只有自己能看懂的文章 吾道不孤 欢迎写信到 ryenxx#gmail.com（#换成@） 进一步交流 了解更多博客折腾记 推荐阅读过去 90 天访问量 top5 文章： 「Rime 鼠须管」小鹤双拼配置指南 从零开始利用 hexo + Github/Coding 搭建个人博客 李宏毅强化学习课程笔记 PG PPO Q-Learing (WDR) Learning to Estimate the Travel Time (FTRL) Follow The Regularized Leader 分享 2018年5月 Standard Template Library PDF 2018年4月 路径规划那些事 PDF 2017年5月 数位 DP PDF 2016年6月 堆及其应用 PDF 2016年6月 树链剖分 PDF 2016年5月 搜索加强 PDF 2016年5月 树型动态规划 PDF 请我喝一杯咖啡 赞助不会给你带来什么，也不会让你失去什么 致谢 20160901: Fighting Heart 通过支付宝赞助 0.50 元 20160928: Jelly 通过微信赞助 0.60 元 20180324: 一位没有留下痕迹的读者 通过简书赞赏 2.00 元 20180712: *儿 通过微信赞助 3 元 20190110: 珍冰乐 通过微信赞助 0.01 元 20190114: 珍冰乐 通过微信赞助 6.66 元 谢谢你看到这里！"},{"title":"categories","date":"2017-10-05T06:29:13.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"categories/index.html","permalink":"https://xiang578.com/categories/index.html","excerpt":"","text":""},{"title":"链接","date":"2020-01-02T14:13:38.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"link/index.html","permalink":"https://xiang578.com/link/index.html","excerpt":"","text":"同学 海虹不老阁：北京某知名高校研究生 浙江某985高校研究生 小仙女？ 币圈大佬 Ocrosoft：有点杂，看他最近对什么感兴趣 Fighting Heart: 「Microsoft - Software Engineer」 浙江财经大学本科生典范 推荐 张驰原 Free Mind：MIT 博士，Google Brain。主题包括但不限于：机器学习、数学、程序设计、阅读、绘画。 刘未鹏 Mind Hacks：思维改变生活 木子：心态平和，语言幽默。关于阅读和折腾。 余舜哲的 One Piece - 时间知道：创业者·加密货币爱好者·文学爱好者。 机器学习 李海波 Semocean – 和自己赛跑的人：滴滴前同事，阿里资深算法专家。机器学习如何在工业界落地。 朱小强 - 知乎：阿里妈妈资深算法专家。CTR。 张俊林 - 知乎：新浪微博 NLP，经常写一些深度长文。 王喆 - 知乎：CTR，《深度学习推荐系统》。 吴海波 - 知乎：蘑菇街。CTR。 yymWater - 知乎：滴滴前同事，3 年P7。CTR。 苏剑林 科学空间|Scientific Spaces：追一科技。NLP 方向论文、代码解析。 石塔西 - 知乎：CTR，文章有深度。 产品 产品沉思录 · Product Thinking：少楠主理的 Newsletter 以及产品数据库，内容包括但不限于产品设计，服务设计，数据分析，互联网技术，经济学，心理学，社会学，决策学，自然科学，城市规划，零售，团队管理等内容。"},{"title":"Log of Me","date":"2020-12-05T14:13:38.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"log/index.html","permalink":"https://xiang578.com/log/index.html","excerpt":"","text":"2021 2021 January book 裴洞篇 禅与摩托车维修艺术 post 合并「博客折腾记：使用 Travis CI 自动部署」到博客折腾记：使用 Travis CI 自动部署博客 blog 6d6f1b9 新年新气象！使用木子修改的 hexo-themes-next 主题。 2020 2020 December book 会饮篇 像哲学家一样生活 乌克兰拖拉机简史 paper [2013] Pathlet Learning for Compressing and Planning Trajectories：区间 DP 用于压缩和规划轨迹的路径学习方法 Before paper CNN 系列 (AlexNet)ImageNet Classiﬁcation with Deep Convolutional Neural Networks FM 系列 (gbdt + lr)Practical Lessons from Predicting Clicks on Ads at Facebook (Wide&amp;Deep) Wide &amp; Deep Learning for Recommender Systems (FM) Factorization Machines course Standford CS231n 2017 课程部分总结 李宏毅强化学习课程笔记 book 机器学习 🍉 书 第 1 章 绪论 ​"},{"title":"tags","date":"2017-09-30T12:41:57.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"tags/index.html","permalink":"https://xiang578.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《会饮篇》读书笔记","slug":"symposium","date":"2021-01-10T12:35:29.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/symposium.html","link":"","permalink":"https://xiang578.com/post/symposium.html","excerpt":"","text":"《会饮篇》记录阿伽通获得酒神节悲剧比赛第一名后，苏格拉底等人在庆祝宴会中「赞美爱神」的比赛过程。实际上记录与会人七种不同的爱情观。最后，酒神狄奥尼索斯决定出当晚的获胜者。 裴卓：勇气之爱 讲述者被人们视为弱小的被爱之人，美少年裴卓 Phaedrus，爱可以给我们勇气！ 为人们要想过美好正当的生活，必须终生遵循一个指导原则，这并不能完全依靠血统，也不能靠威望、财富，只有靠爱才能办到。这原则是什么呢？就是：厌恶丑恶的，爱慕美好的。 如果一个情人在准备做一件丢人的坏事，或者在受人凌辱而怯懦不敢抵抗，这时他被人看见了就会觉得羞耻，但是被父亲、朋友或其他人看见还远远不如被爱人看见那样羞到无地自容。爱人被情人发现他做坏事，情形也是如此。 阿喀琉斯为情人复仇。母亲叮嘱他，杀死仇人赫格多尔之后自己也会死。最后还是奋不顾身杀死仇人，自己被射中脚踵而死。 \"最爱的人心中对我们的期望。「目标的意义」\" 包萨尼亚：精神之爱 讲述者是一个成年的同性恋，阿伽通的情人包萨尼亚 Pausanias，被人们视为追求肉欲的人。爱神爱有天地之分，天之爱是精神之爱，是崇高的爱，只有男男之间才有，是美好的追求；地之爱是肉体之爱，是普遍的低级的爱，也就是大多数人的爱，只有肉欲的爱。 凡间阿莆若狄德引起的爱神确实也是凡俗的，它不分皂白地奔赴它的目的。这种爱情只限于下等人。它的对象可以是娈童，也可以是女子；它所眷恋的是肉体而不是灵魂；最后，它只选择愚蠢的对象，因为它只贪图达到目的，不管达到目的的方式美丑。 至于天上的那位的出身却与女的无关，只是由男子生的，所以其爱情对象只是少年男子。其次，她的年纪较大，所以不至于荒淫放荡。她只鼓舞人们把爱情专注在男性对象上，因为这种对象生来就比较坚强，比较聪明。 他们说，与其暗爱，不如明爱，所爱的人应当在门第和品德上都很高尚，美还在其次。人们对情人都给予极大的鼓励，不认为他在做不体面的事；人们把追求爱情的胜利看成光荣，把这方面的失败看成羞耻。为了争取胜利，他可以做出种种离奇的事。 这种少年男子一定要显现理性，也就是腮帮上长胡须的时候，才能成为爱的对象。我想情人之所以要等爱人达到这种年龄之后才钟爱他，是由于存心要和他终生相守，不是要利用他的年幼无知来哄骗他，碰到另外一个可以宠爱的对象时就把他扔掉。 当时社会能接受：年长男性(智慧，情人)和少年男孩(12-17，肉体，爱人)之间的恋爱关系。包萨尼亚和阿伽通年纪都大了。 我认为道理是这样：这件事并不是十分单纯的，像我开头说的那样。单就它本身来看，它无所谓美，也无所谓丑；做的方式美它就美，做的方式丑它就丑。丑的方式就是拿卑鄙的方式来对付卑鄙的对象，美的方式就是拿高尚的方式来对付高尚的对象。所谓卑鄙的对象就是上面说的凡俗的情人，爱肉体过于爱灵魂的。他所爱的东西不是始终不变的，所以他的爱情也不能始终不变。一旦肉体的颜色衰败了，他就远走高飞，毁弃从前的一切信誓。然而钟爱优美品德的情人却不然，他的爱情是始终不变的，因为他所爱的东西也是始终不变的。 我们的习俗定了两条规矩，头一条是：迅速接受情人是可耻的，应该经过一段时间，因为时间对于许多事物常常是最好的考验；第二条是：受金钱引诱或政治威胁而委身于人是可耻的，无论是不敢抵抗威胁而投降，还是贪图财产和地位，全都一样。因为这些势力、名位和金钱都不是持久不变的，高尚的友谊当然不能由此产生。 按照我们这里的规矩，如果一个人肯侍候另外一个人，目的在于得到那个人的帮助在爱智或其他品德上更进一步，这种卑躬屈节并不卑鄙，也不能指为谄媚。 这里有两条规矩，一条是关于少年男子的爱情，一条是关于学问道德的追求，应该合而为一；如果合而为一，爱人眷恋情人就是一件美事。所以，情人和爱人来往，就各有各的指导原则。情人的原则是爱人对自己既然表现殷勤，自己就应该在一切方面为他效劳；爱人的原则是情人既然使自己在学问道德方面有长进，自己就应该尽量拿恩情来报答。一方面乐于拿学问道德来施教，一方面乐于在这些方面受益，只有在这两条原则合而为一时，爱人眷恋情人才是一件美事，如若不然，它就不美。如果是为了增进学问道德，纵然完全失败也没有什么可耻；如果是为了其他的目的，不管失败与否都是可耻的。假如一个少年男子以为他的情人很富，为了贪财就去眷恋他，后来发现自己看错了，其实他很穷，无利可图。其眷恋还是很可耻的，因为这种行为揭穿了他的性格，证明他这个人为了金钱可以侍候任何人、做出任何事来，这当然是很不光彩的。 我们谈爱情的时候，事实上，我们讨论的正是真实的自己。 鄂吕克锡马柯：功利之爱 讲述者是医生鄂吕克锡马柯，理性大于感性，是技术主义的代表。爱情是和谐的，是有规律的，是有用的，我们可以通过其中的规律得到爱情，也可以通过爱情得到别的有用的东西。 爱情克制论 阿里斯多芬：缺憾之爱 讲述者是喜剧家阿里斯多芬，而喜剧总是悲剧的内核。最初有三种人，男男人、女女人和男女人，像球一样。因为得罪宙斯，人被劈成两半，天生就该追求自己的另一半。不论肉体还是精神，我们都应该追求自己的另一半，以此互补，否则我们孤独，有缺点，不完美。 所以我们每人都是人的一半，是一种合起来才成为全体的东西。所以每个人都经常在寻求自己的另一半。全人类只有一条幸福之路，就是实现自己的爱，找到恰好和自己配合的爱人，总之，还原到自己的本来面目。 对爱情最浪漫的解释。 阿伽通：完美之爱 讲述者是悲剧诗人阿伽通，是十分优秀的人。我们要追求完美，爱神就是完美的，爱情也是完美的，所以我们要追求爱，而且爱是有益于他人的，是可以从满溢之处流向匮乏之处的(对应开头评价苏格拉底的智慧)，可以让大家一起变的完美。 前面的人赞美爱神，其实都是在赞美其索给予的幸福，而不是真正地赞美爱神本身。 苏格拉底：朝圣之爱 讲述者是苏格拉底，从狄欧蒂玛口中听到的爱。爱神不是完美的，介于不死的神和会死的人之间的精灵，介于美和丑之间，介于无知和有知之间。追求爱不是为了爱本身，是为了爱延伸的东西，是在追求爱情过程中得到的美好、是追求到以后收获的智慧和快乐，美好和永恒。 和阿伽通的对话 爱神不仅公正，而且审慎。大家公认审慎是节制快感和情欲的力量。世界上没有一种快感比爱情本身还要强烈。一切快感都比不上爱情，就是因为它们都受爱神节制，而爱神是它们的统治者。爱神既然统治着快感和情欲，岂不是最审慎的吗？ 爱不也是这样：一个人既然爱一件东西，就是还没有那样东西；他盼望它，就是盼望他现在有它，或者将来有它。是不是？” 所以总起来说，在这种和其他情况下，一个盼望的人所盼望的是他缺少的、还没有到手的，总之是他所没有的，是本身不存在的，不在他那里的；只有这样的东西才是他所盼望的、他所爱的。 爱神首先是对某某东西的爱，其次是对他所欠缺的东西的爱。是不是？ 正确的意见而说不出所以然来，就不是有知识（因为没有根据的不能算知识），却也不是无知（因为有正确内容的不能叫无知）。所以很明显，正确的意见就是介于智慧和无知之间的东西。 她说：「说来话长，我还是给你说说吧。当初阿莆若狄德诞生的时候，诸神举行宴会，出席的有智谋女神梅蒂的儿子丰饶神波若。他们宴饮结束时，匮乏神贝尼娅来向他们作节日例行的行乞，站在门口。波若多喝了几杯琼浆（因为那时还没有酒），就走进宙斯的花园，昏昏沉沉地睡着了。贝尼娅由于贫乏，很想和波若生个孩子，于是和他睡在一起，怀下了爱若。爱若也成了阿莆若狄德的随从和仆人，因为他是在阿莆若狄德的生日投的胎，生性爱美的东西，而阿莆若狄德是很美的。」 她说：‘这正是我要启发你的第二点，苏格拉底。爱神就是这个样子，就是这样产生的。他是奔赴美的东西的，像你说的那样。假如有人问我们：爱者从美的东西得到什么呢，苏格拉底和狄欧蒂玛？或者问得更明确一点：那盼望美的东西的是盼望什么呢？你怎么回答？’ 那我给你说清楚点。苏格拉底啊，所有的人都会生育，凭借身体或灵魂生育，到了一定的年龄，就为本性所推动，迫不及待地要求生育。可是他们不能在丑的东西里、只能在美的东西里生育。男人和女人结合就是生育。怀胎、生育是一件神圣的事，是会死的凡夫身上的不朽的因素。但是这件事不可能在不协调的情况下实现，丑的东西与神圣的事情不协调，只有美的东西才与它协调。所以美是引导和帮助生育的女神和决定命运的女神。因为这个缘故，那生育能力旺盛的一遇到美的对象就立刻欢欣鼓舞，精神焕发，同它交配生子；如果遇到丑的，就垂头丧气，毫无兴趣，避开它不去生育，宁愿把沉重的包袱背下去。因此那充满生育的种子和欲望的一遇到美的对象就欣喜若狂，是由于它可以结束他的巨大痛苦。 你就不会觉得奇怪了。因为和上面说的完全一样，那会死的东西也是力求能够永远存在和不朽。要达到不朽，全凭生殖，以新的代替旧的。每一个个体的生物，虽然我们说它一生之中始终是同一个东西。 美者，这时，就只有大愚不解的人才会不明白一切形体中的美是同一个美了。明白了这一点，他就成了爱一切美好形体的人，把他的热情从专注于某一形体推广到一切，因为他把那种专注一点看成渺小的、微不足道的。再则，他必须把灵魂的美看得大大优于形体的美，如果有一个人灵魂值得称赞，即便形貌较次，那也足够了，他也应该对这个人表示爱慕之情，加以照顾，他心里想出来发表的那些美好的话语可以使青年奋发向上，他这样做也使他自己遍览人们各种行动中以及各种风俗习惯中的美，从而见到美是到处贯通的，就把形体的美看成甚为微末的了。可是他必须从各种行动向前更进一步，达到知识，这样就见到知识的美。 至于爱的方面，情形也是这样。一般说来，凡属对于好东西、对于幸福的企盼，都是每个人心中最大的、强烈的爱。然而其余的那些在某一个方面有所追求的，无论是谋求获利的，喜爱体育的，还是爱智慧的，我们都不说他们在爱，不说他们是钟爱者；只有那些以某种方式发挥作用的喜好者，才占有全体的名称，我们说他们在爱，称他们为情人或钟爱者。 一个人如果一直接受爱的教育，按照这样的次序一一观察各种美的东西，直到这门爱的学问的结尾，就会突然发现一种无比奇妙的美者，即美本身。 爱其实就是寻找真正的美的过程。 苏格拉底的颂辞是全篇三大段的中段，也是全篇精义所在。它本身分两部分：和阿伽通的对话以及和狄欧蒂玛的对话。 在和阿伽通的对话里，他说明了： （一）爱情必有对象； （二）钟爱者还没有得到所爱的对象； （三）爱情就是想占有所爱对象那一个欲望； （四）爱情的对象既然是美，如阿伽通所说的，它就还缺乏美，“爱神是美的”一说不能成立； （五）美善同一，所以爱神也不是善的。 这样苏格拉底就把阿伽通的一篇大文章完全推翻了。接着他说他的爱情学问是从女巫狄欧蒂玛那里领教来的。他原来和阿伽通一般见解，她纠正了他。她使他明白： （一）爱神是介乎美丑、善恶、有知与无知、神与人之间的一种精灵，是丰富和贫乏的统一，总之，就是一个哲学家； （二）爱情就是想凡是美的善的永远归自己所有那一个欲望； （三）爱情的目的是在美的对象中传播种子，凭它孕育生殖，达到凡人所能享有的不朽：生殖就是以新代旧，种族与个体都时时刻刻在生灭流转中。这种生殖可以是身体的，也可以是心灵的。诗人、立法者、教育者以及一切创造者都是心灵方面的生殖者； （四）爱情的深密教，也就是达到哲学极境的四大步骤。 阿尔基比亚德：畸形之爱 醉酒阿尔基比亚德（对应开头提到的酒神）的胡言，他爱着苏格拉底却又自惭形秽。爱情的美好没有让他变美好，反而让他与美好背道而驰，离爱越来越远，他矛盾而纠结，爱在折磨着他。 尾声 柏拉图的爱情观：爱是阶梯式的，肉体之爱会让精神之爱更崇高，二者不能单一存在，否则偏向肉体则堕落，偏向精神则虚渺。(寻找本源的行为。) Ref 会饮篇 - 维基百科，自由的百科全书 如何理解柏拉图的《会饮篇》？ - 知乎 我希望你的目的地是星辰大海 【罗翔老师直播课堂】共读柏拉图的《会饮篇》_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili 我们所有的学习都是在回忆 阿尔基比亚德在看到苏格拉底身上看到他原初的美好，现在的匮乏，将来的美好。 \"人最大的痛苦是无法跨越知道和做到的鸿沟\" 最终在战争中背叛了雅典 有追求者介于有知和无知之间，神有知，所以不需要追求，愚人无知，所以不知道要去追求，只有介于二者之间的人才会去追求知识追求美好，这就是追求者，每一个为梦想努力的人就是这样的人。 爱他人就是爱自己。因为爱他人时，你爱上的那些优点和美好，其实都印证了自身。也许过去自己拥有这些美好如今失去了，所以我们爱；也许现在我们正拥有这些美好，我们惺惺相惜所以爱；也许我们看到了将来的自己，自己想要成为的样子，想要拥有的特质，所以我们爱。自恋所恋，其实是恋着自身过去、现在、将来的美好。 所以上次在读书会的时候我给学生说了这样一段话： 我说很少有那个词会像爱一样被庸俗对待，但是爱是可以承载真正的严肃和祟高，我们爱因为我们匮乏，我们爱因为我们希望超越每日的锱铢必较，在爱中我们放弃了自我，我们发现了自我，我们希望重塑我们的自我。 所以整个会饮篇，他提醒我们肉体的爱当然是有意义的，但人不要沉迷于肉体的爱，人要扶级而上，去追求灵魂的爱，去追求更好的爱。因为我们要回忆起我们失落的美好，我们就可以告别，我们当下的平庸苟且，我们要攀登美善的阶梯，一步一步往上爬。 这样，我们才能够达到一种爱的升华，我们才能在一个具体的人中，去发现我们，对于所有人美好的期待，就是这种抽象的爱。因为具体的人，我们在他身上看到了，我们想看到的样子，我们希望变得美好，当我们变得美好的时候，我们又会和具体的人，在具体的人身上，我们又会经营我们的美好。","categories":[{"name":"文渊阁","slug":"文渊阁","permalink":"https://xiang578.com/categories/文渊阁/"}],"tags":[{"name":"book","slug":"book","permalink":"https://xiang578.com/tags/book/"}]},{"title":"深入浅出 BERT 源代码之 BertModel 类","slug":"all-about-bert-code","date":"2020-10-03T12:56:43.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/all-about-bert-code.html","link":"","permalink":"https://xiang578.com/post/all-about-bert-code.html","excerpt":"国庆节前突然对如何计算 BERT 的参数量感兴趣，不过一直看不明白网上的计算过程，索性下载 BERT 源代码阅读一番。这篇文章记录阅读 BertModel 类（核心代码实现）时写的一些笔记，反正我也是纸上谈兵，所以不需要太关注数据处理和 Finetune 相关部分，最后附上计算 BERT 参数量的过程仅供参考。","text":"国庆节前突然对如何计算 BERT 的参数量感兴趣，不过一直看不明白网上的计算过程，索性下载 BERT 源代码阅读一番。这篇文章记录阅读 BertModel 类（核心代码实现）时写的一些笔记，反正我也是纸上谈兵，所以不需要太关注数据处理和 Finetune 相关部分，最后附上计算 BERT 参数量的过程仅供参考。 代码地址：bert/modeling.py at master · google-research/bert BertConfig 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class BertConfig(object): \"\"\"Configuration for `BertModel`.\"\"\" def __init__(self, vocab_size, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=\"gelu\", hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=16, initializer_range=0.02): self.vocab_size = vocab_size self.hidden_size = hidden_size self.num_hidden_layers = num_hidden_layers self.num_attention_heads = num_attention_heads self.hidden_act = hidden_act self.intermediate_size = intermediate_size self.hidden_dropout_prob = hidden_dropout_prob self.attention_probs_dropout_prob = attention_probs_dropout_prob self.max_position_embeddings = max_position_embeddings self.type_vocab_size = type_vocab_size self.initializer_range = initializer_range @classmethod def from_dict(cls, json_object): \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\" config = BertConfig(vocab_size=None) for (key, value) in six.iteritems(json_object): config.__dict__[key] = value return config @classmethod def from_json_file(cls, json_file): \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\" with tf.gfile.GFile(json_file, \"r\") as reader: text = reader.read() return cls.from_dict(json.loads(text)) def to_dict(self): \"\"\"Serializes this instance to a Python dictionary.\"\"\" output = copy.deepcopy(self.__dict__) return output def to_json_string(self): \"\"\"Serializes this instance to a JSON string.\"\"\" return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\" BertConfig 类包含模型参数、几个读取和存储参数的方法。 @classmethod 代表类方法，不需要实例化就可以调用类中的方法。参考其他的文件可以发现它的使用是： 1bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file) 主要参数有： vocab_size: 词表大小 hidden_size: Size of the encoder layers and the pooler layer. 词向量 embedding 大小 num_hidden_layers: Number of hidden layers in the Transformer encoder. 层数 num_attention_heads: Number of attention heads for each attention layer in the Transformer encoder. 多头数量 intermediate_size: The size of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder. FFN 中间层的大小 hidden_act: The non-linear activation function (function or string) in the encoder and pooler. 激活函数 hidden_dropout_prob: The dropout probability for all fully connected layers in the embeddings, encoder, and pooler. dropout 参数 attention_probs_dropout_prob: The dropout ratio for the attention probabilities. max_position_embeddings: position embedding 的最大值 (e.g., 512 or 1024 or 2048). type_vocab_size: next sentence prediction 中的 Segment A 和 Segment B，默认大小是 2 initializer_range: The stdev of the truncated_normal_initializer for initializing all weight matrices. \"\"\" embedding_lookup 根据 input_ids 生成词向量 embedding table 以及对应的 input_id_embeddings。简单一点理解就是向量从 [batch_size, seq_size] 到 [batch_size, seq_size，embedding_size]。 1234567891011121314151617181920212223242526272829303132333435363738394041424344def embedding_lookup(input_ids, vocab_size, embedding_size=128, initializer_range=0.02, word_embedding_name=\"word_embeddings\", use_one_hot_embeddings=False): \"\"\"Looks up words embeddings for id tensor. Args: input_ids: int32 Tensor of shape [batch_size, seq_length] containing word ids. vocab_size: int. Size of the embedding vocabulary. embedding_size: int. Width of the word embeddings. initializer_range: float. Embedding initialization range. word_embedding_name: string. Name of the embedding table. use_one_hot_embeddings: bool. If True, use one-hot method for word embeddings. If False, use `tf.gather()`. Returns: float Tensor of shape [batch_size, seq_length, embedding_size]. \"\"\" # This function assumes that the input is of shape [batch_size, seq_length, # num_inputs]. # # If the input is a 2D tensor of shape [batch_size, seq_length], we # reshape to [batch_size, seq_length, 1]. if input_ids.shape.ndims == 2: input_ids = tf.expand_dims(input_ids, axis=[-1]) embedding_table = tf.get_variable( name=word_embedding_name, shape=[vocab_size, embedding_size], initializer=create_initializer(initializer_range)) flat_input_ids = tf.reshape(input_ids, [-1]) if use_one_hot_embeddings: one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size) output = tf.matmul(one_hot_input_ids, embedding_table) else: output = tf.gather(embedding_table, flat_input_ids) input_shape = get_shape_list(input_ids) output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size]) return (output, embedding_table) 从 embedding_table 取 input_ids 对应的 embedding 有两种方法： 矩阵乘法：先通过 input_ids 构造出 one_hot 矩阵，然后和 embedding_table 相乘得到结果。 tf.gather 根据 input_ids 取 embedding_table 对应行的结果。和 tf.nn.embedding_lookup 方法类似。具体原理可以参考 python - What does tf.nn.embedding_lookup function do? - Stack Overflow 看网上的解释，定义两种方法主要是不同设备（CPU、GPU、TPU）运算速度导致的。 embedding_postprocessor embedding_postprocessor 将 token embeddings segmentation embeddings position embeddings 三个向量相加得到最终的输入向量。 token embeddings 对应单词 embedding segmentation embeddings 代表单词来自哪个句子，在 Next Sentence Prediction 任务中使用。 position embeddings 位置 embedding。在「Attention is all your need」论文中，Google 生成 position embedding 的方法是一个花里胡哨 cos/sin 公式，这一次换成训练 position embedding。猜测在之前的论文中，输入的 seq len 可能长短不一，导致部分 position embedding 训练不充分。BERT 中强行定死 seq len。 最后直接将三个 embedding 相加，可能对新人来说也有点迷惑。我自己的理解是，物理中多个不同波长的波叠加，是可以通过方法区分的。所以三个 embedding 相加，模型也能学到差异。 知乎这个问题为什么 Bert 的三个 Embedding 可以进行相加可以提供更加严谨的理由。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394def embedding_postprocessor(input_tensor, use_token_type=False, token_type_ids=None, token_type_vocab_size=16, token_type_embedding_name=\"token_type_embeddings\", use_position_embeddings=True, position_embedding_name=\"position_embeddings\", initializer_range=0.02, max_position_embeddings=512, dropout_prob=0.1): \"\"\"Performs various post-processing on a word embedding tensor. Args: input_tensor: float Tensor of shape [batch_size, seq_length, embedding_size]. use_token_type: bool. Whether to add embeddings for `token_type_ids`. token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length]. Must be specified if `use_token_type` is True. token_type_vocab_size: int. The vocabulary size of `token_type_ids`. token_type_embedding_name: string. The name of the embedding table variable for token type ids. use_position_embeddings: bool. Whether to add position embeddings for the position of each token in the sequence. position_embedding_name: string. The name of the embedding table variable for positional embeddings. initializer_range: float. Range of the weight initialization. max_position_embeddings: int. Maximum sequence length that might ever be used with this model. This can be longer than the sequence length of input_tensor, but cannot be shorter. dropout_prob: float. Dropout probability applied to the final output tensor. Returns: float tensor with same shape as `input_tensor`. Raises: ValueError: One of the tensor shapes or input values is invalid. \"\"\" input_shape = get_shape_list(input_tensor, expected_rank=3) batch_size = input_shape[0] seq_length = input_shape[1] width = input_shape[2] output = input_tensor # 是否使用有 segmentation embeddings if use_token_type: if token_type_ids is None: raise ValueError(\"`token_type_ids` must be specified if\" \"`use_token_type` is True.\") token_type_table = tf.get_variable( name=token_type_embedding_name, shape=[token_type_vocab_size, width], initializer=create_initializer(initializer_range)) # segmentation vocab 大小一般是 2，所以使用 one-hot 速度比较快 flat_token_type_ids = tf.reshape(token_type_ids, [-1]) one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size) token_type_embeddings = tf.matmul(one_hot_ids, token_type_table) token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width]) output += token_type_embeddings if use_position_embeddings: assert_op = tf.assert_less_equal(seq_length, max_position_embeddings) with tf.control_dependencies([assert_op]): full_position_embeddings = tf.get_variable( name=position_embedding_name, shape=[max_position_embeddings, width], initializer=create_initializer(initializer_range)) # Since the position embedding table is a learned variable, we create it # using a (long) sequence length `max_position_embeddings`. The actual # sequence length might be shorter than this, for faster training of # tasks that do not have long sequences. # # So `full_position_embeddings` is effectively an embedding table # for position [0, 1, 2, ..., max_position_embeddings-1], and the current # sequence has positions [0, 1, 2, ... seq_length-1], so we can just # perform a slice. # position embedding 可以通过学习得到，然后可能输入句子的长度没有到达 512。使用 tf.slice 取对应的向量速度比较快。大小是[seq_length, width] position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1]) num_dims = len(output.shape.as_list()) # Only the last two dimensions are relevant (`seq_length` and `width`), so # we broadcast among the first dimensions, which is typically just # the batch size. # word embedding 的大小是 [batch_size, seq_length, width]，上一步取出的 position embedding 大小是 [seq_length, width]，需要对后面一个矩阵进行广播。 position_broadcast_shape = [] for _ in range(num_dims - 2): position_broadcast_shape.append(1) position_broadcast_shape.extend([seq_length, width]) # 大小为 [1, seq_length, width] position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape) # 通过 broadcast 相加 output += position_embeddings output = layer_norm_and_dropout(output, dropout_prob) return output 三个 embedding 向量相加后，还会过一个 layer_norm_and_dropout 层，都是标准的，没有什么特殊。 123456789101112131415161718192021222324252627def dropout(input_tensor, dropout_prob): \"\"\"Perform dropout. Args: input_tensor: float Tensor. dropout_prob: Python float. The probability of dropping out a value (NOT of *keeping* a dimension as in `tf.nn.dropout`). Returns: A version of `input_tensor` with dropout applied. \"\"\" if dropout_prob is None or dropout_prob == 0.0: return input_tensor output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob) return outputdef layer_norm(input_tensor, name=None): \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\" return tf.contrib.layers.layer_norm( inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)def layer_norm_and_dropout(input_tensor, dropout_prob, name=None): \"\"\"Runs layer normalization followed by dropout.\"\"\" output_tensor = layer_norm(input_tensor, name) output_tensor = dropout(output_tensor, dropout_prob) return output_tensor create_attention_mask_from_input_mask create_attention_mask_from_input_mask 用来构造 attention 时的 mask 矩阵（padding 的单词不参与计算 attention socre）。输入向量 [batch_size, from_seq_length, ...] 和 [batch_size, to_seq_length] 输出向量 [batch_size, from_seq_length, to_seq_length]。 偷个例子来举： 12345678910111213from_tensor = tf.constant([[1,2,3,0,0], [1,3,5,6,1]]) # 中间的 0 代表 padding 的结果to_mask = tf.constant([[1,1,1,0,0], [1,1,1,1,1]]) # 和 from_tensor 对应。如果 1 代表对应位置有词，如果 0 代表对应位置是 padding 的。to_mask = tf.cast(tf.reshape(to_mask, [2, 1, 5]), tf.float32)# print(to_mask_2)broadcast_ones = tf.ones( shape=[2, 5, 1], dtype=tf.float32)mask = broadcast_ones * to_maskinit = tf.global_variables_initializer()with tf.Session() as sess: # print(sess.run(to_mask)) print(sess.run(mask)) 最后的结果是 1234567891011[[[1. 1. 1. 0. 0.] #第一个词可以和前三个词计算 attention [1. 1. 1. 0. 0.] [1. 1. 1. 0. 0.] [1. 1. 1. 0. 0.] [1. 1. 1. 0. 0.]] [[1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.]]] 12345678910111213141516171819202122232425262728293031def create_attention_mask_from_input_mask(from_tensor, to_mask): \"\"\"Create 3D attention mask from a 2D tensor mask. Args: from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...]. to_mask: int32 Tensor of shape [batch_size, to_seq_length]. Returns: float Tensor of shape [batch_size, from_seq_length, to_seq_length]. \"\"\" from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) batch_size = from_shape[0] from_seq_length = from_shape[1] to_shape = get_shape_list(to_mask, expected_rank=2) to_seq_length = to_shape[1] to_mask = tf.cast( tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32) # We don't assume that `from_tensor` is a mask (although it could be). We # don't actually care if we attend *from* padding tokens (only *to* padding) # tokens so we create a tensor of all ones. # # `broadcast_ones` = [batch_size, from_seq_length, 1] broadcast_ones = tf.ones( shape=[batch_size, from_seq_length, 1], dtype=tf.float32) # Here we broadcast along two dimensions to create the mask. # 广播得到最后的 mask 矩阵 [batch_size, from_seq_length, to_seq_length] mask = broadcast_ones * to_mask return mask transformer_model 顾名思议 BERT 最核心的 Multi-headed, multi-layer Transformer 实现过程。Attention is all you need 中的实现在 链接 一个 Transformer 的示意图： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144def transformer_model(input_tensor, attention_mask=None, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, intermediate_act_fn=gelu, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, initializer_range=0.02, do_return_all_layers=False): \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\". This is almost an exact implementation of the original Transformer encoder. See the original paper: https://arxiv.org/abs/1706.03762 Also see: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py Args: input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size]. attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length, seq_length], with 1 for positions that can be attended to and 0 in positions that should not be. 就是前面 create_attention_mask_from_input_mask 产出的结果 hidden_size: int. Hidden size of the Transformer. num_hidden_layers: int. Number of layers (blocks) in the Transformer. num_attention_heads: int. Number of attention heads in the Transformer. intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed forward) layer. intermediate_act_fn: function. The non-linear activation function to apply to the output of the intermediate/feed-forward layer. hidden_dropout_prob: float. Dropout probability for the hidden layers. attention_probs_dropout_prob: float. Dropout probability of the attention probabilities. initializer_range: float. Range of the initializer (stddev of truncated normal). do_return_all_layers: Whether to also return all layers or just the final layer. Returns: float Tensor of shape [batch_size, seq_length, hidden_size], the final hidden layer of the Transformer. Raises: ValueError: A Tensor shape or parameter is invalid. \"\"\" # 最终输出的 hidden_size 能被 num_attention_heads 整除 if hidden_size % num_attention_heads != 0: raise ValueError( \"The hidden size (%d) is not a multiple of the number of attention \" \"heads (%d)\" % (hidden_size, num_attention_heads)) # 定义 attention 每个输出的头的大小 # 最后结果 concat 之后和原始输入大小相同。 attention_head_size = int(hidden_size / num_attention_heads) input_shape = get_shape_list(input_tensor, expected_rank=3) batch_size = input_shape[0] seq_length = input_shape[1] input_width = input_shape[2] # The Transformer performs sum residuals on all layers so the input needs # to be the same as the hidden size. # Transformer 中有残差连接，所以输入和输出 embedding size 要相同 if input_width != hidden_size: raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" % (input_width, hidden_size)) # We keep the representation as a 2D tensor to avoid re-shaping it back and # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on # the GPU/CPU but may not be free on the TPU, so we want to minimize them to # help the optimizer. # TPU 不擅长 reshape 操作，所以把所有的 3D tensor 变成 2D tensor prev_output = reshape_to_matrix(input_tensor) all_layer_outputs = [] # 遍历多层 for layer_idx in range(num_hidden_layers): with tf.variable_scope(\"layer_%d\" % layer_idx): layer_input = prev_output with tf.variable_scope(\"attention\"): attention_heads = [] with tf.variable_scope(\"self\"): attention_head = attention_layer( from_tensor=layer_input, to_tensor=layer_input, attention_mask=attention_mask, num_attention_heads=num_attention_heads, size_per_head=attention_head_size, attention_probs_dropout_prob=attention_probs_dropout_prob, initializer_range=initializer_range, do_return_2d_tensor=True, batch_size=batch_size, from_seq_length=seq_length, to_seq_length=seq_length) attention_heads.append(attention_head) attention_output = None if len(attention_heads) == 1: attention_output = attention_heads[0] else: # In the case where we have other sequences, we just concatenate # them to the self-attention head before the projection. # concat 多头的结果 attention_output = tf.concat(attention_heads, axis=-1) # Run a linear projection of `hidden_size` then add a residual # with `layer_input`. 加上残差 with tf.variable_scope(\"output\"): attention_output = tf.layers.dense( attention_output, hidden_size, kernel_initializer=create_initializer(initializer_range)) # dropout 和 layer_norm attention_output = dropout(attention_output, hidden_dropout_prob) attention_output = layer_norm(attention_output + layer_input) # 全连接层 # The activation is only applied to the \"intermediate\" hidden layer. with tf.variable_scope(\"intermediate\"): intermediate_output = tf.layers.dense( attention_output, intermediate_size, activation=intermediate_act_fn, kernel_initializer=create_initializer(initializer_range)) # 变回原来的大小，才能加上残差 # Down-project back to `hidden_size` then add the residual. with tf.variable_scope(\"output\"): layer_output = tf.layers.dense( intermediate_output, hidden_size, kernel_initializer=create_initializer(initializer_range)) layer_output = dropout(layer_output, hidden_dropout_prob) layer_output = layer_norm(layer_output + attention_output) prev_output = layer_output all_layer_outputs.append(layer_output) # 是不是要输出中间结果 if do_return_all_layers: final_outputs = [] for layer_output in all_layer_outputs: final_output = reshape_from_matrix(layer_output, input_shape) final_outputs.append(final_output) return final_outputs else: final_output = reshape_from_matrix(prev_output, input_shape) return final_output attention_layer attention_layer 中实现 self-attention 和 multi-head，细节在 「Attention is all your need」里面有。query_layer 由 from_tensor 得到，key_layer 和 value_layer 由 to_tensor 得到。由于是 self-attention-encoder，from_tensor 和 to_tensor 相同。 示意图： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190def attention_layer(from_tensor, to_tensor, attention_mask=None, num_attention_heads=1, size_per_head=512, query_act=None, key_act=None, value_act=None, attention_probs_dropout_prob=0.0, initializer_range=0.02, do_return_2d_tensor=False, batch_size=None, from_seq_length=None, to_seq_length=None): \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`. This is an implementation of multi-headed attention based on \"Attention is all you Need\". If `from_tensor` and `to_tensor` are the same, then this is self-attention. Each timestep in `from_tensor` attends to the corresponding sequence in `to_tensor`, and returns a fixed-with vector. This function first projects `from_tensor` into a \"query\" tensor and `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list of tensors of length `num_attention_heads`, where each tensor is of shape [batch_size, seq_length, size_per_head]. Then, the query and key tensors are dot-producted and scaled. These are softmaxed to obtain attention probabilities. The value tensors are then interpolated by these probabilities, then concatenated back to a single tensor and returned. In practice, the multi-headed attention are done with transposes and reshapes rather than actual separate tensors. Args: from_tensor: float Tensor of shape [batch_size, from_seq_length, from_width]. to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width]. attention_mask: (optional) int32 Tensor of shape [batch_size, from_seq_length, to_seq_length]. The values should be 1 or 0. The attention scores will effectively be set to -infinity for any positions in the mask that are 0, and will be unchanged for positions that are 1. num_attention_heads: int. Number of attention heads. size_per_head: int. Size of each attention head. query_act: (optional) Activation function for the query transform. key_act: (optional) Activation function for the key transform. value_act: (optional) Activation function for the value transform. attention_probs_dropout_prob: (optional) float. Dropout probability of the attention probabilities. initializer_range: float. Range of the weight initializer. do_return_2d_tensor: bool. If True, the output will be of shape [batch_size * from_seq_length, num_attention_heads * size_per_head]. If False, the output will be of shape [batch_size, from_seq_length, num_attention_heads * size_per_head]. batch_size: (Optional) int. If the input is 2D, this might be the batch size of the 3D version of the `from_tensor` and `to_tensor`. from_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `from_tensor`. to_seq_length: (Optional) If the input is 2D, this might be the seq length of the 3D version of the `to_tensor`. Returns: float Tensor of shape [batch_size, from_seq_length, num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is true, this will be of shape [batch_size * from_seq_length, num_attention_heads * size_per_head]). Raises: ValueError: Any of the arguments or tensor shapes are invalid. \"\"\" def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width): output_tensor = tf.reshape( input_tensor, [batch_size, seq_length, num_attention_heads, width]) output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3]) #[batch_size, num_attention_heads, seq_length, width] return output_tensor from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) to_shape = get_shape_list(to_tensor, expected_rank=[2, 3]) if len(from_shape) != len(to_shape): raise ValueError( \"The rank of `from_tensor` must match the rank of `to_tensor`.\") if len(from_shape) == 3: batch_size = from_shape[0] from_seq_length = from_shape[1] to_seq_length = to_shape[1] elif len(from_shape) == 2: if (batch_size is None or from_seq_length is None or to_seq_length is None): raise ValueError( \"When passing in rank 2 tensors to attention_layer, the values \" \"for `batch_size`, `from_seq_length`, and `to_seq_length` \" \"must all be specified.\") # Scalar dimensions referenced here: # B = batch size (number of sequences) # F = `from_tensor` sequence length 输入单词长度 # T = `to_tensor` sequence length 输出单词长度 # N = `num_attention_heads` # H = `size_per_head` from_tensor_2d = reshape_to_matrix(from_tensor) # [B*F，hidden_size=N*H] to_tensor_2d = reshape_to_matrix(to_tensor) # [B*T，head_size=N*H] # `query_layer` = [B*F, N*H] 从 from_tensor 得到 query_layer query_layer = tf.layers.dense( from_tensor_2d, num_attention_heads * size_per_head, activation=query_act, name=\"query\", kernel_initializer=create_initializer(initializer_range)) # `key_layer` = [B*T, N*H] key_layer = tf.layers.dense( to_tensor_2d, num_attention_heads * size_per_head, activation=key_act, name=\"key\", kernel_initializer=create_initializer(initializer_range)) # `value_layer` = [B*T, N*H] value_layer = tf.layers.dense( to_tensor_2d, num_attention_heads * size_per_head, activation=value_act, name=\"value\", kernel_initializer=create_initializer(initializer_range)) # 计算多头调整 tensor shape，都是为了方便计算.变成 [batch_size, num_attention_heads, seq_length, width] # `query_layer` = [B, N, F, H] query_layer = transpose_for_scores(query_layer, batch_size, num_attention_heads, from_seq_length, size_per_head) # `key_layer` = [B, N, T, H] key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head) # Take the dot product between \"query\" and \"key\" to get the raw # attention scores. # `attention_scores` = [B, N, F, T] =&gt; [F, H] * [H, T] = [F, T] attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True) attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head))) # 经典缩小 scroe 值，防止落到 softmask 梯度饱和区 # 处理 padding 部分的 score 值， padding 为 0 的在对应的位置上加上 -10000.0， 这样求 exp 之后就是一个接近于 0 的值 if attention_mask is not None: # `attention_mask` = [B, 1, F, T] attention_mask = tf.expand_dims(attention_mask, axis=[1]) # Since attention_mask is 1.0 for positions we want to attend and 0.0 for # masked positions, this operation will create a tensor which is 0.0 for # positions we want to attend and -10000.0 for masked positions. adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0 # Since we are adding it to the raw scores before the softmax, this is # effectively the same as removing these entirely. attention_scores += adder # Normalize the attention scores to probabilities. # `attention_probs` = [B, N, F, T] attention_probs = tf.nn.softmax(attention_scores) # This is actually dropping out entire tokens to attend to, which might # seem a bit unusual, but is taken from the original Transformer paper. attention_probs = dropout(attention_probs, attention_probs_dropout_prob) # `value_layer` = [B, T, N, H] value_layer = tf.reshape( value_layer, [batch_size, to_seq_length, num_attention_heads, size_per_head]) # `value_layer` = [B, N, T, H] value_layer = tf.transpose(value_layer, [0, 2, 1, 3]) # attention 之后的结果 # `context_layer` = [B, N, F, H] context_layer = tf.matmul(attention_probs, value_layer) # `context_layer` = [B, F, N, H] context_layer = tf.transpose(context_layer, [0, 2, 1, 3]) if do_return_2d_tensor: # `context_layer` = [B*F, N*H] context_layer = tf.reshape( context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head]) else: # `context_layer` = [B, F, N*H] context_layer = tf.reshape( context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head]) return context_layer BertModel 构造类 init 方法就是将上面的内容串联起来。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None, use_one_hot_embeddings=False, scope=None): \"\"\"Constructor for BertModel. Args: config: `BertConfig` instance. is_training: bool. true for training model, false for eval model. Controls whether dropout will be applied. input_ids: int32 Tensor of shape [batch_size, seq_length]. input_mask: (optional) int32 Tensor of shape [batch_size, seq_length]. token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length]. use_one_hot_embeddings: (optional) bool. Whether to use one-hot word embeddings or tf.embedding_lookup() for the word embeddings. scope: (optional) variable scope. Defaults to \"bert\". Raises: ValueError: The config is invalid or one of the input tensor shapes is invalid. \"\"\" config = copy.deepcopy(config) if not is_training: config.hidden_dropout_prob = 0.0 config.attention_probs_dropout_prob = 0.0 input_shape = get_shape_list(input_ids, expected_rank=2) batch_size = input_shape[0] seq_length = input_shape[1] if input_mask is None: input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32) # 处理 embedding if token_type_ids is None: token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32) with tf.variable_scope(scope, default_name=\"bert\"): with tf.variable_scope(\"embeddings\"): # Perform embedding lookup on the word ids. (self.embedding_output, self.embedding_table) = embedding_lookup( input_ids=input_ids, vocab_size=config.vocab_size, embedding_size=config.hidden_size, initializer_range=config.initializer_range, word_embedding_name=\"word_embeddings\", use_one_hot_embeddings=use_one_hot_embeddings) # Add positional embeddings and token type embeddings, then layer # normalize and perform dropout. self.embedding_output = embedding_postprocessor( input_tensor=self.embedding_output, use_token_type=True, token_type_ids=token_type_ids, token_type_vocab_size=config.type_vocab_size, token_type_embedding_name=\"token_type_embeddings\", use_position_embeddings=True, position_embedding_name=\"position_embeddings\", initializer_range=config.initializer_range, max_position_embeddings=config.max_position_embeddings, dropout_prob=config.hidden_dropout_prob) with tf.variable_scope(\"encoder\"): # This converts a 2D mask of shape [batch_size, seq_length] to a 3D # mask of shape [batch_size, seq_length, seq_length] which is used # for the attention scores. 获得 attention_mask attention_mask = create_attention_mask_from_input_mask( input_ids, input_mask) # Run the stacked transformer. 计算 transformer 的结果 # `sequence_output` shape = [batch_size, seq_length, hidden_size]. self.all_encoder_layers = transformer_model( input_tensor=self.embedding_output, attention_mask=attention_mask, hidden_size=config.hidden_size, num_hidden_layers=config.num_hidden_layers, num_attention_heads=config.num_attention_heads, intermediate_size=config.intermediate_size, intermediate_act_fn=get_activation(config.hidden_act), hidden_dropout_prob=config.hidden_dropout_prob, attention_probs_dropout_prob=config.attention_probs_dropout_prob, initializer_range=config.initializer_range, do_return_all_layers=True) self.sequence_output = self.all_encoder_layers[-1] # The \"pooler\" converts the encoded sequence tensor of shape # [batch_size, seq_length, hidden_size] to a tensor of shape # [batch_size, hidden_size]. This is necessary for segment-level # (or segment-pair-level) classification tasks where we need a fixed # dimensional representation of the segment. # 分类任务取第一个 [CLS] 对应的 embedding 值 with tf.variable_scope(\"pooler\"): # We \"pool\" the model by simply taking the hidden state corresponding # to the first token. We assume that this has been pre-trained first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1) self.pooled_output = tf.layers.dense( first_token_tensor, config.hidden_size, activation=tf.tanh, kernel_initializer=create_initializer(config.initializer_range)) 模型使用 123456789101112131415# Already been converted into WordPiece token idsinput_ids = tf.constant([[31, 51, 99], [15, 5, 0]])input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])config = modeling.BertConfig(vocab_size=32000, hidden_size=512, num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)# 调用模型model = modeling.BertModel(config=config, is_training=True, input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)label_embeddings = tf.get_variable(...)pooled_output = model.get_pooled_output()logits = tf.matmul(pooled_output, label_embeddings)... Bert 参数量计算 回到写这篇文章的起点，最后通过计算 \\(BERT_{BASE}\\) 的参数量，加深对模型的理解。论文介绍 Layer = 12，Hidden Size = 768，multi head = 12，参数量是 110M 左右。 总的计算公式为 (30522 + 512 + 2)*768 + 768*2 + (3*768*64*12 + 3*64*12 + 64*768*12 + 768 + 768 + 768 + 768*3072 + 3072 + 3072*768 + 768 + 768 + 768) * 12 = 108891648 embedding 部分 (30522 + 512 + 2)*768 + 768*2 embedding size = 768 单词数仅有 30522，比起 CTR 几千万的物品还是少很多。 position size = 512 sentence size = 2 三个 embedding 相加后 Norm 的参数 2 multi attention 部分 (3*768*64*12 + 3*64*12 + 64*768*12 + 768 + 768 + 768 + 768*3072 + 3072 + 3072*768 + 768 + 768 + 768) * 12 一共是 12 层，对应 12 个 Transformer 3*768*64*12 + 3*64*12 12 个 multi-head 对应的 Q K V 参数 64*768*12 + 768 + 768 + 768 multi-head 结果 concat 之后接的全连接层参数以及后面的 norm 768*3072 + 3072 + 3072*768 + 768 + 768 + 768 FFN 以及 norm 的参数 Ref BERT代码阅读 - 李理的博客 BERT encoder参数量计算 | 417's blog BERT源码分析PART I - 知乎","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://xiang578.com/tags/nlp/"},{"name":"google","slug":"google","permalink":"https://xiang578.com/tags/google/"},{"name":"bert","slug":"bert","permalink":"https://xiang578.com/tags/bert/"},{"name":"code","slug":"code","permalink":"https://xiang578.com/tags/code/"},{"name":"transformer","slug":"transformer","permalink":"https://xiang578.com/tags/transformer/"}]},{"title":"Never-Reading 202008 选择记忆","slug":"Never-Reading-202008","date":"2020-09-13T15:55:11.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/Never-Reading-202008.html","link":"","permalink":"https://xiang578.com/post/Never-Reading-202008.html","excerpt":"某一刻突然意识到可以选择自己的记忆，我顿悟了。","text":"某一刻突然意识到可以选择自己的记忆，我顿悟了。 Roam CN 聚会 8 月 29 日参与由 Jessie@FG发起一次北京 RoamCN 微信群聚会，其他到场的还有 pimgeek、Flynn 等 7 位群友。聚会中大家讨论和知识管理、Roam Research 相关的内容。记录一些给我灵感的内容： Anki 做为一个知识管理爱好者，很难没有听说过 Anki。这一次线下的相关内容讨论，给我留下的最大感受就是本期标题的由来「选择记忆」。Flynn 介绍在 Michael Nielsen 的 Augmenting Long-term Memory ，一直坚持使用 Anki，他举的例子是「可以记住全部的正则匹配」。和绝大部分人一样，我记忆内容的方式就是看一遍，然后等到要用的时候，再去查相关的内容。Anki 可以自己制作需要记忆内容的卡片，这个过程不就是选择自己的记忆吗？其实几年前也看过 Michael Nielsen 的文章，可能是当时境界太低，没有理解选择记忆对自己的重要性。现在觉得有几点很重要： 自己制作卡片，网上下载的卡片没有灵魂。 卡片尽量原子化，使用 QA 形式。原子化保证没一张可以快速过去，QA 形式费曼技巧，防止假懂。 坚持使用。 等到自制卡片能到达 1k 张，再来具体分享自己的体验。 Michael Nielsen 的文章实在是太长了，先分享第一部分的摘录： 之前看过中文翻译但是没有实践 【三万字长文】量子物理学家是如何使用 Anki 的？ - 知乎 Solomon Shereshevsky 以超级记忆力闻名 memex 外部记忆机器，汇总全部个人资料 [[Douglas Engelbart]] augmentation of human intelligence [[Ted Nelson]] [[Project Xanadu]] Tim Berners-Lee world wide web Anki makes memory a choice, rather than a haphazard event, to be left to chance. But, as we shall see, there are already powerful ideas about personal memory systems based solely on the structuring and presentation of information. 从信息组织和展示的角度入手。 Anki 卡片之间的复习间隔时间 一张卡片在 20 年间需要花费 4-7 分钟去记忆 制作卡片的标准，Anki make memory a choice：你可以选择自己记忆的内容 if memorizing a fact seems worth 10 minutes of my time in the future, then I do it superseding the first, if a fact seems striking then into Anki it goes, regardless of whether it seems worth 10 minutes of my future time or not. 尝试使用 Anki 很不容易，通过学习 Unix 命令掌握需要的技巧。可以把之前 [[Vim 实用技巧]] 拆解到 Anki 中。 用 QA 的形式来使用 Anki Using Anki to thoroughly read a research paper in an unfamiliar field [[AlphaGo]]还记得多少？ 举例的问题 “What's the size of a Go board?”; “Who plays first in Go?”; “How many human game positions did AlphaGo learn from?”; “Where did AlphaGo get its training data?”; “What were the names of the two main types of neural network AlphaGo used?” 多轮记录问题 通过一年之后阅读 [[AlphaGo Zero]] 检验记忆效果 I find Anki works much better when used in service to some personal creative project. 为了一个明确目标去设置 Anki 的问题，没有温度。 when I'm reading in support of some creative project, I ask much better Anki questions. Using Anki to do shallow reads of papers [how to read a paper] 基于主题去阅读论文，读重要的论文。 选择性阅读，记录关键。I'll add to Anki questions about the core claims, core questions, and core ideas of the paper. 一篇论文 5 到 20 个问题。 避免 Anki 化有误导性的条目，仔细选择提问的方法。 “What does Jones 2011 claim is the average age at which physics Nobelists made their prizewinning discovery, over 1980-2011?” (Answer: 48). “Which paper claimed that physics Nobelists made their prizewinning discovery at average age 48, over the period 1980-2011?” (Answer: Jones 2011). Ankifying figures：知道某张图存在，然后参考。 曲线大概走向？ 图表细节 Syntopic reading using Anki 从 key paper 出发，best 5-10 paper，普通的论文也有助于认识整个领域。 Anki 从设计上不是为了创造性工作而生。 Anki 创造理解新领域的机会。 More patterns of Anki use Effective learning: Twenty rules of formulating knowledge 原子化 Make most Anki questions and answers as atomic as possible How to create a soft link from linkname to filename? ln -s filename linkname 根据答案分解成两部分。 回答错原子化问题，能更清楚在哪一方面不足。 追求大师级使用 Anki use is best thought of as a virtuoso skill, to be developed: 软件简单，但是功能强大。 你就是创造者。 一个卡组 Use one big deck 不同知识混合在一起，会产生意想不到的结果。 避免临时感兴趣的话题 Avoid orphan questions 95% of Anki's value comes from 5% of the features Using Anki for APIs, books, videos, seminars, conversations, the web, events, and places 研讨会记录 3 个左右高质量的问题。 有选择记忆 unmindfully Ankifying everything in sight is a bad habit Ankify things that serve your long-term goals Anki 之前读过的书或者论文 避免判断题 创造性工作需要内化理解部分知识。 知识的流畅度 Fluency matters in thinking。 为什么 Anki 不火？If personal memory systems are so great, why aren't they more widely used? 人们更喜欢临时报佛脚。In experimental research on memory, people consistently underestimate the gains that come from distributing their study in a manner similar to Anki. Instead, they prefer last-minute cramming, and believe it produces better results, though many studies show it does not. 理想难度原理 The psychologist Robert Bjork has suggested the “principle of desirable difficulty”, the idea that memories are maximally strengthened if tested when we're on the verge of forgetting them. This suggests that an efficient memory system will intrinsically be somewhat difficult to use. Human beings have a complex relationship to difficult activities, and often dislike performing them, unless strongly motivated (in which case they may become pleasurable). 用好很难。Systems such as Anki are challenging to use well, and easy to use poorly. 最后，Michael Nielsen 在文章中提到 AlphaGo，分享一个这个相关的例子： RyenX 算法花园 在 Twitter: \"几周前和大老板开会，他突然问我们一个问题，总结一下就是知识层次。 第一层：alpha go 和 alpha zero 的策略网络是如何训练的？（go 学习获胜的下法，zero 学习分布） 第二层：为什么会产出这样的区别？ 第三层：那么这和我们的业务中什么类似？\" / Twitter ### Zettelkasten 最近中文圈比较火的概念，我也多次在 「Never Reading」分享相关的内容。Flynn 在聚会中也分享自己之前写过的一篇文章 拆解Zettelkasten | 卡片盒知识管理体系实践反思 - Flynn，摘录相关的内容： 工作流和笔记方式 Zettelkasten 认可思考的非线性特征，给出了一种脱离现有顺序框架的方式。 本来就没有一个完整的知识管理工作流。 如何判断 是否在各种情景下都可以使用有效方式捕捉素材？ 是否对待读材料有合适的管理方式？是否能够不定期清空待读材料？ 是否建立了自己的关注领域清单，并且建立对应的项目？ 是否知道如何为任何项目快速建立下一步行动？ 是否有种合理的方式检验自己对知识的理解？ 是否确认现有正在执行系统/习惯能可以长期执行？ 是否确定现有系统在遭遇中断甚至奔溃之后可以很快恢复？ [[Evergreen Note]] 比 Permanent Notes 更合适 以能够直接公布为目的逻辑完备的小文章，并且需要不停的更新他们之间的可能存在的逻辑关系，共同点，冲突点。 笔记可以直接公开，比如 [[Andy Matuschak]] 写作当做最重要的事情 引用 [[Simon Eskildsen]] How to Make Yourself Into a Learning Machine - Superorganizers 以及 notes.andymatuschak.org 从他的理解来看，[[Obsidian]] 比 [[Roam]] 更合适，以及在 RoamCN 聚会时提到的，需要对文字数量进行限制。[[Progressive Summarization]] 和 [[Evergreen Note]] 的生长概念类似。 有限与无限的游戏 有本书对我蛮有影响的——叫做《有限与无限的游戏》。有限游戏在边界内玩，无限游戏却是在和边界，也就是和“规则”玩，探索改变边界本身。实际上只有一个无限游戏，那就是你的人生，死亡是不可逾越的边界。与之相比，其他的边界并不是那么重要了。 人人网、美团网、饭否网创始人王兴 有限游戏有明确的游戏结束目标，无限游戏的目标是让游戏一直进行下去。生活中的很多方面更多接近于无限游戏。比如工作之后的学习，没有明确的目标引导，更多是去保持这个习惯。 本期中脱不花的四个面试题中的驱动力也和这两个游戏有关。文章中介绍到谷歌之前从编程比赛中寻找程序员（回想到自己大学参加 ACM 经历），这些人习惯与在边界内做事情（外部刺激和目标）。而谷歌希望的人更多能根据自己的兴趣不断地拓展边界，打破旧有规则。结合到自己的工作中，很多任务没有之前编程时那么明确的目标，更多时候是依赖自己去寻找问题和解决问题。 另外 谁在驱动滴滴发动机？ 中也提到张博最近在看 《有限与无限的游戏》，互联网下半场可能各个大佬更希望自己的企业基业长青。 其他 还有部分提及的内容还没有时间看，记录一下关键字和大家分享：active recall、spaced repetition、learn how to learn、nstigation habit、Atomic Habits、Mini Habits。 阅读 「个人成长」，知乎上最神奇的「专业领域」 厉害的标准 在一个职级体系中通过竞争的手段达到了高位。 留下传世的精品 让足够多的人改善自己的生活 [[产品沉思录]] Product Thinking】Vol.20200816：[[把自己作为方法]] 他用「蜂鸟般悬浮」来描述当下中国人焦虑的现状 反而对这种「温州乡绅式」的做事方式 他还提到了一种温柔而坚定地力量 —— 认命不认输 所以关键是要把自己所在的社会位置想透。在这个现实下如何去超脱自己的角色，和强大的社会和历史力量持续的较劲，不认输的较劲。 Temporary Social Media 事实证明我还是想的肤浅了、Snap 这两篇文章，是从现有社交网络的设计模式出发，从中寻找被忽略掉的边缘行为，然后反思为何社交网络会的「默认值」会被设计为「记录一切」，而这背后会带来什么样的代价。 铭记每一刻，但也带来各种挖坟的效果。 所有的事情都能被记录的时候，往往意味着什么事情都不重要。 所以 Snap 看似用有限的时间和形式，反而促进你能更好的记住某个故事和感觉，而不是像档案馆一样收集你所有的，几乎不会回溯的过往。 温故知新：如何巧妙地达到你沟通的目的？ 文中关于沟通的定义值得着重记录 —— Ronald B. Adler与Russell F. Proctor将沟通定义为：“（沟通是）一种交流的过程，参与者经由交换信息而建立关系。” 不过另一个有趣的地方是关于沟通深度的，即不是两个人在一起聊得越多越好，而是聊得是否是对自己意义重大的事件，或者是极为隐私的事件。 回顾我学心理学的这8年经历 - Discovery - Hi!PDA Hi!PDA #[[fatdragoncat]] 沙盘游戏展示个人心理状态 为了改变这个现状，我用了一个最简单粗暴的办法，就是你不理我，那我就去跟随你，参加你参加的一切活动 我一改过去记者生涯养成的晚上到处约饭的习惯，下班准点回家，周末绝不加班，而且开始主动参加老婆的各种活动，有时是到公园一群妈妈带娃的聚会，有时是做蛋糕，有时是去不同妈妈家聚餐，我本身是极端内向的人，而且前面也说过，我其实是有社交恐惧症的，所以参加这些活动每次都会迅速让我极端疲劳 移情(transference)来访者对分析者产生的一种强烈的情感，将自己过去对生活中某些重要人物的情感会太多投射到分析者身上的过程。 反移情(counter-transference)移情是咨询师把对生活中某个重要人物的情感、态度和属性转移到了来访者身上。 从那时起我就知道了，自己真正渴望的就是一个温暖的家，我想服务的客户，也是渴望家的温暖的人，而不是只想疗愈自己，而不顾伴侣死活的人。 我不想再听一帮只是花钱找人诉苦，却不肯承担家庭责任的人说废话了 我的心理咨询工作终结了，我的婚姻也over了，我以为往后的岁月，就是漂泊江湖，四海为家了。 于是我开始研究，怎么才能背一个双肩背，就能一边工作，一边生活，然后就撞上了知识付费时代的来临。 心理咨询教练很难，但是保持自己的心理健康可能很容易。 【观点】得到CEO脱不花：面试一个人，你只问ta这四个问题就够了 ！ [[面试]] 后来 HR 和 Mentor 告诉我日常要，多积累一些面试相关的题目，这样进去之前就知道，通过什么问题考核什么方面。从此收集一些面试题就成了习惯，而在这个过程中关注点也从对专业内容的考核，变成了对人本身的考核，也变成了对自己的向内思索：这些问题，自己会如何回答呢？ [[少楠]] 的面试题 有哪些事情是别人做起来觉得很难很无趣，但是你自己却乐此不彼的坚持了很久。 最近半年有什么观点改变了你的认知或者行为方式，为什么？ 你迄今为止做过的，最让你有成就感的一件事是什么？ [[脱不花]] 面试题 驱动力：如果你突然有半个月的带薪休假，只有一个条件，就是必须研究一个事儿，你会研究什么? 主要考察的是内驱力，擅长比赛的要靠外部刺激和目标(原文中举例谷歌从编程比赛中寻找程序员)，在边界内做事。 所以我对工作的不适应，除了不了解机器学习，另外一方面来自于大学靠的是外部刺激和目标。现在需要尝试的是突破业务理解？ 而内驱力更能适应变化，他们能根据自己的兴趣不断地拓展边界，打破旧有规则 和最终在谷歌成功概率相关的问题：你几岁开始拥有自己的电脑？ 谷歌的这两条经验,本质上都指向一个问题:一个人的驱动力是从哪里来的?是来自他自己的兴趣或者对自己的要求,还是来自外界什么人给他设定的标准？ 瞬间反应和回答问题的思路。研究的标的、怎么使用这些时间、达到什么目标？ 因为了解个人的内驱力高低,基本上就能判断出来这个人未来应对变化的能力。当环境对他提出新的要求时,他的抗压性强不强,能不能主动适应变化,就能从这个问题中反映出来了。 期望值：你正在做的事，行业里最顶尖的人或公司是谁，他们是怎么做的? 主要是看眼界如何，视野开阔与否。能不能把内驱力转化为行动力 对自己领域的理解 对高手的定义决定了他的认知，定义的过程能看到研究的过程及人脉情况 关键是如何定义和标杆的差异，以及差异如何形成及缩短方式。 人际：你在此之前的人生经历中，做过什么重要的取舍? 如果你做重要取舍都是一个人，是不是验证你是一个孤独的人？ 核心是看决策机制的形成，了解进退感和分寸，是否有清晰的边界意识。 为什么做出这种选择？出发点是什么？为什么这个时候做？你能不能清晰地界定选择的代价是什么？在你做出这些选择的前后都发生了什么，分别怎么解决的？ 重大决策都会有一两个关键人的影响，关注是否有其他相关人被提及，以及这些人在决策中的角色，来判断他的关系网络 啰嗦的人无法带大队伍，因为边界不清晰 反思：针对你刚才提到过的这件事，如果你有机会能重新做一遍，会有哪些地方不一样? 反思能力，自己经历过的事情是否有清醒的觉察，评估他对待机会的敏感度。 对过往是否有总结和复盘，以及对机会的敏感度。颗粒度越细，反思越深。 面试中很难靠一面之词分清哪些是团队的水平，哪些是个人的贡献。通过反思，根据提到的颗粒度，能够判断，他在项目中究竟起到了多大的作用。 对一个人的内驱力、关系建立能力、目标感和反思能力都有充分的了解能更好帮助做出准确的判断。 很幽默以及不温和 最后推荐了 [[奈飞文化手册]] 商业 谁在驱动滴滴发动机？ 平台治理 [有限与无限的游戏] 叶杰平来滴滴的两道面试题： “一道题目，跟出行里抢单、派单相关，问我能不能抽象成数学问题。” 能不能把抢单到派单问题，具体建立成一套算法模型？ 纪念一下人已经走了。 失去字节技术中台支持的 TikTok，还会是曾经那个 TikTok 吗？ [[The Information]] 美国政府：威胁国家安全和违反数据隐私。 国内以什么理由封杀部分软件？美国是实行对等的权力吗？中国和美国之间的战争一部分，没有一个人是无辜的。另外，没有看到国内政府在这一件事情上发表任何的声明？ 联系一下，这件事情对滴滴的国际化会产生什么样的影响？滴滴会不会被更加宽容的处理？还是不要抱有幻想？ 从监管层到潜在买家，试图改写 TikTok 未来的角色变多了[[The Information]] 算法 这个月看起来，看得论文和文章比较少。ai-labs 单机 93w QPS 的模型承保我好几点的笑点。 滴滴KDD2020论文(六) | 滴滴公开ETA新系统，线上推理速度进入微秒时代 [[CompactETA]] #ETA 之前写过他们原来的 ETA 模型文章： (WDR) Learning to Estimate the Travel Time | 算法花园。 数据稀疏 空间稀疏：link 历史数据少 基于路况分布来度量不同 link 的相似性，利用 metric learning 进行训练 时间稀疏：相邻时段的 embedding 设置共享参数，使得相邻时段的 embedding 更加相似 如何解决线上预测耗时？ [[GAN]] 替代 [[LSTM]]，link 之间的依赖关系通过学习路网的拓扑结构来建立 位置编码：保持序列信息 查表 + g 然后过 MLP 算法工程师技术路线图 - 知乎 [[Python]]：[[Learn Python the Hard Way]] [[流畅的 Python]] 能读懂 panads、sklearn 等包的源代码 [[Scala]] Spark快速大数据分析 [[Scala函数式编程]] [[冒号课堂]] [[cpp]] 能够读懂[[LightGBM]]里对于tweedie loss的相关定义代码。 常用设计模式有哪些？ Embedding 技术的非端到端学习方法 - 知乎 下载记录变成一个 session 随机游走扩充数据 #Airbnb 全局 context [[Real-time Personalization using Embeddings for Search Ranking at Airbnb]] 同一个类别的随机负样本，分类成本不高。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"李宏毅强化学习课程笔记 Imitation Learning","slug":"reinforce-learnning-basic-imitation-learning","date":"2020-09-06T15:14:47.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/reinforce-learnning-basic-imitation-learning.html","link":"","permalink":"https://xiang578.com/post/reinforce-learnning-basic-imitation-learning.html","excerpt":"","text":"我的笔记汇总： Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning Actor Critic Sparse Reward Imitation Learning apprenticeship learning 无法从环境中获得 reward。 某些任务中很难定义 reward。 人为设计的奖励可能导致意外的行为。 学习专家的行为。 Behavior Cloning 监督学习，但是样本有限。 Dataset Aggregation 通过行为克隆得到 actor \\(\\pi_1\\) 利用 \\(\\pi_1\\) 和环境交互得到一些新的样本 由专家对上一步采样得到的样本进行标注 利用新得到的样本训练 \\(\\pi_2\\) 如果机器的学习能力有限，可能复制专家多余无用的动作。监督学习无法区分哪些是需要学习、哪些是需要忽视的行为。 Miss match 监督学习中，我们假设训练数据和测试数据有相同的分布。Behavior Cloning 中可能分布不同。 Inverse Reinfofcement Learning 反向强化学习 没有 reward 函数，通过专家和环境互动学到一个 reward function，然后再训练 actor。 类似于 GAN 的训练方法（actor 换成 generator，reward function 换成 discriminator）。 学到 actor 的 pi 后，调整 reward function，保证专家的行为得分大于学到的行为。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://xiang578.com/tags/algorithm/"},{"name":"reinforcement-learning","slug":"reinforcement-learning","permalink":"https://xiang578.com/tags/reinforcement-learning/"}]},{"title":"李宏毅强化学习课程笔记 Sparse Reward","slug":"reinforce-learnning-basic-sparse-reward","date":"2020-09-06T14:14:47.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/reinforce-learnning-basic-sparse-reward.html","link":"","permalink":"https://xiang578.com/post/reinforce-learnning-basic-sparse-reward.html","excerpt":"","text":"我的笔记汇总： Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning Actor Critic Sparse Reward Imitation Learning Reward Shaping 如果 reward 分布非常稀疏的时候，actor 会很难学习，所以刻意设计 reward 引导模型学习。 Curiosity Intrinsic Curiosity module (ICM) 在原来 Reward 函数的基础上，引入 ICM 函数。ICM 鼓励模型去探索新的动作。最后 ICM 和 Reward 和越大越好。 鼓励探索新动作之后，会导致系统风险变大。对比预测的下一个状态和真正的状态的差异程度进行抑制。 Feature Ext 对状态进行抽取，过滤没有意义的内容。 Network 1 预测下一个状态，然后再和真实状态计算 diff 程度。 Network 2 预测 action，和真实的 action 进行对比。如果两个 action 接近，说明 f 可以进行特征提取。重要程度计算。 Curriculum Learning 规划学习路线，从简单任务学习。 Reverse Curriculum Generation Hierarchical Reinforcement Learning 对 agent 分层，高层负责定目标，分配给底层 agent 执行。如果低一层的agent没法达到目标，那么高一层的agent会受到惩罚（高层agent将自己的愿景传达给底层agent）。 如果一个agent到了一个错误的目标，那就假设最初的目标本来就是一个错误的目标（保证已经实现的成果不被浪费）","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://xiang578.com/tags/algorithm/"},{"name":"reinforcement-learning","slug":"reinforcement-learning","permalink":"https://xiang578.com/tags/reinforcement-learning/"}]},{"title":"李宏毅强化学习课程笔记 Actor Critic","slug":"reinforce-learnning-basic-actor-critic","date":"2020-09-06T13:14:47.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/reinforce-learnning-basic-actor-critic.html","link":"","permalink":"https://xiang578.com/post/reinforce-learnning-basic-actor-critic.html","excerpt":"","text":"我的笔记汇总： Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning Actor Critic Sparse Reward Imitation Learning Actor Critic policy gradient 给定在某个 state 采取某个 action 的概率。 baseline b 的作用是保证 reward 大的样本有更大的概率被采样到。 从当前时间点累加 reward，并且当前 action 对后面的 reward 影响很小，添加折扣系数。 PG 效果受到采样数量和质量影响。 Q-learning 状态价值函数 \\(V^{\\pi}(s)\\) 状态行动价值函数 \\(Q^{\\pi}(s,a)\\) Actor-Critic 用 V 和 Q 替换 PG 中的累积 reward 和 baseline。新的模型需要训练两个网络，比较困难。 Advantage Actor-Critic 用 V 去替代 Q，能降低模型整体方差（MC 到 TD)。最下面两个公式转化是由实验得到。 训练过程： tip: actor 和 critic 具有相同的输入 s，可以共享部分网络结构。 output entropy 作为 pi 的正则项，entropy 越大采样效果越好。 Asynchronous Advantage Acotr-Critic A3C 利用多个 worker 去训练。 每个 worker 复制主模型的参数。 每个模型单独采样，并且计算梯度。 更新全局参数。 Pathwise derivative policy gradient 该网络不仅仅告诉 actor 某一个 action 的好坏，还告诉 actor 应该返回哪一个 action。 将这个 actor 返回的 action 和 state 一起输入到一个固定的 Q，利用梯度上升更新 actor。 完整的训练过程和 conditional GAN 类似， actor 是 generator，Q 是 discriminator。 算法： action 由训练的 actor 决定 利用 s 和 a 更新 Q GAN 和 AC 方法对比","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://xiang578.com/tags/algorithm/"},{"name":"reinforcement-learning","slug":"reinforcement-learning","permalink":"https://xiang578.com/tags/reinforcement-learning/"}]},{"title":"Never-Reading 202007 互联网商业模式","slug":"Never-Reading-202007","date":"2020-08-08T15:55:11.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/Never-Reading-202007.html","link":"","permalink":"https://xiang578.com/post/Never-Reading-202007.html","excerpt":"","text":"不知不觉中每月分享已经进行半年，不过前 6 期都没有想到取什么名字。上期的标题「Never Reading」来自稍后读列表名称，仔细一想不正好成为每月分享的名字吗？而且还有致敬「Λ-Reading」的成分。 互联网商业模式 在 202006 Never Reading 中摘录过「即刻半月刊」的一段内容： 所谓“商业模式”其实指的是这家公司的“价值创造模式”，即用什么样的模型创造了更多价值。 世界上现存所有的商业模式无非三种，一是[[边际效应]]（规模效应/协同效应），二是[[双边效应]]，三是梅特卡夫[[网络效应]]。不同的价值创造模型，带来不同的增长动力，继而带来不同的货币化方法。 滴滴是什么模型？ 当时没有找到上面这一段内容的解释，只是觉得有点神奇就记录下来。这个月收听「三五环」中刘飞和少楠关于交易平台两期内容（14、17），其中提到许小年教授的一本书「商业的本质和互联网」。在书中详细的介绍商业平台的效应，感兴趣的可以找来详细的阅读。 [[规模效应]]：[[边际成本]]越低和边际收益越高 [[协同效应]]：依赖于品种增加带来的 1+1 &gt; 2 百货公司拥有协同效应 #problem 协同效应失败的例子？ [[双边市场效应]] 双边供需的进入，都会有正外部性。公式：V=kmn 电商平台的效应弱：需要平台来管控质量，即变成了单边的；滴滴的效应强：司机和乘客的增加，都会带来正向效应（但边际收益未必持续提升）。 [[梅特卡夫效应]] 一个网络的价值与用户量的平方成正比。与常见网络效应的概念基本相同。任何用户的进入，都会有正外部性。 社交网络 曾李青定律：V=k*n²/r²（r 受 T、S、I、C 影响）。 读完之后，滴滴是什么模型这个问题就迎刃而解。这个月 有一个比较火视频 复盘出行大战：被BAT选中的滴滴，如何完成垄断霸业_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili，介绍滴滴创业的故事。做为一名内部人士，也被里面的内容给震撼到。之前沈南鹏在吴海波的采访中说过一句话「未来十年看滴滴」。记录自己看到的三个细节： 滴滴接受腾讯投资后，程维和王刚彻底关上和阿里的联系。 和快的补贴大战中：马化腾建议每次补贴金额在 12-20 中间的一个随机数 和 uber 大战中：腾讯封禁 uber 在微信上的微信号 有一个梦想是能看到程维的传记，之前特意查过他的花名 —— 常遇春。知乎热榜这个月出现过一个奇怪的问题，里面有一段引用： 王保保这个“奇男子”的称号，是朱元璋给的，评价较帝国双壁之一常遇春更高，含金量十足：朱元璋曾大会诸将，问道：“天下奇男子谁也？”诸将都说：“[[常遇春]]将不过万人，横行无敌，真奇男子。”明太祖笑曰：“遇春虽人杰，吾得而臣之。吾不能臣王保保，其人奇男子也。”（《明史》-《扩廓帖木儿传》） 商业 阿里很强调价值观，但[[阿里巴巴]]首先是一家商业组织。 上学时，[[黄峥]]就意识到了机会可贵：“我在上学时就意识到几个事：一是寒门出贵子是小概率事件，大部分富二代，尤其是官二代非常优秀。二是田忌赛马，在整体资源劣势的情况下可以创造出局部优势，进而有机会获得整个战役的胜利。基于此，平凡人可以成就不凡事。第三是钱是工具，不是目的。” 趣头条本质上是一款游戏产品，只不过披着内容信息流的外衣。 不过这一些信息流产品只有学习用户兴趣，真正的影响用户行为还是要靠基于强化学习开发的游戏。 字节系广告变现涉及4个角色：内容消费者，内容生产者，平台，广告主；阿里系广告变现涉及3个角色：消费者，商家，平台；所以阿里的广告变现链条更短，一定程度上也是效率更高的原因。多一个角色，多一份成本，最终效果要多乘一个参数。 Apple’s Relentless Strategy, Execution, and Point of View | by Steven Sinofsky | Jun, 2020 | Learning By Shipping Steven Sinofsky 是前微软 Windows 业务部门的总裁评价苹果架构迁移。 其实没有仔细看这一篇文章。震惊到我的是，这位兄弟管理过 20000 人的工程团队？不知道现在国内有哪一些公司的 CTO 能管理这么多的人。 从苹果的系统更新，理解设计中的「控制」与「自由」 - 少数派 雪城大学建筑学教授理查德·洛萨（Rhichard Rosa）认为“设计的本质即是在于控制与自由。”这句话非常简洁但直接地介入了设计的核心——设计行为为混乱与无序赋予秩序，使之得以承载可控的人类行为；但设计也一定不是绝对的控制，它一定要为使用者预留一些自由。 设计师对美学的自信会落脚在对细节的强制：贝聿铭设计京都东郊的美秀美术馆，下雨天需要领取相应颜色的雨伞才能进入。 有些人说：“消费者想要什么就给他们什么。” 人们不知道想要什么，直到你把它摆在他们面前 设计一定需要控制，但一定不是绝对的控制。平台的搭建者需要预留一些自由给开发者以及用户，对秩序的全盘接手最终会导致自下而上民意的反弹。 [[设计的哲学]]，设计的背后是不是需要有哲学？一味满足用户需求的软件是不是会变得十分的复杂？比如 Emacs 之类自由度高的软件？ 顶级PM的产品观：王慧文看行业-合集 - 简书 互联网 AB 面：A类是供给和履约在线上，B类是供给和履约在线下。 B类又可以分为：以SKU为中心的供给B1和以Location为中心的服务B2。 A 类能力体现在产品设计领域，体现在用户理解上，体现在对于通讯、社交以及内容把握上。 B1里面，主要体现在对于品类的理解，对于供应链的理解，对于定价的理解。 B2里面，如果你们去盘点一下B2的公司，他们总体来说有一个比较共有的特征，大规模的线下团队。 是否有大规模的线下团队是B1和B2一个很大的差别。 在LBS的方向上，中国和美国的企业这样的差距是怎么样发生的？大概有四个因素决定：人力成本、人口密度、人口规模、代际竞争。 互联网 AB 面 阅读 理解世界的一个有效方法是，在人生的某个阶段，把任何之前视为理所当然的事情全都重新研究与思考一遍，并弄清楚它们运行的真正起源与机理。在这个过程中，自问的问题越基础、越显得不需要去质疑，收获往往就会越多——人为什么要每天吃三顿饭、买东西为什么要花钱、书籍和文章为什么会存在——真理通常就藏在这些大多数人想都不会想的事情里。[[张潇雨]] 「银河系漫游指」里面有一句：任何在我出生时已经有的科技都是稀松平常的世界本来秩序的一部分。 什么是第一原理？ [[亚里士多德]] 在[[形而上学]]中提出这个哲学概念，指「公理：无法再分、无法证明且不证自明的命题。」 “第一原理”本身并不是什么原理，它只是个简称。准确地说，应该叫做“从第一原理推理”（reasoning by first principle），是分析问题，找出其不可继续拆分的根本原因，即第一原理，再从第一原理反推出解决方案的思考方式。 第一原理是先验 算法 面试官：会玩牌吧？给我讲讲洗牌算法和应用场景吧！ | 唐磊的个人博客 [[洗牌算法]] 之前的分享中写房租分配 每周分享第 9 期：拼多多 | 算法花园 保证每次的概率是相同 #[[problem]]实现以下算法：一组数，每次不放回抽样，得到一个随机序列。白板编程。分析时间空间复杂度。 follow up：能否时间O(n)完成，能否空间O(1)完成。 Stanford CS230: Deep Learning | Autumn 2018 | Lecture 8 - Career Advice / Reading Research Papers - YouTube：Ag 在课程中介绍如何阅读论文，又一次感受到大佬的真诚。 主题阅读 收集资料 列出一个 list ，标注阅读进度。挑选有价值的论文阅读。 5-20 初步了解 50-100 很好理解前沿工作 如何阅读论文 多遍阅读 第一遍：标题，摘要，图片 第二遍：简介、结论、图片相关材料 第三遍：进入论文主体部分，但是可以跳过数学，明白每个参数的含义。 第四遍：阅读整篇文章，跳过没意义的部分(内容过时，没有火起来过)。 阅读时思考的问题 作者试图解决什么问题？ 研究方法的关键是什么？（最具有开创性） 哪些东西可以为你所用？ 有哪些参考文献可以继续跟进？ 最后发现一篇相关实践文章： How You Should Read Research Papers According To Andrew Ng (Stanford Deep Learning Lectures) | by Richmond Alake | Jul, 2020 | Towards Data Science 另外一篇和论文阅读有关的文章：沈向洋、华刚：读科研论文的三个层次、四个阶段与十个问题 - 知乎 三个层次：速度、精读与研读 四个阶段：Passive Reading、Active Reading、Critical Reading、Creative Reading 这就是本期的 「Never-Reading」，我们下个月再见。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"202006 Never Reading","slug":"monthly-issue-202006","date":"2020-07-14T15:55:11.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/monthly-issue-202006.html","link":"","permalink":"https://xiang578.com/post/monthly-issue-202006.html","excerpt":"","text":"这一份 6 月的阅读总结来的有一点晚。前几年一直断断续续在实践 GTD，人的拖延症超乎想象，学到的一个经验是 「Now or Never」。所以，这个月将自己的阅读列表取名为「Never Reading」。 笔记方法精进 Roam Research 引起现在这一波 Backlink 笔记软件浪潮，本月依然阅读一些和笔记方法相关的文章。 我的 Zettelkasten 卡片盒笔记法实践 | 吕立青的博客 文章链接 上个月介绍过，自己已经从 Roam Research 迁移到 Obsidian，然后就看到吕立青这一篇基于 Obsidian 的 Zettelkasten 实践。文章中将卡片盒笔记分拆分成四步： 1️⃣ 第一步：必须用自己的话写笔记卡片，以确保你将来能够理解。 2️⃣ 第二步：无论何时添加新笔记，主动查找可链接到的已有笔记。 3️⃣ 第三步：通过添加新记录并联系起来，延续这一系列的连续思考。 4️⃣ 第四步：使用 Anki 间隔重复加深记忆，主动由大脑触发远程联想。 搞笑地是，在他写出这篇文章没有多久之后，已经开始尝试往 Roam Research 上迁移……另外，为了更好在国内推广，他参与发起 roam/cn 组织，从英文世界翻译一些推特、视频以及分享一些个人的案例。 Zettelkasten note-taking in 10 minutes 两条原则： Don’t try to get this method perfect from the get go. The advanced practices are useful only when you’ve got close to 1000 notes My productivity app for the past 12 years has been a single .txt file 12 年间使用一个 txt 进行任务管理方法分享。作者提到 to do list 变成 what done list 的过程，每天晚上将日历中第二天的代办事项整理到 txt 中，第二天顺手记录任务相关的信息（比如讨论出的结论，或者得到的信息）。结合自己使用经历，OmniFocus 是一个 to do list，基于纯文本的任务管理方式（org-mode 或 taskpaper）更容易成为 what done list。 商业 滴滴重踩油门_详细解读_最新资讯_热点事件_36氪 不开玩笑地说，看完这里面的分析，我才理解公司很多的战略。 淘宝宣战拼多多的前夜：吕晋杰、陈琪、徐易容和葛永昌的至暗时刻 和很多同学聊天，17 年找工作的时候低估了拼多多的潜力。 淘宝 PC 转移动互联网时，流量入口从网页短 300 个减少到手淘 app 48 个。是不是能解释淘宝这几年涌现出那么多的推荐系统相关的问题。 电视广告投放策略以及如何理解流量暴涨，拼多多上一直很火爆的「百亿补贴」 任何人都可以成为英雄，哪怕是做了一件不起眼的事情。 快手的普通，抖音的美好，算法的价值观 - 知乎 算法没有价值观，算法只会实现设计者的意图。 主动降低算法效率，是为了实现某些短期无法衡量的业务目标。 抖音将流量集中在头部。 快手整体的点击率被牺牲，普通人的流量被保证。 抖音说记录美好生活，快手说记录世界记录你。 阅读 人生总有一刻，我们会开始思考死亡 茨威格在《人类的群星闪耀时》所说的那种幸运是什么 —— 「最大的幸运，莫过于在年富力强的时候，发现了自己的使命」 人生总有一刻，我们会开始思考死亡 - 知乎 [[张潇雨]] 于是我发现，面对死亡最终可能只有两种方法。 - 一种是将自己与一些更宏大的东西联系起来：一个数学定理、一本文学著作、一件艺术作品或一种恒久的信仰。马尔克斯与康德靠《百年孤独》与《纯粹理性批判》遗世独立，米开朗基罗把《创世纪》和《最后的审判》印刻在西斯廷大教堂里，供千万后朝拜——他们肉身虽灭，但精神不朽——反正建筑是永远戳在那儿的。 - 还有一种就是，生活在当下的每个瞬间里，不烦扰过去、不担忧将来。 即刻半月刊 6.18？即刻重新开放，之前对这个社区没有太多印象，尝试关注一些人之后，信息流的质量也不错。「即刻半月刊」是某些爱好者挑选的一些即友发言集合。摘录一些我觉得有意思的内容。 老罗带货首卖小米的中性笔：数量太少。之前看到过一个分析，抖音自己做电商最大的困难点在于没有用户的物流地址。简单想一下，用户可能在填写地址这一步流失。 @Rey_L 我在判断哪家公司的产品会换灰色皮肤的时候，基本都猜对了hhh，滴滴头条一定会做，京东会做，淘宝也许会做，拼多多肯定不做。 微信小程序减少 App Store 在国内的下载量。不过大部分巨头的小程序还是想要从微信中引流的。 底下有回复cite了一个working paper，大意说的是名校（elite education，这里用的是211高校与非211高校的断点回归）能够给学生带来的起薪上 30-45% 的提升，其原因在于名校当中的social network，elite的peer普遍家庭条件更好，成绩也更好。上网课带来不了这种social connection...所以说不单只是知识改变命运，环境也很重要 所谓“商业模式”其实指的是这家公司的“价值创造模式”，即用什么样的模型创造了更多价值。 世界上现存所有的商业模式无非三种，一是[[边际效应]]（规模效应/协同效应），二是[[双边效应]]，三是梅特卡夫[[网络效应]]。不同的价值创造模型，带来不同的增长动力，继而带来不同的货币化方法。 滴滴是什么模型？ A/B test只能做模型的小优化，但找不到大的绝对的增长点，所以除了推荐系统里的参数，即刻的很多产品决策希望尽量远离A/B test 张小龙的饭否，张一鸣的微博，黄峥的微信公众号，很可惜这几个都成了过去形态。现在只剩下王兴的饭否了。 一个反向小思考：兴趣社交刚需归刚需，社交app的本质还是信息分发。RSS订阅20周年，分发的有效性也不是什么相似推荐，^而是真正提供专注力的阅读^。因为个性推荐的本质就是专注力。信息廉价甩卖的今天，打开微博广场/抖音主页/朋友圈（广告），用户最需要的不是新鲜事而是减少噪音的阅读器，人永远看到自己想看到的，产品顺水推舟不也是更讨巧，所以需要设计者克制。沟通永远是双向的，当用户感到一个舒适被接纳的舆论场所，也会更加有表达欲。 芒果系这么多年都在做一件事，就是把握这个时代的情绪。[[乘风破浪的姐姐们]] 机器学习 本月 KDD 2020 的文章应该已经放出，推荐阅读 Airbnb 深度模型实践相关的文章 Managing Diversity in Airbnb Search 以及 Improving Deep Learning For Airbnb Search。 7 月份希望能写一篇博客分享自己的阅读笔记。 神经网络中对需要concat的特征进行线性变换然后相加是否好于直接concat? - 知乎 [机器学习] concat 没有信息融合，也没有信息缺失 add 不同向量之间的权重相同，会导致信息缺失。待融合的特征具有相同分布，或特征属于同一类，直接相加才有可能提升模型性能。 concat + MLP 能够自动学习不同 channel 的权重，MLP 能引入非线性。利于通道间的信息融合，计算量大。 【机器学习】Bootstrap详解 - 知乎：随机森林里面用到的数据重采样方法，老板要求训练两个有差异模型时给我推荐的方法。 BERT 可解释性-从\"头\"说起 - 知乎：蛮有意思的对 BERT 组件效果分析，这位作者举的例子有些蹭热点。 都9102年了，别再用Adam + L2 regularization了 - 知乎：说明为什么要用 AdamW。另外推荐一下，之前一位同事写的 AdamW 实现：L2正则=Weight Decay？并不是这样 - 知乎","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"每月分享 202005 Newsletter","slug":"monthly-issue-202005","date":"2020-06-07T15:55:11.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/monthly-issue-202005.html","link":"","permalink":"https://xiang578.com/post/monthly-issue-202005.html","excerpt":"","text":"Newsletter 从去年开始给我一种 RSS 复兴的感觉，这个月尝试使用 Newsletter。对于创作者来说，RSS 不仅无法统计数据，也很难开展会员模式。Newsletter 通过邮箱订阅的的手段，完美解决这两个问题，国外开始有一站式的解决方案，可能几个月之后也会在国内火起来。推荐自己订阅的一些邮件组给大家。 PRODUCT THINKING · 产品沉思录精选：第一个付费订阅的邮件周刊，目前的价格是 199 元/年。根据少楠自己写的介绍，内容包括但不限于产品设计，服务设计，数据分析，互联网技术，经济学，心理学，社会学，决策学，自然科学，城市规划，零售，团队管理等内容。每周会推荐几篇网上比较好的文章，偶尔也翻译一些英语文章。挑选几篇我觉得不错的公开内容： 原子笔记法：Zettelkasten 如何管理信息：P.A.R.A. 是什么及在 Notion中的应用 Λ-Reading 阅读相关分享，作者的读书笔记网站也值得一看 — Read the Word,Read the World.。推荐内容： 路径依赖和困扰计算机的简单问题 - Λ-Reading 号外：知识管理工具 - Λ-Reading 中文为数不多关于 TiddlyWiki 的介绍。 透明盒子计划 深度阅读分享，盒子对应 Zettelkasten。 透明盒子计划 - 透明盒子计划 Superorganizers 对国外人士的采访，有关于效率、数字生活等。目前只看他的免费内容…… How to Make Yourself Into a Learning Machine - Superorganizers：对一名高中辍学的小哥的采访，介绍来一些自我教育的方法。 阅读 How to take smart notes，方法及工具 - 少数派：Zettelkasten 这种做笔记方法慢慢开始要在国内流行起来，自己已经关注差不多超过半年的时间，接下来也在计划写一篇相关的博客文章。 Human Programming Interface 简单看来一下，利用 py 包和 Emacs 管理所有相关的个人数据，挺疯狂的。 上古论坛差不多十年前的帖子， 我的千书阅读计划 - 意欲蔓延 - Hi!PDA Hi!PDA fatdragoncat 通过阅读成为一名自由职业者。帖子中介绍大量篇幅介绍如何高效阅读、锻炼、自我管理等等。在印象笔记中找到几年前自己写的笔记，现在重新整理一下相关的内容，并分享给大家。 AndyMatuschak - Making sense of Design Unbound vs. prior theories of collaborative design work - Twitch [[Evergreen notes]]的创始人公开展示写作的过程。通过这个视频可以发现他使用的笔记软件是 [[Bear]]，看起来 Reference 和 Backlink 都是手动输入的，不过这样也符合 [[Zettelkasten]] 的原则。只是 [[Roam Research]] 这样的软件让我们变懒。 莫言获得诺贝尔文学奖发表的演讲中有一个故事：到了荒滩上，我把牛羊放开，让它们自己吃草。蓝天如海，草地一望无际，周围看不到一个人影，没有人的声音，只有鸟儿在天上鸣叫。我感到很孤独，很寂寞，心里空空荡荡。有时候，我躺在草地上，望着天上懒洋洋地飘动着的白云，脑海里便浮现出许多莫名其妙的幻象。我们那地方流传着许多狐狸变成美女的故事，我幻想着能有一个狐狸变成美女与我来作伴放牛，但她始终没有出现。但有一次，一只火红色的狐狸从我面前的草丛中跳出来时，我被吓得一屁股蹲在地上。狐狸跑没了踪影，我还在那里颤抖。有时候我会蹲在牛的身旁，看着湛蓝的牛眼和牛眼中的我的倒影。有时候我会模仿着鸟儿的叫声试图与天上的鸟儿对话，有时候我会对一棵树诉说心声。但鸟儿不理我，树也不理我。许多年后，当我成为一个小说家，当年的许多幻想，都被我写进了小说。很多人夸我想象力丰富，有一些文学爱好者，希望我能告诉他们培养想象力的秘诀，对此，我只能报以苦笑。 机器学习 谈谈推荐系统中的用户行为序列建模 - 知乎 一篇关于用户行为序列建模的文章，基本上常用的方法都介绍了。 和上一次 \"从谷歌到阿里，谈谈工业界推荐系统多目标预估的两种范式 - 知乎[[机器学习实践]][[MTL]]\" 属于同一个作者 目前主流推荐系统框架 [[Deep Neural Networks for YouTube Recommendations]] 中的 Matching 和 Ranking。另外可能还有规则模块。 pooling-based architecture 范式，用户行为是无序集合，使用 sum/max pooling 或各种 attention [[Deep Neural Networks for YouTube Recommendations]] 中将用户观看过的视频序列取到 embedding 后，做一个 mean pooling 作为用户历史兴趣的表达 Ranking 阶段：[[DIN]] target item 和行为序列的 item 做一个 attention，得到一个 weight，然后加权求和。 结合 [[Transformer]] 做 self-attention 并行的建模长序列依赖，除去用户行为序列中的噪声：[[Behavior Sequence Transformer for E-commerce Recommendation in Alibaba]] sequential-modeling architecture 范式，用户行为当成一个具有时间属性的序列，使用 RNN、LSTM、GRU 等 [[Perceive Your Users in Depth: Learning Universal User Representations from Multiple E-commerce Tasks]] Property Gated LSTM 推荐中的序列化建模：Session-based neural recommendation - 知乎 上面两种方法都是将用户行为经过 pooling/attention/rnn 的处理，聚合成用户行为序列的 embedding，再和其他的特征 concat 在一起，经过 mlp 后接 sigmod/softmax 抽取聚类出用户多峰兴趣，Capsule 阿里 [[MIND]] 胶囊网络 辅助损失函数 [[DIEN]] 兴趣提取和兴趣演化，以最后一个 hidden state 做为用户兴趣的表达。兴趣提取模块，使用隐状态和下一件商品预测做二分类。不加入辅助loss，GRU 的隐变量完全受限于最终点击的 label，加入后能约束 GRU 每个隐状态表示其本身的兴趣。 提升用户序列长度，可以带来可观的 auc 提升。[[MIMN]] Applying Deep Learning To Airbnb Search：一篇关于从 GBDT 模型迁移到深度模型的工业实践记录 paper。对于我这种没有经历过这种技术迭代的人来说，工业级的深度模型上线比想象中的要困难。作者们针对自己遇到的比如 listing embedding 训练不充分、如何判断 feature 的重要性等问题设计实验去验证以及给出解释。严谨的精神值得吾辈学习。 其他 出于对 Roam Research 开发者的不放心，已将全部文档迁移到 Obsidian。目前还在探索新的工作流，5 月分享不可避免产生拖延。另外还在寻找一种建立 Digital Garden 的方法。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"每月分享 202004 新的尝试","slug":"monthly-issue-202004","date":"2020-05-01T08:55:11.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/monthly-issue-202004.html","link":"","permalink":"https://xiang578.com/post/monthly-issue-202004.html","excerpt":"","text":"不知不觉又到更新每月分享的时间。 想写一下我为什么做这件事情？分享自己平时看到有意思的内容，现实世界认识的人，很少对我关注的内容感兴趣，所幸能借助博客超越时间和空间限制的分享。 另外一点，我希望自己能将这个系列当成一个产品去迭代，每一期都有形式和内容上的进步。这件事看起来很简单，但却需要耗费很大的精力。其实在网上看到很多人通过这种形式分享，到头来还在坚持的大概也没有多少人（比如阮一峰的科技爱好者周刊）。 Zettelkasten 以及 Roam Research Zettlekasten 是一个德语单词，意思是卡片盒。现在主要指一种记笔记的思路。Roam Research 是目前国外很火的一个笔记软件，最大的特点是实现不同笔记之间的双向链接。好几个月前就开始尝试 Zettlekasten 的方法，4 月开始才使用 Roam Research。这里先分享一些我看过的文章。 Roam Research 入门指南（thread） 用 Roam Research 来打草稿 - Dongyang Vic - Medium Roam Fu [Part I: My plan for using @RoamResearch for a thesis](https://twitter.com/kcorazo/status/1247260599760736256) Part II: Using Twitter as an inter-brain zettelkasten Part III: Beyond the Empire 阅读 中文互联网中“讨论”的消亡 | 机核 GCORES：从产品设计角度分析国内主流网站如何限制用户讨论，互联网缩短人与人之间的距离，但我们用来互喷。 互联网是人类历史的一段弯路吗？-虎嗅网：又一篇深度长文，很多观点可以背下来出去装B。 ByteDance程序员生存指南 - 知乎 有些年轻人，在结束一整天的工作后，拖着疲惫不堪的肉体回到出租屋，这时候只想躺着啥都不想干，躺在好几周没有换过的床单上点开了抖音，不一会儿就刷到了凌晨，第二天再拖着疲惫不堪的肉体上班。到了周末也没有任何心力出去玩，只能睡到中午然后随便吃点东西看看剧。没有生活，没有朋友，一晃就单身了四五年，长此以往不仅仅是肉体在亚健康和崩溃的边缘，心理健康更是会出现问题。 孤独、焦虑、易怒等等情绪时刻伴随着一个人。 每个月仅仅只有收到工资短信时可以高兴几分钟。但这笔钱并不敢轻易花出去，因为都是血汗积攒而来，付出的多自然不敢随意花出去。 构建优雅的知识创造系统 [[阳志平]] [[卡片写作法]] 利用电子卡片后，如何量化每天的产出？是不是可以换算成 github 更新多少字？ 卡片是自己看的，不需要分享：在卡片层级最大的误区是：分享。不少人误将卡片、文件和项目三个层级混为一谈，喜欢在卡片层级搞分享。这样每次撰写卡片时就增加了一个选择项：这张卡片我是该分享还是该存着自己看？增加的认知操作加大了认知负荷，从简单反应时变为选择反应时。所以尽可能在卡片层级少做分享。 一周、一月、一年、十年与数十年，进行自己的实践。 所有人问俞军 [[金字塔原理]] 和 [[学会提问]] 优秀的思维方式以及对人性和世界的底层理解 我不是说人工智能不好，而是我自己只关心我能想明白一两年内能给创造什么用户价值的产品(或技术等)，如果我想不明白这东西马上能给用户创造什么价值，我就毫无兴趣。 看《搜索研究院》页首页尾的那句话，“我们若能更妥善地搜寻资料，实在已经改变世界。” 前百度首席科学家吴恩达谈学习 如果你学习，两天后的周一，你不会很快的就在工作中出彩，你的老板也不会知道你花了整天的时间学习，更不会夸奖你什么。你几乎找不到任何东西可以证明你在努力学习。 再谈个人知识管理：革新我的笔记系统 | 吕立青的博客 好久没有看到立青写文章 不同软件之间，单条笔记的迁移相当于一次对知识进行提炼的过程。 关于动物之森的三则 – 苔原带 以前玩家渴望在游戏里杀死巨龙，飞向人马座，击败外星侵入者。而现在我们只需要在游戏里“正常”的生活就乐呵呵了。和朋友一起野餐，请朋友到家里来玩，一个人傻乎乎做开心的事情在现实世界中已经有这么高的门槛了么？又或者只是因为在游戏里做这些事情的成本足够低，多巴胺回报足够快？ 机器学习 内部开始尝试 MCTS 相关的项目。正好接这个机会看一下 DeeepMind 前几年的论文 Mastering the game of Go with deep neural networks and tree search 以及 Mastering the game of Go without human knowledge。推荐去看一下田渊栋在知乎上 AlphaGo的分析 ，当时他在 Facebook 参与类似围棋相关的项目。另外就是木遥的 关于 AlphaGo 论文的阅读笔记 有更多关于现实的思考。最后推荐 阿尔法围棋，记录从 DeepMind 开发 AlphaGo 到战胜李世石的全过程。有一个疑问第四局之后，他们有没有增加使用的 GPU 和 CPU？ 在你做推荐系统的过程中都遇到过什么坑？ - 知乎 [[CTR]] 没有明确的指标：CTR，staytime，read/unread 精准推荐以及兴趣探索 线下auc涨，线上 ctr 跌 从谷歌到阿里，谈谈工业界推荐系统多目标预估的两种范式 - 知乎[[机器学习实践]][[多任务学习]] 范式一：[[MMOE]] 替换 hard parameter sharing [[Recommending what video to watch next: a multitask ranking system]] 范式二：任务序列依赖关系建模 [[ESMM]] 模型负采样，存在 CTR 漂移问题 U5Jvma3de 放弃的事情：Emacs 写作软件中积累一些文章的草稿，不过由于我的兴趣变化太快，很多文章还没有完成就已经被我放弃。借这个机会，展示一些有意思的东西。 程序员圈子中编辑器战争一直是一个绕不过去的话题。自己日常的工作中会使用多种编辑器： Sublime Text 简单处理文本 IDEA 处理 Scala Spark 相关的代码 PyCharm 连接 GPU 服务器处理 python 相关的代码 VS Code 本机上写 python、shell、cpp、sql 等脚本 Vim 服务器上修改文件 不论选择什么编辑器，都推荐大家去看陈斌的一年成为Emacs高手 (像神一样使用编辑器)。 去年底的时候，由于想尝试 org-mode 做任务管理（下个月再分享相关的内容），开始尝试使用 Emacs。Emacs 最大的有点是基于 Elisp 开发，软件中的每一个功能都对应一个函数，一个快捷键对应一个按键和函数的 map。修改功能和配置非常的方法。比如有人完全将 Vim 在文本操作上的功能迁移过来做成 evil 这个插件（号称所有和 Vim 中表现不同的情况都是 bug）。 Emacs 需要大量时间调教才能用起来舒心，对于初学者推荐去网上找一些成熟的配置直接使用。目前比较流行的有 Spacemacs 和 Doom emacs，这些配置维护以及使用的人很多，方便解决你遇到的各种问题。另外那些某些大佬个人分享的配置，如果你和大佬的技术栈不同，没有必要强行 clone。把它当成是一个学习素材，更好的理解 Emacs 背后的哲学。再这些基础上，成为高效的程序员的第一步，就是打造属于你自己的专门的配置文件。 使用好的编辑器是为了更快的工作。那如何更快的工作？ 在加快敲击键盘的速度 减少敲击键盘的次数 减少鼠标和键盘之间的切换 关于 2，我在之前的文章中提到过一点，改变中文的输入方式（从全拼切换到小鹤双拼）。另外一点就是多使用快捷功能，比如 vim 里面的行号跳转。大部分软件的快捷键都是开发商配置好的，不过每一人主要使用的功能其实是完全不一样的。Emacs 中所有的快捷键可以查到定义的文件，从而进行修改。想象一种情况，为了减少我们按快捷键的次数以及难度。我们统计一段时间内使用 Emacs 各个功能的次数（插件 keyfreq），然后重新定义对应的快捷键。 由于我自己之前主要使用的是 vim，所以也给 vim 用户一个相对于合理的替换过程： 当成普通的 vim 使用 逐步接触 org-mode 相关的功能 使用 emacs 其他的特性","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"每月分享 202003","slug":"monthly-issue-202003","date":"2020-03-28T08:55:11.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/monthly-issue-202003.html","link":"","permalink":"https://xiang578.com/post/monthly-issue-202003.html","excerpt":"","text":"读书 呼吸 (豆瓣)：这是一本由 Byte.Coffee 主播 MilkShake 🐑 推荐的一本科幻小说集（前几天看其他东西的时候学会科幻小说的英文 sci-fi）。之前看到过，小说的价值在于作者用一个故事告诉你一个道理。最喜欢的是《商人和炼金术士之门》这篇：在传统的穿越小说无法改变未来和过去的基础上，论证穿越能更深刻理解生活。书中其他探讨的几个问题也很有价值，值得一读。 我看 徐大sao的个人空间 - 哔哩哔哩 ( ゜- ゜)つロ 乾杯~ Bilibili：最近挑着看完大 sao 做饭视频，被他展示的生活激情所吸引。 文章 Seeking the Productive Life: Some Details of My Personal Infrastructure—Stephen Wolfram Writings：这篇超长的文章是 Stephen Wolfram (Mathematica CEO) 介绍自己几十年在家办公的经验，包括如何搭建一个适合自己的工作环境、如何管理文件和个人数据等等。不过这次疫情期间在家办公，大部分同事反应最大的问题是沟通效率降低。另外想想，或许是我们在这方面的思考不够。总之，互联网行业居家办公才是光明的未来。 Naval Ravikant: The Angel Philosopher：AngelList 的 CEO Naval Ravikant 的播客访谈，Naval 的公司投资 Uber Twitter 等 100 多家科技公司。主要介绍 Naval 的一些哲学，文字版在 Naval-Ravikant-TKP.pdf 利用 Kindle 阅读，遇到喜欢的书，购买实体书收藏。对比书的价格，从书中学到可以改变自己人生的内容更重要。 发现一个新的博客后，会在 Archived 页面挑选几篇仔细阅读。读书时也可以使用这个技巧。 How to Make Yourself Into a Learning Machine - Superorganizers：一位高中毕业后离开丹麦来到加拿大创业公司工作人的自我学习之路。 自我定位 T 型人才。 兴趣面广，主题阅读。 高亮关键内容，使用 anki 记忆，相关的想法用 zettelkasten 记录。 zettelkasten 使用纯文本 + 脚本实现。（如果只记录英文 vim 和 emacs 真的是很强大的软件） 利用谷歌检索的数量判断单词的重要性 The PARA Method: A Universal System for Organizing Digital Information - Forte Labs：介绍 PARA 这种数字信息整理方法。 The Best Books and Articles I Read in 2019 – The Blog of Author Tim Ferriss：一篇 2019 年阅读总结文章。对作者介绍的阅读流程比较感兴趣： Evernote 搭配 web clipper 收集文章。 利用 *** 以及高亮在文章中做笔记，方便之后进行快速回顾。 阅读 Kindle 格式的电子书，定期从亚马逊官网导出高亮笔记（国内不支持）。 利用 Readwise 回顾之前提到的高亮。 阅读实体书时，写简单的索引卡片，然后将卡片拍照导入 Evernote 中。 机器学习 为什么有些深度学习网络要加入Product层？ - 知乎：解释为什么 MLP 只包含特征累加而有学习特征交叉的能力，后面展开讲了一些提高模型特征交叉能力的方法。 详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解_网络_nebulaf91的博客-CSDN博客：看过讲 MLE 和 MAP 比较清晰的文章。刚看开头的时候，想到自己大学上过《概率论和统计》居然没有考虑过概率和统计有什么区别……","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"每月分享 202002 「山川异域，风月同天」","slug":"monthly-issue-202002","date":"2020-03-07T07:45:15.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/monthly-issue-202002.html","link":"","permalink":"https://xiang578.com/post/monthly-issue-202002.html","excerpt":"现在看来，又有多少人预测到这一次超级黑天鹅事件。","text":"现在看来，又有多少人预测到这一次超级黑天鹅事件。 这里记录过去一个月，我看到、想到值得分享的东西。 0x03 How Instapaper Changed My Kindle Life For the Better 利用 Instapaper 定时将稍后读文章发送到 kindle 上。 0x02 这是我为武汉雷神山、火神山医院设计的品牌形象标志logo - 步行街主干道 - 虎扑社区 0x01 “山川异域，风月同天” @扎宝：日本汉语水平考试HSK事务所捐赠给武湖北的物资，20000个口罩和一批红外体温计。 标签上写着“山川异域，风月同天”，感动[泪][泪] 求一个英文译文！ p.s. 据记载鉴真事迹的历史典籍《东征传》记载：日本长屋亲王在赠送大唐的千件袈裟上绣“山川异域，风月同天，寄诸佛子，共结来缘”偈。鉴真大师被此偈打动，决心东渡弘法。 @文冤阁大学士：We are created to share Nature and love. 扫了下原博评转，翻得都差我好几座唐招提寺。嘻嘻。 0x00 XGBoost 春节在家，重新把这些经典的内容再拿出来多读几遍。网上写的那些总结感觉都不是很好，还是要回去看论文。说句实话，纸上谈兵这么久，居然没有跑过 xgb 的包……","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"2019 起步","slug":"2019","date":"2020-02-04T13:44:09.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/2019.html","link":"","permalink":"https://xiang578.com/post/2019.html","excerpt":"","text":"受 Free Mind 的影响按这种形式写年度总结 年初的时候看到一句话：「 2019 是过去十年中最差的一年，也是未来十年中最好的一年」。和其他人一样，我害怕不确定性，不过生活除了鼓起勇气前进，还有什么其他选择。 工作 完整在滴滴工作一年，自己没有太多变化，可是环境却变了很多。从年初内部会议上 Will 优化员工开始，很多同事陆续离开，从而我都快要成为团队元老…… 做为食物链低端的算法工程师，工作中杂七杂八的事情干了很多。洗数据、跑模型、改工程代码、测试、上线、实验各个方面都干过。 说回来，算法还是自己的主要工具。今年用的最多的是 FM 和 GBDT，这些都是几年前的技术，但是架不住效果好，性能要求小。自己也写了一些相关的文章，可以供大家参考。 (FM) Factorization Machines | 算法花园 (FTRL) Follow The Regularized Leader | 算法花园 All About GBDT (1) | 算法花园 Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr) | 算法花园 关于深度学习，在我入职前模型就基本迭代完成，今年主要探索个性化场景的解决以及模型性能优化。很遗憾，这两方面的工作目前还没有什么可以写成博客分享的。最后，自己没有参与到组内强化学习的项目中，不过还是通过李宏毅老师的相关课程了解初步的概念，争取 20 年内做一些相关的事情。 9月份开始，leader突然让我准备一些编程题目，开始去面试实习生。通过牛客网以及北邮人论坛大概收到简历60多份，我面试10多个候选人，最终通过的大概五六人，不过过来实习的也就 2 个。印证自己之前的想法，一家已经不是快速发展的公司，很难招到即懂机器学习又会做编程题的实习生。 另外想写的一点是是匿名交流。内部论坛之前有一个匿名区，后来由于一件比较有名的事情，匿名喷得太厉害，被某位海归高管以提高交流效率减少戾气所关闭（目前这位已经离职，有人开玩笑期待干掉他新公司的匿名论坛）。所幸脉脉还有职言（匿名）以及公司圈。在上面混了一年之后，越来越理解匿名交流的必要，说事。比如今年发生的延迟发年终奖，快手可以直接在内部匿名区引起宿华回复。我们的公司圈一堆人才自嗨。本质是国内环境下很难公开交流一些话题。 之前看过[一篇文章]中介绍 Google 的 TGIF： TGIF是Larry和Sergey在公司早期就创立的，一个全公司范围的周会。在这个周会上高管们会透露公司新项目的进展，也安排有答疑环节，员工可以询问两位创始人任何问题。TGIF毫无疑问是为了提高公司内部的透明度，但它在增强员工凝聚力的同时也对公司文化提出了挑战，最直接的就是保密问题。比如Chrome项目在公司内部的公开就是在一次TGIF上发生的，那时离Chrome的正式对外宣布早了一年多的时间。 阅读 今年读过 33 本书，阅读量和前几年基本持平。年底发现自己的一个坏习惯：很多书读了一半就放在那里，导致开的坑很多。应对方法也很简单：一段时间内只读一本书。而且为了提高阅读的质量，将自己读完一本书的定义从读到最后一页改成完成对这本书内容的整理。 阅读的主要工具是 kindle 和微信阅读（iPhone）。kindle 是这么多年一直使用的阅读介质，从前几年的找破解图书到现在的完全中亚购买（以目前看书的频率还不至于承受不住），长时间看电子水墨屏能减轻一些疲劳。微信阅读的特点是白领无限卡后就能全场免费读，实在是太香了。理想状态下用这两个工具读不类型的材料，微信阅读读小说以及人物传记，kindle 看需要大量抄记的书。对于需要反复阅读的内容，实体书则是最佳的选择。 分享读完觉得不错的几本书： 经济学通识/薛兆丰经济学讲义：薛兆丰目前看起来风评不是很好，这两本也不是什么严肃的经济学读物。前一本书是作者专栏文章的合集，后一本是得到专栏的文字版，两本书大量的内容是重复的。书中通过现实中的例子来讲解背后的经济学原理，很适合看完之后做为饭后谈资。反正我运用书中的一些原理，给同事分析好久公司的停车场应该怎么分配车位。 银河帝国：这一套书有很多本，只看完前三本。概括起来，这本小说是以太空为背景讲政治故事，以谢顿的预言为主线，讲述基地对抗各种危机的挑战。另外书中提到看起来有多少分像统计学的心理史学，谢顿一直用这种方法预测未来，而且信徒们一直强调，预测结果不会因为个人而改变。第三本书，围绕寻找第二基地展开，把所有读者能猜的地方都写了出来，选择了一种情理之中，意料之外的结局。 人类简史/未来简史：尤瓦尔·赫拉利是前几年很火的一个历史学家。人类简史主要是按他的框架回顾从原始人类到现代人类的文明发展历程。对于我这种没有系统接受过历史学教育的人，完全是一种震撼。未来简史讨论的是人类未来的发展方向，成神。 房思琪的初戀樂園：讲述一个小女孩被文明所不齿的方式杀死的故事，最让人痛心的这女孩就是作者本人……引用最近很流行的一句话：地狱空荡荡，魔鬼在人间。 基督山伯爵：看完《了不起的盖茨比》后，老板强力推荐的爽文小说。快意恩仇，永远不要丧失对生活的期望。 临高启明：工科党神书，死于历史空无主义，最后放上来缅怀一下。 2020 年开始使用 Notion 记录读书过程，点击 看书也就图一乐 查看。 观影 和去年一样，看电影比起看书来更加容易，豆瓣上轻松标记 60 部。想想原因，打开一个视频放在那里，不用怎么理它就能结束。按类别推荐一下自己觉得不错的影视： 小丑/蝙蝠侠三部曲：去年在观影中大力推荐漫威宇宙，今年看完 DC 这 4 部电影，刷新对超级英雄片的认知。蝙蝠侠：黑暗骑士在是在超级英雄的框架下对人性进行探讨。小丑展示出社会如何逼一个人成为恶魔。 黑客帝国三部曲：经典的电影，赛博朋克风格。之前的神话描述神创造了世界，在这部片子里面，这个神就变成了机器人。多少算是人工智能行业的从业者，强人工智能离我们看起来还是很远。 人生七年9：这应该是拍摄时间最慢长的纪录片，也给我们机会在几十个小时时间里面见证这些主人公 60 多年岁月。很大一个感受，除了 Nick 之外，其他人不过是重复父辈的道路，阶级跃迁又是谈何容易。 哪吒之魔童降世：即大鱼海棠之后，第二部在电影院看的动画电影。之前想过一个问题：为什么一些小说要隔一段时间就翻拍一次？简单的认为要赋予时代主题。这部片子中最喜欢的一个设定：龙族也是妖怪，镇守龙宫，其实也是镇住自己。 邪不压正：电影看到一半的时候，我就觉得自己看不懂。说回来，看这部电影有一种酣畅淋漓的感觉，节奏很快，比《一步之遥》和《让子弹飞》更快。半夜在知乎上看了很多回答之后渐渐地懂得其中的情节，蓝先生的爱国情怀，李的复仇梦想。在历史的框架下演绎，始终无法逃离历史的结果，日本人还会按照发展进入北京城。一句“异父异母的亲兄弟”就值得一看。 游戏 今年新增的一个板块，自从购入 Switch 之后，开始重新接触一些游戏。 隐形守护者：抗战背景下面，一个特工面对选择的游戏。所有的场景都是真实拍摄出来的，比绝大部分国内的抗战剧精美。游戏有很多个结局，当然只有符合社会主义核心价值观的才算善终。最印象深刻是第二号突然的一句：什么都是马尔可夫链。 有氧拳击/健身环大冒险：NS 上的铁人三项之二，充分发挥体感的优势，晚上下班之后健身用。不过从目前的使用频率来看，大概率和买健身卡一个性质。 塞尔达传说：旷野之息：决定买 Switch 很大程度上源于少数派中一篇关于这个游戏的介绍，大意是没有传统的等级增长，只有你真正掌握一个技巧，林克才能使用出来。这款游戏的给我带来看似无限大的空间，但也有一点遗憾，有时候遇到下雨不好攀岩时，我想让林克坐下等雨停，然后发现没有坐下的选项…… 超级马力欧创作家2：大学的时候，经常看超级小桀玩这个游戏。对于我这种连普通的马里奥都要靠无敌才能通关的来说，大部分自制的地图还是有点难的。说回来，买这个游戏就是买一个青春。只可惜物是人非。 暗黑破坏神3：永恒之战版 ：Switch 上的冷饭，自己瞎玩了很久，看完所有的剧情。最终在咸鱼上买了很多强力的装备后，速通 150 层大秘境后索然无味。所以玩游戏还是不要作弊。 未来 世界变化太快，未来可期。 于浙江临海 2017 迷茫 &gt;&gt; 2018 探索","categories":[],"tags":[{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"},{"name":"book","slug":"book","permalink":"https://xiang578.com/tags/book/"},{"name":"movie","slug":"movie","permalink":"https://xiang578.com/tags/movie/"},{"name":"game","slug":"game","permalink":"https://xiang578.com/tags/game/"}]},{"title":"All About GBDT (1)","slug":"gbdt","date":"2020-01-26T06:15:43.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/gbdt.html","link":"","permalink":"https://xiang578.com/post/gbdt.html","excerpt":"","text":"GBDT(Gradient Boosting Decision Tree) 从名字上理解包含三个部分：提升、梯度和树。它最早由 Freidman 在 greedy function approximation ：a gradient boosting machine 中提出。很多公司线上模型是基于 GBDT+FM 开发的，我们 Leader 甚至认为 GBDT 是传统的机器学习集大成者。断断续续使用 GBDT 一年多后，大胆写一篇有关的文章和大家分享。 朴素的想法 假设有一个游戏：给定数据集 \\({(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}\\)，寻找一个模型\\({\\hat y=F(x_i)}\\)，使得平方损失函数 \\({\\sum \\frac{1}{2}(\\hat y_i - y_i)^2}\\) 最小。 如果你的朋友提供一个可以使用但是不完美的模型，比如 \\[F(x_1)=0.8,y_1=0.9\\] \\[F(x_2)=1.4,y_2=1.3\\] 在如何不修改这个模型的参数情况下，提高模型效果？ 一个简单的思路是：重新训练一个模型实现 \\[F(x_1)+h(x_1)=y_1\\] \\[F(x_2)+h(x_2)=y_2\\] \\[...\\] \\[F(x_n)+h(x_n)=y_n\\] 换一个角度是用模型学习数据 \\({(x_1,y_1-F(x_1)),(x_2,y_2-F(x_2)),...,(x_n,y_n-F(x_n))}\\)。得到新的模型 \\({\\hat y=F(x_i)+h(x_i)}\\)。 其中 \\({y_i-F(x_i)}\\) 的部分被我们称之为残差，即之前的模型没有学习到的部分。重新训练模型 \\({h(x)}\\)正是学习残差。如果多次执行上面的步骤，可以将流程描述成： \\[{F_0(x)}\\] \\[{F_1(x)=F_0(x)+h_1(x)}\\] \\[{F_2(x)=F_1(x)+h_2(x)}\\] \\[{...}\\] \\[{F_t(x)=F_t-1(x)+h_t(x)}\\] 即 \\({F(x;w)=\\sum^T_{t=1}h_t(x;w)}\\)，这也就是 GBDT 。 如何理解 Gradient Boosting Decision Tree ? Gradient Boosting Decision Tree 简称 GBDT，最早由 Friedman 在论文《Greedy function approximation: a gradient boosting machine》中提出。简单从题目中理解包含三个部分内容：Gradient Descent、Boosting、Decision Tree。 Decision Tree 即决策树，利用超平面对特征空间划分来预测和分类，根据处理的任务不同分成两种：分类树和回归树。在 GBDT 算法中，用到的是 CART 即分类回归树。用数学语言来描述为 \\({F=\\{f(x)=w_{q(x)}\\}}\\)，完成样本 \\({x}\\) 到决策树叶子节点 \\({q(x)}\\) 的映射，并将该叶子节点的权重 \\({w_{q(x)}}\\) 赋给样本。CART 中每次通过计算 gain 值贪心来进行二分裂。 Boosting 是一种常用的集成学习方法（另外一种是 Bagging）。利用弱学习算法，反复学习，得到一系列弱分类器（留一个问题，为什么不用线性回归做为弱分类器）。然后组合这些弱分类器，构成一个强分类器。上面提到的模型 \\({F(x;w)=\\sum^T_{t=1}h_t(x)}\\) 即是一种 boosting 思路，依次训练多个 CART 树 \\({h_i}\\)，并通过累加这些树得到一个强分类器 \\({F(x;w)}\\)。 为什么 GBDT 可行？ 在 2 中我提到 GBDT 包括三个部分并且讲述了 Boosting 和 Decison Tree。唯独没有提到 Gradient Descent，GBDT 的理论依据却恰恰和它相关。 回忆一下，Gradient Descent 是一种常用的最小化损失函数 \\({L(\\theta)}\\) 的迭代方法。 给定初始值 \\({\\theta_0}\\) 迭代公式：\\({\\theta ^t = \\theta ^{t-1} + \\Delta \\theta}\\) 将 \\({L(\\theta ^t)}\\) 在 \\({\\theta ^{t-1}}\\) 处进行一阶泰勒展开：\\({L(\\theta ^t)=L(\\theta ^{t-1} + \\Delta \\theta) \\approx L(\\theta ^{t-1}) + L^\\prime(\\theta ^{t-1})\\Delta \\theta}\\) 要使 \\({L(\\theta ^t) &lt; L(\\theta ^{t-1}) }\\)，取 \\({\\Delta \\theta = -\\alpha L^\\prime(\\theta ^{t-1})}\\) 其中 \\({\\alpha}\\) 是步长，可以通过 line search 确定，但一般直接赋一个很小的数。 在 1 中提到的问题中，损失函数是 MSE \\({L(y, F(x))=\\frac{1}{2}(y_i - f(x_i))^2}\\)。 我们的任务是通过调整 \\({F(x_1), F(x_2), ..., F(x_n)}\\) 最小化 \\({J=\\sum_i L(y_i, F(x_i))}\\)。 如果将 \\({F(x_i)}\\) 当成是参数，并对损失函数求导得到 \\({ \\frac{\\partial J}{\\partial F(x_i)} = \\frac{\\partial \\sum_i L(y_i, F(x_i))}{\\partial F(x_i)} = \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} = F(x_i)-y_i}\\)。 可以发现，在 1 中提到的模型 \\({h(x)}\\) 学习的残差 \\({y_i-F(x_i)}\\)正好等于负梯度，即 \\({y_i-F(x_i)=-\\frac{\\partial J}{\\partial F(x_i)}}\\)。 所以，参数的梯度下降和函数的梯度下降原理上是一致的： \\({F_{t+1}(x_i)=F_t(x_i)+h(x_i)=F(x_i)+y_i-F(x_i)=F_t(x_i)-1\\frac{\\partial J}{\\partial F(x_i)}}\\) \\({\\theta ^t = \\theta ^{t-1} + \\alpha L^\\prime(\\theta ^{t-1})}\\) GBDT 算法流程 模型 F 定义为加法模型： \\[{F(x;w)=\\sum^{M}_{m=1} \\alpha_m h_m(x;w_m) = \\sum^{M}_{m=1}f_t(x;w_t)}\\] 其中，x 为输入样本，h 为分类回归树，w 是分类回归树的参数，\\({\\alpha}\\) 是每棵树的权重。 通过最小化损失函数求解最优模型：\\({F^* = argmin_F \\sum^N_{i=1}L(y_i, F(x_i))}\\) 输入: \\({(x_i,y_i),T,L}\\) 初始化：\\({f_0(x)}\\) 对于 \\({t = 1 to T}\\) ： 计算负梯度（伪残差）： \\({ \\tilde{y_i} = -[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x)}]_{F(x)=F_{m-1}(x)} ,i=1,2,...,N}\\) 根据 \\({\\tilde{y_i}}\\) 学习第 m 棵树： \\({w^*=argmin_{w} \\sum_{i=1}^N(\\tilde{y_i} - h_t(x_i;w))^2}\\) line searcher 找步长：\\({\\rho^* = argmin_\\rho \\sum_{i=1}^{N}L(y_i, F_{t-1}(x_i)+\\rho h_t(x_i;w^*))}\\) 令 \\({f_t=\\rho^*h_t(x;w*)}\\)，更新模型：\\({F_t=F_{t-1}+f_t}\\) 输出 \\({F_T}\\) 说明： 初始化 \\({f_0}\\) 方法 求解损失函数最小 随机初始化 训练样本的充分统计量 每一轮拟合负梯度，而不是拟合残差，是为方便之后扩展到其他损失函数。 最小化问题中，如果有解析解，直接带入。否则，利用泰勒二阶展开，Newton Step 得到近似解。 这一篇就先到这里，之后还会分享 GBDT 常用损失函数推导以及 XGboost 相关内容。如果有任何想法，都可以在留言区和我交流。 Reference 李航, 《统计学习方法》8.4 提升树 Freidman，greedy function approximation ：a gradient boosting machine 【19年ML思考笔记】GBDT碎碎念（1）谈回归树的分裂准则 - 知乎 机器学习-一文理解GBDT的原理-20171001 - 知乎 GBDT入门详解 - Scorpio.Lu|Blog python - Why Gradient Boosting not working in Linear Regression? - Stack Overflow GBDT基本原理及算法描述 - Y学习使我快乐V的博客 - CSDN博客 GBDT的那些事儿 - 知乎","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://xiang578.com/tags/ml/"},{"name":"gdbt","slug":"gdbt","permalink":"https://xiang578.com/tags/gdbt/"}]},{"title":"(FTRL) Follow The Regularized Leader","slug":"ftrl","date":"2020-01-03T13:26:06.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/ftrl.html","link":"","permalink":"https://xiang578.com/post/ftrl.html","excerpt":"","text":"FTRL 是 Google 提出的一种优化算法。常规的优化方法例如梯度下降、牛顿法等属于批处理算法，每次更新需要对 batch 内的训练样本重新训练一遍。在线学习场景下，我们希望模型迭代速度越快越好。例如用户发生一次点击行为后，模型就能快速进行调整。FTRL 在这个场景中能求解出稀疏化的模型。 基础知识 L1 正则比 L2 正则可以产生更稀疏的解。 次梯度：对于 L1 正则在 \\(x=0\\) 处不可导的情况，使用次梯度下降来解决。次梯度对应一个集合 \\(\\{v: v(x-x_t) \\le f(x)-f(x_t)\\}\\)，集合中的任意一个元素都能被当成次梯度。以 L1 正则为例，非零处梯度是 1 或 -1，所以 \\(x=0\\) 处的次梯度可以取 \\([-1, 1]\\) 之内任意一个值。 FTL FTL(Follow The Leader) 算法：每次找到让之前所有损失函数之和最小的参数。 \\[W=argmin_W \\sum^t_{i=1}F_i(W)\\] FTRL 中的 R 是 Regularized，可以很容易猜出来在 FTL 的基础上加正则项。 \\[W=argmin_W \\sum^t_{i=1}F_i(W) + R(W)\\] 代理函数 FTRL 的损失函数直接很难求解，一般需要引入一个代理损失函数 \\(h(w)\\)。代理损失函数常选择比较容易求解析解以及求出来的解和优化原函数得到的解差距不能太大。 我们通过两个解之间的距离 Regret 来衡量效果： \\[ \\begin{array}{c}{w_{t}=\\operatorname{argmin}_{w} h_{t-1}(w)} \\\\ {\\text {Regret}_{t}=\\sum_{t=1}^{T} f_{t}\\left(w_{t}\\right)-\\sum_{t=1}^{T} f_{t}\\left(w^{*}\\right)}\\end{array} \\] 其中 \\(w^{*}\\) 是直接优化 FTRL 算法得到的参数。当距离满足 \\(\\lim _{t \\rightarrow \\infty} \\frac{\\text {Regret}_{t}}{t}=0\\)，损失函数认为是有效的。其物理意义是，随着训练样本的增加，两个优化目标优化出来的参数效果越接近。 推导过程 参数 \\(w_{t+1}\\) 的迭代公式： \\[{w_{t+1}=argmin_w\\{ g_{(1:t)}w + \\frac{1}{2} \\sum_{s=1}^t \\sigma_s \\lVert w - w_s \\rVert ^2 + \\lambda_1 \\lVert W \\rVert_1 + \\frac{1}{2} \\lambda_2 \\lVert W \\rVert^2 \\}}\\] 其中 \\(g_{(1:t)}=\\sum^{t}_{s=1}g_s\\)，\\(g_s\\) 为 \\(f(w_s)\\) 的次梯度。参数 \\(\\sum^t_{s=1}\\sigma_s=\\frac{1}{\\eta _t}\\)，学习率 \\(\\eta _t = \\frac{1}{\\sqrt{t}}\\)，随着迭代轮数增加而减少。 展开迭代公式 \\[{F(w)= g_{(1:t)}w + \\frac{1}{2} \\sum_{s=1}^t \\sigma_s \\lVert w - w_s \\rVert ^2 + \\lambda_1 \\lVert W \\rVert_1 + \\frac{1}{2} \\lambda_2 \\lVert W \\rVert^2 }\\] \\[{F(w)= g_{(1:t)}w + \\frac{1}{2} \\sum_{s=1}^t \\sigma_s ( w^Tw - 2w^Tw_s + w_s^Tw_s) + \\lambda_1 \\lVert W \\rVert_1 + \\frac{1}{2} \\lambda_2 \\lVert W \\rVert^2 }\\] \\[{F(w)= (g_{(1:t)} - \\sum_{s=1}^t \\sigma_s w_s)w + \\frac{1}{2} (\\sum_{s=1}^t \\sigma_s + \\lambda_2) w^Tw + \\lambda_1 \\lVert W \\rVert_1 + const }\\] \\[{F(w)= z_t^Tw + \\frac{1}{2} (\\frac{1}{\\eta _t} + \\lambda_2) w^Tw + \\lambda_1 \\lVert W \\rVert_1 + const }\\] 其中 \\({z_{t-1}=g^{(1:t-1)} - \\sum_{s=1}^{t-1} \\sigma_s w_s}\\)。 对 \\(F(w)\\) 求偏导得到： \\[{z_t + (\\frac{1}{\\eta _t} + \\lambda_2) w + \\lambda_1 \\partial \\lvert W \\rvert = 0}\\] \\(w\\) 和 \\(z\\) 异号时，等式成立。 根据基础知识里面提到的对于 L1 正则利用偏导数代替无法求解的情况，得到： \\[ \\partial|W|=\\left\\{\\begin{array}{ll}{0,} &amp; {\\text { if }-1&lt;w&lt;1} \\\\ {1,} &amp; {\\text { if } w&gt;1} \\\\ {-1,} &amp; {\\text { if } w&lt;-1}\\end{array}\\right. \\] 当 \\({ z_t &gt; \\lambda_1}\\) 时，\\({w_i &lt; 0}\\) , \\({w_i = \\frac{- z_t + \\lambda_1 }{\\frac{1}{\\eta _t} + \\lambda_2 }}\\) 当 \\({ z_t &lt; - \\lambda_1}\\) 时，\\({w_i &gt; 0}\\) , \\({w_i = \\frac{- z_t - \\lambda_1 }{\\frac{1}{\\eta _t} + \\lambda_2 }}\\) 当 \\({ \\lvert z_t \\rvert &lt; \\lambda_1}\\) 时，当且仅当 \\({w_i=0}\\) 成立 因此可得： \\[ w_{i}=\\left\\{\\begin{array}{ll}{0,} &amp; {\\text { if }\\left|z_{i}\\right| \\leq \\lambda_{1}} \\\\ {\\frac{-\\left(z_{i}-\\text sgn(z_i) \\lambda_{1}\\right)}{\\eta_{t}+\\lambda_{2}},} &amp; {\\text { if others }}\\end{array}\\right. \\] FTRL 和 SGD 的关系 将 SGD 的迭代公式写成：\\({W^{t+1}=W^t - \\eta _tg_t}\\) FTRL 迭代公式为：\\({W^{t+1}=argmin_w\\{ G^{(1:t)}W + \\lambda_1 \\lVert W \\rVert_1 +\\lambda_2 \\frac{1}{2} \\lVert W \\rVert \\}}\\) 代入 \\({\\sum^t_{s=1}\\sigma _s= \\frac{1}{\\eta _t}}\\) 到上面的公式中，得到 \\({W^{t+1}=argmin_w\\{ \\sum_t^{s=1}g_sW + \\frac{1}{2} \\sum^t_{s=1}\\sigma _s\\lVert W - W_s \\rVert_2^2 \\}}\\) 求偏导得到 \\({\\frac{\\partial f(w)}{\\partial w} = \\sum^t_{s=1}g_s + \\sum^t_{s=1}\\sigma _s( W - W_s )}\\) 令偏导等于 0 ：\\({\\sum^t_{s=1}g_s + \\sum^t_{s=1}\\sigma _s( W^{t+1} - W_s ) = 0}\\) 化简得到：\\({(\\sum^t_{s=1}\\sigma _s) W^{t+1} = \\sum^t_{s=1}\\sigma _s W^{s} - \\sum^t_{s=1}g_s}\\) 代入 \\(\\sigma\\)：\\({\\frac{1}{\\eta _t} W^{t+1} = \\sum^t_{s=1}\\sigma _s W^{s} - \\sum^t_{s=1}g_s}\\) 根据上一个公式得出上一轮的迭代公式：\\({\\frac{1}{\\eta _{t-1}} W^{t} = \\sum^{t-1}_{s=1}\\sigma _s W^{s} - \\sum^{t-1}_{s=1}g_s}\\) 两式相减：\\({\\frac{1}{\\eta _t} W^{t+1} - \\frac{1}{\\eta _{t-1}} W^{t} = (\\frac{1}{\\eta _t} - \\frac{1}{\\eta _{t-1}}) W_t - g_t}\\) 最终化简得到和 SGD 迭代公式相同的公式：\\({W_{t+1} = W_t - \\eta_t g_t}\\) FTRL 工程化伪代码 引用自论文 Ad Click Prediction: a View from the Trenches 下面的伪代码中学习率和前面公式推导时使用的一些不一样： \\(\\eta_{t_{i}}=\\frac{\\alpha}{\\beta+\\sqrt{\\sum_{s=1}^{t} g_{s_{i}}^{2}}}\\)。Facebook 在 GBDT + LR 的论文中研究过不同的学习率影响，具体可以参看博文 Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr) | 算法花园。 FTRL 例：FM 使用 FTRL 优化 FM 是工业界常用的机器学习算法，在之前博文 (FM)Factorization Machines 中有简单的介绍。内部的 FTRL+FM 代码没有开源，所以也不好分析。从 FM+FTRL算法原理以及工程化实现 - 知乎 中找了一张 FTRL+FM 的伪代码图片。 Reference Online Learning算法理论与实践 - 美团技术团队 FTRL公式推导 - 知乎 每周一文】Ad Click Prediction: a View from the Trenches(2013)_机器学习,CTR,online_fangqingan_java的专栏-CSDN博客","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"google","slug":"google","permalink":"https://xiang578.com/tags/google/"},{"name":"ml","slug":"ml","permalink":"https://xiang578.com/tags/ml/"},{"name":"fm","slug":"fm","permalink":"https://xiang578.com/tags/fm/"},{"name":"ftrl","slug":"ftrl","permalink":"https://xiang578.com/tags/ftrl/"}]},{"title":"每月分享 202001 Fine-Tune Your Days","slug":"monthly-issue-202001","date":"2020-01-01T08:55:11.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/monthly-issue-202001.html","link":"","permalink":"https://xiang578.com/post/monthly-issue-202001.html","excerpt":"","text":"[TOC] 这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。 0x04 你见过哪些让你目瞪口呆、脑洞大开的骗局？ - SME情报员的回答 - 知乎 推荐其中的吉普赛读心术，当成脑筋急转弯来看。 首先任选一个两位数，在心里默默记住，然后用这个两位数再依次减去它的十位和个位，最后用得数查表，找到对应的怪符号。 比如67,相应的计算就是67-6-7=54， 现在，在表中找到你心中数字经过计算后所对应的符号。 最后的答案都会是 0x03 Deep Neural Networks for YouTube Recommendations #paper #ml Youtube 几年前的论文，最近拿过来看一下。工业界的论文最大的价值是提到的一些 tick，比如这篇论文中分析到用户对新视频的偏好，引入 example age 代表视频的上传到预测时的时间。再比如，给用户推荐视频时，考虑用户看过这个视频相关频道次数以及这个视频在用户实现中出现的次数。所以，做算法实现需要深入理解自己所处的场景。 推荐知乎上一些关于这篇论文的解读： Deep Neural Network for YouTube Recommendation论文精读 - 知乎 重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文 - 知乎 YouTube深度学习推荐系统的十大工程问题 - 知乎 揭开YouTube深度推荐系统模型Serving之谜 - 知乎 0x02 Fine-Tune Your Days with the Scientific Method 这个小标题出自 Make Time，翻译成中文是利用科学的方法每天微调你的习惯。 0x01 2020 阅读看板 参考部分网友 Notion 的用法，搭建一个自己的阅读看板 看书也就图一乐。目前挑选出来的书远远超过前两年的阅读量，加油一起读书。 Notion 这种一个数据库 + 可选的 View 很接近我心目中任务管理软件的极限。 reading board 0x00 大佬们的年度总结 新的一年开始时，最期待翻看大佬们的年度总结，罗列一些我觉得有总结。 Growing a Result-Driven Mindset - Yiming Chen：英文总结，Yiming 的博客给我带来学习英语并且用之来表达的动力。 2019 总结与 2020 计划 | 小土刀 2.0：从不同角度回顾自己的 2019 年。 2019 时光小偷：这位博主每年总结的标题都是一首歌，也是几年前看他的总结才开始尝试写自己的总结。 致敬时间的价值：一品十年 - 知乎：和这个主题没有太大的关系，看一下其他人十年的总结，也能很好的指导自己的生活。 2019年：下个十年路口，Farewell | Crossairplane的博客：读这篇文章的时候突然想到一点，之后再看年终总结时，留言一句「新年快乐」。 Create vs. Consume - ends 2019 then starts 2020 - Ziting Li：真诚的思考。 我的千书阅读计划 - 意欲蔓延 - Hi!PDA Hi!PDA：fatdragoncat 13 年在 Hi!PDA 上立下愿望，这么多年过去，不知道数量上有没有达到，但是读书的收获已经改变他的生活。难得可贵这篇帖子展示他的变化过程。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"李宏毅强化学习课程笔记 PG PPO Q-Learing","slug":"reinforce-learnning-basic","date":"2019-12-26T13:14:47.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/reinforce-learnning-basic.html","link":"","permalink":"https://xiang578.com/post/reinforce-learnning-basic.html","excerpt":"","text":"Info 课件下载：Hung-yi Lee - Deep Reinforcement Learning 课程视频：DRL Lecture 1: Policy Gradient (Review) - YouTube Change Log 20191226: 整理 PPO 相关资料 20191227: 整理 Q-Learning 相关资料 20200906: 拖延半年多没有整理笔记，将剩下的内容整理到单独的笔记中。 我的笔记汇总： - Policy Gradient、PPO: Proximal Policy Optimization、Q-Learning - Actor Critic - Sparse Reward - Imitation Learning RL 基础 强化学习基本定义： Actor：可以感知环境中的状态，通过执行不同的动作得到反馈的奖励，在此基础上进行学习优化。 Environment：指除 Actor 之外的所有事务，受 Actor 动作影响而改变其状态，并给 Actor 对应的奖励。 on-policy 和 off-policy 的区别在于 Actor 和 Environment 交互的策略和它自身在学习的策略是否是同一个。 一些符号： State s 是对环境的描述，其状态空间是 S。 Action a 是 Actore 的行为描述，其动作空间是 A。 Policy \\(\\pi(a|s)=P[A_t=a|S_t=s]\\) 代表在给定环境状态 s 下 动作 a 的分布。 Reward \\({r(s,a,s^{\\prime})}\\) 在状态 s 下执行动作 a 后，Env 给出的打分。 Policy Gradient Policy Network 最后输出的是概率。 目标：调整 actor 中神经网络 policy \\(\\pi(\\theta)\\)，得到 \\(a=\\pi(s, \\theta)\\)，最大化 reward。 trajectory \\(\\tau\\) 由一系列的状态和动作组成，出现这种组合的概率是 \\(p_{\\theta}(\\tau)\\) 。 \\[ \\begin{array}{l}{p_{\\theta}(\\tau)} \\\\ {\\quad=p\\left(s_{1}\\right) p_{\\theta}\\left(a_{1} | s_{1}\\right) p\\left(s_{2} | s_{1}, a_{1}\\right) p_{\\theta}\\left(a_{2} | s_{2}\\right) p\\left(s_{3} | s_{2}, a_{2}\\right) \\cdots} \\\\ {\\quad=p\\left(s_{1}\\right) \\prod_{l=1}^{T} p_{\\theta}\\left(a_{t} | s_{t}\\right) p\\left(s_{t+1} | s_{t}, a_{t}\\right)}\\end{array} \\] reward ：根据 s 和 a 计算得分 r，求和得到 R。在围棋等部分任务中，无法获得中间的 r（下完完整的一盘棋后能得到输赢的结果）。 需要计算 R 的期望 \\(\\bar{R}_{\\theta}\\)，形式和 GAN 类似。如果一个动作得到 reward 多，那么就增大这个动作出现的概率。最终达到 agent 所做 policy 的 reward 一直都比较高。 \\[ \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\] 强化学习中，没有 label。需要从环境中采样得到 \\(\\tau\\) 和 R，根据下面的公式去优化 agent。相当于去求一个 likelihood。 \\(\\nabla f(x) = f(x) \\frac{\\nabla f(x)}{f(x)}= f(x) \\nabla \\log f(x)\\) ，这一步中用到对 log 函数进行链式求导。 \\[ \\nabla \\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau) \\] \\[ \\begin{array}{l}{=E_{\\left.\\tau \\sim p_{\\theta}(\\tau)[R(\\tau)] \\log p_{\\theta}(\\tau)\\right]} \\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right)} \\\\ {=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)}\\end{array} \\] 参数更新方法： 在环境中进行采样，得到一系列的轨迹和回报。 利用状态求梯度，更新模型。如果 R 为正，增大概率 \\(p_{\\theta}(a_t|s_t)\\), 否则减少概率。 重复上面的流程。 PG 的例子 训练 actor 的过程看成是分类任务：输入 state ，输出 action。 最下面公式分别是反向传播梯度计算和 PG 的反向梯度计算，PG 中要乘以整个轨迹的 R。 PG tip 1： add a baseline 强化学习的优化和样本质量有关，避免采样不充分。Reawrd 函数变成 R-b，代表回报小于 b 的都被我们当成负样本，这样模型能去学习得分更高的动作。b 一般可以使用 R 的均值。 tip 2: assign suitable credit 一场游戏中，不论动作好坏，总会乘上相同的权重 R，这种方法是不合理的，希望每个 action 的权重不同。 引入一个 discount rate，对 t 之后的动作 r 进行降权重。 利用 Advantage Function 评价状态 s 下动作 a 的好坏 critic。 Assign Suitable Credit PPO: Proximal Policy Optimization importance sampling 假设需要估计期望 \\(E_{x~p[f(x)]}\\)，x 符合 p 分布，将期望写成积分的形式。由于在 P 分布下面很难采样，把问题转化到已知 q 分布上，得到在 p 分布下计算期望公式。 上面方法得到 p 和 q 期望接近，但是方差可能相差很大，且和 \\(\\frac{p(x)}{q(x)}\\) 有关。 原分布的方差： \\[ \\operatorname{Var}_{x-p}[f(x)]=E_{x-p}\\left[f(x)^{2}\\right]-\\left(E_{x-q}[f(x)]\\right)^{2} \\] 新分布的方差： \\[ \\begin{array}{l}{\\operatorname{Var}_{x \\sim p}[f(x)]=E_{x \\sim p}\\left[f(x)^{2}\\right]-\\left(E_{x \\sim p}[f(x)]\\right)^{2}} \\\\ {\\operatorname{Var}_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]=E_{x \\sim q}\\left[\\left(f(x) \\frac{p(x)}{q(x)}\\right)^{2}\\right]-\\left(E_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]\\right)^{2}} \\\\ {=E_{x \\sim p}\\left[f(x)^{2} \\frac{p(x)}{q(x)}\\right]-\\left(E_{x \\sim p}[f(x)]\\right)^{2}}\\end{array} \\] 在 p 和 q 分布不一致时，且采样不充分时，可能会带来比较大的误差。 Issue of Importance Sampling 从 On-policy 到 Off-policy on-policy 时，PG 每次参数更新完成后，actor 就改变了，不能使用之前的数据，必须和环境重新互动收集数据。引入 \\(p_{\\theta \\prime}\\) 进行采样，就能将 PG 转为 off-ploicy。 和之前相比，相当于引入重要性采样，所以也有前一节中提到的重要性采样不足问题。 \\[ J^{\\theta^{\\prime}}(\\theta)=E_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)\\right] \\] PPO/TRPO 为了克服采样的分布与原分布差距过大的不足，PPO 引入 KL 散度进行约束。KL 散度用来衡量两个分布的接近程度。 \\[ J_{P P O}^{\\theta^{\\prime}}(\\theta)=J^{\\theta^{\\prime}}(\\theta)-\\beta K L\\left(\\theta, \\theta^{\\prime}\\right) \\] TRPO(Trust Region Policy Optimization)，要求 \\(K L\\left(\\theta, \\theta^{\\prime}\\right)&lt;\\delta\\)。 \\[ J_{T R P O}^{\\theta^{\\prime}}(\\theta)=E_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)\\right] \\] KL 散度可能比较难计算，在实际中常使用 PPO2。 A&gt;0，代表当前策虑表现好。需要增大 \\(\\pi( \\theta )\\)，通过 clip 增加一个上限，防止 \\(\\pi( \\theta )\\) 和旧分布变化太大。 A&lt;0，代表当前策虑表现差，不限制新旧分布的差异程度，只需要大幅度改变 \\(\\pi( \\theta )\\)。 参考 【点滴】策略梯度之PPO - 知乎 PPO algorithm 系数 \\(\\beta\\) 在迭代的过程中需要进行动态调整。引入 \\(KL_{max} KL_{min}\\)，KL &gt; KLmax，说明 penalty 没有发挥作用，增大 \\(\\beta\\)。 Q-Learning value-base 方法，利用 critic 网络评价 actor 。通过状态价值函数 \\(V^{\\pi}(s)\\) 来衡量预期的期望。V 和 pi、s 相关。 Monte-Carlo MC: 训练网络使预测的 \\(V^{\\pi}(s_a)\\) 和实际完整游戏 reward \\(G_a\\) 接近。 Temporal-difference TD: 训练网络尽量满足 \\(V^{\\pi}(s_t)=V^{\\pi}(s_{t+1}) + r_t\\) 等式，两个状态之间的收益差。 MC: 根据策虑 \\(\\pi\\) 进行游戏得到最后的 \\(G(a)\\)，最终存在方差大的问题。\\(\\operatorname{Var}[k X]=k^{2} \\operatorname{Var}[X]\\) TD: r 的方差比较小，\\(V^{\\pi}(s_{t+1})\\) 在采样不充分的情况下，可能不准确。 Another Critic State-action value function \\(Q^{\\pi}(s, a)\\)：预测在 pi 策略下，pair(s, a) 的值。相当于假设 state 情况下强制采取 action a。 对于非分类的方法： Q-Learning 初始 actor \\(\\pi\\) 与环境互动 学习该 actor 对应的 Q function 找一个比 \\(\\pi\\) 好的策虑：\\(\\pi \\prime\\)，满足 \\(V^{\\pi \\prime}(s,a) \\ge V^{\\pi}(s,a)\\), \\(\\pi^{\\prime}(s)=\\arg \\max _{a} Q^{\\pi}(s, a)\\) 在给定 state 下，分别代入 action，取函数值最大的 a，作为后面对该 state 时采取的 action。 证明新的策虑存在： Target NetWork 左右两边的网络相同，如果同时训练比较困难。简单的想法是固定右边的网络进行训练，一定次数后再拷贝左边的网络。 Exploration Q function 导致 actor 每次都会选择具有更大值的 action，无法准确估计某一些动作，对于收集数据而言是一个弊端。 Epsilon Greedy 小概率进行损失采样 Boltzmann Exploration 利用 softmax 计算选取动作的概率，然后进行采样 Replay buffer 采样之后的 \\((s_t, a_t, r_t, s_{t+1})\\) 保存在一个 buffer 里面（可能是不同策虑下采样得到的)，每次训练从 buffer 中 sample 一个 batch。 结果：训练方法变成 off-policy。减少 RL 重复采样，充分利用数据。 Typical Q-Learning Algorithm Q-Learning 流程： Double DQN DDQN Q value 容易高估：目标值 \\(r_t + maxQ(s_{t+1}, a)\\) 倾向于选择被高估的 action，导致 target 很大。 选动作的 Q' 和计算 value 的 Q(target network) 不同。Q 中高估 a，Q' 可能会准确估计 V 值。Q' 中高估 a ，可能不会被 Q 选中。 Dueling DQN 改 network 架构。V(s) 代表 s 所具有的价值，不同的 action 共享。 A(s,a) advantage function 代表在 s 下执行 a 的价值。最后 \\(Q(s, a) = A(s, a) + V(s)\\)。 为了让网络倾向于使用 V（能训练这个网络），得到 A 后，要对 A 做 normalize。 Prioritized Reply 在训练过程中，对于经验 buffer 里面的样本，TD error 比较大的样本有更大的概率被采样，即难训练的数据增大被采样的概率。 Multi-step 综合 MC 和 TD 的优点，训练样本按一定步长 N 进行采样。MC 准确方差大，TD 方差小，估计不准。 Noisy Net Noise on Action：在相同状态下，可能会采取不同的动作。 Noise on Parameters：开始时加入噪声。同一个 episode 内，参数不会改变。相同状态下，动作相同。 更好探索环境。 Distributional Q-function Q 是累积收益的期望，实际上在 s 采取 a 时，最终所有得到的 reward 为一个分布 reward distribution。部分时候分布不同，可能期望相同，所以用期望来代替 reward 会损失一些信息。 Distributional Q-function 直接输出分布，均值相同时，采取方差小的方案。这种方法不会产生高估 q 值的情况。 Rainbow rainbow 是各种策略的混合体。 DDQN 影响不大。 Continuous Actions action 是一个连续的向量，Q-learning 不是一个很好的方法。 \\[ a=\\arg \\max _{a} Q(s, a) \\] 从 a 中采样出一批动作，看哪个行动 Q 值最大。 使用 gradient ascent 解决最优化问题。 设计一个网络来化简过程。 \\(\\sum\\) 和 \\(\\mu\\) 是高斯分布的方差和均值，保证矩阵一定是正定。 最小化下面的函数，需要最小化 \\(a - \\mu\\)。 Reference 强化学习基础知识 - 知乎","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://xiang578.com/tags/algorithm/"},{"name":"reinforcementlearning","slug":"reinforcementlearning","permalink":"https://xiang578.com/tags/reinforcementlearning/"}]},{"title":"每月分享 201912","slug":"monthly-issue-201912","date":"2019-12-16T08:55:11.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/monthly-issue-201912.html","link":"","permalink":"https://xiang578.com/post/monthly-issue-201912.html","excerpt":"","text":"[TOC] 这里记录过去一个月，我看到、想到值得分享的东西，每周六滚动更新。 0x02. Org-mode Workflow 国外一名 CS 学生的 org mode workflow 教程，包括 GTD 和 Zettelkasten 两个主要的部分，分别对应时间管理和知识管理，是一份很好的参考资料。 0x01. HHKB 更新 改めてとなりますが今回新しく登場したHHKBは3機種になります。それぞれの特徴をまとめたものがこちらになります。#HHKBミートアップ pic.twitter.com/GVVxNI6H72 — HHKB OFFICIAL (@PFU_HHKB) December 10, 2019 HHKB 好久之后终于更新了！不过价格也变得更贵……还是很喜欢自己 18 年买的 HHKB BT 版。不过在使用 Emacs 之后，出现没有方向键的烦恼。之前通过映射 Ctrl + HJKL 替代方向键，然后和 Emacs 的一些快捷键冲突……等买一个新的机械键盘。 0x00. My GTD Workflow (2019 ver.) - Yiming Chen #gtd 很少看到国人用英文写的 GTD 相关文章，年初自己也想按 Workflow 这种形式写一篇，不过一直拖到现在都没有完成。 对任务设置优先级：A B C 如何设置任务优先级，对目标进行分解 每年一月份设定年度目标 每月一号根据年度目标设定月度目标 每周日根据月度目标设定每周目标 每天早上设定当天目标 任务安排优先级和截止日期后，可以使用四象限法则。 回顾技巧 追求 100% 完成，可以接受 70%。 一个任务多次延迟之后，考虑是否还是重要。 如果任务还是重要，对任务进行拆分。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[{"name":"monthly-issue","slug":"monthly-issue","permalink":"https://xiang578.com/tags/monthly-issue/"}]},{"title":"2019 年软硬件指北","slug":"2019-consumer-report","date":"2019-12-15T13:29:38.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/2019-consumer-report.html","link":"","permalink":"https://xiang578.com/post/2019-consumer-report.html","excerpt":"","text":"呼吸不止，折腾不停。记录在过去的一年，自己选择的软件和硬件。去年写指南并不能指南，所以今年直接写成指北。 硬件更新 iPhone XR 和 Apple Watch Series 4 iPhone XR 刚出来的时候，一直被吐槽是大边框。不过随着在电商网站上不断降价，越来越被当成是无边框手机……在忍受不了使用多年 iPhone 6 的卡顿，以及很难脱离 iOS 生态的现实。终于在苏宁上下单 \\((Product)^{read}\\) 版的 XR。经过半年多的使用，这部手机实用但是不出彩。 Apple Watch 圆环图 购买 Apple Watch Series 4(AW) 理由很简单：在去公司健身房锻炼的时候，希望有一个可以记录运动数据的设备。事实 AW 自带的运动软件很不错，可以满足我的运动记录需求。但是例如 Keep 之类的第三方 App 适配不好。另外，AW 很好扩展 iPhone 和 Mac 的使用，比如可以解锁 Mac、查看 iPhone 上的信息等等。到头来，AW 还只是一块需要一天一充的电子表。 Nintendo Switch Nintendo Switch(NS) 是任天堂在 2017 年推出的，集掌机和主机于一体的游戏机。买 NS 的理由也很简单，Mac 上游戏太少，我需要一个设备玩游戏。更深层次的来说，感觉自己的反应太慢，想通过玩游戏来锻炼快速决策能力。 简单统计一下，我在 NS 上花费的时间大概有 200 多小时，购入游戏也花费上千元……反应能力不知道有没有上去，但是享受到游戏的快乐。 任天堂就是世界的主宰 Nintendo Switch 目前可以选的有 Nintendo Switch、Nintendo Switch 续航版、Nintendo Switch Lite。 Kindle Oasis 2 大概是 5 月末，通过公司内部闲置群出了使用 5 年多的 Kindle PaperWhite 2 后，从淘宝上购入美版 Kindle Oasis 2 。可惜的是，1 个月不到的时间，亚马逊推出 Kindle Oasis 3 …… kindle oasis 2 和 kindle 2 对比图 和 KPW2 相比，KO2 主要带来一下几个方面的提升： 7寸屏幕，更高的分辨率。看的更多，看的更清晰，更加逼近纸书的感觉。 不对称设计，电池集中在一边，握持比较舒服。 金属机身。前几代 kindle 都是亚马逊祖传的类肤质塑料机身，很容易沾上油脂，这一代采用金属机身，看起来更加富有科技感。毕竟当年小米用上金属边框的时候，都敢去吹一块钢板的艺术之旅。 两个翻页实体按键，按起来比较有安全感。 上面说这么多，kindle 主要功能还是看书。这几年，很多 kindle 电子书分享站都由于版权问题陆续关闭，优质的资源比较难下载。不过，去年自己办信用卡时，领了一年的 Kinle U 会员（今年又领到一年的会员），在中亚上借阅很多本小说。从体验上来说，KU 会员不能实现全场自由借，而且大部分书籍都只是滥竽充数。一对比微信读书会员就是十分实惠，多期待微信读书可以出电子书阅读器吧。 软件实践 18 年开始，着手准备构建自己的数字化系统。19 年在前面的基础上，进行了很多迁移。 信息管理 11 月份看到一句话：input 做的越多，知识管理越差。这个很好形容我之前的状态，在印象笔记中囤积待看的剪藏、OF 里面有很多想写的主题、MWeb 遗留大量没有写完的文章。 年中的时候，想把自己写的一些笔记更好的管理起来。最初想到的是搭建 wiki ，实现知识的网状化连接。不过市面上常用的一些个人 wiki 方案都不是很满意。最终选择 hexo 搭配一个 wiki 主题 Wikitten。另外，后来了解到有一种基于纯文本的知识管理方案：zettelkasten。感兴趣的可以去看一下。 写日记是这么多年以来坚持的一件小事情。之前一直是在笔记本上写，后来慢慢的尝试通过印象笔记来写。2月份，订阅 Day One ，开始尝试迁移到它上面去。作为一个专业的软件，体验真的比之前的方式不知道好多少。Day One 上也有很多数据统计，多少可以拿来得瑟用。另外，自己干的一件事情就是把印象笔记中的日记慢慢转移到 Day one 。写在笔记本上的日记，也被我拍成一张又一张的照片，只不过这个迁移起来比较麻烦。 Day one 按年统计图 任务管理 这个问题一直是一个大坑，花费很多时间在多个软件中试来试去。在现在这个时间点，自己开始选择混合使用 OmniFocus 和 Org mode。具体怎么搭配使用，等再坚持几个月再出来分享。不过说回来，任务管理的关键不在于软件，而在于执行。 其他实践 下面这一些今年自己做的选择，都有一个共同的特点：从商业软件到开源项目。很多人选着使用的开源项目的出发点在于害怕商业公司无休止的使用个人隐私数据，而吸引我的主要是自由软件自由开放的精神。 从 MoneyWiz 到 Beancount MoneyWiz 是在少数派上了解到记账软件，Setapp 中可以免费使用。和国内那些整天搞社区和卖理财的记账软件相比，只是纯粹的一个记账软件。Beancount 是无意中从BYvoid文章中了解的一款纯文本记账软件。最大的优点是扩展性强。在使用过程中，搭配一些简单的脚本，可以实现每月底花一个小时就能把这个月的开销记录明白。 Beancount fava 从 1Password 到 KeePass 之前看过一个结论：密码破解的难度主要在于长度而不是复杂度。所以借助密码软件辅助记忆密码是不二之选。1Password 是在去年感恩节活动中获得的长达一年的免费体验。快要到期前，没有选择转向订阅（今年感恩节活动依然是新用户 长度一年的免费使用），反而是选择开源的 KeePass。KeePass 在不同的平台上有多个客户端可以选择，目前我主要用的是 MacPass 和奇密。KeePass 中所有的密码数据都保存在一个文件中，跨平台使用只需要简单同步这个文件。 从搜狗输入法到鼠须管 网上关于搜狗输入法的声讨一直不绝于耳，我也长时间忍受搜狗动不动给你跳出来的斗图功能提示。在花费一番力气，配置鼠须管后，彻底删除搜狗，详见 「Rime 鼠须管」小鹤双拼配置指南 | 算法花园。另外 Mac 上自带的输入法的体验也没有那么差。 博客上和这个主题相关的文章： Best of iPhone 2019 软件清单 | 算法花园 2018 年消费指南 | 算法花园 iPhone软件清单 | 算法花园 Mac软件清单 | 算法花园","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"}]},{"title":"Standford CS231n 2017 课程部分总结","slug":"cs231n-summary","date":"2019-12-04T09:58:39.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/cs231n-summary.html","link":"","permalink":"https://xiang578.com/post/cs231n-summary.html","excerpt":"","text":"去年学习这门做的部分笔记，现在分享出来。 笔记格式有些问题，持续整理中。 大量内容参考 mbadry1/CS231n-2017-Summary Table of contents Table of contents Course Info 01. Introduction to CNN for visual recognition 02. Image classification 03. Loss function and optimization 04. Introduction to Neural network 05. Convolutional neural networks (CNNs) 06. Training neural networks I 07. Training neural networks II 09. CNN architectures Reference Course Info 主页: http://cs231n.stanford.edu/ 视频：斯坦福深度学习课程CS231N 2017中文字幕版+全部作业参考_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili 大纲：Syllabus | CS 231N 课件：Index of /slides/2017 笔记：贺完结！CS231n官方笔记授权翻译总集篇发布 作业仓库：MachineLearning/CS231n at master · xiang578/MachineLearning 总课时: 16 01. Introduction to CNN for visual recognition 视觉地出现促进了物种竞争。 ImageNet 是由李飞飞维护的一个大型图像数据集。 自从 2012 年 CNN 出现之后，图像分类的错误率大幅度下降。 神经网络的深度也从 7 层增加到 2015 年的 152 层。截止到目前，机器分类准确率已经超过人类，所以 ImageNet 也不再举办相关比赛。 CNN 在 1998 年就被提出，但是这几年才流行开来。主要原因有：1) 硬件发展，并行计算速度提到 2）大规模带标签的数据集。 Gola: Understand how to write from scratch, debug and train convolutional neural networks. 02. Image classification 图像由一大堆没有规律的数字组成，无法直观的进行分类，所以存在语义鸿沟。分类的挑战有：视角变化、大小变化、形变、遮挡、光照条件、背景干扰、类内差异。 Data-Driven Approach Collect a dataset of images and labels Use Machine Learning to train a classifier Evaluate the classifier on new images 图像分类流程：输入、学习、评估 图像分类数据集：CIFAR-10，这个数据集包含了60000张32X32的小图像。每张图像都有10种分类标签中的一种。这60000张图像被分为包含50000张图像的训练集和包含10000张图像的测试集。 一种直观的图像分类算法：K-nearest neighbor(knn) 为每一张需要预测的图片找到距离最近的 k 张训练集中的图片，然后选着在这 k 张图片中出现次数最多的标签做为预测图片的标签（多数表决）。 训练过程：记录所有的数据和标签 \\({O(1)}\\) 预测过程：预测给定图片的标签 \\({O(n)}\\) Hyperparameters：k and the distance Metric Distance Metric L1 distance(Manhattan Distance) L2 distance(Euclidean Distance) knn 缺点 Very slow at test time Distance metrics on pixels are not informative 反例：下面四张图片的 L2 距离相同 Hyperparameters: choices about the algorithm that we set ranther than learn 留一法 Setting Hyperparameters by Cross-validation: 将数据划分为 f 个集合以及一个 test 集合，数据划分中药保证数据集的分布一致。 给定超参数，利用 f-1 个集合对算法进行训练，在剩下的一个集合中测试训练效果，重复这一个过程，直到所有的集合都当过测试集。 选择在训练集中平均表现最好的超参数。 Linear classification: Y = wX + b b 为 bias，调节模型对结果的偏好 通过最小化损失函数来，来确定 w 和 b 的值。 Linear SVM: classifier is an option for solving the image classification problem, but the curse of dimensions makes it stop improving at some point. @todo Logistics Regression: 无法解决非线性的图像数据 03. Loss function and optimization 通过 Loss function 评估参数质量 比如 \\[L=\\frac{1}{N}\\sum_iL_i\\left(f\\left(x_i,W\\right),y_i\\right)\\] Multiclass SVM loss 多分类支持向量机损失函数 \\[L_i=\\sum_{j \\neq y_j}\\max\\left(0,s_j-s_{y_i}+1\\right)\\] 这种损失函数被称为合页损失 Hinge loss SVM 的损失函数要求正确类别的分类分数要比其他类别的高出一个边界值。 L2-SVM 中使用平方折叶损失函数\\[\\max(0,-)^2\\]能更强烈地惩罚过界的边界值。但是选择使用哪一个损失函数需要通过实验结果来判断。 举例 根据上面的公式计算：\\[L = \\max(0,437.9-(-96.8)) + \\max(0,61.95-(-96.8))=695.45\\] 猫的分类得分在三个类别中不是最高得，所以我们需要继续优化。 Suppose that we found a W such that L = 0. Is this W unique? No! 2W is also has L = 0! Regularization: 正则化，向某一些特定的权值 W 添加惩罚，防止权值过大，减轻模型的复杂度，提高泛化能力，也避免在数据集中过拟合现象。 \\[L=\\frac{1}{N}\\sum_iL_i\\left(f\\left(x_i,W\\right),y_i\\right) + \\lambda R(W)\\] R 正则项 \\[\\lambda\\] 正则化参数 常用正则化方法 L2\\[\\begin{matrix} R(W)=\\sum_{k}\\sum_l W^2_{k,l} \\end{matrix}\\] L1\\[\\begin{matrix} R(W)=\\sum_{k}\\sum_l \\left\\vert W_{k,l} \\right\\vert \\end{matrix}\\] Elastic net(L1 + L2): \\[\\begin{matrix} R(W)=\\sum_{k}\\sum_l \\beta W^2_{k,l} + \\left\\vert W_{k,l} \\right\\vert \\end{matrix}\\] Dropout Batch normalization etc L2 惩罚倾向于更小更分散的权重向量，L1 倾向于稀疏项。 Softmax function： \\[f_j(z)=\\frac{e^{s_i}}{\\sum e^{s_j}}\\] 该分类器将输出向量 f 中的评分值解释为没有归一化的对数概率，通过归一化之后，所有概率之和为1。 Loss 也称交叉熵损失 cross-entropy loss \\[L_i = - \\log\\left(\\frac{e^{s_i}}{\\sum e^{s_j}}\\right)\\] 12345f = np.array([123, 456, 789]) # 例子中有3个分类，每个评分的数值都很大p = np.exp(f) / np.sum(np.exp(f)) # 不妙：数值问题，可能导致数值爆炸# 那么将f中的值平移到最大值为0：f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # 现在OK了，将给出正确结果 SVM 和 Softmax 比较 评分，SVM 的损失函数鼓励正确的分类的分值比其他分类的分值高出一个边界值。 对数概率，Softmax 鼓励正确的分类归一化后的对数概率提高。 Softmax 永远不会满意，SVM 超过边界值就满意了。 Optimization：最优化过程 Follow the slope 梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量，它是各个维度的斜率组成的向量。 Numerical gradient: Approximate, slow, easy to write. (But its useful in debugging.) Analytic gradient: Exact, Fast, Error-prone. (Always used in practice) 实际应用中使用分析梯度法，但可以用数值梯度法去检查分析梯度法的正确性。 利用梯度优化参数的过程：W = W - learning_rate * W_grad learning_rate 被称为是学习率，是一个比较重要的超参数 Stochastic Gradient Descent SGD 随机梯度下降法 每次使用一小部分的数据进行梯度计算，这样可以加快计算的速度。 每个批量中只有1个数据样本，则被称为随机梯度下降（在线梯度下降） 图像分类任务中三大关键部分： 评分函数 损失函数：量化某个具体参数 \\({W}\\) 的质量 最优化：寻找能使得损失函数值最小化的参数 \\({W}\\) 的过程 04. Introduction to Neural network 反向传播：在已知损失函数 \\({L}\\) 的基础上，如何计算导数\\({\\nabla _WL}\\)？ 计算图 由于计算神经网络中某些函数的梯度很困难，所以引入计算图的概念简化运算。 在计算图中，对应函数所有的变量转换成为计算图的输入，运算符号变成图中的一个节点（门单元）。 反向传播：从尾部开始，根据链式法则递归地向前计算梯度，一直到网络的输入端。 绿色是正向传播，红色是反向传播。 对于计算图中的每一个节点，我们需要计算这个节点上的局部梯度，之后根据链式法则反向传递梯度。 Sigmoid 函数：\\({f(w,x)=\\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}}\\) 对于门单元 \\({\\frac{1}{x}}\\)，求导的结果是 \\({-\\frac{1}{x^2}}\\)，输入为 1.37，梯度返回值为 1.00，所以这一步中的梯度是 \\({(\\frac{-1}{1.37^2})*1.00=-0.53}\\)。 模块化思想：对 \\({\\sigma(x)=\\frac{1}{1+e^{-x}}}\\) 求导的结果是 \\({(1-\\sigma(x))\\sigma(x)}\\)。如果 sigmoid 表达式输入值为 1.0 时，则前向传播中的结果是 0.73。根据求导结果计算可得局部梯度是 \\({(1-0.73)*0.73=0.2}\\)。 Modularized implementation: forward/backwar API 1234567891011121314class MultuplyGate(object): \"\"\" x,y are scalars \"\"\" def forward(x,y): z = x*y self.x = x # Cache self.y = y # Cache # We cache x and y because we know that the derivatives contains them. return z def backward(dz): dx = self.y * dz #self.y is dx dy = self.x * dz return [dx, dy] 深度学习框架中会实现的门单元：Multiplication、Max、Plus、Minus、Sigmoid、Convolution 常用计算单元 加法门单元：把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。 取最大值门单元：将梯度转给前向传播中值最大的那个输入，其余输入的值为0。 乘法门单元：等值缩放。局部梯度就是输入值，但是需要相互交换，然后根据链式法则乘以输出值得梯度。 Neural NetWorks (Before) Linear score function \\[f = Wx\\] (Now) 2-layer Neural NetWork \\[f=W_2\\max(0,W_1x)\\] ReLU \\[\\max(0,x)\\] 是激活函数，如果不使用激活函数，神经网络只是线性模型的组合，无法拟合非线性情况。 神经网络是更复杂的模型的基础组件 05. Convolutional neural networks (CNNs) 这一轮浪潮的开端：AlxNet 卷积神经网络 Fully Connected Layer 全连接层：这一层中所有的神经元链接在一起。 Convolution Layer： 通过参数共享来控制参数的数量。Parameter sharing Sparsity of connections 卷积神经网络能学习到不同层次的输入信息 常见的神经网络结构：INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC 使用小的卷积核大小的优点：多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好地特征。而且使用的参数也会更少 计算卷积层输出 stride 是卷积核在移动时的步长 通用公式 (N-F)/stride + 1 stride 1 =&gt; (7-3)/1 + 1 = 5 stride 2 =&gt; (7-3)/2 + 1 = 3 stride 3 =&gt; (7-3)/3 + 1 = 2.33 Zero pad the border: 用零填充所有的边界，保证输入输出图像大小相同，保留图像边缘信息，提高算法性能 步长为 1 时，需要填充的边界计算公式：(F-1)/2 F = 3 =&gt; zero pad with 1 F = 5 =&gt; zero pad with 2 F = 7 =&gt; zero pad with 3 计算例子 输入大小 32*32*3 卷积大小 10 5*5 stride 1 pad 2 output 32*32*10 每个 filter 的参数数量：5*5*3+1 =76 bias 全部参数数量 76*10=760 卷积常用超参数设置 卷积使用小尺寸滤波器 卷积核数量 K 一般为 2 的次方倍 卷积核的空间尺寸 F 步长 S 零填充数量 P Pooling layer 降维，减少参数数量。在卷积层中不对数据做降采样 卷积特征往往对应某个局部的特征，通过池化聚合这些局部特征为全局特征 Max pooling 2*2 stride 2 避免区域重叠 Average pooling 06. Training neural networks I Activation functions 激活函数 不使用激活函数，最后的输出会是输入的线性组合。利用激活函数对数据进行修正。 Sigmoid 限制输出在 [0,1]区间内 firing rate 二分类输出层激活函数 Problem 梯度消失：x很大或者很小时，梯度很小，接近于0（考虑图像中的斜率。无法得到梯度反馈。 输出不是 0 均值的数据，梯度更新效率低 exp is a bit compute expensive tanh 输出范围 [-1, 1] 0 均值 x 很大时，依然没有梯度 \\({f(x)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}}\\) \\({1-(tanh(x))^2}\\) RELU rectified linear unit 线性修正单元 一半空间梯度不会饱和，计算速度快，对结果又有精确的计算 不是 0 均值 Leaky RELU leaky_RELU(x) = max(0.01x, x) 梯度不会消失 需要学习参数 ELU 比 ReLU 好用 反激活机制 Maxout maxout(x) = max(w1.Tx + b1, w2.Tx + b2) 梯度不会消失 增大参数数量 激活函数选取经验 使用 ReLU ，但要仔细选取学习率 尝试使用 Leaky ReLU Maxout ELU 使用 tanh 时，不要抱有太大的期望 不要使用 sigmoid 数据预处理 Data Preprocessing 均值减法：对数据中每个独立特征减去平均值，从几何上来看是将数据云的中心都迁移到原点。 归一化：将数据中的所有维度都归一化，使数值范围近似相等。但是在图像处理中，像素的数值范围几乎一致，所以不需要额外处理。 12X -= np.mean(X, axis = 1)X /= np.std(X, axis =1) 图像归一化 Subtract the mean image AlexNet mean image 32,32,3 Subtract per-channel mean VGGNet mean along each channel = 3 numbers 如果需要进行均值减法时，均值应该是从训练集中的图片平均值，然后训练集、验证集、测试集中的图像再减去这个平均值。 Weight Initialization 全零初始化 网络中的每个神经元都计算出相同的输出，然后它们就会在反向传播中计算出相同的梯度。神经元之间会从源头上对称。 Small random numbers 初始化权值要非常接近 0 又不能等于 0。将权重初始化为很小的数值，以此来打破对称性 randn 函数是基于零均值和标准差的高斯分布的随机函数 W = 0.01 * np.random.rand(D,H) 问题：一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度。会减小反向传播中的“梯度信号”，在深度网络中就会出现问题。 Xavier initialization W = np.random.rand(in, out) / np.sqrt(in) 校准方差，解决输入数据量增长，随机初始化的神经元输出数据的分布中的方差也增大问题。 He initialization W = np.random.rand(in, out) / np.sqrt(in/2) Batch normalization 保证输入到神经网络中的数据服从标准的高斯分布 通过批量归一化可以加快训练的速度 步骤 首先计算每个特征的平均值和平方差 通过减去平局值和除以方差对数据进行归一化 Result = gamma * normalizedX + beta 对数据进行线性变换，相当于对数据分布进行一次移动，可以恢复数据之前的分布特征 BN 的好处 加快训练速度 可以使用更快的而学习率 减少数据对初始化权值的敏感程度 相当于进行一次正则化 BN 适用于卷积神经网络和常规的 DNN，在 RNN 和增强学习中表现不是很好 Babysitting the Learning Provess Hyperparameter Optimization Cross-validation 策略训练 小范围内随机搜索 07. Training neural networks II Optimization Algorithms: SGD 的问题 x += - learning_rate * dx 梯度在某一个方向下降速度快，在其他方向下降缓慢 遇到局部最小值点，鞍点 mini-batches GD Shuffling and Partitioning are the two steps required to build mini-batches Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128. SGD + Momentun 动量更新：从物理学角度启发最优化问题 V[t+1] = rho * v[t] + dx; x[t+1] = x[t] - learningRate * V[t+1] rho 被看做是动量，其物理意义与摩擦系数想类似，常取 0.9 或0.99 和 momentun 项更新方向相同的可以快速更新。 在 dx 中改变梯度方向后， rho 可以减少更新。momentun 能在相关方向加速 SGD，抑制震荡，加快收敛。 Nestrov momentum v_prev = v; v = mu * v - learning_rate * dx; x += -mu * v_prev + (1 + mu) * v AdaGrad \\(n_t=n_{t-1}+g^2_t\\) \\(\\Delta \\theta _t = -\\frac{\\eta}{\\sqrt{n_t+\\epsilon}}\\) 下面根号中会递推形成一个约束项。前期这一项比较大，能够放大梯度。后期这一项比较小，能约束梯度。 gt 的平方累积会使梯度趋向于 0 RMSProp RMS 均方根 自适应学习率方法 求梯度的平方和平均数：cache = decay_rate * cache + (1 - decay_rate) * dx**2 x += - learning_rate * dx / (sqrt(cache) + eps) 依赖全局学习率 Adam RMSProp + Momentum It calculates an exponentially weighted average of past gradients, and stores it in variables \\(v\\) (before bias correction) and \\(v^{corrected}\\) (with bias correction). It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables \\(s\\) (before bias correction) and \\(s^{corrected}\\) (with bias correction). 一阶到导数累积，二阶导数累积 It updates parameters in a direction based on combining information from \"1\" and \"2\". The update rule is, for \\(l = 1, ..., L\\): \\[\\begin{cases} v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\ v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\ s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\ s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\ W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon} \\end{cases}\\] where: t counts the number of steps taken of Adam L is the number of layers \\(\\beta_1\\) and \\(\\beta_2\\) are hyperparameters that control the two exponentially weighted averages. \\(\\alpha\\) is the learning rate \\(\\varepsilon\\) is a very small number to avoid dividing by zero 特点： 适用于大数据集和高维空间。 对不同的参数计算不同的自适应学习率。 Learning decay 学习率随着训练变化，比如每一轮在前一轮的基础上减少一半。 防止学习停止 Second order optimization Regularization Dropout 每一轮中随机使部分神经元失活，减少模型对神经元的依赖，增强模型的鲁棒性。 Transfer learning CNN 中的人脸识别，可以在大型的模型基础上利用少量的相关图像进行继续训练。 09. CNN architectures 研究模型的方法：搞清楚每一层的输入和输出的大小关系。 LeNet - 5 [1998] 60k 参数 深度加深，图片大小减少，通道数量增加 ac: Sigmod/tanh AlexNet [2012] (227,227,3) （原文错误） 60M 参数 LRN：局部响应归一化，之后很少使用 VGG - 16 [2015] 138 M 结构不复杂，相对一致，图像缩小比例和通道增加数量有规律 ZFNet [2013] 在 AlexNet 的基础上修改 CONV1: change from (11 x 11 stride 4) to (7 x 7 stride 2) CONV3,4,5: instead of 384, 384, 256 filters use 512, 1024, 512 VGG [2014] 模型中只使用 3*3 conv：与 77 卷积有相同的感受野，而且可以将网络做得更深。比如每一层可以获取到原始图像的范围：第一层 33，第二层 55，第三层 77。 前面的卷积层参数量很少，模型中大部分参数属于底部的全连接层。 GoogLeNet 引入 Inception module design a good local network topology (network within a network) and then stack these modules on top of each other 该模块可以并行计算 conv 和 pool 层进行 padding，最后将结果 concat 在一起 Reset ResNet 目标：深层模型表现不应该差于浅层模型，解决随着网络加深，准确率下降的问题。 Y = (W2* RELU(W1x+b1) + b2) + X 如果网络已经达到最优，继续加深网络，residual mapping会被设置为 0，一直保存网络最优的情况。 Reference 深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam） - 知乎","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"ml","slug":"ml","permalink":"https://xiang578.com/tags/ml/"},{"name":"course","slug":"course","permalink":"https://xiang578.com/tags/course/"}]},{"title":"算法花园写作风格清单","slug":"blog-writing-checklist","date":"2019-11-03T10:03:23.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/blog-writing-checklist.html","link":"","permalink":"https://xiang578.com/post/blog-writing-checklist.html","excerpt":"","text":"李如一在 写作风格手册 中提到写作风格的作用是 「保持机构和组织内部的文体统一，提高沟通效率。」 本清单会持续更新，如果有相关的建议，可以在留言中告诉我。 算法花园定位为个人博客，也是我和这个世界沟通的窗口。为提高读者阅读体验，参考相关文章后，推出该清单统一网站文章的基础风格。 写作 减少形容词使用，尽可能删除 「的」和「了」。 给出引用图片及引文来源。 文章如果发布后大幅度修改，在末尾给出版本信息。 写完文章后，整体阅读一遍。 排版 中文、英文、数字中间加空格，数字与单位之间无需增加空格，全角标点与其他字符之间不加空格。链接前后增加空格用以区分。 不重复使用标点符号。 中文使用直角引号 「」以及『 』。 使用全角中文标点，数字使用半角字符。中文中出现英文部分，仍然使用中文标点。 遇到完整的英文整句、特殊名词，其內容使用半角标点。 专有名词使用正确的大小写，使用公认的缩写。 todo 如何处理图片排版和命名。 使用英文命名文档，使用 - 来连接。为保证搜索引擎效果，尽量不要修改文档名称。 每篇文章开头添加简单介绍 &lt;!--more--&gt;。 发布后，在网页中确认格式是否符合预期、链接能否点击以及图片能否展示。 ChangeLog 20191103: 第一版 参考 写作风格手册_设计词典_好奇心日报 中文文案排版指北（简体中文版） — 码志 简体中文文本排版指南 - 知乎 少数派写作排版指南 - 少数派 城堡制作检查清单 0.1 版 - 简书","categories":[{"name":"站务","slug":"站务","permalink":"https://xiang578.com/categories/站务/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"},{"name":"writing","slug":"writing","permalink":"https://xiang578.com/tags/writing/"}]},{"title":"(WDR) Learning to Estimate the Travel Time","slug":"wdr","date":"2019-07-28T14:14:33.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/wdr.html","link":"","permalink":"https://xiang578.com/post/wdr.html","excerpt":"","text":"严重申明：本篇文章所有信息从论文、网络等公开渠道中获得，不会透露滴滴地图 ETA 任何实现方法。 这篇论文是滴滴时空数据 2018 年在 KDD 上发表的关于在 ETA 领域应用深度学习的文章，里面提到的深度学习方法大家都耳熟能详，主要是属于工业界的创新。说点题外话，你为什么从滴滴出行离职？ - 知乎 中提到一点： 8.同年大跃进，在滴滴中高层的眼里，没有BAT。滴滴单量超淘宝指日可待，GAFA才是滴滴要赶超的对象。百度系，LinkedIn系，学院派，uber帮，联想系，MBB就算了，据说连藤校都混成了一个小圈子。。一个项目A team ，B team。一个ETA，投入了多少人力自相残杀？MAPE做到0%又如何？用户体验就爆表了吗？长期留存就高枕无忧了吗？风流总被雨打风吹去，滴滴是二龙山，三虫聚首？是不是正确的事情不知道，反正跟着公司大势所趋，升D10保平安。 简单介绍一下背景：ETA 是 Estimate Travel Time 的缩写，中文大概能翻译成到达时间估计。这个问题描述是：在某一个时刻，估计从 A 点到 B 点需要的时间。对于滴滴，关注的是司机开车把乘客从起点送到终点需要的时间。抽象出来 ETA 就是一个时间空间信息相关的回归问题。CTR 中常用的方法都可以在这里面尝试。 对于这个问题：文章中提到一个最通用的方法 Route ETA：即在获得 A 点到 B 点路线的情况下，计算路线中每一段路的行驶时间，并且预估路口的等待时间。最终 ETA 由全部时间相加得到。这种方法实现起来很简单，也能拿到一些收益。但是仔细思考一下，没有考虑未来道路的同行状态变化情况以及路线的拓扑关系。针对这些问题，文章中提到滴滴内部也有利用 GBDT 或者 FM 的方法解决 ETA 问题，不过没有仔细写实现的方法，我也不好继续分析下去。 评价指标 对于 ETA 问题来说，工业界和学术界常用的指标是 MAPE(mean absolute percentage error)，\\({y_i}\\) 是司机实际从 A 点到 B 点花费的时间，\\({f(x_i)}\\) 是 ETA 模型估计出来的时间。得到计算公式如下： \\[{min_f \\sum_{i=1}^{N}\\frac{|y_i - f(x_i)|}{y_i}}\\] 多说一句，如果使用 GBDT 模型实现 ETA 时，这个损失函数的推导有点困难，全网也没有看见几个人推导过。 这个公式主要考虑预估时间偏差大小对用户感知体验的影响，目前我们更加关心极端 badcase 对用户的影响。 特征 特征： 空间特征：路线序列、道路等级、POI等 时间特征：月份、星期、时间片等 路况特征：道路的通行速度、拥堵程度 个性化信息：司机特征、乘客特征、车辆特征 附近特征：天气、交通管制 模型 WDR 模型，包含 3 个部分： - Wide Learning Models：利用交叉积学习信息，泛化能力。 - Deep Neural networks：对 sparse feature 做一次 Embedding，使用 3 层 MLP 和 ReLU 的网络。 - Long-Short Term Memory：解决 Wide &amp; Deep 没用使用路线的顺序特征，利用 LSTM 学习 link 信息以及序列信息，最后一个单元的隐藏状态作为输出。 - Regressor： 将 3 个模型的输出综合起来，作为最后的 ETA 预估。MAPE 作为损失函数，利用 BP 训练模型。 -w962 上面模型中使用的特征分类： - Dense feature：行程级别的实数特征，比如起终点球面距离、起终点 GPS 坐标等。 - Sparse feature：行程级别的离散特征，比如时间片编号、星期几、天气类型等。 - Sequential feature：link 级别的特征，实数特征直接输入模型，而离散特征先做 embedding 再输入模型。注意，这里不再是每个行程一个特征向量，而是行程中每条 link 都有一个特征向量。比如，link 的长度、车道数、功能等级、实时通行速度等。 评估 包括两部分：离线评估和在线评估。 离线评估中取滴滴 2017 年北京前6个月的订单数据，分成两类 pickup （平台给司机分单后，司机开车去接乘客的过程）和 trip （司机接到乘客并前往目的地的过程）。具体数据集划分如下。 离线使用 MAPE 来评价模型。在线评估时，为了更好的与用户体验挂钩，采用多个指标来衡量 ETA 的效果。包括： - APE20: absolute percentage error 小于 20% 的订单占比。（越大越好） - Badcase率：APE 大于 50% 或者 AE 大于 180s 的订单占比，定义为对用户造成巨大影响的情况。（越小越好） - 低估率：低估订单的比例。（越小越好） 离线结果如下图所示，说来汗颜 PTTE 和 TEMP 是什么算法我都不知道…… WD-MLP 指的是将 WDR 中的 R 部分换成 MLP 。最终 WDR 较 route-ETA 有巨大提升，而且 LSTM 引入的序列信息也在 pikcup 上提升了 0.75%。文章的最后还提出来，LSTM 也可以换成是 Attention，这样替换有什么优点和缺点留给大家思考。 在线实验结果如下图所示，滴滴 ETA MAPE 明显小于 com1、com2、com3 ，这三家地图公司具体是哪三家，大家也能猜到吧。 ETA 服务工程架构： -w486 从上面的图中可以看出 ETA 服务工程架构主要包括三个部分： Data Aggregation：包括利用 Map Matching 将司机上传到平台的 GPS 对应到滴滴的 Map Info 中得到司机真实行驶过的路线信息，Order Context 指的是订单相关的信息，augmented Data 额外数据比如上文说的交通情况相关信息。 Offline Training：利用上一步得到的历史数据训练模型。这里可以值得一提的是，ETA 模型是和时间强相关的（节假日和工作日的数据分布明显不同），所以在文章中作者指出将拿出最新的一部分数据用来 fine-tune 训练出来的 WDR 模型。 Online Service：这里需要一个完整的模型服务系统，其他公司也有很多分享，所以原文没有多提。 FMA-ETA: Estimating Travel Time Entirely Based on FFN With Attention 大学实验报告级别的论文 简单记录一下，不详细评价 WDR 模型中 RNN 耗时长，探索基于 Attention 机制的模型 将特征分组（multi-factor）去做 Attention 效果比多头要好 实验结果给出的理由有点牵强。 &gt; The deep modules with attention achieve better results than WDR on MAE and RMSE metrics, which means attention mechanism can help to extract features and sole the long-range dependencies in long sequence. 说预测时延减少，也没有提供线上数据。 最后，不公开代码、不公开数据、SOTA 是 WDR，图一乐。 总结 从上面简单的介绍来看，ETA 可以使用 CTR 和 NLP 领域的很多技术，大有可为。最后，滴滴 ETA 团队持续招人中（社招、校招、日常实习等），感兴趣者快快和我联系。 参考 KDD 2018：滴滴提出WDR模型显著提升ETA预测精度 | 雷锋网 LBS工业界ETA应用及滴滴WDR技术 – Semocean","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"lstm","slug":"lstm","permalink":"https://xiang578.com/tags/lstm/"},{"name":"widedeep","slug":"widedeep","permalink":"https://xiang578.com/tags/widedeep/"},{"name":"didi","slug":"didi","permalink":"https://xiang578.com/tags/didi/"}]},{"title":"(FM) Factorization Machines","slug":"fm","date":"2019-07-28T10:18:52.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/fm.html","link":"","permalink":"https://xiang578.com/post/fm.html","excerpt":"","text":"Factorization Machines(FM) 由日本 Osaka University 的 Steffen Rendle [1] 在 2010 年提出,是一种常用的因子机模型。 FM 假设现在有一个电影评分的任务，给定如下如所示的特征向量 x（包括用户名、当前在看的电影、已经打分的电影、时间特征、之前看的电影），预测用户对当前观看电影的评分。 电影评分 作者在线性回归模型的基础上，添加交叉项部分，用来自动组合二阶特征。 \\[\\hat y(x):= w_0 + \\sum_{i=1}^{n} w_ix_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \\left \\langle v_i,v_j \\right \\rangle x_ix_j\\] 其中交叉特征的权重由两个向量的点积得到，可以解决没有在模型中出现的特征组合权重问题，以及减少参数数量。 \\[W_{i,j}=\\left \\langle v_i,v_j \\right \\rangle = \\sum_{f=1}^kv_{i,f} \\cdot v_{j,f}\\] 通过下面的方法来化简交叉项权重计算，算法复杂度降到线性。 \\[\\sum_{i=1}^n \\sum_{j=i+1}^n \\left \\langle v_i,v_j \\right \\rangle x_iy_i = \\frac{1}{2}\\sum^k_{f=1} \\left( \\left(\\sum_{i=1}^nv_{i,f}x_i \\right)^2 - \\sum^n_{i=1} v^2_{i,f} x_i^2 \\right)\\] 对交叉项部分的求导： \\[\\frac{\\partial}{\\partial \\theta} \\hat y \\left( x \\right) = \\begin{cases} 1, &amp; \\text{ if $\\theta$ is $w_0$} \\\\ x_i, &amp; \\text{ if $\\theta$ is ${w_i}$} \\\\ x_i\\sum^n_{j=1} v_{j,f}x_j - v_{i,f}x_i^2, &amp;\\text{if $\\theta$ is ${v_{i,f}}$} \\end{cases}\\] 其中 \\({\\sum^n_{j=1} v_{j,f}x_j}\\) 与 \\({x_i}\\) 无关，可以在计算导数前预处理出来。 FM vs SVM 对于经典的特征组合问题，不难想到使用 SVM 求解。Steffen 在论文中也多次将 FM 和 SVM 做对比。 在考虑 SVM 的 Polynomial kernel 为 \\({K(\\mathbf{x}, \\mathbf{z}) :=(\\langle\\mathbf{x}, \\mathbf{z}\\rangle+ 1)^{2}}\\)，映射 \\[ \\begin{array}{l}{\\phi(\\mathbf{x}) :=\\left(1, \\sqrt{2} x_{1}, \\ldots, \\sqrt{2} x_{n}, x_{1}^{2}, \\ldots, x_{n}^{2}\\right.} {\\sqrt{2} x_{1} x_{2}, \\ldots, \\sqrt{2} x_{1} x_{n}, \\sqrt{2} x_{2} x_{3}, \\ldots, \\sqrt{2} x_{n-1} x_{n} )}\\end{array} \\] SVM 的公式可以转化为： \\[ \\begin{aligned} \\hat{y}(\\mathbf{x})=w_{0}+\\sqrt{2} \\sum_{i=1}^{n} w_{i} x_{i}+\\sum_{i=1}^{n} w_{i, i}^{(2)} x_{i}^{2} &amp;+\\sqrt{2} \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} w_{i, j}^{(2)} x_{i} x_{j} \\end{aligned} \\] 论文中提到一句上面的公式中 \\({w_{i}}\\) 和 \\({w_{i,i}}\\) 表达能力类似，我猜这也是为什么 FM 中没有自身交叉项的原因吧。 FM 相比于 SVM 有下面三个特点： 1. SVM 中虽然也有特征交叉项，但是只能在样本中含有相对应的特征交叉数据时才能学习。但是 FM 能在数据稀疏的时候学习到交叉项的参数。 2. SVM 问题无法直接求解，常用的方法是根据拉格朗日对偶性将原始问题转化为对偶问题。 3. 在使用模型预测时，SVM 依赖部分训练数据（支持向量），FM 模型则没有这种依赖。 Rank FM 用来做回归和分类都很好理解，简单写一下如何应用到排序任务中。以 pairwise 为例。假设排序结果有两个文档 \\({x_i}\\) 和 \\({x_j}\\)，显然用户点击文档有先后顺序，如果先点击 \\({x_i}\\)，记 label \\({y_{ij}=1}\\)，反之点击 \\({x_j}\\)，label \\({y_{ij}=0}\\)。模型需要去预测 \\({\\hat y_{ij} = sigmoid(\\hat y_i - \\hat y_j)}\\)。 参考逻辑回归，用最大似然对参数进行估计，得到损失函数为 \\({L=\\log(1+\\exp(-(\\hat y(x_i)-\\hat y(x_j))}\\)。优化过程和前面提到类似。 NFM NFM 和 AFM 两篇论文是同一个作者写的，所以文章的结构很相近。 FM 模型由于复杂度问题，一般只使用特征二阶交叉的形式，缺少对 higher-order 以及 non-liner 特征的交叉能力。NFM 尝试通过引入 NN 来解决这个问题。 NFM 的结构如下：第一项和第二项是线性回归，第三项是神经网络。神经网络中利用 FM 模型的二阶特征交叉结果做为输入，学习数据之间的高阶特征。与直接使用高阶 FM 模型相比，可以降低模型的训练复杂度，加快训练速度。 \\[ \\hat{y}_{N F M}(\\mathbf{x})=w_{0}+\\sum_{i=1}^{n} w_{i} x_{i}+f(\\mathbf{x}) \\] NFM 的神经网络部分包含 4 层，分别是 Embedding Layer、Bi-Interaction Layer、Hidden Layers、Prediction Score。 NFM Embedding Layer 层对输入的稀疏数据进行 Embedding 操作。最常见的 Embedding 操作是在一张权值表中进行 lookup ，论文中作者强调他们这一步会将 Input Feture Vector 中的值与 Embedding 向量相乘。 Bi-Interaction Layer 层是这篇论文的创新，对 embedding 之后的特征两两之间做 element-wise product，并将结果相加得到一个 k 维（Embeding 大小）向量。这一步相当于对特征的二阶交叉，与 FM 类似，这个公式也能进行化简： \\[ f_{B I}\\left(\\mathcal{V}_{x}\\right)=\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} x_{i} \\mathbf{v}_{i} \\odot x_{j} \\mathbf{v}_{j} =\\frac{1}{2}\\left[\\left(\\sum_{i=1}^{n} x_{i} \\mathbf{v}_{i}\\right)^{2}-\\sum_{i=1}^{n}\\left(x_{i} \\mathbf{v}_{i}\\right)^{2}\\right] \\] Hidden Layers 层利用常规的 DNN 学习高阶特征交叉 Prdiction Layer 层输出最终的结果： \\[ \\begin{aligned} \\hat{y}_{N F M}(\\mathbf{x}) &amp;=w_{0}+\\sum_{i=1}^{n} w_{i} x_{i} +\\mathbf{h}^{T} \\sigma_{L}\\left(\\mathbf{W}_{L}\\left(\\ldots \\sigma_{1}\\left(\\mathbf{W}_{1} f_{B I}\\left(\\mathcal{V}_{x}\\right)+\\mathbf{b}_{1}\\right) \\ldots\\right)+\\mathbf{b}_{L}\\right) \\end{aligned} \\] 实验结果： AFM AFM(Attentional Factorization Machine), 在 FM 的基础上将 Attention 机制引入到交叉项部分，用来区分不同特征组合的权重。 \\[ \\hat{y}_{A F M}(\\mathbf{x})=w_{0}+\\sum_{i=1}^{n} w_{i} x_{i}+\\mathbf{p}^{T} \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} a_{i j}\\left(\\mathbf{v}_{i} \\odot \\mathbf{v}_{j}\\right) x_{i} x_{j} \\] 单独看上面公式中的第三项结构： Embedding Layer 与 NFM 里面的作用一样，转化特征。 Pair-wise Interaction Layer 是将特征两两交叉，如果对这一步的结果求和就是 FM 中的交叉项。 Attention 机制在 Attention-based Pooling 层引入。将 Pair-wise Interaction Layer 中的结果输入到 Attention Net 中，得到特征组合的 score \\({a_{i j}^{\\prime} }\\)，然后利用 softmax 得到权重矩阵 \\({a_{ij}}\\)。 \\[ \\begin{aligned} a_{i j}^{\\prime} &amp;=\\mathbf{h}^{T} \\operatorname{Re} L U\\left(\\mathbf{W}\\left(\\mathbf{v}_{i} \\odot \\mathbf{v}_{j}\\right) x_{i} x_{j}+\\mathbf{b}\\right) \\\\ a_{i j} &amp;=\\frac{\\exp \\left(a_{i j}^{\\prime}\\right)}{\\sum_{(i, j) \\in \\mathcal{R}_{x}} \\exp \\left(a_{i j}^{\\prime}\\right)} \\end{aligned} \\] 最后将 Pair-wise Interaction Layer 中的二阶交叉结果和权重矩阵对应相乘求和得到 AFM 的交叉项。 和前一节的实验结果对比，AFM 效果比 NFM 要差一些。这大概就能说明为什么论文中提到 NFM，但是最后没有把 NFM 的结果贴出来，实在是机智。又回到，发论文是需要方法有创新，还是一味追求 state-of-the-art。 参考资料 深入浅出Factorization Machines系列 | Kubi Code'Blog FM模型在LTR类问题中的应用 - 知乎","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"fm","slug":"fm","permalink":"https://xiang578.com/tags/fm/"}]},{"title":"(Wide&Deep) Wide & Deep Learning for Recommender Systems","slug":"wide-and-deep","date":"2019-07-02T12:56:43.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/wide-and-deep.html","link":"","permalink":"https://xiang578.com/post/wide-and-deep.html","excerpt":"","text":"背景 这是一篇推荐系统相关的论文，场景是谷歌 Play Store 的 App 推荐。文章开头，作者点明推荐系统需要解决的两个能力： memorization 和 generalization。 memorization 指的是学习数据中出现过的组合特征能力。最常使用的算法是 Logistic Regression，简单、粗暴、可解释性强，而且会人工对特征进行交叉，从而提升效果。但是，对于在训练数据中没有出现过的特征就无能为力。 generalization 指的是通过泛化出现过特征从解释新出现特征的能力。常用的是将高维稀疏的特征转换为低维稠密 embedding 向量，然后使用 fm 或 dnn 等算法。与 LR 相比，减少特征工程的投入，而且对没有出现过的组合有较强的解释能力。但是当遇到的用户有非常小众独特的爱好时（对应输入的数据非常稀疏和高秩），模型会过度推荐。 综合前文 ，作者提出一种新的模型 Wide &amp; Deep。 模型 从文章题目中顾名思义，Wide &amp; Deep 是融合 Wide Models 和 Deep Models 得到，下图形象地展示出来。 Wide &amp; Deep Models Wide Component 是由一个常见的广义线性模型：\\({y=w^Tx+b}\\)。其中输入的特征向量 \\({x}\\) 包括两种类型：原始输入特征（raw input features）和组合特征（transformed features）。 常用的组合特征公式如下： \\[{\\phi_k(x)=\\prod_{i=1}^dx_i^{c_{ki}},c_{ki}\\in\\{0,1\\}}\\] \\({c_{ki}}\\) 代表对于第k个组合特征是否包含第i个特征。\\({x_i}\\)是布尔变量，代表第i个特征是否出现。例如对于组合特征 AND(gender=female, language=en) 当且仅当 x 满足(“gender=female” and “language=en”)时，\\({\\phi_k(x)=1}\\)。 Deep Component 是一个标准的前馈神经网络，每一个层的形式诸如：\\({a^{(l+1)}=f(W^{(l)}a^{(l)} + b^{(l)})}\\)。对于输入中的 categorical feature 需要先转化成低维稠密的 embedding 向量，再和其他特征一起喂到神经网络中。 对于这种由基础模型组合得到的新模型，常用的训练形式有两种：joint training 和 ensemble。ensemble 指的是，不同的模型单独训练，且不共享信息（比如梯度）。只有在预测时根据不同模型的结果，得到最终的结果。相反，joint training 将不同的模型结果放在同一个损失函数中进行优化。因此，ensmble 要且模型独立预测时就有有些的表现，一般而言模型会比较大。由于 joint training 训练方式的限制，每个模型需要由不同的侧重。对于 Wide&amp;Deep 模型来说，wide 部分只需要处理 Deep 在低阶组合特征学习的不足，所以可以使用简单的结果，最终完美使用 joint traing。 预测时，会将 Wide 和 Deep 的输出加权得到结果。在训练时，使用 logistic loss function 做为损失函数。模型优化时，利用 mini-batch stochastic optimization 将梯度信息传到 Wide 和 Deep 部分。然后，Wide 部分通过 FTRL + L1 优化，Deep 部分通过 AdaGrad 优化。 实验 本篇论文选择的实验场景是谷歌 app 商店的应用推荐，根据用户相关的历史信息，推荐最有可能会下载的 App。 使用的模型如下： 一些细节： - 对于出现超过一定次数的 categorical feature，ID 化后放入到模型中。 - Continuous real-valued features 通过 cumulative distribution function 归一化到 [0, 1] 区间。 - categorical feature 由 32 维 embedding 向量组成，最终的输入到 Deep 部分的向量大概在 1200 维。 - 每天在前一天 embedding 和模型的基础上进行增量更新。 实验结果： 实验结果 Wide &amp; Deep 模型相对于其他两个模型毫无疑问有提升。但结果中也一个反常的现象：单独使用 Deep 模型离线 AUC 指标比单独使用 Wide 模型差，但是线上对比实验时却有较大的提升。论文中作者用了一句：线下实验中的特征是固定的，线上实验会遇到很多没有出现过的特征组合，Deep 相对于 Wide 有更好的模型泛化能力，所以会有反常现象。由于笔者工作中不关注 AUC，也没有办法继续分析。 总结 作者从推荐系统的的 memorization 和 generalization 入手，设计出新的算法框架。通过线上和线下实验实验，证明 Deep 和 Wide 联合是必须的且有效的。最终也在自己的业务场景带来提升。 Reference Wide &amp; Deep Learning for Recommender Systems - 知乎 详解 Wide &amp; Deep 结构背后的动机 - 知乎","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"lr","slug":"lr","permalink":"https://xiang578.com/tags/lr/"},{"name":"dnn","slug":"dnn","permalink":"https://xiang578.com/tags/dnn/"},{"name":"ctr","slug":"ctr","permalink":"https://xiang578.com/tags/ctr/"}]},{"title":"Best of iPhone 2019 软件清单","slug":"best-of-iphone-2019","date":"2019-06-22T13:32:22.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/best-of-iphone-2019.html","link":"","permalink":"https://xiang578.com/post/best-of-iphone-2019.html","excerpt":"","text":"一直想做一个推荐软件的系列文章，不过完成 2017 年的iPhone软件清单 后就没有动力……很多时候在思考，自己为什么一定要使用 iPhone？iOS 中的软件正是最好的答案，让每一个人享受科技带来的快乐。 这一篇文章和 2017 年的形式一样，删除常用的软件，推荐一些我认为有趣的软件。自从 iOS 12 中引入屏幕时间，今年的推荐顺序就按照屏幕时间中的排序。 目前使用设备：iPhone XR &amp; Apple Watch Series 4 Inoreader：RSS 订阅服务商以及 RSS 阅读器。随着这两年对推荐算法相关的新闻阅读器批判，RSS 大有一股复新之势。可惜的是在去年年底左右，inoreader 中每个免费帐号只能关注 150 个订阅源。目前，我通过使用多个帐号来临时解决。 脉脉：职场社交软件。有匿名区以及公司圈，定期查看公司内部的吐槽和爆料。用 Leader 的话来说，整天操 M6 的心。 Keep/Nike Running/健身记录：锻炼相关 App 集合。Keep 和 Apple Watch 的配合太差，很多数据不能同步。自带的健身记录展示数据不够详细。Nike Running 不太用。 Overcast：免费且功能强大的播客软件，良心到只有少量的播客相关广告。喜欢智能播放和人声增强功能。最近，也支持章节播放功能。 AutoSleep：配合 Apple Watch 实现睡眠监控功能。原来大致为读取 Apple Watch 写入健康中的数据判断是否入睡以及睡眠质量。由于硬件的限制，很多时候还是不太准确。不过我也和网上说的一样，每天起来需要看这软件中的评分，判断昨天睡的如何…… Kindle：管它微信阅读如何火爆，不忘初心依旧选择 Kindle。配合 Kindle Unlimted，可以阅读很多好书。 Google 相册：免费强大的照片管理软件，允许谷歌优化照片质量的后，可以无限存储。 Quantumult：梯子软件，美区下载，比小火箭好用。 OmniFoucs：GTD 软件，居然使用的正版…… Anki：多年之后，我的摘抄终于有了存放之地。 Day One：日记软件，喜欢里面的去年今天功能，陆续把日记导入中。 Pocket：稍后读。 1 Password：密码软件，全平台通用。 熊猫吃短信：垃圾短信过滤，可以自己配置规则把公司的报警短信给屏蔽了。 Ulysses：Markdown 软件，iOS 版也包含在 SetApp 订阅。 在与 2017 版相比，有两个趋势：1. 减少很多与学习强相关的软件。 2. 越来越多的付费或者订阅制软件。期待明年。","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://xiang578.com/tags/iOS/"},{"name":"app","slug":"app","permalink":"https://xiang578.com/tags/app/"},{"name":"best of","slug":"best-of","permalink":"https://xiang578.com/tags/best-of/"}]},{"title":"Practical Lessons from Predicting Clicks on Ads at Facebook(gbdt + lr)","slug":"gbdt_lr","date":"2019-06-16T12:56:43.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/gbdt_lr.html","link":"","permalink":"https://xiang578.com/post/gbdt_lr.html","excerpt":"","text":"主题：Facebook 2014 年发表的广告点击预测文章。最主要是提出经典 GBDT+LR 模型，可以自动实现特征工程，效果好比于人肉搜索。另外，文章中还给出一个 online learning 的工程框架。 问题： GBDT 如何处理大量 id 类特征 广告类对于 user id 的处理：利用出现的频率以及转化率来代替 id 特征放在 lr 中处理。 GBDT+LR 和 RF+LR 的区别 选出能明显区分正负样本的特征的变换方式，转换成 one hot 有意义 RF + LR 可以并行训练，但是 RF 中得到的区分度不高 收获： 数据支撑去做决策，收获和实验数量成正比。 CTR click through rate，点击率 评价指标： Normalized Entropy：越小模型越好 Calibration：预测点击数除以真实点击数 AUC 正样本出现在负样本前面的概率。 数据新鲜度：模型天级训练比周级训练在 NE 下降 1%。 GBDT 和 LR 模型采用不同的更新频率，解决训练耗时不同。但是 GBDT 重新训练之后，LR 必须要重新训练。 网络： GBDT + LR 利用 GBDT 模型进行自动特征组合和筛选，然后根据样本落在哪棵树哪个叶子生成一个 feature vector 输入到 LR 模型中。这种方法的有点在于两个模型在训练过程从是独立，不需要进行联合训练。 GBDT 由多棵 CART 树组成，每一个节点按贪心分裂。最终生成的树包含多层，相当于一个特征组合的过程。根据规则，样本一定会落在一个叶子节点上，将这个叶子节点记为1，其他节点设为0，得到一个向量。比如下图中有两棵树，第一棵树有三个叶子节点，第二棵树有两个叶子节点。如果一个样本落在第一棵树的第二个叶子，将它编码成 [0, 1, 0]。在第二棵树落到第一个叶子，编码成 [1, 0]。所以，输入到 LR 模型中的向量就是 [0, 1, 0, 1, 0] Online Learning 文章中提到的 Online Learning 包括三个部分： - Joiner 将每次广告展示结果（特征）是否用户点击（标签） join 在一起形成一个完成的训练数据； - Trainer 定时根据一个 small batch 的数据训练一个模型； - Ranker 利用上一个模块得到模型预测用户点击。 注意的点： - waiting window time：给用户展示广告之后，我们只能知道用户点击的广告，也就是模型中的正样本。负样本需要设置一个等待时间来判断，即超过某一个时间没有观测到用户点击某一个广告，就认为这是一个负样本。另外设置这个时间也是一个技术活，时间过短导致click没有及时join到样本上，时间太长数据实时性差以及有存储的压力。最后，无论如何都会有一些数据缺失，为了避免累积误差，需要定期重新训练整个模型。 - request ID：人家的模型是分布式架构的，需要使用 request ID 来匹配每次展示给用户的结果以及click。为了实现快速匹配，使用 HashQueue 来保存结果。 - 监控：避免发生意向不到的结果，导致业务损失。我们的实时模型也在上线前空跑了好久。 实验： 有无 GBDT 特征对比 训练两个 LR 模型，一个模型输入样本经过 GBDT 得到的特征，另外一个不输入。混合模型比单独 LR 或 Tree 学习率选择 5 种学习率，前三个每一个特征设置一个学习率，最后两种全局学习率。 结果：应该给每一个特征设置一个不同的学习率，而且学习率应该随着轮次缓慢衰减。 GBDT 参数相关实验 前面的树会带来大量的收益，但是树越多训练越慢。 特征重要程度，累加不同树上某个特征的得分减少贡献。 两种特征： 上下文，冷启动的时候比较重要，与数据新鲜度有关。 历史史特征，权重比较大，关键在于长时间积累。 采样 训练数据大多，需要进行采样。 uniform subsampling ：无差别采样。使用 10 % 的样本，NE 减少 1 % negative down subsampling ：对负样本进行下采样。但不是负采样率越低越好，比如下面的图中0.0250就可能是解决了正负样本不平衡问题。最后的CTR指标结果需要重新进行一次映射。 Reference 回顾Facebook经典CTR预估模型 - 知乎","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"machine learing","slug":"machine-learing","permalink":"https://xiang578.com/tags/machine-learing/"},{"name":"gbdt","slug":"gbdt","permalink":"https://xiang578.com/tags/gbdt/"},{"name":"lr","slug":"lr","permalink":"https://xiang578.com/tags/lr/"}]},{"title":"「Rime 鼠须管」小鹤双拼配置指南","slug":"rime","date":"2019-06-15T13:21:14.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/rime.html","link":"","permalink":"https://xiang578.com/post/rime.html","excerpt":"","text":"引言 如何将汉字输入到计算机中是一个编码有关的问题，目前市面上主流的方案包括音码、形码、音形码。和大多数人一样，之前我一直使用全拼，而且得益于 NLP 技术发展，使用搜狗输入法搭配云词库，输入效率可以媲美五笔输入法。 但是今天要和大家分享，是从年初开始使用的全新音码输入方案——小鹤双拼。最初关于双拼的概念来自李笑来《把时间当作朋友》： 在很长的一段时间里，我常言之凿凿地对同学们说“练习打字完全是浪费时间。”我当时的逻辑是这样的。首先，我认为王码五笔字型输入法是给打字员用的。为什么要学它？难道你将来想要当个打字员？我总觉得五笔字型知识一种抄写输入法，因为用他输入时只能边看边打。而对真正创造内容的人来说，先用纸和笔写出来再录入电脑，还有比这更荒诞的事情吗？学习拆字学法已经很累人了，还要练什么指法，见鬼。更不用说这种所谓的输入法对思考的干扰——不仅要把字拆开再输入，还要按照莫名其妙的方法拆字。其次，盲打。我现在不是盲打，只是两根手指输入速度就已经很快了（至少比手写快）。 这样看来，我还有必要学习什么五笔字型和盲打吗？ 在我有了这些定见很久之后，发生了一件事情。 那是在1997年，我25岁。当时互联网除了聊天室和论坛，几乎没有什么实际的应用。适逢windows捆绑了哈尔滨工业大学开发的“微软拼音输入法1.0”，某天下午，当我在网上和一位永远都不会知道是谁的女生放肆地聊了两个小时之后，突然发现自己竟已无师自通地学会了所谓的“盲打”了！在这之后的一段时间里，我身边甚至很多人羡慕我打字的速度。为了让自己的打字速度再快一点，我索性花了差不多20分钟，把原本默认的“全拼输入”改成了“”双拼输入“。而这还远远不够。后来，我增设了”慢放模糊音“（不区分z/zh、c/ch、s⇧），又把打字速度提高了一些。这时我第一次意识到‘有些认识，哪怕是简单的常识，也需要亲身经历后才能真正体会”。只有拥有无与伦比的打字速度，才会体会打字速度快的好处。 打字速度提升后，我发现自己不再讨厌在读书的时候做笔记了，因为在键盘上敲字相对于笔写字来说轻松太多。我开始大段地纪录感悟，有时甚至干脆整篇摘抄原文！ 李笑来思考的问题正是如何利用最小的代价快速的输入文字。这里代价包括两个方面，输入方案的学习成本以及输入文字的速度。简单分析一下，主流输入法需要用户在键盘上敲击一些字符，然后映射到汉字。就是说输入文字的速度和两个因数有关：敲击键盘的次数（对应计算机中的码长）以及重码的字符数量（对于拼音输入法来说，这里指的是每个拼音对应多少的字）。 从这两个指标来说，五笔应该是接近输入法的极限，五次敲击键盘肯定能选定你需要的汉字。但是学习五笔需要记忆大量的说是有序其实没有太多规律的字根，学习曲线不是一般的陡峭。几年前，自己尝试跟着网上的视频教程学习，最后还是太复杂而放弃。 这时候要介绍一下双拼输入法，它是一种基于全拼的激进改良版拼音输入法。简单来说，它将一个汉字的拼音分成声母和韵母两个部分，输入一个拼音只需要按两个键（减少键盘的敲击次数但是没有减少重码率）。比如对于“拼音”两个字来说，全拼需要输入 pinyin 六个字符，换成是双拼，只需要输入 4 个字符 pbyb。具体介绍可以看 做少数派中的少数派：双拼输入快速入门。 由于双拼有声母和韵母，需要先从键盘上进行一次映射，所以有很多的输入方案。我自己入门使用的是小鹤双拼，也推荐大家使用这个方案。主要原因有两点：声母和韵母全部放在字母键上（微软双拼中要用到 ; 键，以及 iOS12 和 MacOS 中自带的输入法都支持这个方案（国内那几个流氓输入法更不用提）。 下图是一张小鹤双拼的输入法键位图，其中黑色的字符代表是键盘上的什么键，棕色的代表这个键表示声母时是什么，蓝色的代表这个键表示韵母时是什么。学习双拼的过程也很简单，拼音本身就会，无非是熟悉键位。从我自己的角度来说，每天抽出几分看一下键位图，再在 双拼练习 @ BlueSky （相关的介绍可以看：快速上手双拼，可以尝试这个练习平台 - 少数派）网站上练习 5 分钟，一周后可以完全脱离键位图来打字，之后就是孰能生巧的过程。 说回来当你学习了这么强大的内功之后，自然需要神兵来辅助你输入。在饱受国内的流氓输入法侵害之后（比如输入到一半给你跳一个什么斗图功能提示），我遇到今天的主角 RIME | 中州韻輸入法引擎，它是由佛振开发的一种开源输入框架，业内人士称之为「神级输入法」。上一个有类似拉风的称号的软件还是「神级编辑器」—— Vim。Rime 有趣的一点是在不同的平台上有不同的名字，包括 Linux 上的「中州韵」，Win 上的「小狼毫」以及 Mac 上的「鼠须管」。稍微有文化的人可以反应过来，后面两个正是两种不同的毛笔名字。 简单总结一下为什么要使用鼠须管：一是安全，不会出现什么输入法读取你个人信息更甚者是密码发送到服务器，也不知道他们用来机器学习什么；二是配置全平台同步，解决多台设备的输入法配置问题；三是快，不会出现输入法跟不上我的打字速度而导致思路中断的情况。 安装 在 下載及安裝 | RIME | 中州韻輸入法引擎 主页，你可以找到对应不同平台的安装方法。 对于 MacOS 来说，最简单的方式莫过于在终端中输入 brew cask install squirrel 安装软件本体。如何你想使用 rime/plum: 東風破可以在终端中输入 curl -fsSL https://git.io/rime-install | bash -s -- :preset double-pinyin 。其中 preset double-pinyin 指定下载时默认包括小鹤双拼输入方案。 配置 防止配置时候出现各种意想不到的情况，首先推荐阅读官方文档 CustomizationGuide · rime/home Wiki。 Rime 的配置文件默认放在 ~/Library/Rime，而且是一种扩展 yaml 文件。默认的文件名为 .schema.yaml，比如小鹤双拼相关的默认配置在 double_pinyin_flpy.schema.yaml 中。如果我们自己想添加一些设置，推荐写在以.custom.yaml 结尾的新文件中，比如 double_pinyin_flypy.custom.yaml default.custom.yaml 这个文件写一些全局的配置。 123456789101112131415161718192021222324252627282930patch: switcher: caption: 〔方案选单〕 hotkeys: Control+grave # 候选词数量 menu: page_size: 9 # 使用的输入方案 schema_list: - schema: luna_pinyin_simp - schema: luna_pinyin - schema: double_pinyin_flypy # 输入法中英文状态快捷键 ascii_composer/switch_key: Caps_Lock: commit_code Control_L: noop Control_R: noop # 按下左 shift 英文字符直接上屏，不需要再次回车，输入法保持英文状态 Shift_L: noop Shift_R: noop # 在一些软件中默认使用英文输入状态 app_options: com.apple.finder: &amp;a ascii_mode: true no_inline: true com.googlecode.iterm2: *a com.alfredapp.Alfred: *a com.runningwithcrayons.Alfred-2: *a org.vim.MacVim: *a com.apple.Terminal: *a 最后在修改配置时，可以查阅 Rime_collections/Rime_description.md at master · LEOYoon-Tsaw/Rime_collections 寻找相关信息。 installation.yaml 配置文件多平台同步相关文件，sync_dir 指定同步文件夹的位置，配合例如坚果云之类的软件实现备份同步。 12345678distribution_code_name: Squirreldistribution_name: \"鼠鬚管\"distribution_version: 0.11.0install_time: \"Sun Dec 23 23:42:01 2018\"installation_id: \"mac_didi\"sync_dir: \"/Users/didi/Documents/rime_sync\"rime_version: 1.4.0update_time: \"Mon Jun 3 07:18:30 2019\" 从 自由输入法RIME简明配置指南 - 少数派 中了解到，rime 个人词典双向同步，用户配置单向同步。另外需要把配置文件上传到 git 中，方便不同设备使用。 double_pinyin_flypy.custom.yaml 小鹤双拼相关的自用配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849patch: # 引用 `symbols.custom` 文件里面的符号 # 'punctuator/import_preset': symbols.custom 'recognizer/patterns/punct': \"^/([a-z]+|[0-9])$\" # 載入朙月拼音擴充詞庫 \"translator/dictionary\": ryen # 更改‘西文’为‘英文’，‘增广’为‘扩展集’ punctuator: import_preset: symbols.custom half_shape: \"#\": \"#\" \"`\": \"`\" \"~\": \"~\" \"@\": \"@\" \"=\": \"=\" \"/\": [\"/\", \"÷\"] '\\': [\"、\", '\\'] \"'\": &#123;pair: [\"「\", \"」\"]&#125; \"[\": [\"【\", \"[\"] \"]\": [\"】\", \"]\"] \"$\": [\"¥\", \"$\", \"€\", \"£\", \"¢\", \"¤\"] \"&lt;\": [\"《\", \"〈\", \"«\", \"&lt;\"] \"&gt;\": [\"》\", \"〉\", \"»\", \"&gt;\"] switches: - name: ascii_mode reset: 0 states: [\"中文\", \"英文\"] - name: full_shape states: [\"半角\", \"全角\"] - name: zh_simp reset: 1 states: [\"漢字\",\"汉字\"] - name: ascii_punct states: [\"，。\", \"，．\"] - name: extended_charset #生僻字开关 states: [\"通用\", \"扩展集\"] - name: show_emoji # 该项为表情输入，具体内容可见下文中 [关于表情输入] 部分 reset: 1 states: [ \"🈚️️\\uFE0E\", \"🈶️️\\uFE0F\" ] # 输入双拼码的时候不转化为全拼码 translator/preedit_format: &#123;&#125; simplifier: option_name: zh_simp # 分号上屏二候选词；引号上屏三候选词 \"key_binder/bindings\": - &#123; when: has_menu, accept: semicolon, send: 2 &#125; - &#123; when: has_menu, accept: apostrophe, send: 3 &#125; - &#123; when: paging, accept: bracketleft, send: Page_Up &#125; - &#123; when: has_menu, accept: bracketright, send: Page_Down &#125; squirrel.custom.yaml 自定义皮肤相关文件 1234567891011patch: style: color_scheme: psionics horizontal: true inline_preedit: true candidate_format: \"%c\\u2005%@ \\u2005\" # 用 1/6 em 空格 U+2005 来控制编号 %c 和候选词 %@ 前后的空间。 font_point: 16 # 候选文字大小 label_font_point: 14 # 候选编号大小 corner_radius: 5 # 候选条圆角 border_height: 0 # 窗口边界高度，大于圆角半径才生效 border_width: 0 # 窗口边界宽度，大于圆角半径才生效 最后，我的配置在 xiang578/rime 同步。 调试 Debug 是折腾 rime 不得不面对的一步，主要通过查看 rime 部署的時候 log 文件实现。对于鼠鬚管， log 文件保存在 $TMPDIR/rime.squirrel.* 中。首先在命令行中输入 echo $TMPDIR 获得路径，然后将地址输入到 「访达-前往-前往文件夹...」跳转。点击 rime.squirrel.WARNING 选择显示原身，利用文本编辑器打开。 文件格式类似于下面： 123456789101112131415161718Log file created at: 2019/06/07 23:31:54Running on machine: didideMiBook-Pro.localLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msgW0607 23:31:54.549346 162086912 config_compiler.cc:391] inaccessible node: punctuation.custom:/patchW0607 23:31:54.554167 162086912 config_compiler.cc:391] inaccessible node: key_bindings.custom:/patchW0607 23:31:54.560144 162086912 deployment_tasks.cc:179] schema list not defined.W0607 23:36:13.133517 161013760 config_compiler.cc:391] inaccessible node: punctuation.custom:/patchW0607 23:36:13.135843 161013760 config_compiler.cc:391] inaccessible node: key_bindings.custom:/patchW0607 23:36:13.149406 161013760 config_data.cc:62] nonexistent config file '/Users/didi/Library/Rime/luna_pinyin_simp.custom.yaml'.W0607 23:36:13.154920 161013760 config_compiler.cc:391] inaccessible node: punctuation.custom:/patchW0607 23:36:13.156643 161013760 config_compiler.cc:391] inaccessible node: key_bindings.custom:/patchW0607 23:36:13.331344 161013760 config_compiler.cc:391] inaccessible node: key_bindings.custom:/patchW0607 23:36:13.333066 161013760 config_compiler.cc:391] inaccessible node: punctuation.custom:/patchW0607 23:36:13.668072 161013760 config_data.cc:62] nonexistent config file '/Users/didi/Library/Rime/stroke.custom.yaml'.W0607 23:36:13.670761 161013760 config_compiler.cc:391] inaccessible node: key_bindings.custom:/patchW0607 23:36:13.672724 161013760 config_compiler.cc:391] inaccessible node: punctuation.custom:/patchW0607 23:36:14.281919 161013760 config_compiler.cc:391] inaccessible node: punctuation.custom:/patchW0607 23:36:14.283246 161013760 config_compiler.cc:391] inaccessible node: key_bindings.custom:/patch 按右 shift 切换输入法 之前使用搜狗输入法时，特别喜欢的一个功能：按右 shift 切换输入法的输入状态，实现暂时切换到英文状态。Rime 作者在 使用 Control 鍵切換中西文，上屏已輸入的編碼；令 Caps Lock 改變字母的大小寫 中提到一种方案。例如下面： 12345678patch: ascii_composer/good_old_caps_lock: true ascii_composer/switch_key: Caps_Lock: commit_code Shift_L: noop Shift_R: commit_code Control_L: commit_code Control_R: commit_code 然而这样修改完成之后，不论按哪个 Shift 键，都会切换到英文输入状态。看前面那个网页下面作者与其他人的讨论中发现，鼠须管无法区分 Shift 键。 网上查了一下，简单实现的方法是通过 karabiner 软件来改键。详细步骤可以参考 禁用 Squirrel 英文模式，使用左侧 Shift 切换中英 · rime/squirrel Wiki。 Reference 双拼 · Issue #37 · xiang578/xiang578.github.io 基于Rime的鼠须管输入法配置记录 Rime 输入法配置记录 Mac 下调校 Rime - 漠然的博客 | mritd Blog 最新版 Rime 输入法使用 - jdhao's blog 「鼠须管」的调教笔记 「鼠须管」输入方案的添加 RIME输入法在Finder中自动切换成英文 @ Why &amp; How 鼠须管配置 2019 禁用 Squirrel 英文模式，使用左侧 Shift 切换中英 · rime/squirrel Wiki rime-aca/dictionaries: Rime詞庫","categories":[],"tags":[{"name":"rime","slug":"rime","permalink":"https://xiang578.com/tags/rime/"}]},{"title":"博客折腾记","slug":"blog-log","date":"2019-06-15T05:39:49.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/blog-log.html","link":"","permalink":"https://xiang578.com/post/blog-log.html","excerpt":"","text":"记录博客修修补补的故事 210123 我也想学习微信不写更新日志 beta 0.3 换到 Github Action 构建 换到木子修改的 hexo-themes-next 主题 beta 0.2 长达 9 个月闲置之后，终于迎来一轮对博客的更新。 本着闭环的原则，将评论系统从 Disqus 替换为基于 Github Issues 的 Gitalk。不过，依旧没有多少留言量。 博客主题从 Next 切换到 fi3ework/hexo-theme-archer: 🏹 A smart and modern theme for Hexo. 添加一个新的固定栏目 「每月分享」，目前正在持续探索具体形式中。 除继续使用百度统计之外，加上 Google Analytics 进行数据分析。 beta 0.1 开始使用版本号来记录博客更新。从 beta 0.1 开始，预计完成博客发布会的计划后（参见：博客发布会 · Issue #23，切换到正式版本。 本次更新带来最大的变化是在博客中加入豆瓣个人阅读和观影记录。感谢 hexo-douban: A simple plugin for hexo that helps us generate pages for douban books ,movies and games. 提供技术支持。 beta 0 早期对博客进行的相关修改有： 从零开始利用 hexo + Github/Coding 搭建个人博客 为博客添加返回顶部按钮：WordPress 博客折腾记：使用 Travis CI 自动部署博客 博客折腾记：修复七牛云测试域名失效问题 博客折腾记：主题更新、迁移博客到腾讯云COS以及解决百度收录 博客折腾记：hexo-leancloud-counter-security 与标题中的引号冲突 博客专栏 【每周分享】：每周六更新，记录过去一周，我看到值得分享的内容 【月读】：每月更新，推荐本月我阅读的一本书 【数字生活】：我的数字生活实践 【博客公告】：分享与这个博客维护相关的内容 博客记录 本博客采用hexo搭建，使用Even主题 托管：Coding Pages 域名：腾讯云 评论：Disqus 统计：百度统计 图床：七牛 20151006：恢复订阅功能 20160303：开启分类和请我喝一杯咖啡 20160623：重新开启评论和百度统计，与自己和解 20160624：更换主题大道至简Maupassant，增加favicon和apple-touch-icon 20170104：放弃github+hexo,投入vps+wordpress怀抱,依旧使用maupassant主题 20170722：主机系统升级失败,从备份中恢复博客,并采用 Twenty Twelve 主题 20171005：重新投入hexo怀抱，并托管于Coding Pages 20180101：开启功德箱 20180501：PV: 657 | UV: 242 20180528：使用 Travis CI 自动部署 除虫记录 Error: Cannot find module 'node-sass-magic-importer' ERROR in Cannot find module 'node-sass'（已解决） - line - CSDN博客 1cnpm install node-sass@latest","categories":[],"tags":[{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"},{"name":"hexo","slug":"hexo","permalink":"https://xiang578.com/tags/hexo/"},{"name":"vps","slug":"vps","permalink":"https://xiang578.com/tags/vps/"}]},{"title":"博客折腾记：hexo-leancloud-counter-security 与标题中的引号冲突","slug":"meet-leancloud-counter-security-problem","date":"2019-05-25T13:21:14.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/meet-leancloud-counter-security-problem.html","link":"","permalink":"https://xiang578.com/post/meet-leancloud-counter-security-problem.html","excerpt":"","text":"昨天按照 hexo-theme-next/LEANCLOUD-COUNTER-SECURITY.md at master · theme-next/hexo-theme-next 这个文档配置博客阅读次数时，遇到 hexo-leancloud-counter-security 插件的一个冲突。 完成配置使用 hexo -d 时，终端中出现下面的错误提示： 12345678910111213 ATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlSyntaxError: Unexpected token h in JSON at position 30 at JSON.parse (&lt;anonymous&gt;) at /Users/didi/Documents/personal/xiang578.github.io/node_modules/hexo-leancloud-counter-security/index.js:92:42 at arrayEach (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_lodash@4.17.11@lodash/lodash.js:516:11) at Function.forEach (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_lodash@4.17.11@lodash/lodash.js:9344:14) at Hexo._callee$ (/Users/didi/Documents/personal/xiang578.github.io/node_modules/hexo-leancloud-counter-security/index.js:83:27) at tryCatch (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:62:40) at Generator.invoke [as _invoke] (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:296:22) at Generator.prototype.(anonymous function) [as next] (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:114:21) at step (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_babel-runtime@6.26.0@babel-runtime/helpers/asyncToGenerator.js:17:30) at /Users/didi/Documents/personal/xiang578.github.io/node_modules/_babel-runtime@6.26.0@babel-runtime/helpers/asyncToGenerator.js:28:13 at process._tickCallback (internal/process/next_tick.js:68:7) 看提示貌似是利用 Json 解析字符串的时候出现问题。打开 node_modules/hexo-leancloud-counter-security/index.js:92，对应出现一个解析 JSON的： 1y = JSON.parse(memoData[memoIdx].substring(0, memoData[memoIdx].length - 1)); js 没有怎么接触过，不知道能不能单步调试之类的，只好祭出输出调试大法，加上两个输出： 123console.log(memoIdx)console.log(memoData[memoIdx])y = JSON.parse(memoData[memoIdx].substring(0,memoData[memoIdx].length - 1)); 然后再执行 hexo -d 命令，命令行输出为： 12345678910111213141528&#123;\"title\":\"System.out.println(\"hello world!\");\",\"url\":\"/post/hello-world.html\"&#125;,FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlSyntaxError: Unexpected token h in JSON at position 30 at JSON.parse (&lt;anonymous&gt;) at /Users/didi/Documents/personal/xiang578.github.io/node_modules/hexo-leancloud-counter-security/index.js:92:42 at arrayEach (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_lodash@4.17.11@lodash/lodash.js:516:11) at Function.forEach (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_lodash@4.17.11@lodash/lodash.js:9344:14) at Hexo._callee$ (/Users/didi/Documents/personal/xiang578.github.io/node_modules/hexo-leancloud-counter-security/index.js:83:27) at tryCatch (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:62:40) at Generator.invoke [as _invoke] (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:296:22) at Generator.prototype.(anonymous function) [as next] (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_regenerator-runtime@0.11.1@regenerator-runtime/runtime.js:114:21) at step (/Users/didi/Documents/personal/xiang578.github.io/node_modules/_babel-runtime@6.26.0@babel-runtime/helpers/asyncToGenerator.js:17:30) at /Users/didi/Documents/personal/xiang578.github.io/node_modules/_babel-runtime@6.26.0@babel-runtime/helpers/asyncToGenerator.js:28:13 at process._tickCallback (internal/process/next_tick.js:68:7) JSON 在解析字符串{\"title\":\"System.out.println(\"hello world!\");\",\"url\":\"/post/hello-world.html\"} 时出现错误。对应的正是之前写的一篇名为 System.out.println(\"hello world!\"); 的文章，由于 JSON 格式中字符串是需要用\"\" 修饰，导致JSON 中出现了一个 \"title\":\"System.out.println(\"hello world!\");\" key-value 组合。然而实际上 JSON 只会将 \"System.out.println(\"h 解析成 value，之后出现的 h 被当成非法字符报错。 定位问题之后，暂时修改文章的标题为 hello world! | 算法花园，绕过部署失败。","categories":[{"name":"站务","slug":"站务","permalink":"https://xiang578.com/categories/站务/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"},{"name":"hexo","slug":"hexo","permalink":"https://xiang578.com/tags/hexo/"},{"name":"hack","slug":"hack","permalink":"https://xiang578.com/tags/hack/"}]},{"title":"博客折腾记：主题更新、迁移博客到腾讯云COS以及解决百度收录","slug":"use-cos-to-store-blog","date":"2019-05-19T07:44:09.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/use-cos-to-store-blog.html","link":"","permalink":"https://xiang578.com/post/use-cos-to-store-blog.html","excerpt":"","text":"本周有空对博客进行新一轮折腾，现在将这些尝试记下来和大家分享。 1. 主题更新 我在 博客折腾记：使用 Travis CI 自动部署 中提到将主题以 modules 的形式加入主仓库。而且现在使用的主题 git 仓库是我自己 fork 的，也有一些修改。几个天之前，hexo-theme-even 的 master 接受 feat: add LaTeX support by JieJiSS · Pull Request #236 ，完成对 LaTeX 公式的支持。所以，我需要将使用的代码和最新的代码合并。 这里使用的是 github Pull request 功能。在你自己 fork 的仓库的网页上点击 new pull request，然后按照下图修改。就会生成一个新的 Pull request 。 而且，如果你没有修改过原来的代码，PR 能自动合并。不过由于我对代码做了一些修改，会产生一些冲突，需要手动解决冲突（这里推荐使用 VS code）。出现下图的情况即成功合并两个库。 完成 PR 后，进入你站点下面的对应主题目录，使用 git checkout master 切换到主题的 master 分支，使用 git pull origin master 拉取最新的代码。回退到站点目录下，利用 git add 更新。 2. 迁移博客到腾讯云COS 利用腾讯云存储博客的静态文件，并配合使用 CDN 可以加快国内的访问速度。参考 Hexo博客迁移之旅（Coding到腾讯云COS）+ Travis CI持续集成 - 个人文章 - SegmentFault 思否 以及 如何在腾讯云COS部署HEXO博客 - 云+社区 - 腾讯云 。 记录两个我遇到的坑。 新的域名解析 完成 COS 配置后，需要将博客域名解析到腾讯提供CDN节点上的地址。 添加持续集成自动发布到COS（Travis CI） 为了发布到 COS，站点的 _config.yml 会添加下面的代码。 1234567 deploy:- type: cos secretId: XXX_ID secretKey: XXX_KEY appId: 1252086360 bucket: blog-1252086360 region: ap-shanghai 其中出现的 secretId 以及 secretKey 是私钥，不要在公开仓库展示。通过Travis-ci 中添加 Environment Variables 解决。 很多教程里，他们的 _config.yml 不会出现 secretId 和 secretKey 这两行，取而代之的是让你在 .travis.yml 添加几行。 123456script - hexo denv: global: - secretId: $&#123;secretId&#125; # Environment Variables 中配置 - secretKey: $&#123;secretKey&#125; # Environment Variables 中配置 按照这样设置，build 时，出现错误提示如下： 12345678910111213141516171819202122&#123; error: &#123; Code: &apos;InvalidAccessKeyId&apos;, Message: &apos;The access key Id format you provided is invalid.&apos;, Resource: &apos;blog-1252086360.cos.ap-shanghai.myqcloud.com/2012/01/23/2011/index.html&apos;, RequestId: &apos;NWNlMDU3NzlfNWI5ZDA4MDlfNWVlMF81ZWUzNTg=&apos;, TraceId: &apos;OGVmYzZiMmQzYjA2OWNhODk0NTRkMTBiOWVmMDAxODc0OWRkZjk0ZDM1NmI1M2E2MTRlY2MzZDhmNmI5MWI1OTQyYWVlY2QwZTk2MDVmZDQ3MmI2Y2I4ZmI5ZmM4ODFjMDU3YThkNThjZmQ1NWVkMGY2ZDBiNGM1YTEyNGIzMGM=&apos; &#125;, statusCode: 403, headers: &#123; &apos;content-type&apos;: &apos;application/xml&apos;, &apos;content-length&apos;: &apos;513&apos;, connection: &apos;keep-alive&apos;, date: &apos;Sat, 18 May 2019 19:05:29 GMT&apos;, server: &apos;tencent-cos&apos;, &apos;x-cos-request-id&apos;: &apos;NWNlMDU3NzlfNWI5ZDA4MDlfNWVlMF81ZWUzNTg=&apos;, &apos;x-cos-trace-id&apos;: &apos;OGVmYzZiMmQzYjA2OWNhODk0NTRkMTBiOWVmMDAxODc0OWRkZjk0ZDM1NmI1M2E2MTRlY2MzZDhmNmI5MWI1OTQyYWVlY2QwZTk2MDVmZDQ3MmI2Y2I4ZmI5ZmM4ODFjMDU3YThkNThjZmQ1NWVkMGY2ZDBiNGM1YTEyNGIzMGM=&apos; &#125; &#125;FATAL Something&apos;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlTypeError: Cannot read property &apos;statusCode&apos; of undefined at uploadFileToCOS.catch.then.data (/home/travis/build/xiang578/xiang578.github.io/node_modules/hexo-deployer-cos/lib/deployer.js:42:16) at process._tickCallback (internal/process/next_tick.js:68:7) 出现这个问题是 hexo -d 时，_config.yml 无法获得环境变量 secretId 和 secretKey 的。会导致没有秘钥。 参考 使用 Travis CI 部署你的 Hexo 博客 - 知乎 ，在 .trvis.yml 文件的 hexo d 命令前，加入下面两行即可解决。 12- sed -i &quot;s~XXX_ID~$&#123;secretId&#125;~&quot; _config.yml- sed -i &quot;s~XXX_KEY~$&#123;secretKey&#125;~&quot; _config.yml 之后build 时，会自动利用环境变量中 secretId 和 secretKey 的值替换 _config.yml 文件缺省的值。 最后提供我的两份配置文件给大家参考：_config.yml、.travis.yml。 3. 百度收录 之前，我一直将博客的静态文件存储在 github 的项目中，也使用插件生成 baidusitemap 文件。但是由于一些不为人知的秘密，百度的爬虫实际上无法爬取 github 上的资源，导致博客最新的文章没有被收录到百度中。 而且从百度提供的抓取诊断上来看，配置腾讯云 COS 后，百度的爬虫依然访问的是 github 上的仓库。 -w558 一顿搜索之后，找到一个主动提交 hexo 博客链接至百度的插件 huiwang/hexo-baidu-url-submit。 参考 Hexo插件之百度主动提交链接 | 王辉的博客 以及 Hexo百度主动提交链接 - 简书 完成配置。 安装插件 cnpm install hexo-baidu-url-submit --save 修改根目录下面的 config.yml 文件，配置 baidu_url_submit 和 deploy。 1234567891011121314baidu_url_submit: count: 100 ## 比如3，代表提交最新的三个链接 host: xiang578.com ## 在百度站长平台中注册的域名 token: your_token ## 请注意这是您的秘钥， 请不要发布在公众仓库里! path: baidu_urls.txt ## 文本文档的地址， 新链接会保存在此文本文档里deploy:- type: cos secretId: XXX_ID secretKey: XXX_KEY appId: 1252086360 bucket: blog-1252086360 region: ap-shanghai- type: baidu_url_submitter 上面的代码中出现一个 token，由于这是一个私有的，不能出现在 github 公开的仓库中。所以也需要 Travis-ci 中添加 Environment Variables 解决。和前文提到相同，在 .travis.yml 中添加 - sed -i \"s~your_token~${BD_TOKEN}~\" _config.yml 解决私钥问题。 最终在 travis-ci 中发现下面的日志即配置成功。另外一点，百度的站长平台的数据不能及时展示我们提交后的结果，需要耐心等待。 -w866","categories":[{"name":"站务","slug":"站务","permalink":"https://xiang578.com/categories/站务/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"},{"name":"travis","slug":"travis","permalink":"https://xiang578.com/tags/travis/"},{"name":"cos","slug":"cos","permalink":"https://xiang578.com/tags/cos/"}]},{"title":"ImageNet Classiﬁcation with Deep Convolutional Neural Networks(AlexNet)","slug":"alexnet","date":"2019-05-18T12:56:43.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/alexnet.html","link":"","permalink":"https://xiang578.com/post/alexnet.html","excerpt":"","text":"作者以及相关性 Alex Krizhevsky Ilya Sutskever Geoffrey E. Hinton 本文被认为是这一轮深度学习浪潮的开端 主题 将 CNN 技术最先应用到图像识别领域，利用 CNN 参数共享的特性，减少网络的规模 解决深度网络难训练（速度慢）以及容易过拟合问题（更多数据或者网络技巧） 数据集与指标 ImageNet LSVRC-2010 contest: 图片 1000 分类 top-1 和 top-5 错误率为指标 模型/实验/结论 模型 由于当时的 GPU 显存限制，无法将所有的数据加载到单独 GPU 中，作者使用两个 GPU 并行训练。 整个模型如下图所示，由 5 个卷积层以及 3 个全连接层组成。其中在 CONV3、FC1、FC2、FC3 层进行两个 GPU 的数据交互。 [227, 227, 3] INPUT: 原始论文中 224 为笔误。 [55, 55, 96] CONV1: (11*11*3,96) filters, Stride 4, pad 0 [27, 27, 96] MAX POOL1: (3*3) filters, Stride 2 [27, 27, 96] NORM1: Normalization layer [27, 27, 256] CONV2: (5*5,256) filters, Stride 1, pad 2 [13, 13, 256] MAX POOL2: (3*3) filters, Stride 2 [13, 13, 256] NORM2: Normalization layer [13, 13, 384] CONV3: (3*3,384) filters, Stride 1, pad 1 [13, 13, 384] CONV4: (3*3,384) filters, Stride 1, pad 1 [13, 13, 384] CONV5: (3*3,256) filters, Stride 1, pad 1 [6, 6, 256] MAX POOL3: (3*3) filters, Stride 2 [4096] FC1: 两个 GPU 中的 CONV 层结果进行全连接 [4096] FC2: FC1 进行全连接 [1000] FC3: FC2 进行全连接，最后输出分类结果 参数数量 60 million 使用 ReLU 作为激活函数：比 tanh 计算开销小，以及收敛速度快。根据问题的特点选择激活函数（大模型、大数据集） Local Response Normalization(Norm Layers)：局部响应归一化层，后来很少使用。 在经过 ReLU 作用之后，对相同空间位置上（\\({b_{x,y}}\\)）的相邻深度（\\({b^j}\\) ）的卷积结果做归一化。n 指定相邻卷积核数目，N 为该层所有卷积的数目。\\({k, n, \\alpha, \\beta}\\) 都是超参数。本文使用 \\({k=2, n=5, \\alpha=10^{-4}, \\beta = 0.75}\\), 分别降低 top-1 和 top-5 错误 1.4% 和 1.2% \\[ b_{x, y}^{i}=a_{x, y}^{i} /\\left(k+\\alpha \\sum_{j=\\max (0, i-n / 2)}^{\\min (N-1, i+n / 2)}\\left(a_{x, y}^{j}\\right)^{2}\\right)^{\\beta} \\] Pooling：s=2 &lt; z=3，有部分重叠，作者通过实验发现这种方法可以更好地避免过拟合。 data augmentation： 对图像进行裁剪以及翻转，扩大数据。这种策略对测试带来影响，测试时裁剪出图片四个角落以及中间部分，得到 5 张图片，另外翻转得到 5 张图片，最后分类结果又这 10 图片的平均得分确定。 利用 PCA 改变 RGB 通道的强度。 Dropout：每次训练的时候，从模型中 sample 出一个小的模型，减少过拟合。 实验 参数：dropout 0.5，batch size 128， SGD Momentum 0.9， Learning rate 1e-2 reduce by 10，L2 weight decay 5e-4 测试集上结果 取出 CONV1 相关的 filters卷积侧重点不同，GPU1 颜色无关，GPU2 颜色相关。多次实验发现都存在这种现象，说明使用多个 GPU 训练是必要的，模型可以捕捉更多信息。 取所有最后一个隐层向量，找到与测试图片欧拉距离最小的训练图片（下图中第一列为测试图片，之后几列是欧拉距离最小的训练集中图片）。肉眼可以发现，同一分类的图片有很大关联性。证明模型能学习图片之间的关系。 结论 通过移除 AlexNet 网络中的某几层发现错误率均有提高，这个网络时必要以及有效的。 文章中作者通过大量的实验确定模型的细节问题，值得我们学习。 当时的 GPU 限制作者的想象力……","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"cnn","slug":"cnn","permalink":"https://xiang578.com/tags/cnn/"},{"name":"alexnet","slug":"alexnet","permalink":"https://xiang578.com/tags/alexnet/"}]},{"title":"2018 探索","slug":"2018","date":"2019-04-10T04:40:45.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/2018.html","link":"","permalink":"https://xiang578.com/post/2018.html","excerpt":"","text":"全文混乱。拖了 4 个月之后，强行完结。 毕业 2018 最大的一件是自己终于艰难地从学校毕业。本来在学校属于 easy 模式，原本以为毕业很轻松。不过出于一些原因，比其他人多待一个月才拿到毕业证书，给我不太美好的大学生涯又多添几份痛苦。本来还准备写篇文章来总结一下大学生涯，拖到现在毕业都快一周年，也只能当成是毕业一周年的回忆文章。 工作 毕业之后，用我外婆说的一句话“一个人拉着两个行李箱就去工作了”。误打误撞和机器学习挂上一些关系。每天属于虽然工作很开心，但是好像没有干什么事情的状态。更多地吐槽也准备写在工作一周年的文章中。 自我管理 这个概念是年初感觉自己太混乱时提出来的，如果成为更好的自己。一年来有过很多想法和实践，但是现在还探索出来完整的系统。有机会再写。 年度阅读 说来惭愧，今年没有读多少本书，而且绝大部分都是在没有毕业时候读的。工作之后，完整看完地也只有一本《九败一胜》。这本书讲的是王兴的创业故事，总的感受是创业维艰。感觉王兴是为了创业而生的人，有知识基础，又有经济基础。在多次创业之后，培养了商业上的灵敏，管理上的艺术。最终能在千团大战中走出来，成就今天的美团帝国。可惜这个冬天，美团有些艰难，脉脉上给予他裁团（裁员，特别是应届生）、C团（绩效打 C，逼你走）的名声。比起王兴的故事，我更感兴趣的是程维创立滴滴的故事，不知道什么时候可以读到。 说回来在读过的书中，最推荐 软技能，之前也写过简单的介绍。用时髦的话来说，这本书教你成为一个斜杠青年。在基础的工资外，还有通过其他渠道有第二职业的收入，最后是睡后收入（表名上说的是睡觉时候获得的收入，第二层含义是一次生产，可以多次贩卖）。后来想想，自己可以二次出售什么？无法是什么时间管理、知识管理、理财、读书、写作之类的烂大街的东西。所以，自己还是需要加强抗击职业风险的能力，尽快找到自己的第二职业收入。 另外，自己也进行了一些主题阅读。年初的时候，对时间管理和知识管理感兴趣。读过Evernote 100个做笔记的好方法、Evernote超效率数字笔记术、印象笔记留给你的空间、有道云笔记：记录，成为更好的自己、你的知识需要管理，看完这些书多少有些收获，但也没有完全解答我的疑问，说回来，也不太推荐你们去看。不过，时间管理方面的两本书，小强升职记和搞定Ⅰ，却是五星推荐，看一看，多少能提高一些工作效率。 年度观影 今年看过的电影倒是比书多一些。不过，其中好多都是漫威的超级英雄片。自己感觉漫威伟大的地方在于创造了一个包括神话、物理、外星文明的电影宇宙，这个宇宙也许会成为我们这一代人的回忆。 说回来，今年看过的片子中，最推荐的是无问西东。这部片子讲述了不同时期 4 个不同年代清华学子关于选择的故事，也许是因为没有他们这样的大学经历才会嫉妒。看完片子后，还抄录一些台词，大概能更加清晰的表达电影对我的影响。 吴岭澜（文科很好，理科很差）面对梅校长时候询问为什么不去读文科时的回答。 &gt; 因为最好的学生都读实科 我只知道，不管我将来做什么 在这个年纪，读书，学习都是对的 我何用管我学什么？ 每天把自己交给书本，就有种踏实 吴岭澜重新找到自己的目标之后，成为了清华大学的一名教授。在西南联大给学生上课时回忆自己的大学时光： &gt; 当我在你们这个年纪，有段时间，我远离人群，独自思索，我的人生到底应该怎样度过？某日，我偶然去图书馆，听到泰戈尔的演讲，而陪同在泰戈尔身边的人，是当时最卓越的一群人，这些人站在那里，自信而笃定，那种从容让我十分羡慕。而泰戈尔，正在讲“对自己的真实”有多么重要，那一刻，我从思索生命意义的羞耻感中，释放出来。原来这些卓越的人物，也认为花时间思考这些，谈论这些，是重要的。今天，我把泰戈尔的诗介绍给你们，希望你们在今后的岁月里，不要放弃对生命的思索，对自己的真实。 对吴岭澜的总结： &gt; 梅校长说：“人把自己置身于忙碌当中，有一种麻木的踏实，但丧失了真实，你的青春也不过只有这些日子。” 什么是真实？ 你看到什么，听到什么，做什么，和谁在一起 有一种，从心灵深处，满溢出来的不懊悔，也不羞耻的平和与喜悦 后来吴岭澜领悟到了： 看到和听到的，经常令你们沮丧，世俗是这样强大，强大到生不出改变它们的念头。可是如果有机会提前了解了你们的人生，知道青春也不过只有这些日子，不知你们是否还会在意的，那些世俗让你们在意的事情，比如占有多少，才更荣耀，拥有什么，才能被爱。 等你们长大，你们因绿芽冒出土地而喜悦，会对出生的朝阳欢呼雀跃，也会给别人善意和温暖，但是却会在赞美别的生命的同时，常常，甚至永远忘了自己的珍贵。愿你在被打击的时，记起你的珍贵，抵抗恶意；愿你在迷茫时，坚信你的珍贵，爱你所爱，行你所行，听从你心，无问西东。 富家子弟沈光耀放弃学业，决定参加飞行队时，母亲不远万里来联大劝他。 “当初你离家千里，来到这个地方读书，你父亲和我都没有反对过，因为，是我们想你，能享受到人生的乐趣，比如读万卷书行万里路，比如同你喜欢的女孩子结婚生子。注意不是给我增添子孙，而是你自己，能够享受为人父母的乐趣，你一生所要追求的功名利禄，没有什么是你的祖上没经历过的，那些只不过是人生的幻光。我怕，你还没想好怎么过这一生，你的命就没了啊！” 同学在他牺牲后，去看望沈母时，屏幕上展现出一幅对联：三代五将护国定疆青史留正气，六韬三略擅用筹边御旨赞英豪。 这部电影的彩蛋标题是致敬时代的风骨，快速回顾在电影中出现过的时代名人。可惜自己没有认出多少个，真是悲哀。 后记 这篇文章写的有点杂，我只是看着 MWeb 中的存稿有点多，趁着这次机会消灭一些，来年有机会写些新的东西。 于北京回龙观 其他文章： 2017 迷茫","categories":[],"tags":[{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"},{"name":"book","slug":"book","permalink":"https://xiang578.com/tags/book/"},{"name":"movie","slug":"movie","permalink":"https://xiang578.com/tags/movie/"}]},{"title":"每周分享第 11 期","slug":"week-issues-11","date":"2019-03-16T12:56:43.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issues-11.html","link":"","permalink":"https://xiang578.com/post/week-issues-11.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六不定时更新。 观点 美团和滴滴为什么要互相开展对方的业务： 我记得罗振宇在某一年时间的朋友中提到一个观点：O2O 领域会出现一个超级公司，业务横跨所有场景。 从技术角度分析，外卖中比较难的点在于分单、时间估计以及路径规划。这也是出行服务中的技术难点。不过各有侧重，外卖中路径规划相当于最多不超过 20 个点（10 个商家处取餐，10 个顾客中送餐）的 TSP 问题。出行服务最关键的是如何有效的获取道路信息。（我有一个观点，人是有主观能动性的，会抄各种小道） 这两个服务都严重依赖地推服务，所以双方本身都有一支庞大的城市地推人员。我想开展不同业务的切换难度应该会小一些。 读书《刻意练习》 听这个名字很容易认为是一本鸡汤书，英文小标题中提到 New science of Experitse。本书打破的是之前很流行的 1 万小时天才理论，这个理论认为很多天才之所以是天才是他们的技能经过长时间的训练，有量变引起质变。新的研究发现，学习的本质是在大脑中建立心理表征，它是一种长时记忆单元，也是我们习得技能的结果。刻意练习就是如何高效地获得这个心理表征。鉴于这是一本脑科学的书，方法是否正确还应该是自己亲身体验才能知道。 文章 经过一年的思考，我重新梳理了我的印象笔记使用方法 - 少数派：不知道为什么，我对这类文章特别地喜欢。这篇文章带来的新意是对知识的六种分类，之前我自己没有考虑过。不要过度沉溺于研究工具，而忘记学习的初心。 教授说没有写过一千行代码就别想上大公司，这种说法对吗？ - 知乎：之前看到这个问题就想直接喷，写一千行代码怎么去大公司。看完其他人的回答，才发现又一次陷入到了思维定势。从数学角度来说，写一千行代码去大公司是一个必要不充分条件，去大公司确实需要写代码，一千行是被包括的，但可能远远不够。 iPhone X 发布会乔布斯录音的中译（或当代汉语现状） – 一天世界：一段英文白话翻译以及文言翻译对比，又一次体会到古典之美。 There’s lots of ways to be as a person, and some people express their deep appreciation in different ways. But one of the ways that I believe people express their appreciation to the rest of humanity is to make something wonderful and put it out there. You never meet the people, you never shake their hands, you never hear their stories or tell yours. But somehow in the act of making something with a great deal of care and love, something’s transmitted there. And it’s a way of expressing to the rest of our species our deep appreciation. So we need to be true to who we are and remember what’s really important to us. That’s what’s gonna keep Apple, Apple, is if we keep us, us. 「李如一」译文：人生而有别，感恩之心亦可谓十人十色。创造神奇，示之于人，此为对人性感恩之一种也。彼二人或未曾握手相见，对各自生平事故复无相闻，惟细巧体贴之创造即足以千里传音。此即吾等人类传递感恩之心之法门也。故人必诚于其本色，忠于其所信。林檎之为林檎，端赖吾辈不忘初心也。愿与诸君共勉。 「@东莞大唐和尚」译文：人与人的生活方式千差万别，表达感恩的方式也是多种多样。但我认为，做出一件绝妙的东西，才是向这个世界表达感恩的最最深刻的方式。你没见过那些人，没跟他们握过手，你没听过他们的故事，也没跟他们讲过自己的经历，但就是通过你倾注心血做出的这样一件东西，你传达给了他们一些东西。这是向其他人表达我们感激的最深刻的方式。我们需要真诚地面对自我，永远记住对自己最重要的东西是什么。苹果之所以是苹果，我们之所以是我们，正在于此。 视频 人生一串 _ 纪录片 _ bilibili _ 哔哩哔哩弹幕视频网：属于国人的深夜食堂，再搭配上神级文案，完美展示烧烤江湖。 后记 这半年自己写的东西有点少，从第 10 期到这一次拖了好久。最初，是在阮一峰的鼓舞下开始这种形式的分享。不过后来由于种种原因，又展现出自己的本质。有些事情还是要坚持的，所以将这些存货发布出来。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"2018 年消费指南","slug":"2018-consumer-report","date":"2019-01-13T11:05:14.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/2018-consumer-report.html","link":"","permalink":"https://xiang578.com/post/2018-consumer-report.html","excerpt":"","text":"去年在总结中提到了一些知识付费的内容，今年将内容扩展，和大家分享我在这一年购买的实物以及虚拟产品。 实体购物 工作之后，感觉自己每个月留不下多少钱，很大一部分用来购买一些号称提高工作效率有关的物件。现在毕竟钱都花出去了，至少要装模作样地总结它们给我带来的提升。 键鼠 入职之后，地主只给我提供了一把锄头（Macbook Pro 13 with touchbar），长时间在蝴蝶键盘上敲击不是很愉悦的感觉。所以自己产生了购买键盘和鼠标的念头。 众所周知，机械键盘是码农的标配，用手指在键盘上噼里啪啦快速敲击，想想就有画面感。之前在上学的时候，我拥有一把 IKBC G87 的青轴键盘。IKBC 的优点在于价格便宜，不过和同学的 Filco 圣手对比，手感不是那么的清脆。而且本着一步到位的想法，这次准备购买的键盘可以贵一些。再加上一些其他的条件，将自己的选择限定在了 Filco 和 HHKB 上。众所周知，HHKB 的价格差不多可以买两个 Filco。最后是遇到了少数派的优惠活动，才痛下决心买了一个 HHKB Professional BT（其实是多送了少数派的贴纸而已）。 HHKB 评价 HHKB 最好的方式是引用其创始人和田英一下面这一段话： 美国西部的牛仔们，会将死去的马儿留在原地，但是仍然会扛着马鞍长途跋涉，穿越一望无垠的沙漠。因为马儿是消耗品，而马鞍却是与人体融合在一起的“知己”。我们要有这样的观念：现在，电脑是消耗品，键盘却是传递情感，陪伴我们一生的“挚友”。 HHKB 给人最大的感觉就是与众不同，一共只有 60 个按键。整个键盘长度和一张 A4 纸相当。看下面的布局图不难发现： 没有 F1-F12 功能按键 没有方向键 Caps 键的位置上是 Control 键 HHKB_Pro2_Layout 咋一看，很难满足一般的工作需求。但是经过对工作方式的一些调整，可以很好的完成日程任务，而且键盘的手感不错（从 v2 上看到的形容是少女酥胸的手感，具体是不是我也没有体验过），长时间敲击没有疲劳感。我的编程主要在服务器上用 vim 完成，所以 hjkl 才是我的方向键，而且我也在 Jetbrain 的编辑器中安装了 vim 插件。对于其他情况下，使用 Karabiner Elements 对键盘进行一些改造（按住 Control 开启 vi 模式，hjkl 变成方向键），最后还可以用 Mac 系统自带的一些文本编辑相关的快就键。 Karabiner_Elements 有了键盘之后，不能没有鼠标。其实这里面也有两个选择：罗技 的 MX Master 2 和苹果的触摸板。不得不说， mnp 自带的触摸板用起来非常的爽快，但是单独购买触摸板价格也很感人，提前退出了购买范围。趁着双十一，在京东买下了 MX Master 2。 MX Master 2 看上面的图片可以知道，这是一款人体工程学的鼠标，而且有一些按键可以编程（配合软件）。支持蓝牙以及接受器连接，可以记忆三个设备。据说，还实现了在一台电脑上复制，再另外一台电脑上粘贴。总体用下来也是中规中矩，除了中间的滚轮阻尼感有点差之外（侧边的滚轮手感很好，但是不能修改成上下滚动的效果），也没有太多缺点。 Bose QC 35 二代 购买降噪耳机多少是出于无奈，离开学校的图书馆之后，很少能找到一个安静的地方，让自己静下心来干一些事情。特别是在开放的办公室中，不仅有其他同事的讨论声，还有空调的噪音。带上降噪耳机，在放上一曲喜欢的音乐，就感觉来到了另外一个世界。之前在知乎上看到的一个评价正好能形容这种感觉： 有多安静我来描述一下，孩子数学成绩不好，你在银行做经理，维护客户关系，不上不下，有房贷和车贷，每月按揭五千。你老婆在市人民医院做护士，她妈有尿毒症透析多年，她不爱你。你年轻的时候觉得能成一番事业，但现在也就这样，朋友们混的都比你好，你下班在车库停稳车，关掉引擎，呜一声安静了下来。太安静了，你生命中少有这么安静的时刻，你打算发十分钟呆再上楼吃饭。 以上就是 BOSE QC 35 的降噪效果测评。 所以充分证明，现在的耳机评测多么注重编故事的能力。说回来，带上耳机之后，空调之类的噪音基本上会被隔绝，其他人声只是轻微的减弱，就像他们在远处处聊天。总体来说，这副耳机达到了我的预期，也算是一笔合理消费。 米家宇宙 用这里来调侃一下，小米出的那么多智能家电。自己入手了米家台灯和小饭煲，搭配米家的 APP ，可以实现晚上当你拖着身体回来时有一盏灯为你亮起，清晨又有一锅粥等你去品尝。最近，米家 APP 通过捷径配合 Siri 使用，大大扩展了便利程度。未来真的快要来了。 mijia 年度虚拟产品 与上面提到的实体产品相反的，就是虚拟产品，比如软件、文章、教程、视频等。用虚拟产品更好的总结这些消费的特点。其实很多人会觉得这些东西不值得花钱，网上找盗版的即可。但自己赞同一个观点，给优秀的内容付费，才能激励作者分享更多的知识。 订阅服务 在这一年中持续付费的产品有滴答清单、印象笔记、SetApp。滴答清单是最早购买的 GTD 软件，对于初学者来说，这是一个大而全的软件，从清单、日历到番茄时间应有尽有。不过，我没有打算继续在新的一年为它付费，我对 GTD 软件的要求是有强大的过滤功能，而不是那一些花里胡哨的噱头功能。印象笔记是自己选择的主力笔记软件，之前使用为知笔记，但是他在 Mac 上的功能很少，也很久没有大的更新了，而印象笔记特别是在中国区独立之后，有很多大的动静。 Setapp 是一个软件集中订阅服务，简单来说，你订阅了这个服务，可以使用很多需要购买的 Mac App。自己目前常用的有 Moneywiz（记账软件，mac 和 ios 都可以使用）、Timing（自动记录 Mac 上软件使用时间，可以看成是一个统计工作效率的软件）、Bartender（隐藏状态栏，看起来更加清爽）、iStat Menus（在状态栏显示网速、cpu使用率等系统状态）、MarginNote（比较强大的 PDF 阅读软件）、Ulysses（Markdown 写作软件）。除此之外，SetApp 还提供了 100 多款软件，总的来说是超值的服务。 -w1252 买断服务 相比起付费服务，直接买断的软件就显得有点少。其中的原因有很多，最重要的是很多国外软件一次性买断价格是参考国外的物价，也许对于他们来说是一顿饭的价格，对应到国内就是好几百。 MacOS 上购买了 MWeb 3，也就是现在使用的 Markdown 软件。从二代开始入手使用，用来写一些文章的草稿。另外一点，用来管理博客文档也比较方便，可以一键将文章中出现的本地图片上传到图床。不过，上面提到 SetApp 中提供了更强的 Ulysses，也许明年的总结我会用 Ulysses 完成。 iOS 中，主要购买的是一些工具。Cloud Speed，测试不同国外云服务商的不同机房的速度，买了之后没有想象中的那么好。Taskmator，搭配 Mac 上的 Taskpaper 使用，之前想用他来做任务管理，不过后来放弃了。MoneyWiz 2，超值的记账软件，帮我养成了记账的习惯。之前用过网易有钱，无法忍受他的理财社区而卸载了。无相，一款神奇的浏览器，你可以指定一些其他网站的 CSS 样式（软件中有一个商店可以下载 CSS 文件），从而提升阅读体验，间接实现去除页面上的广告…… 知识付费 罗振宇在 2016 年提出知识付费元年，可从我的角度来说，2018 才是我的知识付费元年。今年主要在两大平台进行内容消费，闲鱼以及少数派。 对的，你没有看错，闲鱼是我上半年的一个主要消费场所。有一句话，评价一个知识付费好不好，看它在闲鱼上有没有买就可以了。闲鱼上有很多倒卖的人，很可能是 N 道贩子，主要是通过百度云进行交易。比起原生的，体验是非常差的（得到的文章是长图片形式），胜在价格便宜（一两块到十几块不等）。可以用来简单判断一下内容，再决定是否需要去原网站购买。回过头来看，自己购买的绝大部分课程内容还静静地躺在百度云中……但也发现了一个精品课程，小能熊——陈华伟的《知识管理训练营》，这里面讲了很多老师自己使用 Mac 和 iPhone 进行知识管理的方法和体会。对于不是高阶的用户很是值得一看，原价是 99 元非常值得（可以在印象笔记公众号中找到）。我也做了一些笔记，一直比较忙，没有时间整理分享。搜索了一个其他人的笔记，大家可以看一下了解 21天知识管理训练营总结【笔记版1.0】 - 简书 课程大纲 至于在少数派中，就花了很多钱够买其中的专栏。少数派是我看了好几年的一个数字资讯网站，他们的口号是「少数派致力于更好地运用数字产品或科学方法,帮助用户提升工作效率和生活品质」。自己购买了他们的会员通讯 Power+ 1.0 以及还在持续更新中的 Power+ 2.0，具体的内容介绍可以查看这两个网页。如果你也是那种喜欢折腾软件的人，这个东西非常超值。任务管理系列（用 OmniFocus 3 搭建任务管理系统、用更现代的方式做任务管理、TaskPaper 使用指南），其实购买这三个完全是没有必要，你喜欢哪一个软件做 GTD，直接购买对应的教程就好了。最后也很推荐的是 从零开始做好个人记账，教你使用 Moneywiz 记一手明白账（原理通用，也可以使用其他软件。） sspai","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"}]},{"title":"博客折腾记：修复七牛云测试域名失效问题","slug":"fix-qiniu-test-url-error","date":"2018-12-20T19:39:14.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/fix-qiniu-test-url-error.html","link":"","permalink":"https://xiang578.com/post/fix-qiniu-test-url-error.html","excerpt":"","text":"毕业之后开始工作快要 5 个月了，然后也快有 3 个月没有更新博客。其实文本编辑器中还有很多的草稿，但是一直没有力量驱动自己完结他们，并且分享出来。另外，这一段时间也不是完全没有分享。在这个页面的上方有一个 Tech 的标签，可以连接到我新搭的博客。受限于当前使用的 hexo 主题无法配置 latex 数学公式，所幸新开博客分享算法学习的笔记。大家感兴趣的可以访问一下，不过也没有太多的内容。 这次在博客公告中要告诉大家的确是另外一件事情。屋漏偏风连夜雨，不知道从什么时候开始，七牛云开始图片使用测试域名，毫无疑问这个博客的图片都挂了。自己也一直没有动力修复，让这一段时间访问我博客的小伙伴受累了。 今天研究了一下如何修复这个图床问题。官方有一个帮助页面如何配置域名的 CNAME - 七牛开发者中心，大概就是你的存储空间之前有一个测试域名（比如我的是 7xkpe5.com1.z0.glb.clouddn.com），现在不允许通过测试域名访问图片，需要绑定一个备案过的域名才可以。所以我们需要两个步骤完成改造：首先，给空间绑定一个域名（比如现在使用的是 media.xiang578.com ）；最后，在域名解析平台添加一个 CNAME，将你指定的域名转发到七牛的记录上。 完成上一步后，图片还是不能正常显示。因为之前的文章中，图片的链接都是以测试域名开头的，比如7xkpe5.com1.z0.glb.clouddn.com/15283589946007.jpg ，现在我们要将它改成 下面的形式 media.xiang578.com/15283589946007.jpg。简单的方法是打开文本编辑软件，然后使用查找替换功能，一个一个文件处理。显然这很无聊，而且进入 source/_posts 目录下利用 grep 7xkpe5 *.md | wc 统计了一个，我大概需要修改的有 142 处。 需要替换的字符串 幸运地是 linux 系统下有两大文本处理利器 sed 和 awk。我们使用 sed 可以将一个字符串转换为另外一个字符串。网上搜索了一下用法，很快写了出来 1sed -i -r \"s/7xkpe5\\.com1\\.z0\\.glb\\.clouddn\\.com/media\\.xiang578\\.com/g\" *.md 这条命令中原始形态可以表示为 sed 's/原字符串/替换字符串/g'。其中参数 -i 代表替换文件中的所有匹配项，-r 代表批量替换支持扩展表达式。在原字符串和替换字符串中都出现了 \\.，应为 . 在 sed 命令中代表匹配任意单个字符，加上转移字符后可以代表它本身。最后 *md 代表对目录下的 md 文件进行处理。 运行完成之后，我们在统计一下测试域名和正式域名的数量，可以发现完美的解决了这个问题，图片又能正常显示。 修改后 所以，写下今天这一篇博客一切都是因为贫穷。如果有钱直接在主机上放置图片，有带宽提供出来访问，也就不会依赖七牛云了…… 2019.11.09 为了减少博客依赖服务，参考 图床从七牛云迁移到腾讯COS折腾笔记 | 思想就是武器 将所有的图片全部从七牛上下载，利用 COS 存储。","categories":[{"name":"站务","slug":"站务","permalink":"https://xiang578.com/categories/站务/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"},{"name":"hack","slug":"hack","permalink":"https://xiang578.com/tags/hack/"}]},{"title":"字体的重要性","slug":"ImportanceOffont","date":"2018-09-03T23:41:14.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/ImportanceOffont.html","link":"","permalink":"https://xiang578.com/post/ImportanceOffont.html","excerpt":"","text":"字体的重要性 最近开始工作，基本上都和终端打交道，碰到几个误认字符的尴尬场面，记录一下。 第一个遇到的问题发生在输入账户密码时，公司发的小册子上写的密码大概形式如xxxxxi|6xxx。由于打印密码的字体是黑体，难免产生疑问 | 到底是 I 还是 l？观察到的细节是 | 这个字符下面比其他的字符长，不过由于之前很少在密码中使用过这个字符，所以以为这个细节是区分I 和 l 的。在密码错误 n 次后，眼光扫到键盘才发现回车键上面的 | 键。 第二个遇到的问题是在终端中，公司的堡垒机登陆比较复杂，一般都会写脚本来快速登陆。写完之后，运行指令的格式为 jump ip 'auth' 其中的 'auth' 部分为调用另外一个脚本生成一个二次验证的并作为 jump 命令的参数。其中这个 ' 符号被我认为是引号，后来查阅相关的 shell 命令（相关文章参考linux下命令执行结果作为其他命令输入参数 - CSDN博客），才明白为反引号（一般位于ESC 的下方）。 说完这两个问题，回到主题，每天和字母打交到，选着一款合适的字体是非常重要的。推荐一款我在几年前就使用的编程字体——Hack: A typeface designed for source code。 Hack 字体示意 上图就是这款字体的示意，最喜欢的点是 0 中间有一个小竖点，非常的传神。 以至于现在 IDE 中的 0 不是想上面这样处理，我都感觉不会编程了。","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"font","slug":"font","permalink":"https://xiang578.com/tags/font/"}]},{"title":"博客折腾记：使用 Travis CI 自动部署博客","slug":"use-travis-ci-to-auto-build-blog","date":"2018-09-03T23:05:50.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/use-travis-ci-to-auto-build-blog.html","link":"","permalink":"https://xiang578.com/post/use-travis-ci-to-auto-build-blog.html","excerpt":"","text":"已切换到 Github Action 之前一周在封闭接受公司培训，最重要的任务是熟悉「项目开发全流程」。其中有一点：服务的稳定性。不知道为什么，前几天自己的博客崩溃了，输入域名只能看到 404 页面。当时以为是 Travis CI 的原因，所以进行了全面的一次排查： 问题出现在 Github Pages 的 Custom domain 设置中。 具体表现：通过 Travis CI 推送博客静态文件到仓库中的 master 后，下图框中的域名就会变成空的，导致无法访问。 解决方法：在源文件的 source 目录下创建一个 CNAME 文件，写上你自己的域名。 Travis CI 其实看一眼就应该知道，我的博客是基于 hexo 搭建的，文件托管在 github 仓库中。不过，按照之前的设想博客应该在 Coding 中也有一份备份。后来由于一些原因，在利用 hexo 生成静态文件之后，自动推送到 Coding 上的命令不起作用。自己也没有时间去排查问题，所以最近访问速度有点慢。 传统的 hexo 博客更新过程是：在完成写作之后，利用命令行调用 hexo g &amp;&amp; hexo d 来生成静态博客文件以及并推送到远端的仓库中。这种方法会产生三个痛点： 每一次修改源文件后都需要重新生成一边静态文件，当大量修改时，步骤就变得繁琐且无趣。 生成静态文件依赖电脑中的 hexo 和 node.js 环境，不方便在外出时临时写或修改博客。 博客源文件没有自动的备份功能，不符合安全原则。 Travis CI 是一种持续集成开发所使用的工具，在写作过程中引入他可以解决上面我提到的痛点。Travis CI 具体的含义也不是很清楚，直接介绍我是怎么使用的。 博客依赖 3 个 git 仓库： 原始文件 xiang578/blog 主题文件 xiang578/hexo-theme-even: A super concise theme for Hexo（我对这个主题有一些修改，所以自己 fork 了一份 发布文件 xiang578/xiang578.github.io: Welcome to My blog!。 和大部分人一样，这个博客的静态文件保存在 github 的 xiang578.github.io 仓库 master 分支中。但是，我还创建了一个新的分支 hexo，用来保存博客源文件。每一次修改博客源文件之后，我不在本地生成静态文件，而是利用 git 命令，将所有的修改内容推送到仓库中的 hexo 分支。Travis CI 服务监听到新的 push 时，会根据你的配置将 git 仓库拉倒他的服务器上，编译源文件成为静态文件，并推送生成的文件到指定仓库的指定分支中。而且，如果编译静态文件失败，他也会通过邮件通知你结果。 编译成功截图 流程 将 github 上存放静态博客源文件的仓库拉下来，利用 git checkout -b hexo 创建并进入新的分支，删除分支内所有的文件。 将博客源文件复制到第一步中的文件夹中。 添加一个 .travis.yml 文件，文件内容可以参考下一节 Travis-ci 配置文件。 https://travis-ci.org/ 提供免费的持续集成服务，可以通过 github 登入，直接选择需要管理相关的项目。 第一次将源文件上传到 github 时，可能会遇到问题。主题 themes/xxx 是通过 git clone 下载的，无法直接 push 包含嵌套关系的 git 库。删除 themes/xxx/.git 正确的姿势是，最开始就使用 git modules 引用依赖库。所幸还能编辑 .git/config，添加下面几行代码解决： 打开 themes 对应的 github 网页，你会看到主题链接到其他仓库（其中 @commitid 控制对应的版本） 完成这样的设置，修改主题文件后，需要先将修改 push 到主题的仓库，然后在博客文件夹下 push 修改到远端仓库（修改依赖的 commitid）。最终，才能再网页上看到修改效果。 Travis-ci 配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041language: node_jsnode_js:- 9.11.1cache: directories: - node_modulesbefore_install:- export TZ=&apos;Asia/Shanghai&apos;- npm install hexo-cli -ginstall:- npm installscript:- hexo clean- hexo generateafter_script: - git clone https://$&#123;GH_REF&#125; .deploy_git # GH_REF是最下面配置的仓库地址 - cd .deploy_git - git checkout master - cd ../ - mv .deploy_git/.git/ ./public/ - cd ./public - git config user.name &quot;xiang578&quot; - git config user.email &quot;xiang578@foxmail.com&quot; - git add . # - git commit -m &quot;Deploy at $(date +&quot;%Y-%m-%d %T&quot;)&quot; - git commit -m &quot;Travis CI Auto Builder at `date +&quot;%Y-%m-%d %H:%M&quot;`&quot; # Github Pages - git push --force --quiet &quot;https://$&#123;CI_TOKEN&#125;@$&#123;GH_REF&#125;&quot; master:master # Coding Pages # - git push --force --quiet &quot;https://xiang578:$&#123;Coding_TOKEN&#125;@$&#123;CO_REF&#125;&quot; master:masterbranches: only: - hexoenv: global: # Github Pages - GH_REF: github.com/xiang578/xiang578.github.io # Coding Pages # - CO_REF: git.coding.net/xiang578/xiang578.git hexo 两个错误 在这一次的过程中，又遇到两个本地编译 hexo 的错误，一同记录一下。错误表现如下： 123456789ERROR Plugin load failed: hexo-renderer-sassError: Cannot find module 'node-sass' at Function.Module._resolveFilename (internal/modules/cjs/loader.js:581:15) ...ERROR Plugin load failed: hexo-renderer-scssError: Node Sass does not yet support your current environment: OS X 64-bit with Unsupported runtime (64)For more information on which environments are supported please see:https://github.com/sass/node-sass/releases/tag/v4.8.3 ... 网上的建议是修改 npm 的源地址为淘宝的镜像，并且重新下载这两个包。 123sudo npm config set registry https://registry.npm.taobao.orgnpm install hexo-renderer-sass --savenpm install hexo-renderer-scss --save Reference 用TravisCI持续集成自动部署Hexo博客的个人实践 - CSDN博客 关于 git-submodule 的一些基本操作 - 个人文章 - SegmentFault 思否 安装 npm install hexo-renderer-sass --save 出错，有什么办法没 - V2EX hexo 发布之后 gitpage 自定义域名失效 - CSDN博客 使用 Travis 自动构建 Hexo 到 GitHub | Zthxxx's Blog 使用travis-ci自动部署Hexo到github和coding - 掘金 ## ChangeLog 180904：完成初稿","categories":[{"name":"站务","slug":"站务","permalink":"https://xiang578.com/categories/站务/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"},{"name":"hexo","slug":"hexo","permalink":"https://xiang578.com/tags/hexo/"},{"name":"github","slug":"github","permalink":"https://xiang578.com/tags/github/"},{"name":"travisci","slug":"travisci","permalink":"https://xiang578.com/tags/travisci/"}]},{"title":"每周分享第 10 期","slug":"week-issue-10","date":"2018-08-05T08:40:14.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-10.html","link":"","permalink":"https://xiang578.com/post/week-issue-10.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 文章 深入FFM原理与实践 -：美团技术博客的文章之一，感觉比网上很多文章讲解的都深刻。不知道为什么文章中很多的参考资料都是网上的博文，有点遗憾。 「华为公积金下调到 5%」，刚毕业的应届生可能不懂，这当中有什么套路：前一段时间的热门，华为真是一家神奇的企业。从奋斗者协议到今天的调整公积金比例，不断地压榨着员工的极限。之前还看到有人说夸华为，至少给得钱对得起员工的时间，这次的调整无疑给已经买房的员工致命一击。 Readme驱动开发 - Erning.write()：这种开发方式又称 RDD。目前在公司开发时，发现大部分项目没有开发文档或者 wiki，项目的内容也只能靠口口相传。造成这样的局面，可能是公司之前发展速度太快。从另外一个角度来说，没有外部的文档是否可以开发人员本身有一些优势，防止被快速的替代？ 图片 新奇的创意，通过缺失表达战争造成的伤害。 金句 我经常骂孙笑川，心机自私膨胀，后来才发现我自己就是孙笑川，骂的都是现实中不争气的自己，人人都是孙笑川，却又都不想当孙笑川。每个人都在骂孙笑川，每个人都恶心孙笑川，因为每个人都知道自己就是孙笑川，但每个人都不想承认自己就是孙笑川。—— 网络时代，每个人都是小丑","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"每周分享第 9 期：拼多多","slug":"week-issue-9","date":"2018-07-28T07:07:55.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-9.html","link":"","permalink":"https://xiang578.com/post/week-issue-9.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 文章 拼多多的多和少：拼多多应该会在本周上市，这篇文章是上周的微信热门文章之一，也是我见过分析拼多多最仔细的文章。说句实话，我对拼多多了解不多，也没有研究过他们的商业模式，从文章中可以看到拼多多的成长。现在也有一点为当初拒掉拼多多的 offer 而后悔，虽然辛苦但是给的多啊。 My Approach to Getting Dramatically Better as a Programmer – malisper.me：程序员成长相关。常思考两个问题： Learning how to solve problems I didn’t know how to solve before. Learning how to write correct programs faster. 以及掌握方法： Reading a paper. Learning a new tool. Reading several chapters of a book. Recording my screen as I write a program. Then reviewing the footage and seeing how I could have written the program faster. 云风的 BLOG: 三人合租的房租公平分配方案：现实中我也遇到了租房问题，不过还不太需要这些高深的知识来处理。这篇文章提供了2 、3 人分房的方案。 2 人版：A 和 B 分两间房，由 A 写下对两间房的报价，然后由 B 选择自己住哪间房。如果两个人都在理性下完成这个任务，房间的价格会趋向于它本身的价值。 2 人版另外一种解法：A 和 B 同时写下对两间房的心里价格（两个房间的价格总和一定），然后每间房由开价最高的人居住，每个月需要支付的价格是两个人对这间房子开价的平均值。 3 人版：这个问题比前面两种情况更加复杂。A、B、C 三个人有不同的职责。 A：写下对三间房的报价（报价的总和一定），A 是最后一个选择房间。 B：首先查看 A 对三间房的报价，思考是否有两间价格是合理的。如果是，那么在 C 选择一个房间后，依然有一个价格合理的房间可以选择。如果不是，那么标记两间他认为价格不合理的房间。 C：也是查看 A 对三间房的报价，思考是否有两间房的价格是合理的。如果是，那么他可以让 B 先进行选择，即使 B 认为有两间房价格是不合理的，还是有一间价格是合理的，C 也有至少有一间价格合理的房子可以选择。如果不是，那么标记两间价格不合理的房子，如果 B 认为是合理的，让 B 先进行选择，C 第二个进行选择。否则，B 与 C 标记的房子中重复的分给 A ，然后由 B 和 C 进入两人分房间。 如何「收集」知识 | MacTalk-池建强的随想录：这一段时间一直思考的如何与知识更好相处的问题，这篇文章可以当成是入门参考，不要陷入研究工具的怪圈，而是找到一个适合自己的知识处理系统。 视频 How to Read a Paper Efficiently (By Prof. Pete Carr) - YouTube：虽然没有去读研究生，但由于工作的性质也需要去大量的阅读论文。这一段小视频在如何读论文这个问题上提出了一种框架化的方法，适合新人去模仿实践。 金句 第三届国际华语辩论公开赛复赛，重温了经典辩题“金钱是不是万恶之源”。正方恐龙复生队对“恶”做了全新的定义：人世间所有的恶，都可以概括为“不把人当人”。—— 有感于前一段时间的疫苗问题，看到这一段文字依旧有力量。 工具 hunkim/DeepLearningZeroToAll: TensorFlow Basic Tutorial Labs：非常好的 tensorflow 教程。另外说一句，利用 tensorflow 来写机器学习真是轻松。 哔哩哔哩·猜你喜欢：BiliBili 的客户端中有一个猜你喜欢的功能，杀时间利器。在信息化的时代，还有什么比调试一个符合自己口味的信息流更有成就感的事情？通过使用这个插件，可以在网页版上实现猜你喜欢功能。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"月读 | 睡眠革命","slug":"Sleep","date":"2018-07-21T09:28:48.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/Sleep.html","link":"","permalink":"https://xiang578.com/post/Sleep.html","excerpt":"","text":"前一段时间作息混乱，晚上躺在床上睡不着，然后玩手机，导致更加睡不着。第二天下午又很困，只好趴着睡一会儿。从而导致晚上睡不着又玩手机的恶性循环。当时一度觉得自己都不能从这个圈里面走出来了。不过，想起来之前在少数派看到过一篇文章，推荐过一本和睡眠相关的书——睡眠革命，抱着死马当活马医的心态看了一下。 书中作者主要介绍昼夜节律、睡眠类型、睡眠周期、睡前醒后、日间小睡、睡眠环境等多个方面的内容，我将自己在阅读时候记录的笔记整理一下，与大家分享。 核心收获 读完这本书最大的几个收获： 人的睡眠类型有两种：早睡类型和晚睡类型。 失眠是过度清醒造成的，过度清醒又是担心自己会失眠所产生的。 昼夜节律 昼夜节律是生命体24小时的内循环，受我们的内置生物钟的管理。我们大脑中的这一生物钟，24小时调节着我们的多个内部系统，包括睡眠和饮食习惯、激素的分泌、体温、灵敏度、情绪和消化，使其与地球的自转相一致。我们的生物钟是根据一些外部线索而设定的，其中最主要的是日光，此外还包括温度、进食时间等其他因素。 上面是书中的一张插图和一段文字。核心思想是，人的一天活动受自然规律控制，所以要在正确的时间干正确的事情。如同我最开始描述的状态一样，人体内也有一种正常的循环状态。在晚上褪黑素产生的时候，开始准备睡觉，我们会在2~3点时进入一个高效的睡眠阶段，从而在早晨可以按时起床，那么晚上对于睡眠的需求又会达到峰值，为我们再一次提供最佳的入睡时机。 睡眠类型 一直以来，我们的文化都提倡一点：早睡早起。但是在现实中，我们很多人却很喜欢睡懒觉。作者在书中指出，从基因上来看，人的睡眠分成两类：早睡早起类型 和 晚睡晚起类型。不应该强制两类人在相同的时间躺下和起床，而是寻找最合适他们的睡眠规律，如果昼夜节律图中描述的内容一样。当然，作者强调需要严格遵守自己的睡眠规律，不要轻易的打破，比如周末睡懒觉之类的。 睡眠类型：智慧睡眠的7个要点 1．了解你和你的亲朋好友属于哪种睡眠类型。如果你不能确定，可以使用慕尼黑大学的问卷调查表。 2．巧妙规划好你一天的日程，在你状态最佳时做最重要的工作。 3．把咖啡因当成高效的表现增强剂使用，而不是出于习惯去喝咖啡，并且一天的咖啡因摄入量不要超过400毫克。 4．晚睡星人——如果你想要克服社交时差，就不要在周末睡懒觉。 5．在会议室、办公室和办公桌配置日光灯，提高员工的灵敏度、工作效率，并改善他们的工作情绪。 6．知道何时该上前、何时该退后。如果你是一个早起型的人，那么你该不该自告奋勇地在深夜比赛中参加点球大战呢？ 7．如果你和你的伴侣属于不同的睡眠类型，你们应该学会彼此协调、彼此适应。 睡眠周期 在我们的常识中，一直有一个8小时的推荐睡眠时间。作者澄清，8小时是每晚人均睡眠时间，一味追求8小时的睡眠会造成巨大的压力，反而对我们的睡眠起着破坏性极强的反作用，让我们无法获得真正需要的、因人而异的睡眠时间。 作者提倡一种新的睡眠时间方法： &gt;“R90”指的是以90分钟为一个周期，获得身体修复。“90”这个数字，并不是我从1—100中随意选择的。从临床上说，90分钟是一个人经历各个睡眠阶段所需的时间。这些睡眠阶段组成了一个睡眠周期。 我们的睡眠周期由4个（有时是5个）不同的睡眠阶段组成。你可以把度过不同的睡眠阶段、完成一个睡眠周期的过程，想象成走下几段楼梯、完成一段行程。当我们关灯上床、准备睡觉时，就像站在这几段楼梯之上，而想要得到的深睡眠，就像走到了这几段楼梯之下。 实践这个 R90 睡眠方法的关键是，对睡眠进行统计。以周为统计周期，争取每周达到 35 个睡眠周期。然后根据自己的工作性质，确定一个固定的起床时间，反向推导出你应该在什么时候入睡。 睡前醒后 一般而言，我们只关心什么时候睡觉，而忽视了睡眠前以及醒来后的90分钟。从作者观点来看，应该利用好这一段时间，提高睡眠的质量。 睡眠前后的例行程序：智慧睡眠的7个要点 1．睡眠前后的例行程序将直接影响你睡眠的质量，以及你清醒的一天：如果予以重视，整个白天和整个晚上就会更加高效。 2．在日间偶尔脱离电子设备，将此作为一种犒劳，并通过这种方式训练你的身体和心灵。 3．对于晚睡星人来说，睡眠后例行程序至关重要，如果他们不想输给早起星人的话。不要因为青睐于止闹按钮，而轻易放弃这些睡眠后例行程序。 4．不要在头脑不清醒的时候发送消息！先让自己清醒一点，再去拿手机。 5．让你的身体感受从温暖到凉爽的变化，有利于带来自然的体温下降。快速洗个温水浴，然后选择较凉爽的睡眠环境，能帮助你达到目的。 6．在上床前整理你的卧室，放空你的大脑，“下载”你的一天，就不会在该睡觉的时候想东想西、难以入眠。 7．执行睡眠前例行程序，是为了结束过去的一天——用鼻子呼吸、放松心情、实现从明亮到昏暗的过渡。执行睡眠后例行程序，是为了开启不匆不忙的新一天。这两段时间都只属于你，不属于别人。 日间小睡 日间小睡分成两个部分：午睡以及傍晚的小憩。这一部分的睡眠的作用是对夜晚睡眠周期的补充，一般而言30分钟的小睡可以媲美晚上的一整个睡眠周期。 书中引用到美国国家航空航天局的一项调查专门研究了日间小睡的功效，在对执行长途飞行的飞行员进行调查后，他们得出结论：“日间小睡有助于维持或改善随后的表现，提高生理和心理的灵敏度，并能有效改善情绪。” 日间小睡：智慧睡眠的7个要点 1．利用午后休憩时机（下午1—3点）给自己安排一个可控修复期，是弥补夜间睡眠周期的完美方法，这也与你的昼夜节律彼此协调。 2．黄昏（下午5—7点）是次优的休憩时机，因为此时人的睡眠需求极高。但这一时段的日间小睡应控制在30分钟之内，这样才不会影响晚上的睡眠。 3．白天睡不着？没关系。只要花30分钟放松一会儿，暂时脱离周围的世界片刻。 4．至少每隔90分钟休息一会儿，消除大脑的疲劳，提高注意力的集中水平。在休息时不要使用电子设备，你无需让自己自始至终地受到电子设备的控制。 5．不要受到你所在的企业文化的影响，切莫先入为主地给那些白天睡觉的人贴上“懒惰”的标签，而应着手构建一种接受日间小睡和休憩的企业文化——你若不打盹儿，就会输得很惨。 6．使用冥想或正念应用软件，或者把玩某个珍爱的私人物品，暂时脱离当前的环境。 7．如果你真的无法脱身，那就巧妙地安排一天的工作。在午后倦怠期，不要让自己困于太费神的工作 睡眠环境 上面讲了那么多，都是一些比较虚的东西，最后分享一些作者对于睡眠环境的建议。首先一点，作者提倡在卧室中尽量只睡觉，不干其他活动。其次，作者提倡采用像婴儿一样的侧卧睡眠方式，要点包括：膝盖自然弯曲、两条手臂放在身前，并轻轻交叠在一起，颈部、脊柱和臀部形成一条平滑的直线。 最后，是一些关于寝具套装选择的建议： 1．学会以胎儿姿势睡眠，躺向非主要的身体一侧（惯用左手的人向右侧睡，惯用右手的人向左侧睡）。 2．检测床垫，了解什么样的床垫对你来说软硬适中。让你的伴侣也这样做。 3．循序渐进：7年间分两次各花500英镑在你的床垫上，而不是一次花1000英镑。可以考虑购买能够经常换洗的床垫。 4．使用低过敏性、透气舒适的床上用品，无论你是否属于过敏体质。避免潜在的睡眠障碍因素，并注意调节好温度。 5．床的尺寸很重要——能买多大，就买多大。特大号床垫是值得一对儿夫妇考虑的最小型号（只要卧室中放得下）。双人床是给一个人睡的。 6．不要盲目购置寝具！可以根据销售人员的介绍，了解你能买到什么样的寝具。但在做出最后决定时，记得运用你在本章中学到的知识。 7．记住床垫和床架重要性比率：你可以把100％的预算全部花在购置床垫上，因为床架主要是装饰性的","categories":[{"name":"文渊阁","slug":"文渊阁","permalink":"https://xiang578.com/categories/文渊阁/"}],"tags":[{"name":"Sleep","slug":"Sleep","permalink":"https://xiang578.com/tags/Sleep/"}]},{"title":"每周分享第 8 期","slug":"week-issue-8","date":"2018-07-20T13:37:00.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-8.html","link":"","permalink":"https://xiang578.com/post/week-issue-8.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 文章 唐爽：坦途的人生有时候经不起一次搭错车：前一段时间很火的周立波事件，当事人之一回复，唐爽说的话很有逻辑，不愧是博士。 每周转载：德鲁克谈《自我管理》——《哈佛商业评论》史上最受欢迎的文章 @ 编程随想的博客：开始工作之后，越来越关注自我提升的内容。 Shell 中的极品-- Zsh - Richard Wei：推荐的几个插件都很有用，shell 的美观程度又提高了一个层次。 演进：在工作的前三年里快速成长（练习篇） - Phodal | Phodal - A Growth Engineer：职业成长系列。 金句 为什么越长大觉得时间过得越快？因为小时候是在发现发掘，而长大后是在不断重复。“有的人30岁就死了，80岁才埋葬。”这句话说的就是这个理，小时候，我们把一年过成了365天，长大后，我们把一年过成了365次。“回程总比去程快”佐证了一件事情：“时间”和我们接收了多少信息有关——信息能够延长时间。 区块链是一种分布式数据存储、点对点传输、共识机制、加密算法等计算机技术的新型应用模式，这是一个典型的去中心化应用，建立在 p2p 网络之上。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"每周分享第 7 期","slug":"week-issue-7","date":"2018-07-15T23:05:03.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-7.html","link":"","permalink":"https://xiang578.com/post/week-issue-7.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 文章 I have forgotten how to read：碎片化时代对于阅读的反思。 技术演讲中最容易被忽视的问题：参考文章，内容不错 金句 感谢大学四年里遇到的所有人，或萍水，或至交；或平平淡淡四年的柴米油盐，或惊鸿一瞥不期的遇见；或心酸的讥讽，不屑的嘲笑；或热情的洋溢，真心的赞美；所有形形色色的人和林林总总的事，无论是相逢于青山绿水还是最终相忘于江湖，感谢你们漫步庭前看花谢，坐卧塔下笑清风。人生匆忙，百年过往，他日相逢，此情莫忘。泛菊杯深，吹梅角远， 同在京城，聚散匆匆， 教人怎不伤情？ 觉几度，魂飞梦惊。 后夜相思，尘随马去，月逐舟行。诸位，一世顺意！（毕业论文结束语） 直到毕业的时候我才明白，当你要致谢的人和环境并没有那么美好时，有些话是无论如何都说不出口的。于是乎，我也使用了前人的套路，留下几句客套话后便匆匆赶往下一站了。 高考做错的每一道题，都是为了遇见对的人。 那些办公室的白领们，总是自以为自己的表现领先于父母。认为父母落伍了……其实，这不过是因为经济结构转型造成的误会而已。 现在各种写字楼里格子间里，哼哧哼哧做重复 ppt 的年轻人，和当年踩着缝纫机的女工们，其实没有本质区别。 同理，当年父母在菜场讨价还价，多一份还是少一分，和现在年轻人在群里争先恐后的抢红包，状态也都差不多。 父母当年非要给电视剧遥控器套个塑料袋，和现在年轻人非要给手机套个壳差不多。 当然，现在每天拿着手机刷朋友圈和微博的人，和当年蹲在墙角嗑瓜子的人，也没什么区别！ 点评：从某种意义上来说，社会从未改变。它只是在不停的轮回。所谓的变化，只不过是外在形式和工具而已。 也同样，随着新一轮经济的转型，这些在写字楼里做 PPT 的白领，也即将重复当年那些纺织女工被淘汰时的情景…… 互联网时代的学习，学得太多，做得太少。恰恰正是“学习”知识剥夺了我们实践和内化知识的机会。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"我常用的自动化流程","slug":"auto-workflow","date":"2018-07-07T06:09:22.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/auto-workflow.html","link":"","permalink":"https://xiang578.com/post/auto-workflow.html","excerpt":"","text":"什么是自动化 日常工作学习中，我们需要使用不同的工具来应对各种各样的任务。比如，用印象笔记收集网页，利用图库管理博客中的图片。但是，由于大部分工具不是同一家公司发布的，彼此之间很难进行简单的协作。如果你有编程能力，可以利用很多工具提供的 API（应用程序接口），让信息在不同的软件中流动。 对于普通人而言，目前市面上也有很多 App 提供这种功能，可以解决少部分的问题。我对这些功能的定义就是自动化流程，通过使用这些功能，可以减少一些重复机械的工作，使我们更好的享受生活。老罗在吹锤子系统时，提到过「One Step」，在我心目中也是自动化的一种。自动化流程指的是我自己在实践中，配置的一些软件使用流程。 Workflow Workflow (新手入门请看iOS 效率神器 Workflow 怎么用？跟着这篇入门指南从零开始 | 新手问号）是 iOS 上的一款自动化软件。似乎是去年被苹果收购，现在在 App Store 中可以免费下载。前一段时间的 WWDC 上，也公布了进行版本，能利用 Siri 的「捷径」App。 在 iPhone 上，我最常用的 workflow 动作是碎片知识摘记，实现利用 iPhone 阅读时，将值得多次阅读的内容，集中保存到印象笔记中。有关这个动作更多的介绍可以查看 Workflow + 印象笔记，定制高效碎片知识管理工作流 | Matrix 精选 - 少数派。 《奇特的一生》在时间管理领域如雷贯耳，很多人都按照柳比歇夫的那种方法进行时间。如果你希望可以使用 Workflow 来记录时间，可以进一步阅读少数派中 想学柳比歇夫？用这个Workflow来帮你一把。 最后，授人以鱼不如授人以渔，少数派的 Workflow Gallery 提供很多相关的工作下载，可以自行前往挑选。 Aflred Aflred 是 Mac 上的一款效率工具。之前我在自己的博客中也有分享过，Alfred 使用记录 - RyenX。这里，我就补充介绍几个自己目前在用的功能。 由于我平时是使用 Markdown 进行写作的，难免会遇到需要插入很多链接的时候，比如现在正在写的这一篇文章。Afled 中有一个 workflows 「fallroot/copy-url-for-alfred」，通过这个动作可以将 Chrome 或者 Safari 中的当前正在浏览的网页的标题以及网址以 Anchor 、URL、Title、Markdown 等多种形式复制到当前的输入框或者是剪贴板。 ngreenstein/alfred-process-killer: An Alfred 2 workflow that makes it easy to kill misbehaving processes. It is, in essence, a way to easily find processes by name and kill them using kill -9.：快速结束 Mac 中的某些进程。 IFTTT Workflow 和 Aflred 需要在自己的 iPhone 或者 Mac 上才可以运行，说起来多少还有些不自动。我经常使用的另外一个自动化服务「IFTTT」就可以解决这个问题。IFTTT 是什么？它是英文 IF This Then That 的首字母缩写，它是一个提供自动化服务的网站。它的功能可以从字面猜测出来大概，完整地解释是「如果A完成事件1，那么就让B完成事件2」。简单来说，你可以通过设置一些规则，让它帮你自动完成一些重复工作。比如设置规则实现当你发布一条微博后，IFTTT 自动将微博同步到印象笔记中。所以，IFTTT 对我的意义是将不同的应用连接在了一下，方便我造各种工作流。 与上面的形式相同，继续介绍一下我目前在使用的规则。 碎片阅读 互联网的发展，使每个人有更多表达自己的机会，越来越多的人开始通过文字展示自己。碎片阅读指的正是阅读这些散落在不同平台上的文章。对于我而言，一般是先将在网上看到的想要阅读的文章放到稍后读软件 Pocket 中。之后在自己空闲时，打开 Pocket 网站阅读。比起直接阅读这样的好处有二： 1. 不是每一个网站或者平台都有良好的阅读环境，比如很多广告、字体字号。 2. Pocket 提供丰富的 API 接口，通过 IFTTT 可以很好的和 Evernote、Airtable 等软件写作。 If New favorite item, then Append to a note in Evernote - IFTTT：古语「不动笔墨不读书」，很多文章有值得记录地方，而且也希望可以将自己所做的笔记集中在一处。针对这个问题，很多人会复杂，并且再粘贴到某一个文档中。这种方法做起来简单粗暴，却打断了阅读的连续性。通过我这个动作，可以将在 Pocket 中高亮的句子，直接添加到印象笔记的一条特定笔记中。 印象笔记中效果 If item archived , then create record in airtable：这一个动作是实现阅读数据的统计，当我在 Pocket 中归档一篇文章之后，会在 Airatble 指定的表单里添加一条相关的阅读记录。 Airtable record 任务管理 Github issues assigned to me Todoist：这个规则可以实现，在 Github 中有人分配给我 issue 时，自动在 Todoist 中创建相关任务。 Log completed Todoist Tasks to Airtable：记录所有完成的任务到 Airtable 中。 If Todoist task with label is completed, create new task - IFTTT：实现 Todoist 中的任务循环。 创意 IFTTT 中有一个与 RSS 相关的触发器，而且如下图所示，提供了对文章进行关键字匹配的选项。 前一阵子是 618 年中大促，正好我想购买一个 QC35 的耳机。为了追求优惠，大部分人会不停的查看想购买商品的页面，看看有没有什么新的变化。而我利用 IFTTT 搞了一个监控 QC35 全网优惠的监控。具体思路如下：什么值得买中有一个精选好价的频道，很多人会在上面爆料商品的优惠信息。幸运的是，这个频道提供 RSS 订阅链接。所以，配合上面提到的 RSS 相关触发器，我写了 If new feed item matches 'qc35' from http://faxian.smzdm.com/feed, then send me an email at xiang578@foxmail.com - IFTTT 以及 If new feed item matches 'QC35' from http://feed.smzdm.com, then send me an email at xiang578@foxmail.com - IFTTT 两个规则。从而实现，有人爆料 QC35 相关的价格优惠时，我会收到一条由 IFTTT 发送的邮件。 qc Reference 玩转 IFTTT，互联网自动化也可以很简单 - 少数派 Changelog 20180707：完成初稿","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"workflow","slug":"workflow","permalink":"https://xiang578.com/tags/workflow/"},{"name":"alfred","slug":"alfred","permalink":"https://xiang578.com/tags/alfred/"},{"name":"ifttt","slug":"ifttt","permalink":"https://xiang578.com/tags/ifttt/"}]},{"title":"每周分享第 6 期","slug":"week-issue-6","date":"2018-07-07T02:06:17.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-6.html","link":"","permalink":"https://xiang578.com/post/week-issue-6.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 卡片写作 熊猫慢递 上周《纪实72小时》，北京熊猫慢递。写给未来的一封这个概念很早就接触过，自己之前也尝试写过。再看那封信会有一些伤心，辜负了过去努力的自己。有一段时间为了使自己在考试时集中注意力，都会在试卷写上一句，不要辜负过去点点滴滴的努力。有机会，就去这个熊猫慢递写给两年后的自己一封信。 文章 细枝末节都交给 App，我只负责享受生活 | 2016与我的数字生活 - 沨沄的博客 | FoxGeeker Blog：博主从意识到数字生活开始的一系列改变，文章内容很充实。 LPL心理教练黄菁：“心”备竞赛（上）——RNG的锐变：看了半年多的 LPL，了解一下背后的故事。 卡片助力输入输出，工具我选 WorkFlowy | ishanshan's blog：使用印象笔记之后，创建了很多只有几句话的笔记。这种情况下印象笔记很难管理，需要去寻找另外一个工具，有幸发现 Workflowy 这类大纲软件。 用心写周报的同事，绩效不会太差 | ishanshan's blog：与上一篇文章的作者相同，最近自己也要入职了，拿这一篇文章来参考一下，其实不光工作可以写周报，学习也可以尝试写周报，反思是取得进步的途径。题外话，作者是来自开智学堂的，从我侧面的一些了解，这家公司对于工作规范要求比较高，也可以当成数字生活的参考。 金句 哪里会有人喜欢孤独，不过不喜欢失望。（挪威森林） 感谢大学四年里遇到的所有人，或萍水，或至交；或平平淡淡四年的柴米油盐，或惊鸿一瞥不期的遇见；或心酸的讥讽，不屑的嘲笑；或热情的洋溢，真心的赞美；所有形形色色的人和林林总总的事，无论是相逢于青山绿水还是最终相忘于江湖，感谢你们漫步庭前看花谢，坐卧塔下笑清风。人生匆忙，百年过往，他日相逢，此情莫忘。泛菊杯深，吹梅角远， 同在京城，聚散匆匆， 教人怎不伤情？ 觉几度，魂飞梦惊。 后夜相思，尘随马去，月逐舟行。诸位，一世顺意！ 直到毕业的时候我才明白，当你要致谢的人和环境并没有那么美好时，有些话是无论如何都说不出口的。于是乎，我也使用了前人的套路，留下几句客套话后便匆匆赶往下一站了。 高考做错的每一道题，都是为了遇见对的人。 工具 Mweb 3：是 Mac 上一款专业的 markdown 写作软件。目前，我自己用它来管理平时的卡片写作、机器学习笔记以及博客文章更新。最近，这个软件正在搞首发优惠，比较惊喜得是发现软件中隐藏的沉浸模式。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"每周分享第 5 期","slug":"week-issue-5","date":"2018-06-29T23:52:43.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-5.html","link":"","permalink":"https://xiang578.com/post/week-issue-5.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 文章 想尝鲜 iOS 12，请先看这份 Public Beta 升降级指南 - 少数派：前几周就尝试开发者预览版了，升级之后，明显可以感觉到流畅度的提升，而且也多了屏幕时间等一些统计数据。 澎湃新闻的英文版发表这篇 Outside the Green Bubble of China’s Super-App，介绍一些人为什么要逃离微信？从自己的角度来看，这一年多以来，加了很多好友和群，有时候打开微信就是一排的群消息，不想看又不好退群，朋友圈还能通过屏蔽来净化。不可否认微信已经成为一个超级 APP，更进一步可以说是小型操作系统。手机只要能装上微信，几乎可以不安装其他软件。但是一群人却反其道而行之，接受种种的不变从而达到放弃微信。微信最大的问题是封闭，公众号中的文章很难被外部检索，人为的造成了一些信息的孤岛。第二就是野蛮，从最早朋友圈封杀支付宝，到现在的抖音、今日头条，完全不顾用户的感受。最后一点是自由，聊天消息被监控，严重的会转移到司法机关处理。以及谣传利用聊天数据进行广告推荐。不知道有没有其他一款通讯软件能挑战微信的地位。 技术演讲中最容易被忽视的问题 | 唐巧的博客 图片 推特上新学到的名词：无产中产阶级，自己也将成为其中的一员，却又没有机会逃离。 金句 能脱离原来的阶层思考问题，却与家人朋友有更深的隔阂，却依旧在贫穷与疾病里挣扎。命运并没有改变，只是更理解命运的无奈而已。 功利性的学习是对知识的一种浪费，异化的知识观念让学习变成了一场场知识与世俗的交易。 如今的朋友圈已经成为了每个人自我形象塑造和对外表达的关键平台。 一方面，优质教育资源可以向更广泛的地区辐射，惠及有需要的人群。另一方面，师生关系不再如以往那般界限分明。每个人都是相互学习、共同成长的分享者与传播者。 大多数互联网和技术人都还是有种理想主义情怀（非贬义）在身上的；我们希望通过技术让世界更好，我们也希望能把前辈留给我们的那个包容开放自由的互联网传承下去，有些先驱们甚至用生命捍卫了这些价值与理念。 卡片写作 《派出所的故事》 这几天一直在看《派出所的故事》，电视剧由《炊事班的故事》原班人马打造，每个人的性格也差不多继承。看完这部剧的第一感受，艺术源于生活，又高于生活，所有的巧合都集中发生。不过后来仔细一下，这部电视剧也是奶头乐中的一种，没有深度可以思考的内容。 《地下交通站》 这两天又高强度看完了一部情景喜剧《地下交通站》。这部片子以抗战时期河北安邱（虚拟）的鼎香楼为背景，展示我军情报人员如何与敌人斗志斗勇的故事。情节虽然短平快，却难逃抗日篇的通病，敌我黑白分明，好人聪明机智，某些特殊的还出场自带BGM，坏人这是一事无成。或许我能得出这种结论是因为站在了全知视角，没有将自己代入到故事中。有趣的一点，最近看的电视剧，经常发现某些演员重复出现，弹幕则疯狂地刷这个人之前的台词，为观剧添加了更多的期待。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"每周分享第 4 期","slug":"week-issue-4","date":"2018-06-23T15:29:43.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-4.html","link":"","permalink":"https://xiang578.com/post/week-issue-4.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 文章 一个程序员的成长之路 · Issue #41 · fouber/blog：之前一直在思考如何将一些分享展示出来，从这篇文章中得到启发，将 ppt 全部转换成图片，并且配上文字解说。 超一流作家的写作利器：用卡片提升创意的密度：卡片写作法，下一个阶段学习的重点之一。离线杂志的内容和形式也很有趣，不过目前停更了。 校长钟晓敏寄语2018届毕业生：拥抱时代，创造幸福：毕业季，贴出自己学校校长的毕业典礼演讲稿。从内容上来说平淡无奇，毫无在互联网上火爆的可能。听完之后，只记下了一句「我们每个人都是由自己一再重复的行为所铸造的。因而优秀不是一种行为，而是一种习惯。」 图片 微博中图片，有一种共建二流的错觉。好的学校创建世界一流，普通学校创建国内一流，就没有安心当二流的学校。 视频 2018世界杯足彩竞猜：一个稳赚不赔的方法是什么？李永乐老师教你足球彩票对冲套利（2018最新） - YouTube 最近世界杯，网上一大堆赌球的截图。这个视频虽然标题写的是稳赚不赔的方法，看完这个方法的数学原理之后才发现足彩公司的强大。 金句 这一改革，适应新时代我国社会主要矛盾的变化，聚焦发展所需、基层所盼、民心所向，为的就是建设服务型政府，更好服务人民，更加让人民满意。 ［一棵妒忌另外一棵树，恨不得自己变成斧头］：雪山下有一只二头鸟名为共命鸟，其两头分别值守日夜彼此轮流休息。由于一个头总是得享美味果实，另一头心生嫉恨，于是吞下毒果，两头同归于尽。 大多数互联网和技术人都还是有种理想主义情怀（非贬义）在身上的；我们希望通过技术让世界更好，我们也希望能把前辈留给我们的那个包容开放自由的互联网传承下去，有些先驱们甚至用生命捍卫了这些价值与理念。 但是作为个人，我希望的互联网应该是更开放的，这也是为什么我们会说微信做的不好：因为互联网应该让信息更流通，而非人为地制造信息孤岛。 一个人是有多自恋，才会觉得几十光年以外，老久以前发生的天文现象，就为了暗示他的人生里的那点事。 工具 autojump：命令行中实现目录快速跳转","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"007 践行反思","slug":"007-aciton-reflection","date":"2018-06-21T14:24:31.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/007-aciton-reflection.html","link":"","permalink":"https://xiang578.com/post/007-aciton-reflection.html","excerpt":"","text":"写这篇文章之前，查看了一下 007 公众号里面的数据，加入 83 班已经 134 天。但是自我感觉出了一些问题，借这次作业反思一下。 问题 主题。刚开始践行的时候，为了解决每周量产一篇文章的问题，自己想过每月文章主题的安排，但是现在更多地是随心所欲。 拖延。最近几次作业，虽然很早就知道要写什么内容，但是总拖到最后一天才打开会电脑开始写。 评论。说点题外话，由于自己一直有独立博客，所以认为评论数量是评价一个博客质量的标准。对上下楼文章的评论，也是 007er 每周作业中一个重要环节。其实每周对战友的评价很多时候是草草了事，蛮对不起战友的。 写作流。写作流属于工作流中的一个概念，工作流是一种规范化重复劳动的方法。刚开始有想过写作流，但也没有按照之前所写的实践。 1234567891011121314151617181920## 写作工作流1. 输入环节：通过阅读寻找灵感2. 加工整理：通过印象笔记里面的积累，来拓展思路3. 建立体系：搭建思考框架，通过回答 Why，What，How，How Good4. 写作输出 1. 快速输出草稿，使用录音宝App，口头费曼 2. 卡片式费曼，番茄工作法，幕布列出大纲 3. 文字素材迭代，文章中引入准确的数据、翔实的事例、有说服力的论述和观点 4. 图片素材及排版，复制html文本到简书中（图片设置居中） 5. 校对## 写作工作流 1. 收集资料2. 使用幕布列出大纲3. 按照大纲生成初稿4. 修改初稿5. 按照少数派稿件要求修改初稿并校对6. 以 html 格式复制到简书，在博客上发布7. 如果还发现错误，只修改博客上的文本 反思 针对上面发现的问题，自己想了一下解决的方案。 根据自己的特点，整理出四个主题。未来以月为单位，每周写其中的一个主题相关文章。主题如下： 月读：整理分享本月所读的一本书 卡片写作法：前几天加入的《笨方法学写作》课程，需要完成教练所提供的习题 数字生活：我的效率类软件实践。 机器之心：下一阶段学习的重点，机器学习入门的笔记。 关于拖延。每月确定这个月需要写的文章类型，在写作软件中搭好框架，争取每天写一点。更重要的，预留出一些文章作为备份，应对一些冲击。 关于评论。学习每个组长的方式，将自己对战友的点评整理成文章，发表在简书上。 最后，重新整理写作工作流。尝试按照少数派编辑部的写作方式，利用 todoist 管理选题，在 Mweb 中创作。 尾声 在反思之后，发现还有几个遗留的问题，先在这里罗列一下。 图片问题。很多文章需要配图，mac 有什么简单的软件可以压缩图片？ 简书与个人博客的同步。一般而言，自己还是使用 Mweb 配合 Markdown 语法进行写作，但是复制到简书时会遇到问题，外部图床不能使用，每次都需要重新传一遍图片。如何解决这个麻烦？","categories":[{"name":"七年之旅","slug":"七年之旅","permalink":"https://xiang578.com/categories/七年之旅/"}],"tags":[{"name":"7","slug":"7","permalink":"https://xiang578.com/tags/7/"},{"name":"wrtiting","slug":"wrtiting","permalink":"https://xiang578.com/tags/wrtiting/"}]},{"title":"每周分享第 3 期","slug":"week-issue-3","date":"2018-06-16T09:39:04.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-3.html","link":"","permalink":"https://xiang578.com/post/week-issue-3.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 文章 1、 GitHub 和开源是对穷人的恩赐 - 来自知乎专栏，作者: 园长 想起之前PAT老师过来宣传PAT时（我不是很看好这个考试），将到企业通过学校来筛选候选人是因为应聘者太多，企业只需要找到合适的人，不需要考虑社会公平。这篇文章中谈到的观点，通过 Github 给开源世界贡献代码，能记录你学习成长的过程（git commit 还可以修改，是不是区块链技术适合这个场景），让更多的人认识你，逃过企业的一些过滤器。校招时，也在简历上写过 github 地址，在网易游戏面试，面试官还夸了一下。某一刻突然认识到，你需要找到属于自己的产品。 2、重新捡起GTD —— 读《软技能》有感 - 少数派 少数派上关于工作效率探讨的文章之一，我在月读中也推荐过《软技能》，这篇文章的作者结合里面的原理，打造了属于自己的工作流，有一定的参考意义，也可以当成是开始实现 GTD 的范本。 3、矩阵求导术（上） 学习机器学习时遇到的困难之一，这篇文章介绍的很详细，最后也分析了几个机器学习中的例题。 图片 1、中国看待世界的方法。小时候总以为世界上只有两个国家，中国和外国。 视频 1、一条穿过河北农村的海底隧道｜大史记 Vol.6... 来自史里芬Schlieffen - 微博：魔幻现实主义，拥有中国特色的海洋馆，之前没有想过还可以建造龙宫。大史记还有好几个更这种类似的视频，介绍国内一些疯狂的建筑。 2、第1集 长沙：解放西路旁的小餐厅_纪实72小时(中国版)_腾讯视频：纪实72小时是日本 NHK 电视台的节目，每期选择一个地点连续拍摄 72 小时。之前在 B 站上看过这个系列很多的，现在腾讯搞了一个中文版的。看完第一集之后，没有日版的味道，感觉日版选择餐厅主题时，会平衡食物与故事之间的分配，长沙这个餐厅中，注重煽情。还是期待未来能去更多的地方拍摄。 金句 1、币圈生存法则不是已经告诉你了么 倾家荡产四大捷径：1，追涨杀跌；2，期货杠杆；3，融资融币；4，短线神操作。 人生巅峰四条大道：1，踏实工作；2，闲钱投资；3、长线持有；4，按时吃饭睡觉。 还有再加一条: 远离空气币。——勃学日常反投机语录之一 2、我对于主播粉丝感到非常费解的一个地方，就是能够把极高的信任授予某一本质上并非双向认识的一个人。 ——目睹之前绝地求生主播圈内的开挂风波的感叹 3、当别人知识体系已经汇编成执行节点，不仅达到成熟运转，还设计了升级调整的节奏，你的知识体系却还远远没有完善。 4、无非三本的学生不仅仅在初筛的时候会失去很多机会，并且在推销证明自己的时候，需要拿出比985学生更多的东西出来。 ——又快要到了高考填志愿的时候，不同选择的差距所在。前面也提到过这个观点。 微博写作训练 1、从少数派 Power+ 中看到的文章——颜色与身份统一。很多软件设置项目的颜色（比如日历软件中事件的颜色，todoist 中项目的颜色等等），思考将这些颜色对应一个现实中的身份进行统一（比如个人-绿色、家庭-紫色、工作/学校-棕色、其他-黄色），在不同的软件中进行统一（todoist、toggl、日历、文件夹）。在无法设置颜色的软件中利用 Emoji 解决。#效率思维# 少数派中文章是专属的，所以不放链接。放上知乎上一篇类似的文章：彩虹分类法：用七种颜色管理时间 。 工具 1、苹果设计奖的赢家，集日历、任务管理、笔记于一身的应用：Agenda - 少数派：看起来不错的一款多功能应用，由于之前的系统迁移成本有些太高，这一次都没有尝试的兴趣。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"大学书单","slug":"universiy-booklist","date":"2018-06-14T14:27:44.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/universiy-booklist.html","link":"","permalink":"https://xiang578.com/post/universiy-booklist.html","excerpt":"","text":"大学书单 这几年学校一直有一个传统，会将毕业生大学期间在图书馆的借阅记录整理打印，形成一份特殊的书单。前几天，我就收到了这一份礼物，正好用来回忆一下自己在图书馆借过的书。 书单 书单制作的很精美，基于成本考虑没有能做成一个小册子倒是蛮遗憾的。图书馆寄语：「世态炎凉于指尖低回婉转，人生百态于页上妙趣横生。流年不虚度，不负诗与书。」第二页写的是，四年里你最常去的阅览室。我感觉这个数据是错误的，阅览室门口没有可以统计次数的仪器，大概推测是分别在哪个阅览室借过多少本书吧。由于自己所学的是计算机专业，毫无疑问，在自然类的阅览室借阅最多。之后是哲学社科以及文学艺术，也没有太大的惊讶。 第三页开始，就是罗列我的借书记录了。这份书单上没有写，查询知道大学四年总借图书121本，号称打败了 93.4% 的小伙伴。从最后一行记录开始查看，多少可以见证自己思维的成长。刚开始借的书很杂，什么小说、教辅、人文社科都有。之后由于求职的压力，更多看地是专业技能的书籍。 书单很长也很杂，推荐 5 本我看过比较喜欢的图书。 1、我是一只IT小小鸟：这本是大四才看的书，我却想把他放在第一本。这本书一些博文总集，邀请一些毕业后从事计算机领域的工程师、教授写的关于如何走上 IT 的故事。很多人是大学科班出身，他们讲到自己是如何学习计算机专业知识的，对迷茫的新生应该会有很多帮助。也有一些是转行干 IT，也很符合目前知乎上的劝退风潮。 2、万历十五年：黄仁宇的作品，国内一直很火。去年热映的《人民的名义》中，高小凤正是因为这本书才与高育良搭上关系。这本书分析不同的人物（张居正、申时行、戚继光、海瑞、李蜇），去揭示明朝衰败的开端。黄教授多年在海外教学，书中的叙事逻辑更偏西式。对于我这种深受国内历史教科书迫害的人来说，有醍醐灌顶之效。 3、浪潮之巅：吴军博士的《XX之X》系列中的一本（还有文明之光、数学之美、大学之路）。讲述站在技术浪潮之巅的科技公司的故事，可以快速了解目前美国的一些大型公司成长的历程与规律，提高专业素养。 4、搞定：GTD 的开山之作，引领了一股风潮。于我而言，重要的是接触 GTD 之后，思维方式的转变，对事情开始进行规划和反思。每一个人都不一定需要去实践完整地 GTD 流程，但是不能不知道这种方法。 5、暗时间：刘未鹏的博文集，一本看完之后，直接在网上下单购买的书。内容很杂，很难简单的讲清楚，以后有机会再详细推荐。 借阅记录 1 软技能:代码之外的生存指南:the software developer's life manual (美) John Z. Sonmez著 2 Qt 5开发实战 (韩) 金大〓著 3 大教堂与集市 (美)Eric S. Raymond著 4 人月神话 (美) 小弗雷德里克·布鲁克斯著 5 数学之美 吴军著 6 我是一只IT小小鸟 胡江堂主编 7 美语音标 赖世雄编著 8 统计学习方法 李航著 9 硅谷钢铁侠:埃隆·马斯克的冒险人生 (美) 阿什利·万斯著 10 构建之法:现代软件工程:effective software engineering 邹欣著 11 小狗钱钱 (德)博多·舍费尔著 12 Word排版艺术 侯捷著 13 乔布斯的魔力演讲.第3版 (美) 卡迈恩·加洛 (Carmine Gallo) 著 14 搞定.III,平衡工作与生活的艺术,Winning at the game of work and business of life:最新版.第2版 (美) 戴维·艾伦著 15 小强升职记 邹鑫著 16 不要等到毕业以后.修订版 张志著 17 硅谷钢铁侠:埃隆·马斯克的冒险人生 (美) 阿什利·万斯著 18 奇特的一生:柳比歇夫坚持56年的“时间统计法” (俄)格拉宁著 19 搞定.I,无压工作的艺术,The art of stress-free productivity:最新版.第2版 (美) 戴维·艾伦著 20 别闹了, 费曼先生:科学玩童的故事 (美) R·费曼著 21 好好学习:个人知识管理精进指南 成甲著 22 有道云笔记:记录，成为更好的自己 有道云笔记主编 23 公务员考试,申论的规矩 粉笔科技编著 24 你的知识需要管理 田志刚著 25 论可计算数:图灵与现代计算的诞生:the birth of computer science (美) 克里斯·伯恩哈特著 26 苏东坡传 林语堂著 27 统计学习方法 李航著 28 Word排版艺术 侯捷著 29 高频交易员:华尔街的速度游戏:a Wall Street revolt (美) 迈克尔·刘易斯著 30 人月神话 (美) 小弗雷德里克·布鲁克斯著 31 盖洛普优势识别器2.0 (美) 汤姆·拉思著 32 毛泽东选集.第1卷 毛泽东著 33 穿越抑郁的正念之道:freeing yourself from chronic unhappiness (英) 马克·威廉姆斯 ... [等] 著 34 大学之路:陪女儿在美国选大学 吴军著 35 大学之路:陪女儿在美国选大学 吴军著 36 编程之美:微软技术面试心得 《编程之美》小组著 37 Redis设计与实现 黄健宏著 38 尽在双11:阿里巴巴技术演进与超越 阿里巴巴集团双11技术团队著 39 程序员修炼之道:从小工到专家 (美)Andrew Hunt, (美)David Thomas著 40 编码:隐匿在计算机软硬件背后的语言:the hidden language of computer hardware and software (美)Charles Petzold著 41 日本围棋故事.修订本 薛至诚编译 42 统计学习方法 李航著 43 编程之美:微软技术面试心得 《编程之美》小组著 44 剑指Offer:名企面试官精讲典型编程题 何海涛著 45 淘宝技术这十年 子柳著 46 Linux内核完全剖析 赵炯编著 47 计算机考研指导全书 赵霖 48 技术之瞳:阿里巴巴技术笔试心得 阿里巴巴集团校园招聘笔试项目组著 49 人生十八局:现在我将这样下 (日]吴清源著 50 阶梯围棋教室.从入门到业余初段.第2版 黄希文主编 51 布局 日本棋院编 52 计算几何:算法与应用:algorithms and applications Mark de Berg ... [等] 著 53 数据库原理及应用.第2版 雷景生, 叶文珺, 楼越焕编著 54 只是为了好玩:Linux之父林纳斯自传:the story of an accidental revolutionary (美) Linus Torvalds, (美) David Diamond著 55 围棋入门.第3版 胡懋林, 马自正编著 56 向死而生:我修的死亡学分 李开复著 57 神奇的老大日记 有时右逝著 58 计算几何:算法与应用:algorithms and applications Mark de Berg ... [等] 著 59 数字电子技术 (美) Thomas L. Floyd著 60 经济学原理.宏观经济学分册 (美) 曼昆著 61 旧制度与大革命 (法)托克维尔著; 冯棠译 62 离线·黑客.NO.002 主编李婷 63 数字电子技术 (美) Thomas L. Floyd著 64 黑客与画家:硅谷创业之父Paul Graham文集:big ideas from the computer age (美)Paul Graham著 65 菊与刀:日本文化模式论 (美)鲁思·本尼迪克特著 66 酒国 莫言著 67 电路与电子学习题解答与实验指导 李景宏, 刘淑英主编 68 煮酒探西游:吴闲云详解西游记 吴闲云著 69 悟空传 今何在作品 70 晨间日记的奇迹 (日) 佐藤传著 71 学习vi和Vim编辑器 Arnold Robbins，Elbert Hannah，Linda Lamb著 72 Evernote 100个做笔记的好方法:数字化重整你的工作和人生 异尘行者著 73 善用佳软:高效能人士的软件应用之道 张玉新, 陈勇, 吴放著 74 黑天鹅:如何面对不可预知的未来:the impact of the highly improbable (美) 纳西姆·尼古拉斯·塔勒布著 75 寻秦记:终结篇.陆 黄易著 76 寻秦记.伍 黄易著 77 构建之法:现代软件工程:effective software engineering 邹欣著 78 寻秦记 黄易著 79 寻秦记.叁 黄易著 80 寻秦记 黄易著 81 寻秦记 黄易著 82 1984 (英) 奥威尔 (G. Orwell) 著 83 四十一炮 莫言著 84 红高粱 莫言著 85 数据结构学习与实验指导 陈越 ... [等] 编著 86 电路与电子学习题解答与实验指导 李景宏, 刘淑英主编 87 怎样解题:数学思维的新方法-1版 (美)G·波利亚著 88 万历十五年 黄仁宇著 89 图论算法理论、实现及应用 王桂平, 王衍, 任嘉辰主编 90 老子 汤漳平, 王朝华译注 91 魔方宝典:风靡世界的智力玩具之终极指南 (美) 杰瑞·斯洛克姆 ... [等] 著 92 中国哲学简史:修订译本 冯友兰著 93 挑战程序设计竞赛 (日)秋叶拓哉，(日)岩田阳一，(日)北川宜稔著 94 文明之光 吴军著 95 数学之美 吴军著 96 把时间当作朋友:运用心知获得解放 李笑来著 97 浪潮之巅.第2版 吴军著 98 浪潮之巅.第2版 吴军著 99 暗时间 刘未鹏著 100 Vim实用技巧:edit text at the speed of thought (英) Drew Neil著 101 编程之美:微软技术面试心得 《编程之美》小组著 102 货币战争.2,金权天下 宋鸿兵编著 103 北大往事:珍藏版:纪念北京大学建校110周年 橡子，谷行著 104 C++ Primer Plus (第6版) 中文版 (美) Stephen Prata著 105 数据结构学习与实验指导 陈越 ... [等] 编著 106 C++ Primer Plus (第6版) 中文版 (美) Stephen Prata著 107 鸟哥的Linux私房菜:基础学习篇.3版 鸟哥著 108 搞定.Ⅱ,提升工作与生活效率的52项原则,52 productivity principles for work and life (美) 戴维·艾伦著 109 搞定.Ⅲ.Ⅲ,平衡工作与生活的艺术,Winning at the game of work and business of life (美)戴维·艾伦著 110 搞定.Ⅰ.Ⅰ,无压工作的艺术,the art of stress-free productivity (美) 戴维·艾伦著 111 美国种族简史 (美)托马斯·索威尔著 112 一万小时天才理论 (美)丹尼尔·科伊尔(Daniel Coyle)著 113 番茄工作法图解:简单易行的时间管理方法 (瑞典) Staffan Noteberg著 114 越读者 郝明义著 115 C++ Primer中文版 Stanley B. Lippman, Josee Lajoie, Barbara E. Moo著 116 Vim实用技巧:edit text at the speed of thought (英) Drew Neil著 117 计算机科学导论 (美)贝鲁扎.佛罗詹著 118 ACM/ICPC算法训练教程 余立功主编 119 C程序设计 谭浩强著 120 数学分析习题集.2版 Б. П. 吉米多维奇著 121 高等数学全程学习指导与习题精解:同济六版 滕加俊, 滕兴虎编著","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"zufe","slug":"zufe","permalink":"https://xiang578.com/tags/zufe/"},{"name":"reading","slug":"reading","permalink":"https://xiang578.com/tags/reading/"}]},{"title":"每周分享第 2 期","slug":"week-issue-2","date":"2018-06-11T15:34:50.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-2.html","link":"","permalink":"https://xiang578.com/post/week-issue-2.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 毕业季脱更几天 新闻 1、EffectiveMac - EffectiveMac 一份 Mac 系统教程 2、Evernote 和印象笔记终于拆分了，国内的公司独立开发。为知笔记半死不活的情况下，还是希望印象笔记能给我们带来更多的惊喜。来源 3、离开毛坦厂，去吻我爱的女孩：又是一年高考时，这篇文章从不同的角度去感受被称为高考军工厂的毛坦厂，可惜当年我没有这样的感受。 工具 Thought Train for MacOS – Simple Note App for MacOS：有意思的小工具，在 Mac 状态栏上显示一些话。 Rooster for Chrome™：在新标签页统计每天浏览各个网站的时间。 文摘 1、看板使用指南：从三列看板到灵活的布局设计 思考了一下，个人管理在于项目比较少的时候不是很适合这种方法，还是比较适合软件团队使用。看完这篇文章，学习到的是泳道这个概念。 2、如何理解傅里叶变换公式？ - 知乎 3、机器学习最佳实践：Google 官方指南 图片 1、索引卡的妙用 视频 1、預告分析 《蜘蛛人：返校日》 ： 少年超級英雄的進化(一) | 誰不重要 - YouTube 台湾一名电影自媒体，看问题的方式更加的深刻。 金句 1、这是创业者最坏的时代，因为你很难再打情怀牌同情牌，要直面巨头们的烧钱大战。你能做的极限就是杀进决赛接受巨头投资。这也是创业者最好的时代，因为靠包装和营销的骗子们更加难以持续，劣币的离场速度显然是快于良币的。世界最终未必是你的，但一定是属于和你一样的人的。 2、评家罗世宏说，脸书作恶并非首次，“脸书强大的信息过滤和投放能力，代替人们开展正常的社交活动，也代替了人们自行选择接触什么信息的决定权。这次脸书再度陷入丑闻，或许最好的结果并不是脸书从此幡然改悟，善尽社交媒体平台的社会责任，而是扎克伯格的总统梦破灭了。（我并）不期待它从此改邪归正，但盼最终能把脸书这样的网络巨人和它所掌控的超大权力，逐步关到笼子里面去。 3、在一个知识更新如此迅猛的时代里，人们借助各种工具，将日常碎片化的时间充分利用起来，以更少的时间快速获取知识。这一点本来无可厚非。鲁迅先生也曾说，“哪里有天才,我是把别人喝咖啡的时间都用在写作上了”。但是，碎片化的学习不代表知识获得可以有捷径可走，更不等于打着知识付费的旗号去做知识的买卖。 4、其实，消费的本质就是考量成本与收益。这些新的名词后面的内核，是重新回到消费的本源，是为了使用价值而不是符号价值去消费；是为了提高生活品质的需要去消费，而不是追求炫耀的资本；为了获得身体和内心的享受去消费，而不是仅仅满足自己被裹挟的欲望。 微博写作训练 1、淘宝与拼多多的商业模式 淘宝由于通过商品展示赚钱，倾向于推荐客单价比较大的商品，很多长尾商家就没有那么多导入流量。拼多多正是瞄准这一点，利用微信进行裂变式发展，做低价标品的拼团。此外为了促进消费，拼多多在系统的设计上取消了购物车、收藏夹、评论等功能。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"《机器学习》 第 3 章 线性模型 读书笔记","slug":"machine-learning-ch-03","date":"2018-06-09T08:29:21.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/machine-learning-ch-03.html","link":"","permalink":"https://xiang578.com/post/machine-learning-ch-03.html","excerpt":"","text":"西瓜书 周志华 2016 年 12 月第 14 次印刷 3.1 基本形式 线性模型的预测函数为： \\[{f(x)=w_1x_1+w_2x_2+...+w_dx_d+b}\\] 写成向量模式得到： \\[{f(x)=w^Tx+b}\\] 3.2 线性回归 线性回归能在给定数据集 \\({D=\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}}\\)，其中 \\({x_i =\\{x_{i1};x_{i2};...;x_{id}\\},y_i\\in\\mathbb{R}}\\)学到一个线性模型从而进行预测。 考虑最简单情况，当 \\({x_i}\\) 为一维时，问题转换为求下式： \\[{f(x_i)=wx_i+b，使得f(x_i)\\simeq y_i}\\] 使用平方损失函数作为衡量线性规划模型性能的指标，\\({f(x)}\\) 与 \\({y}\\) 越接近，代表平方损失函数越小。即得到：","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://xiang578.com/tags/MachineLearning/"}]},{"title":"那些年听过的播客","slug":"podcast-list","date":"2018-06-07T06:59:22.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/podcast-list.html","link":"","permalink":"https://xiang578.com/post/podcast-list.html","excerpt":"","text":"前几天在刷微博时，突然发现 Checked 休更。蛮震惊的，于是在他下面留了言。 后来仔细一想，接触播客也快四年多了。刚开始疯狂地时候，走路的时候也带着耳机在听。不过现在胆子越来越小，主要在跑步的时候通过无线耳机收听节目。四年多的时间里面，有播客听过，有播客开始，也有些播客半死不活。正好借这个机会分享一下我听过的播客。 播客是什么？其实有两种解释：一是一种数字广播，制作方将节目以音频的形式放在指定网站上，用户可以将节目下载到自己喜欢的硬件中收听。二是特指 iOS 中的一个软件——Podcast，通过这个软件能订阅以及收听音频节目。 科技 IT 公论 说起中文播客，就不得不提 IPN 播客网络，他们旗下有很多的博客节目，IT 公论正是其中的一个。由李如一和 Rio 主持，主要探讨科技和人文的关系，由于主播的限制，更多关心 Apple 生态下的新闻。可惜的是于 2106 年停播，不过早期的一些节目还是值得一听。 Checked 本文最开始提到的节目，有文刀汉三、千千、Hum三个人主持，主要讨论如何使用 App 进行数字化生活。比较经典的有#1: 日历/待办事项/GTD、#2: 如何记一手明白账、#49: 访谈「也谈钱」: 你的钱是可以给你赚钱的，也可以说是我听过后实践最多的节目。 生活 太医来了 一档医生谈话类节目。之前很喜欢的节目，高考之前也有学医的想法，听三位太医讲他们大学生活以及医院工作都有一种触动。更关键地是，这个节目也是很好的获得一些基础医学常识的渠道。比如其中有一期节目讲解了医生的字，听完之后才了解到龙飞凤舞的字迹主要是不希望患者看明白。 狗熊有话说 主播大狗熊，注重分享设计、生活、运动、阅读等方面的知识。更重要的是，通过收听这么长的节目，可以看见狗熊的成长轨迹。从当初的云南昆明，如何通过自己的规划，最终全家移民新西兰。 Byte.Coffee 比较杂的节目，主播 MilkShake 羊，斯坦福博后，目前在南京一所大学任教。 在北大不吐槽会死 三名北大学生的吐槽会，取材也是方方面面，从他们的大学生活到现在毕业之后对工作的感悟。内容有些时候比较脏。 纽约文化沙龙 在美国留学的学生主办，每期会邀请一些嘉宾，讲述特定主题的内容。我知道的类似组织还有安城文化沙龙，不过他们只分享视频。 蒋勋细说红楼梦 蒋勋的红楼梦应该是很有名的节目，某一位粉丝通过播客形式将这些音频分享出来。其实之前我也搞过这方面的内容，WordPress 有专门的播客插件，从技术角度来说发布播客还是很简单的。 编程 内核恐慌 这是另外一档由 Rio 主持的节目，听名字就很硬派。讨论过程序员的方方面面，比如编辑器、字体、键盘、自由职业等等。可惜目前保持半年更新的频率。 Teahour.fm 以及代码时间 这两个都是访谈类节目，邀请过各种各样的程序员都有。对于立志投身于软件产业的人具有一定的参考价值。 工具 收听播客大概有两种主要的工具，第一种是专用的软件，比如喜马拉雅、蜻蜓 FM等，能直接在软件内部搜索你想听的节目收听，而且我在上面的介绍的节目大多都会在这些平台发布。但是，我更推荐第二种方法，使用泛用性客户端收听节目。在 iOS 中，系统自带播客软件，也可以去下载如Overcast或者Castro Podcasts等第三方软件。目前我自己常用的是 Overcast，比起系统自带的，提供了人声增强（能解决部分播客音质太差问题）以及智能变速（自动调整速度）等功能。在 Android 中，推荐 360 Podcast ，老周的公司出品的良心应用，良心到国内都没有推广过…… 最后，对于一些只有在喜马拉雅等地方可以收听的节目，可以使用播客 RSS Feed这个网站获得订阅链接。","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"podcast","slug":"podcast","permalink":"https://xiang578.com/tags/podcast/"}]},{"title":"《机器学习》 第 1 章 绪论 读书笔记","slug":"machine-learning-ch-01","date":"2018-06-06T14:31:16.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/machine-learning-ch-01.html","link":"","permalink":"https://xiang578.com/post/machine-learning-ch-01.html","excerpt":"","text":"西瓜书 周志华 2016 年 12 月第 14 次印刷 1.1 引言 机器学习：利用经验来决策 1.2 基本术语 根据数据是否拥有标记信息分类： 监督学习 supervised learning 分类 classification 回归 regression 无监督学习 unsupervised learning 聚类 clustering 1.3 假设空间 假设空间指的是所有跟问题相关的假设所组成的空间，学习过程是从假设空间中进行搜索，目标是找到与训练集「匹配」（fit）的假设。 在这么多的假设中，可能存在一些假设，得出的结果和训练集一致，这些假设组成的空间叫做「版本空间」（version space）。 1.4 归纳偏好 对于数据集中没有出现过的情况，算法可能会按照自己的偏好来预测结果，这种情况称为「归纳偏好」。为算法选取偏好时，可以使用「奥卡姆剃刀」原则，即有多个假设与观察一致，则选最简单的那个。但是什么是最简单的也需要仔细思考。 没有免费的午餐定理（No Free Lunch Theorem, NFL） 在所有问题出现的机会相同时，所有的算法的期望性能相同。 任何一个算法都有表现好的问题，也有表现差的问题。 针对具体的学习问题研究算法。脱离具体问题，研究什么算法更好毫无意义。 1.7 阅读材料 其他科学研究中采用的假设选择原则 古希腊哲学家伊壁鸠鲁 「多释原则」：保留与经验观察一致的原则。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"}],"tags":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://xiang578.com/tags/MachineLearning/"}]},{"title":"每周分享第 1 期","slug":"week-issue-1","date":"2018-06-01T15:15:33.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/week-issue-1.html","link":"","permalink":"https://xiang578.com/post/week-issue-1.html","excerpt":"","text":"这里记录过去一周，我看到的值得分享的东西，每周六更新。 新闻 1、伊朗迷你裙消亡史_手机网易网 历史居然可以导流…… 2、欧盟GDPR有多狠？未合规的数据处理活动或将被叫停！ - 大数据 - DBAplus社群——围绕数据库、大数据、PaaS云，运维圈最专注围绕“数据”的学习交流和专业社群 关于 GDPR 你需要了解的一切 - 少数派 最近收到很多跟这个相关的邮件 3、2018互联网女皇报告中文完整版呈现！ 4、如何评价ry(Ryan Dahl)的新项目deno? - 知乎 这个新闻关键不在于这个新的项目，而是在于国内和多的开发者在项目的 Issues 页面灌水，引起其他人的反感。 工具 Kanbanist | The Missing Kanban Board for Todoist 前几天还在考虑如何将 todoist 与看板结合，也看了一下 todoist 相关的 api 文档，之后在网上搜索时就发现了这个产品。功能还不是很强大，不知道作者什么时候可以完善。 文摘 1、 大学的双休日应该如何充实地度过？ - 知乎 这一个回答是我见过关于大学如何渡过比较好的解释，希望还没有毕业的同学可以不辜负四年。 你如果找准了目标，自然自己会有一个答案。每个人只要认准一件事情好好做下去，最后的结果不一定会差。虽然我自己是走传统路线的学生，GPA+英语+竞赛都稍微拿得出手，但是其实我并不喜欢现在的自己。其实你发现，除了顶级大牛，大多数大学所谓的好学生，其实都是一个模子刻画出来的。国家奖学金(其他综合奖学金)+90左右的均分+各类学科竞赛+各类荣誉称号+各类传奇的社会经历活动等等。当然，本科毕业后，人生不会太多波折，但是回首望去，这样的本科未必真的是自己想要的本科。 2、前几周加了民科微信群（不知道有没有人还记得当年发现电荷不存在的“民间科学家”），很难理解他为什么会这么执迷不悟，下面这一段话大概可以解释一下。 一个人不需要做什么惊天动地的大事，但是对自己做的事情必须有认同感，如果做学术的自己都觉得自己在灌水，理由是身边大多数人也是在灌水，那么我不明白我为什么要去做它。 3、哪些经济学论文让你发出「脑洞才是第一生产力」的感叹？ - 童话李的回答 - 知乎 生活处处皆皆学问，从篮球比赛中的「手感」出发，抽象「这个球命中概率是否与下一个球命中概率正相关」，然后去统计数据研究，最后给出下面的结论。 The belief in the hot hand and the “detection” of streaks in random sequences is attributed to a general misconception of chance according to which even short random sequences are thought to be highly representative of their generating process. （一小段完全随机的结果，却被认为对整体有很高的代表性。） 4、我的5个经济学思维 – 左岸读书 折现、机会成本、边际效用递减、沉没成本、复利 图片 1、以 GDPR 为背景创建的图片 视频 1、2018 4 18 德云一队新街口剧场《朱夫子》高峰 栾云平 - YouTube 金句 1、坚持读书，坚信自己可以读完一些东西，坚信自己可以承受读书的强度——无论读的是什么，这过程本身就是一种信念，并且这份信念会让我们抵达心中的目的地。 2、许多时候，我们身处信息传导的末端。当一个问题反复多年都没有解决方法的时候，就应该怀疑是否信息上游的路径已经锁死？可能在你自己原有的知识体系和知识框架内并不存在解决方法，无论耗费多少时间和精力都不能解决问题。那么，这时候应该考虑迁移到其它陌生的领域中去，也许，你要的答案早就放在那里，只是你一直没有发现而已。你的问题，也许对那个领域里的人而言，只是他们工作的一个副产品，甚至都不会太在意。——和菜头 3、所有的科技公司，解决的都是人、信息、物品三方之间的关系。简化来说：谷歌解决的是人与信息的关系，亚马逊解决的是人与物的关系，Facebook解决的是人与人的关系，微软，解决的是企业人的问题。 ——如同答辩老师对我所说，做硬件才有意义。 4、元认知是对认知的认知。元认知是自省的能力。一个人能够理性的反思自我，自然就会知道许多麻烦，并非是外界的存在，而是自己暴戾所引发的。执迷于无知，甚至以无知为傲骄，才是最可怕的贫穷。 5、我觉得大多数幸福的人的生活就是这样的，没有什么高大上，也没有那么多的风花雪月，而就是这样简简单单、快快乐乐地用自己的努力和自己心爱的人一起为更美好的生活脚踏实地地奋斗着。 6、当一个学生从某一所知名大学毕业后，他不需要再把母校的名字天天挂在嘴边；当一个学生从一所二三流大学毕业后，那所学校今后会因为曾经出了这样一个学生感到自豪。这样的大学经历就堪称完美了。——快要离开学校时，思考自己的价值。","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[]},{"title":"月读|软技能——代码之外的生存指南","slug":"soft-skills","date":"2018-05-28T14:21:29.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/soft-skills.html","link":"","permalink":"https://xiang578.com/post/soft-skills.html","excerpt":"","text":"本月推荐的书是 John Z. Sonmez 写的《软技能——代码之外的生存指南》。本书作者之前是一名程序员，目前更多的是充当程序员的人生导师角色，通过博客、播客、vlog等多种形式介绍自己的软技能，借此希望能推动其他人事业进步，过上更充实的生活。 本书总共分成七大篇： 职业篇，介绍了程序员从公司选择、面试、工作、辞职、创业等职业相关的方方面面内容 自我营销篇，从博客开始，打造自己的个人品牌 学习篇，作者自己领悟的「十步学习法」学习新的知识 生产力篇，自由职业者的自我管理方法 理财篇，探讨收入、投资、债务以及退休等问题 健身篇，下面的图片就是作者本人，和一般的程序员形象有很大的区别 精神篇，心灵、情感等问题探讨 作者博客 从上面的介绍可以看出，这本书基本上把一个程序员可以面对的所有问题，都给囊括了。从形式上来看，这本书像是各种主题的博文合集，有些散乱。总的来说都是关注于「更好地经营生活」。 自我营销 这一个主题是书中我最感兴趣的内容。知识付费的兴起，朋友圈中经常能看到有人分享付费课程的图片，图片中的老师各种头衔酷炫狂霸叼咋天，自我营销的成本真的是已经低到一种程度。万物互联，每一个人都有一次成名的机会。 本书中的自我营销，介绍了一系列的过程，作者推荐个人品牌的建立从博客开始。对于程序员来说，维护一个自建博客是很轻松的事情，而且可以分享的内容也很广泛，工作中的技能学习、遇到的问题分析。将博客当成自己的名片，从而实现让更多的人认识你，认可你。 书中介绍维护一个博客，要从两个方面入手，一是内容质量，二是更新计划。之前，自己的博客都是随心所欲的更新。重新思考一下，给博客增加了几个栏目： 【每周分享】：每周六不定时更新，记录过去一周，我看到值得分享的内容 【月读】：每月不定时更新，推荐本月我阅读的一本书 【数字生活】：我的数字生活实践 【博客公告】：分享与这个博客维护相关的内容 另外一方面，对于我这样的博客来说，最主要的流量来源是搜索引擎，所以花时间进行了一些 SEO 优化。通过在百度搜索和谷歌搜索相关页面提交网站，实现文章更快的收入。除此之外，需要在社交账号中更多的绑定博客，寻求其他的主动点击。 生产力 这个主题，是我大半年以来一直感兴趣的。之前没有注意到，像作者这样的自由职业者更应该是效率大师。其实各种效率方法流程大致相同，更多的是细节的实现。作者在做周计划时，倾向于利用看板来展示任务，这个给我一个新的启示。我考虑过利用 Github + Zenhub 来管理一些大的项目，不过平时没有什么动力打开 Github 网站。 Github Issues 所以，需要一种更加简单地方法。网上查找了一下，发现一个网站 Kanbanist | The Missing Kanban Board for Todoist。Kanbanist 最大的优势是能直接读取 Todoist 的数据，用起来会更加方便吧。不过，如何融合进入日常工作流中，又值得我自己仔细思考一下。 Kanbanist 总结 读这本书，最大的感受是读到很多点的时候，我都打开了电脑进行实践。功利地说，这是一本实用的书，推荐大家有空都来挑选自己感兴趣的内容阅读。","categories":[{"name":"文渊阁","slug":"文渊阁","permalink":"https://xiang578.com/categories/文渊阁/"}],"tags":[{"name":"book","slug":"book","permalink":"https://xiang578.com/tags/book/"},{"name":"gtd","slug":"gtd","permalink":"https://xiang578.com/tags/gtd/"}]},{"title":"博客折腾记：使用 Travis CI 自动部署","slug":"use-travis-ci-to-auto-update","date":"2018-05-28T07:44:09.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/use-travis-ci-to-auto-update.html","link":"","permalink":"https://xiang578.com/post/use-travis-ci-to-auto-update.html","excerpt":"","text":"前几天，看到其他人在 V2 上讨论利用 Docker 更新 hexo 博客。不过自己对使用 Docker 不是很感兴趣，倒是了解到 Travis CI 的作用。 参考使用 Travis 自动构建 Hexo 到 GitHub | Zthxxx's Blog和使用travis-ci自动部署Hexo到github和coding - 掘金这两篇文章，完成了博客自动部署的修改，也解决了一些，之前没有在意的问题，写下来备忘一下。 介绍一下与这个博客先关的 git 项目。博客相关的原始文件放在 xiang578/blog，主题文件放在 xiang578/hexo-theme-even: A super concise theme for Hexo（我对这个主题有一些修改，所以自己 fork 了一份，最后发布的文件放在xiang578/xiang578.github.io: Welcome to My blog!（实际上这也是一个备份，访问时的文件是从 coding 服务器上读取的）。 按照上面两篇博文配置好 Travis CI 之后，每次向存放博客原始文件的仓库 push 时，travis-ci.com 都会拉取代码进行 build ，成功之后会出现下图。 之前都是使用 git clone 下载主题的，如果把本机上的博客相关文件直接推送到 github 上会遇到问题。简单地说就是一个 git 文件夹包含了另外一个 git 文件夹。所以，需要使用 git modules 来解决。通过这种方法可以在 clone 主仓库时，会自动clone子仓库。 博客仓库中会链接到其他仓库，其中 @ 之后的那一串就是 commit 编号，主要是用来做版本控制的。 完成这样的设置，修改主题文件后，需要先将修改 push 到主题的仓库，然后在博客文件夹下 push 修改到远端仓库。最终，才能再网页上看到修改效果。","categories":[{"name":"站务","slug":"站务","permalink":"https://xiang578.com/categories/站务/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://xiang578.com/tags/hexo/"},{"name":"travis","slug":"travis","permalink":"https://xiang578.com/tags/travis/"}]},{"title":"月读 | 数学之美","slug":"the-beauty-of-math","date":"2018-05-07T14:24:30.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/the-beauty-of-math.html","link":"","permalink":"https://xiang578.com/post/the-beauty-of-math.html","excerpt":"","text":"其实从《数学之美》这书名上看就可以知道，这是一本科普类的图书。这本书收集吴军博士早期发布在谷歌黑板报上解释搜索相关的数学原理的文章。吴军博士之前在谷歌和腾讯都从事搜索相关领域的研究，自然而然的有很多独特见解。除了这本书之外，他还有《浪潮之巅》、《大学之光》等科普图书，也是值得一读。 虽然是科普类读物，但是本书还是有一些阅读门槛的。大二的时候也看过这本书，当时的数理基础不够，囫囵吞枣翻完后。事后大概只记得自然语言处理、马尔科夫链等等的名词。这一段时间正好自己也在做文本搜索相关的工作，所以又从图书馆中借来一读。 本书大概介绍了 Google 整个搜索引擎的基本框架，包括资料获取、分词、索引、排名、分类、相似度计算等知识。这一次读完之后，印象最深刻的是关于从通信模型推导出语言翻译模型的转变。这里和大家分享一下我的简单见解，如果想要更加深入的理解可以阅读《数学之美》的第 5 章 隐含马尔可夫模型。 大二的时候上过《通信原理》这门课，其中讲解如下图所示的简单通信系统。一般包括五个最基础的要输，分别是信源、变换器、信道、反变换器、信宿。从字面上很难理解这些东西是，所以简单的解释一下。通信系统是对通信过程的抽象概括，比如你和其他人发微信时，你就是信源，信息的发送方。变换器的作用是编码，计算机只能处理 0 和 1，所以需要将你的消息通过一定的方法（协议）转化成为由 0 和 1 组成的序列。之后的信道指的是信息发送的通道，可以理解为发送微信时需要有网。反变换器顾名思义作用和变换器相反，从一个二进制序列还原成和你聊天的人（信宿，和你聊天的人）可以理解的消息。 当时学到这里的时候，没有去发散思维，思考这个模型在其他领域的应用。《数学之美》书中，讲到统计语言模型时，将它衍生到机器翻译的过程中。在日常生活中，我们和其他人交流的时，一般是在脑子中有一个想法（信源），然后在说出来（编码），声音在空气中传播（信道），最后被另外的人（信宿）听到，并且在脑子中思考理解（解码）做出相应的回应。对于翻译问题，可能我说出来的是中文，如果听到的是美国人，他需要将你说的转化为英文。所以产生的一个问题是，如何实现将中文与英文对应起来，也就是翻译问题了。为了解决这个问题，科学奖们运用了贝叶斯概率、隐含马尔科夫模型、大数定理等数学知识来处理。 这只是书中的一个简单例子，吴军博士在自然语言处理与搜索领域的经历令人难以望其项背，书中关于这两个领域在近代的发展的小故事比比皆是，作者更是和这些人谈笑风声。所以，我推荐所有对数学有爱好的人，都可以看一下这本书，感受一下数学原理的力量。","categories":[{"name":"文渊阁","slug":"文渊阁","permalink":"https://xiang578.com/categories/文渊阁/"}],"tags":[]},{"title":"利用 GTD 原则完成一次讲座","slug":"use-gtd-to-speach","date":"2018-04-28T10:04:32.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/use-gtd-to-speach.html","link":"","permalink":"https://xiang578.com/post/use-gtd-to-speach.html","excerpt":"","text":"前几天，在导师的邀请下，给17计算机班的同学们进行了一次分享讲座。内容大概是关于去年在滴滴实习时做的一些工作以及主观意愿下的大学学习建议。作为一个号称实践 GTD 的人，必不可少的要将 GTD 原则使用到这一次分享准备过程中。 自然式计划模式 这样子的一次活动，正好符合 GTD 中关于项目的定义。《搞定1》第 3 章中提到了「控制项目：纵向管理项目的5个阶段」。现在结合五个阶段，来复盘一下我准备讲座的过程。 1. 定义目标和原则 你为什么做这件事？ 做这件事的真正目的或动机是什么？ 做到什么程度才算是成功？ 你的目标是否足够清晰明确？ 什么样的行为可能会损害我正在进行的工作？ 我怎样才能防止这类情况的发生？ 我决定做这一次分享时，主要有下面的4方面原因。 第一，整理。每一次分享都是一次自己我反思。所以我需要整理一下之前在滴滴工作时留下的资料，以及梳理前几年的学习感悟。 第二，演讲。说句实话，我的演讲的能力很弱。体现在，口齿不清楚、演讲时语速太快和听众没有交流。众所周知，演讲能力是现代社会必不可少的一项能力。提升演讲的关键在于实践，所以给其他人讲解我熟悉的内容正是一次绝好的机会。 第三，回馈。虽然私下里和其他人骂过很多学校和学院的规章制度之类，但也不能忽视学校所提供给我的成长机会和空间。之前书记也在一次党会上说过，我们很多党员的材料中，只有那一年考了多少名，又获得什么奖这些的，没有和群众交流。 第四，参考。作为一个软弱的人，我很少会去谈给其他人有什么影响，也不会整天去批评这个那个，只希望自己的经历给其他人一个参考。所以，我很喜欢学院里面搞的《榜样的力量》栏目。通过这些，可以了解到学长学姐有多么努力，获得了什么样的成就。事实是，我们这样的普通学校中，很多优秀的人都已经模板化了，集中在绩点多少高，获得过什么奖学金，当过什么部门的什么而已。真正的大学教育，应该培养的学生达到「千人千样」。 2. 展望结果 成功是一番什么景象？听上去会怎样？有什么感觉？专注！创造出清晰可见的结果。只做积极性思考，不考虑不利因 素 对于我个人来说，这一次分享是一次宝贵的锻炼机会。是实践 GTD 原则，是完成一次公众演讲，也是完成一次和学弟学妹的交流。不过，由于个人经历的限制，我所分享的东西，终将是少数同学所感兴趣的，只希望这部分人听到我的一些观点，未来的学习会有那么一点点不同。也许多年之后，互联网上有一段文字记录，提起那个晚上我所讲的观点。 3. 头脑风暴/集思广益 在这张的纸的反面的正中央写上项目名称，开始头脑风暴。追求数量，不求质量，不判断，不质疑，不评估，不批判。 不要急于分析组织。1）目的？2）害怕什么？3）我所不知道的？4）有哪些不利因素？ 上面写了这么多，终于到了最关键的一步。由于这是一次个人分享，所以也谈不上什么集思广益。不知道现阶段有没有什么头脑风暴的工具，我直接掏出几张草稿纸就开始搞了。上面的介绍也提到了，头脑风暴的原则，将大脑中的想法排空，为下一步工作做准备。 头脑风暴 4. 组织整理 在完成头脑风暴之后，我开始使用幕布来整理之前想到的想法。上图中蓝色的对勾就是在整理时的记号，标记整理到大纲中的内容。最后导出一张思维导图，到这里为止我就清晰的知道自己要分享的内容。 路径规划那些事大纲 5. 明确下一步的行动方案 目前我是在使用 Emacs 中的 org-mode 管理这些项目。所以，我就将自己要做的事情放到一个文件中。下图是完成这个项目之后的结果，当然也有些行动被我取消了。 组织 回顾 来总结一下这次讲座之后的想法。 从辅助工具上来说，自己的 PPT 水平很差，也就认为不能把时间都放在准备 PPT 上。所以思考过其他一些工具，但都不是很乐观。考虑过两种类型的工具，可以生成 ppt 的软件，比如 Marp（利用 markdown 生成）以及 org-ioslide （利用 org-mode 生成），这两个工具很难实现图文混排。另外，就是幕布的演示模式，使用时需要展开和折叠大纲，有点麻烦，也容易导致出错。最后，还是只能硬着头皮去做了一个简单的 ppt 。 幕布演示模式 从演讲效果来说，内容是很丰富的，但是我的演讲能力很差，之前在上面提到的问题，都暴露无疑，还是需要加强锻炼。也由于时间上的一些原因，我的准备不够充分，试讲次数也太少了，导致节奏控制也不好。还要对浪费17计算班同学的时间，表示一些歉意。 点击查看我使用的ppt，文字形式的分享等我有空再来整理吧。 参考 自然式计划模式 【搞定GTD】用iPhone打造GTD实践1年后的心得体会 - 申龙斌的程序人生 - 博客园 《搞定1:无压工作的艺术》 · Issue #136 · JimmyLv/jimmylv.github.io","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"zufe","slug":"zufe","permalink":"https://xiang578.com/tags/zufe/"},{"name":"gtd","slug":"gtd","permalink":"https://xiang578.com/tags/gtd/"}]},{"title":"采访一则","slug":"interview","date":"2018-04-21T05:34:05.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/interview.html","link":"","permalink":"https://xiang578.com/post/interview.html","excerpt":"","text":"前一整子学院内有关部门对我进行的采访，正好放在这里记录一下。【】这个内部是我现在补充的内容。 Q1：在大学学习的这几年里，可有一事令你感触颇深，对你影响巨大？能否与大家分享一下？ 【没有回答，现在很难被一件事情所影响】 Q2：很少有人在人生的道路上总是一帆风顺，你可曾遇到过难以解决的挫折？你是怎样克服它的？ 【没有回答，向前看，不回头】 Q3：学长一直以来都是成绩优异，然而大学生活必定不能只有学习，学长是如何合理安排自己的时间的呢？ 最简单的方法是善于利用日历软件。每学期将自己的课表以及固定时间要做的事情（比如每周二、四下午都要训练）输入到软件里面，作为已经分配的时间。每周、每天将必须要完成的任务找到一个截止日期前的时间块填入。复杂一点可以去尝试学习 GTD。 【比起和安排时间，更重要的是控制欲望】 Q4：作为信工学子，大家都知道你的代码十分厉害，能否跟学弟学妹透露一下学习的妙招？ 平时多写算法题目或者是多做项目。刚开始可以从书籍、博文、mooc等渠道找到自己喜欢的内容模仿，然后自己在这个基础上尝试添加的代码。 【上一篇博文中有过类似的感叹，其实我的基础也很不扎实】 Q5：学长曾经参加过许多国家级乃至国际的竞赛，初次参加大型竞赛时可会紧张？你是如何克服这些心态上的问题的呢？ 一般来说，编程比赛时间比较长，相对来说是比较自由的。紧张时可以尝试先趴着睡一觉，然后等到心情平复之后，再发挥出自己的正常水平就好了。不要有太多成绩上的压力或者是幻想。 【刚开始以为自己参加比赛能有奇迹发生，后来开始期待队友能 Carry。最后发现只能做自己，顺其自然。】 Q6：学长刚毕业就接到滴滴出行的offer，令人羡慕不已，每个人都想变得优秀，学长可不可以分享一下努力的过程？ 凡事预则立不预则废。 很多时候，我们想要做的事情都是有前人做过的，所以可以从网上收集大量的资料。对于找工作来说，很多人会在博客或者论坛上分享面经，通过收集这些，看一下企业一般会考察本科生哪些方面知识，以及什么样的经历会给自己加分。然后朝着这些方向去努力。 【这些话是我自己经历找工作痛苦之后，才总结出来。人生就是一张信息战。】 Q7：图书馆是许多同学经常待的地方，你最喜欢哪一类的书籍？对哪本书印象最为深刻？ 好像没有特别喜欢的一类书，大概什么类型的书都看，一般是了解到某一本感兴趣的书，就会去图书馆找一下。另外，我大部分时候去图书馆都是看报纸和杂志。 这么多时间下来，看完之后收获最大的书是《搞定I：无压工作的艺术》。这本书是 GTD( Getting things done) 的入门读物，主要介绍了一套个人任务管理的方法。如果你感觉生活杂乱无章，可以尝试学习这套方法来规划生活。 【不知道为什么，我一直想给身边的人推广 GTD 的原则。】 Q8：在学习与代码之余，你有什么十分热爱的东西，并为它坚持不懈吗？能否与大家分享一二？ 写日记。这个爱好从高中开始，最初是写在笔记本上，由于不会搞手账之类的东西，日记的形式很单调。后来受到《晨间日记的奇迹》启发，开始使用为知笔记来记日记，可以同时记录今天拍摄的照片或者是视频。另外，每个日期开一篇笔记，每年都在同一篇笔记上更新，可以手动实现去年今日功能。 【记日记可以发现自己吧】 Q9：每个人都拥有属于自己的生活习惯，学长有哪些良好的生活习惯能与学弟学妹分享一下吗？ 充分挖掘手机或者电脑的功能来体验数字化生活。推荐一些我自己使用的软件给大家吧。 个人任务管理：OmniFocus 笔记软件：Evernote、Wiznote 时间记录：Toggl、Forest 记账：MoneyWiz 如果想要具体了解这些软件如何提高工学学习效率，推荐大家去少数派上找相关文章阅读。 【其实我想说的是一个「数字游民」的概念，通过使用这些优秀的软件，记录自己的成长以及提高自己的效率。】 Q10：最后，请学长给学弟学妹们分享一段寄语，鼓励一下大家吧！ 大学时光是短暂的，希望你们能早点找到自己想做的事情，并为之奋斗。 新闻稿的链接 查看更多我的随笔，请点击 ZenTalk","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"zufe","slug":"zufe","permalink":"https://xiang578.com/tags/zufe/"}]},{"title":"构建之法 第 0 次个人作业","slug":"build-to-win-homework-0x00","date":"2018-04-16T04:01:41.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/build-to-win-homework-0x00.html","link":"","permalink":"https://xiang578.com/post/build-to-win-homework-0x00.html","excerpt":"","text":"最近重新看邹欣写的构建之法，发现他的第一章课后作业很有意思，所以决定自己也来写一下。有关的作业说明在2017BUAA软工助教 第0次个人作业 - ChildishChange - 博客园中。 结缘计算机 你为什么选择计算机专业？你认为你的条件如何？和这些博主比呢？（必答） 计算机是你喜欢的领域吗？是你擅长的领域吗？ 你热爱这一专业吗？你对计算机的热爱是怎样的？仅仅是口头的吗？ 与大部分人差不多，在高考前我也没有对规划未来。考的也不是很好，没有太多的选择。所幸自认为还是有学习的劲头，想着大学找机会去转专业一波。当时也没有太在意专业的填报，最后机缘巧合来到了财经读计算机。大一下时，确实得到了转专业的机会。在面试时，突然感觉自己对转过去的专业也没有太多的好感，就和面试我的老师说放弃转专业，他们只能口头上鼓励我，男生学计算机也不错。大概就是这样子，就在计算机专业混到了大四。 比起这些博主来说，我没有他们那么强的主观能动性吧，能自觉地了解学习计算机各个领域的知识。而我从除了学校内的课程安排之外，自己了解其他知识都是浅尝而止。从大一开始差不多就点弯了天赋树，基本上课余时间都在写算法题目，参加各种各样的算法竞赛。幸运地拿了一些大大小小的奖，但是这也导致在知识面上和企业认为的计算机专业学生有很大的区别。目前来说，正在通过自己的努力，加强学习基础知识，希望能补齐自己的短板。 记得之前有一句话说，「数学是神造学科，计算机是人造学科」。在三年多的学习中，无时无刻不感受到前人思维的巧妙与严谨。比如 CPU 中的加法器，能通过简单的元器件和门电路搭配实现二进制加法以及进位。另外，计算机专业也能及激发最深处的好奇心。当你查阅相关的文档，实现软件的某些功能时，是否能想起童年某个午后拼成的模型？ 在计算机系学习 你对你的大学生活有什么想要吐槽的地方吗？你理想的大学教育应该是什么样子的？跟学校给你的有什么区别？比较你在中国大学的经历，你的老师和学校能做到和国外那样吗？如果不能，请分析一下为什么。（必答） （8.26修改） 迄今为止，你写了多少代码，描述你做的最复杂的软件项目/作业。（必答）（8.24修改） 科班出身和北大青鸟有什么区别？ 速成的培训班和打基础的大学教育还有mooc之间有区别吗？ 学线性代数和概率论的时候，你是否有过这样的疑问「我们为什么要学这么多数学，这和我们的计算机有关系吗」，你现在是否还有这样的疑问？对这个问题，你有自己的解答了吗？那么其他学科呢？ 对于专业来说，我记得辅导员曾经在年级会议上讲过这样一段话：「有些人和我说，为什么高等数学这么难？这些书都是在大学没有扩招前编的，以你们的成绩当时都可能上不了大学。而且，我们学校搞计算机系，只是发现有很多教计算机基础的老师才开设的。」所以，也可以知道这个专业在学校的尴尬地位。 托尔斯泰说「幸福的家庭总是相似的,不幸的家庭却各有各的不幸」。反而对于计算机专业来说，感觉「普通学校的问题总是相似的，强势学校的优点却各有不同」。蒋宇东在梦断计院 为梦前行中写道他自己的学校计院有三大问题：1.学生基础薄弱 2.学风不正，溃散成性 3. 实践环节薄弱，人才位置错放。之前，我也一直在想一件事情：是老师水平太差学生懒得学还是学生水平太差还是老师懒得教？看到上面的三个问题才明白，冰冻三尺非一日之寒，事物发展都是相辅相成的。学生大部分都是过来享受大学美好生活的，对于有挑战的专业基础课比如编译原理、离散数学等都会用脚投票让他离开课程表。有幸能勉强开下去的课程，也会被改成能突击几天通过的样子。 记得大二导师前往台湾进修前找我谈过一次话。他希望我能自己养成一些良好的学习习惯，多去钻研计算机专业知识。并且还说，学院每年毕业的学生中，没有超过十个学生明白计算机是怎么一回事？三年多的时间里，我也接触过网上的一些 Mooc，不过差不多完全听完的只有浙江大学陈越教授的数据结构和吴恩达教授的机器学习。能坚持学习完这两门课，背后都是兴趣使然。数据结构是 ACM 比赛中很重要的一个知识点，当时自己学校还没有开设这门课，而且看书自学也不是很懂，所以就花大半学期的时间在网上去学习。机器学习则是希望未来能从事机器学习相关方面的工作才了解的。 陈越教授放在网上的数据结构课程相对于浙大内部来说，最大的变化是将教学语言从英文变成了中文，而且也适当的删除了部分内容（不是很确定？）。之前也在知乎上看到浙大计算机专业的课程安排，主干课程选用外国原版教材，以及使用英文授课。另外陈老师在讲解相关算法时，只介绍原理和思想。相关的实践需要你课后花时间进行，并且要求提交程序设计平台上进行黑箱测试。对比后来在自己学校开设的数据结构课程中，直接给你展示代码，也不强求你写的代码能正确运行，只要粘贴在实验报告中提交即可。《机器学习》的课程展示了美国教育的特点，每一周的课程都会包含一个小 quiz ，以及在课程中插入适量的编程大作业，学习的困难程度也就可想而知。 这些授课方式的不同，我认为是不同大学对于教育的理解不同。对于浙大之类的重点大学来说，所给予学生的是精英教育，侧重于培养学生的计算机科学基础。而下面的学校更多是大学教育成为通识教育的产物，本应该培养专业型人才，传授计算机技术，却由于种种原因没有这样开展。Mooc 的出现，打破了原有的时间、空间的限制，让我这样的普通学习接触到了国内外顶尖大学的教育资源，有利于教育公平。但失败之处也在于它的灵活性，不得不承认需要很强的自我约束能力才能完整地学习完网上的课程，不知你还记不记得那个当时看得热血沸腾的课程时，被你遗忘在哪个角落了？ 最后，我觉得 Mooc 还缺少的同辈压力。吴军博士在《大学之路》中写道「当许多聪明、求知欲强、具有同情心而又目光敏锐的年轻人聚到一起时，即使没有人教，他们也能互相学习。他们互相交流，了解到新的思想和看法、看到新鲜事物并且掌握独到的行为判断力。」这大概就是学什么不重要，重要的是和谁一起学吧。一些 Mooc 虽然提供在线社区交流，但还是没有线下那种碰出火花的感觉。 对比完这些，计算机专业学生的另外一个主题不可能忽视——“自学”。最开始的作业说明给出的很多参考博文也说明了这一点。我印象最深的一个方法是徐宥在掉进读书的兔子洞中写道的「一字不漏敲入一本书的程序成了我推荐别人学习语言的最好办法」。古语读书百遍其义自见，计算机是一门实践的学科，只有在实践中才会学习到技术的本义。目前，自己在看《机器学习实战》时（相关代码和读书笔记见参考链接5 ），也差不多用的是这个方法，将书中提到的机器学习算法一个字符一个字符地敲进编辑器，一次又一次地调试运行直到出现满意的结果。 在这样的学习中，我也可以回答这一部分的最后一个问题，我们为什么要学这么多数学，这和我们的计算机有关系吗？对于机器学习来说，本身是建立在概率论和线性代数的基础上，毕竟解释机器为什么能学习都需要用到霍夫丁不等式等相关知识。数学是一种工具，通过数学能将很多的计算机实践经验理论化，以此得到更好的发展。对比功力性很强的培训机构来说，接受科班教育的结果是有机会受到严格的数理化训练，这也能间接决定你在这个行业从事工作的层次和上限。 未来规划 对于你未来在IT行业的发展，你有什么样的梦想或者未来想从事什么样的工作？你准备怎样来规划你技术道路，职业道路和社会道路？（必答） 你对于实现自己的梦想已经做了或者计划做什么样的准备？ 你们马上就要面临实习了，你打算在企业内实习还是在实验室实习？ 实习经验究竟有多重要？是否需要马上开始积累实习经验？ 前文也提到了，以我的目前的情况来看，大学中接受的教育和自己所学的技能都是和市场脱节的。所以，前一段时间也很迷茫，自己能做什么。所幸看到之前公司的同事所写的知乎专栏文章「谈“圈外”在校生如何更靠谱的拿到大厂算法/ML实习机会」（不知道为什么被删除了，所以无法提供链接），大意就是可以曲线救国，先就业再择业。 所以，我还是推荐出去实习的。不仅是走出象牙塔，感受一下工业界的需求。更重要的是大一点的企业都会安排专人对实习生进行相应的指导，使其能更快地朝工程师方向发展。 参考 现代软件工程 第一章 【概论】练习与讨论 - SoftwareTeacher - 博客园 2017BUAA软工助教 第0次个人作业 - ChildishChange - 博客园 蒋宇东：梦断计院 为梦前行 徐宥：掉进读书的兔子洞 xiang578/MachineLearningAction: 《机器学习实战》","categories":[{"name":"文渊阁","slug":"文渊阁","permalink":"https://xiang578.com/categories/文渊阁/"}],"tags":[{"name":"shcool","slug":"shcool","permalink":"https://xiang578.com/tags/shcool/"}]},{"title":"使用 TaskPaper 实现 3 Things","slug":"how-to-use-taskpaper-to-do-3-thigns","date":"2018-04-08T07:51:23.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/how-to-use-taskpaper-to-do-3-thigns.html","link":"","permalink":"https://xiang578.com/post/how-to-use-taskpaper-to-do-3-thigns.html","excerpt":"","text":"这几个月，一直在尝试 GTD 方法。有没有改善生活不知道，反正是软件尝试了一个又一个。前几天，又接触到了一款纯文本任务管理软件——Taskpaper。简单的尝试了一下，发现可以将这款软件结合进入 3 things 体系。 3 Things 看过很多时间管理的方法之后，才明白时间是不可以管理的。一个人独处的时候，明知道有很重要的考试需要准备，但还是会不知不觉地打开手机刷微信。很多时候，需要管理的不是时间，而是欲望。所以，在自我控制力很弱时，时间管理会加重你的疲劳感。 说到底，一个人的精力优先，能做的事情也很少。有一个很有名的原则叫做“二八原则”，套入到这里来就是，我们的生活是由 20% 的关键任务 + 80% 的普通任务组成。在理想的状况下，我们应该将 80% 的时间投入到那些 20% 的重要任务中。如何安排这 20% 的任务时，3 Things 的体系就孕育而生了。3 Things 指的是每天优先挑选出最关键的三件事情，优先处理。这三件事情的选择，需要结合你自身的目前情况、长久目标而决定。对于这个选择，你可以问自己，如果完成了其他事情，没有完成这三件事情，这一天不合格吗？如果完成了这三件事情，而没有完成其他事情，这一天合格吗？ Taskpaper 如同前文所说，Taskpaper 是一款纯文本软件，所以界面非常的简洁。下图右侧就是他的主界面，和大家经常使用的文本编辑器没有太大的区别。软件通过一些语法来实现任务管理，比如 : 实现一个项目，用 - 实现一条任务，再使用 @开头实现标签。软件更强大的地方在于，可以自定义 CSS 文件，所以可以很轻松地将一些标签搞上不同的颜色。 有点遗憾的是，TaskPaper 只用 macOS 版本，其他平台只能通过文本编辑器+插件的方式实现，比如 Sublime Text 中有一个能实现类似功能的插件 aziz/PlainTasks: An opinionated todo-list plugin for Sublime Text editor (version 2 and 3)。 系统工作流 其实，能想起来用 Taskpaper 做三件事情，是受到了《18分钟》：如何用18分钟变身效率达人？| 狗熊月读64影响。视频中，大狗熊老师介绍了一个习惯，会每天在文本文档中，列出5项需要完成的任务（对的，比三件事情多出了两项），而且会对事情进行估计，包括四象限中的位置、预计完成时间以及和自己人生的那个目标有关。 四象限法则是时间管理中的一个重要理论，它将我们需要处理的事情分成了下图显示的四类，并且指导在日常生活中应该关注重要但是不紧急的事情，也就是第二象限。 一般的 GUI 任务管理软件，列出事情还算比较方便，但是做评论却没有那么便利。TaskPaper 比起在文本文档中直接写，体现出强大的整合能力。比如，我可以选定 MachineLerninginAction 的标签，可以快速的将我之前安排过的相关任务整合到一起。 所以，每天晚上睡觉前的 10 分钟，我会回顾一下今天完成的事情，并给第二天安排需要做的 3 Things。确保第二天醒来，我可以直接开始做当天的重点。达到从头到尾、逐一处理、不避不拖、一次一事的 GTD 执行境界。 参考 纯文本任务管理思考 · Issue #11 · xiang578/xiang578.github.io 重回纯文本，兼谈电子化时代 GUI 工具的弊端与反思 GTD · Issue #288 · JimmyLv/jimmylv.github.io TaskPaper 使用指南的所有文章 - 少数派 IMG_0233","categories":[{"name":"七年之旅","slug":"七年之旅","permalink":"https://xiang578.com/categories/七年之旅/"}],"tags":[{"name":"gtd","slug":"gtd","permalink":"https://xiang578.com/tags/gtd/"},{"name":"taskpaper","slug":"taskpaper","permalink":"https://xiang578.com/tags/taskpaper/"}]},{"title":"「只言片语」| 2018/03/07","slug":"excerpt-2018-03-07","date":"2018-03-07T13:13:09.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/excerpt-2018-03-07.html","link":"","permalink":"https://xiang578.com/post/excerpt-2018-03-07.html","excerpt":"","text":"多年之前阅读《读者》时，最喜欢杂志中的语丝栏目。所以准备向其致敬，收集每周我在阅读中收获的只言片语。 1. 学习 “当我在你们这个年纪，有段时间，远离人群，独自思索，我的人生到底应该怎样度过。某日，我偶然去图书馆，听到泰戈尔的演讲，而陪同在泰戈尔身边的人，是当时最出名的学者。那些人站在那里，自信而笃定，那种从容让我十分羡慕，而泰戈尔正在讲对自己的真实有多么重要，那一刻我从思索生命意义的羞耻感中释放出来，原来这些卓越的人物也会花时间思考这些，他们也觉得这些是重要的。” 语出《无问西东》的吴岭澜给学生介绍泰戈尔时。“自信而笃定”，简单五个字，回味无穷。 凡是机器或者人工智能能够代替的能力，就是不值得花时间去训练的能力；凡是不能迁移到其他领域的能力，也是不重要的能力。 机器学习的火热带来思考，如何才能避免自己在未来被替代。 只要你停下来认真思考一下就会明白，投资比特币简直愚蠢至极。这是不安好心的坏人们鼓吹的疯狂泡沫，引诱人们追求不劳而获、一夜暴富的妄梦…… 你知道，黄金之所以具备流通价值，是因为人类根本无法轻易地得到黄金，因此黄金才显得稀有。相信我，人类完全有能力创造出更多的比特币——尽管他们会告诉你由于种种限制无法这么做。千万不要相信这些鬼话，当好处足够多的时候，坏事就会发生。 根据所学为数不多知识判定，比特币之类的数字货币已经成为24小时不断线的赌场 2. 理性 总体来说，给我的感悟就是老罗成长了，做实事才是真正锻炼人的，相比罗振宇哪个挤眉弄眼心高气傲的样子，罗永浩讲起来自己的产品那种娓娓道来朴实坦诚的样子让我能容易喜欢。 语出知乎网友评价罗振宇对罗永浩的专访。记得有一段时间，疯狂地看罗永浩的演讲视频，完全被他的行为所喜迎，记得印象最深的一句是通过干干净净的赚钱，让人相信干干净净的赚钱是可能的。可不知道为什么不喜欢看《罗辑思维》，现在找到答案了。 对熟人宽容大度，对陌生人零容忍。 有另外一种印象：坏人远在天边，好人都近在眼前。 罗辑思维搞了四年，知乎上大家骂了四年。四年中罗辑思维从最初一个靠打赏的小书童，成长为一个创业者，继而成长成一个做平台的知识服务提供商，甚至邀请了一众精英在主流媒体和新媒体上大搞跨年演讲，钱赚得盆满钵满，影响力也是一日千里。而那些罗辑思维的黑粉，四年来又收获了什么？罗胖的一句话，黑粉真的可以好好琢磨琢磨，原话忘了，大概的意思是：我不会再否定一个人，因为否定他，是关闭了我自己观察世界的一扇门。 知乎网友对得到的评价。因为这个，开始接触一些自己之前无名抵触的东西，还是有收获的。 毕业20年之后，去参加一下同学聚会，不是要你去攀比人生成就，也不是要你去利用同学资源，而是看看人生20年长跑之后，决胜千里的东西究竟是什么，输掉人生长跑的东西又是什么？ 我想不起来有什么真正的机会是稍纵即逝，追悔莫及的，我想不明白为什么人们非得那么迫不及待，惶惶不可终日。错过了成为 Netscape、Yahoo 的机会，你可以成为 Google ；错过了成为 Google 的机会，你可以成为 Facebook ；错过了成为 Facebook 的机会，你可以成为微信，成为滴滴，成为 ofo。如果区块链真的是一个大机会，它怎么可能会是一个手慢无的机会？况且，有哪个真正的大机会，从一开始就孕育出了空前绝后的大项目？有哪项有前景的伟大技术，不是慢慢成就它的伟大？ 区块链或者数字货币火热的反思 3. 欢喜 世界于你而言，无意义无目的，却又充满随心所欲的幻想，但又有谁知，也许就在这闷热令人疲倦的正午，那个陌生人，提着满篮奇妙的货物，路过你的门前，他响亮地叫卖着，你就会从朦胧中惊醒，走出房门，迎接命运的安排。 泰戈尔《爱者之殆》，可能现实中叫醒你的就是你自己。 好看的皮囊是确实好看，有趣的灵魂是自以为有趣。 取“好看的皮囊千篇一律，有趣的灵魂独一无二”之反 4. 番外 Timetrack：计时软件，点击图标就能开始计时，配合 URL Scheme 直接跳转到你预设的 App 中。 IMG_0233","categories":[{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"}],"tags":[{"name":"timetrack","slug":"timetrack","permalink":"https://xiang578.com/tags/timetrack/"}]},{"title":"解决 OmniFocus 中 Applescrpit 脚本输出文件中文乱码问题","slug":"AppleScript-encode-error","date":"2018-03-05T08:45:30.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/AppleScript-encode-error.html","link":"","permalink":"https://xiang578.com/post/AppleScript-encode-error.html","excerpt":"","text":"在学习OmniFocus 入门与进阶 - 少数派教程中，了解到了一个 Applescript 脚本 Weekly project report generator（周报生成器）。这个脚本可以将 OmniFocus 中前七天活跃的项目以及完成的动作导出成为一个.md文件。这样子，可以大大减轻周回顾时回忆本周完成任务的压力，也可以将生成的文本粘贴到周报中使用。 但是当我第一次运行这个脚本时，生成的文件中所有的中文字符都变成了问号。 错误 由于，之前没有接触过中文显示为问号的问题。所以，与少数派教程的作者联系，他很快给我回复了一份邮件，并给出两个建议。 回信 按照邮件中的建议，我进行了下面三个尝试。 Sublime Text中安装转化编码方式的插件ConvertToUTF8，最终发现无法识别出生成文件的默认编码。 使用系统自带的文本编辑打开时，还是显示编码错误。 利用homebrew安装了enca软件，使用enca -L zh_CN file无法检测出文件的编码方式。 无奈这些方式都没有找出问题的所在，只好打开脚本编辑器，查看这个脚本是怎么写的。 如下图所示，该脚本将需要输出的字符串指定为Unicode text 格式，用 write 将这个字符串写进文本中。 根据之前的编程经验，输出文本一般都能指定编码方式。所以又用 Dash 来查看 AppleScript 中 Write 的相关语法。 最终，找到 as 可以用来指定编码方式，成功解决乱码问题。 IMG_0233","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"applescript","slug":"applescript","permalink":"https://xiang578.com/tags/applescript/"},{"name":"omnifocus","slug":"omnifocus","permalink":"https://xiang578.com/tags/omnifocus/"},{"name":"dash","slug":"dash","permalink":"https://xiang578.com/tags/dash/"}]},{"title":"GTD 实践 1 个月心得","slug":"gtd-one-month","date":"2018-02-28T08:03:32.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/gtd-one-month.html","link":"","permalink":"https://xiang578.com/post/gtd-one-month.html","excerpt":"","text":"不知不觉混到大四，突然感觉有一堆事情把我压的喘不过气来。所以就想学习一种任务管理方法。GTD 正好在这个时候重新进入我的视线中。根据 wikipedia 上的介绍：GTD，全称 Getting Things Done ，中文一般翻译为搞定，是一种行为管理的方法，也是戴维·艾伦写的一本书的书名。GTD 的方法根据一个理念，大脑是用来思考的，好比计算机中的 CPU。所以，我们需要把要做的事情从大脑中移除，放在一个可靠的外部系统中，这就是 GTD 系统。 根据网上的教程，学习 GTD 第一步是阅读戴维·艾伦的书《搞定I：无压力工作的艺术》（读书笔记链接）。这本书，其实我在大一就借来看过，但是由于翻译的实在是太差了，所以没有看完就还回去了。所以这一次重新学习时，我选择从阅读《小强升职记》（读书笔记链接）开始，这一本书通过讲述一个职场新人小强的虚拟故事，介绍了很多任务管理、项目管理、个人心智成长的方法。最后，还阅读前人实践的心得。貌似 GTD 这几年不是很热门，很多文章都很旧了，没有太多的实际意义。我只推荐两个系列文章，第一个是申龙斌的博客园博客搞定GTD - 随笔分类，他践行 GTD 五年多的时间，写下很多心得体会，是很不错的参考资料，目前他选择在微信公众号（申龙斌的程序人生）中更新为主。最关键的一点，他也是一名007er。第二个是滴答清单与GTD时间管理知乎专栏，从名字中可以看出这个系列的特点。不要因为你不用滴答清单而错过里面的内容，毕竟 gtd 方法是通用的。 开始打造 gtd 系统之前，还需要挑选一些工具。《搞定》书中是介绍的是利用纸笔和篮子来完成gtd，这是一个不错的方法。但是，结合实际，我没有太多的实体材料，所以选择使用软件。前几年，我也使用过很多任务管理软件，有 2Do、奇妙清单和滴答清单等。考虑到目前使用的组合是 iPhone + MacBook Pro，所以我选择了 OmniFocus 。第一次打开这个软件简直让我头皮发麻，功能很强大，但是需要深度学习才能运用自如。我是跟着这一个教程OmniFocus 入门与进阶 - 少数派，才掌握初步的使用方法。OmniFocus 吸引我的是任务过滤功能，也就是透视。简单来说，通过定义一些规则，来展现你的任务。 介绍 gtd 时，我将结合下面这张图，来讲讲我是怎么做的。一般来说，gtd包括 5 个步骤：收集、厘清、组织、回顾和执行。 （一） 收集 对应图中开始的一步，将材料放入系统的工作篮（inbox）。这里的材料是一个很广泛的概念，可以是你要做的事、想读的一本书或者是要学的技能等等。根据最开始的介绍，成为一个可靠的系统，需要将你所有的材料都放进这个inbox。所以，你在打造系统前，需要找个空闲的时间（书中说是2天）和过去做一个告别，好好的把任务整理出来。 在第一次收集前，我利用幕布把自己的材料来源整理了一下，主要分布在手机和电脑的一些软件中。根据这张大纲的顺序，最后整理完成时大概有300多件。完成这一次整理后，再有要做的任务记录到inbox中，自己定时清空就完成这一步了。 GTD整理思考 （二）厘清 在完成上一步收集后，我们需要开始第一次过滤这些事情。从inbox的顶端开始一件件评估我们收集的事情，对于每一件事情，考虑这件事件需要现阶段去做吗？对于不需要去做的事情，我们需要将他们分成三类：垃圾（没有任何价值，直接抛弃）、将来/也许（以后有机会做，放在一个专门的列表中）、参考资料（需要时再来参考）。 （三）组织 这是GTD中最复杂的一步。我们把需要现阶段去做的事情继续分类：第一类是行动，只需要一步完成的任务，第二类是项目，需要执行多个步骤才能完成的任务。首先是项目，书中提供了一种分析项目的方法：自然项目分析法，通过这个方法可以有效的把项目搞明白。当然，对于我们这样的新手，虽然有了这个强力的武器，但还是不容易一开始就将项目拆解的明明白白。这时候，我们只需要考虑一个问题：下一步行动是什么？比如说，如果你看了我的文章，也想要学习 GTD ，这时候就有了一个项目：学习 GTD。由于你更本不知道这是什么东西，所以无法分析项目，所以只能考虑下一步行动：利用搜索引擎来搜索 GTD 相关的介绍。完成这个行动之后，下一步行动就可能变成去买几本书了。通过这种每次思考一步的方法，来完成大的项目。说完项目，接下来就是动作。这里又诞生了一个很厉害的方法：两分钟原则。如果在组织的时候，这个动作可以在两分钟之内完成，那就直接去完成它，不用下面的步骤，特别声明这里的两分钟指一种很短的时间。对于其他动作，你还要考虑是否可以由其他人完成，如果是直接去委托他人完成。对于有时间限制的事情，比如明天下午开一个会之类的，就要写进日程表来提醒了。如果都不是，那么就放进你的文件夹中，等待后续执行。 （四）回顾 可能有些奇诡，事情还没有执行，怎么就开始回顾了。是的，书中就是这么安排的。这里的回顾是指，看看你做了什么，有什么需要反思的。主要用两种方式，第一种：完成项目之后回顾；第二种：定时回顾，可以是日回顾、周回顾、月回顾以及年回顾。目前，我主要做的是日回顾和周回顾，日回顾主要是简单看一下做了什么，周回顾有专门的检查清单，以及会写回顾记录。 （五）执行 这里就要认识到任务管理的本质是管理任务，而不是帮你完成任务。所以，还是需要你去执行。GTD 提供的只是一种简单的挑选方法。当你没有空闲时，考虑当前的情景（在什么户外还是家中，有手机还是电脑），有多少时间可以用，当前精力（精力旺盛还是昏昏欲睡）以及有没有重要的事情需要做。综合这些情况，找出需要做的事情，然后去做吧。 最后，根据一些人的说法。GTD 执行一年之后，才能算是懂了，上面就是我这个执行一个月的菜鸟的见解。由于时间有限，GTD 的六个层次、三个原则、自然分析法都没有写到。更关键的是，也没有结合 OmniFocus 来写我是具体怎么执行的。总结一下，这一个多月来，我第一次认识到要做的事情居然有这么多，真是时不我待。用原来的方法确实能做很多事情，但是接触 GTD 之后，我开始关注目标制定以及回顾方面的实践。 IMG_0233","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"omnifocus","slug":"omnifocus","permalink":"https://xiang578.com/tags/omnifocus/"},{"name":"gtd","slug":"gtd","permalink":"https://xiang578.com/tags/gtd/"},{"name":"evernote","slug":"evernote","permalink":"https://xiang578.com/tags/evernote/"},{"name":"mubu","slug":"mubu","permalink":"https://xiang578.com/tags/mubu/"}]},{"title":"小学十周年同学聚会","slug":"primary-school-classmate-10th-anniversary-party","date":"2018-02-21T06:58:10.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/primary-school-classmate-10th-anniversary-party.html","link":"","permalink":"https://xiang578.com/post/primary-school-classmate-10th-anniversary-party.html","excerpt":"","text":"不知不觉中，小学毕业快要十年了（2008-2018）。前几个月，就有同学开始策划组织聚会。就这样把我的思绪拉回十年前的。 学校校门 看上面的图可以知道，学校的名字叫做——晓村小学。值得注意地是，晓村不是地名。根据一般的套路，这样取名都是有故事的。查阅百科可以看到下面的介绍：「晓村小学位于沿海的椒北平原，创办于1929年2月26日，当年，中共地下党员、著名教育家林迪生、地下党的外围组织—“乙丑读书社”社员陈鹤亭，留日学生陈诗斋，当地豪绅陈孔彰等发起创办了这所红色的学校。当年他们效法南京晓庄师范“教学做”合一的模式创办,故把把学校取名为晓村小学。随后，陈叔亮、徐明清等一大批地下党员相继来晓村任教，宣传马克思主义，传播革命真理，创造了晓村校歌，颁布了晓村校训，办学方向十分明确。大革命时期，他们以学校作为地下党活动场所，曾先后有十多位地下党员驻足晓村小学，从事革命活动并创造了许多惊天动地。可歌可泣的辉煌业绩，被誉为“荒原里的灯塔”，享誉椒江南北两岸。」所以，这是一所有革命传统的学校。目前来说，学校以及异址重建，和百度百科中的介绍还有关系的只有校名和校歌。 校歌 学校旧样 如果让我来评价的话，说到底是一所农村学校。记得我自己当时的时候，教育水平是很差的，不知这么多年过去了有没有提高。当年的校舍也很朴素，但是前几年，也进行了现代化改造，总的来说比我们当年的条件不知道好到哪里去。 教室旧样 聚会定于大年初五举行，下午一点的时候我就差不多达到了学校。找到原来班级的时候，也有很多人在里面了，很多同学我们都十年没有见过了，却发现还能叫出他的名字。和各位老板聊到了3点左右，才开始活动。 学校新样 翻出下面的老照片，后面一排的左边罗伟迪大佬已经出国工作。中间的陈子翔同学没有来参加聚会，也不知道近况如何。右边就是我。前面一排左边的是陈凌超，右边是罗飞。见到他们，发现他们是越来越强壮。说起来，我还和陈哥一起演过小品。陈哥当时演的角色，用现在来说就是女装大佬。演完之后，我还听见当时有人说“原来这个是男的”。可见我陈哥功力深厚。罗哥是我同桌，被我坑过很多次。 WechatIMG26 教室里大概来了三十多位同学以及三位老师。在班长的提一下，我们首先进行了自我介绍，然后三位老师讲了一些话。 第一位是六年级那年教数学的郑必君老师。还记得当年最后一节数学课时，老师讲起自己的小学同学，很多人到那时候都没有再见过面了。谁知道，再次见到他也是十年后了。他又讲了自己的其他故事，82年高中毕业，高考失败之后，顶替老爸过来教书，很多同学进行高复，现在都当了官。自己由于有了工作，便没有继续去高考了，安心从事小学教育工作。当有同学和他联系举办同学会时，非常高兴，特地找出毕业时拍的照片。 第二位是教过我最多的数学陈平芬老师。最早是在二年级的时候，当时教我的数学老师怀孕了，然后她过来代课。之后一直到五年级时，她自己怀孕了，由其他老师代课，就再也没有教过我们了。陈老师回忆自己刚开始教书时，就像我们现在这么大。没有太多想法，只想着怎么把我们教好。印象最深的是，有一次找我上去写题目，写完之后，看见我最后算错了答案，说了一句“太粗心了，昨天的试卷也这样算错了”。不知道，当时如果改正了这个错误，现在会在哪里…… 最后一位是四到六年的教授语文同时也是班主任的陈菊红老师。印象中，听写没有通过，我还抄过50遍的课后词组。 完成介绍之后，班长开始播放精心制作的十周年会议视频。视频播放的时候，点点停停，给我们介绍老照片中的每一个人。十年光阴，弹指一挥间就过去了。 教室新样 最后，在小雨中留下了一张合影。很可惜，许多同学由于种种原因，没有机会过来合影。也不知道剩下的时光。还可以留下多少张这样合影。 2018年照片 2008年毕业照 晚餐选在了一家海鲜餐厅，不过大家有些拘束，酒准备好了，都没有讲故事的人。反倒是吃完饭之后的抢红包调动了全场的气氛，微信群中出现的红包之后，伴随着激动和叹息，可惜我自己抢到了很多0分的红包。天下无不散的宴席，最终我们散了。下次再赴十年之约。 在这次聚会之前，我看到了下面的两段话。 毕业十年的同学聚会，大概是最“丑陋”的同学会。此时，人生的分水岭已清晰可见。同学已完全褪去学生时代的单纯。每个人都变得居心叵测，面目狰狞。华丽的包厢里全是人生的演员，华丽的衣服里全是生活的褶皱。同学们言语之间未必是炫耀，但我们会以为他在炫耀；同学之间未必没真情，但我们已经扭曲了真情。因为人到中年，纯真已经退却，人生尚未悟透；危机四伏，刀光剑影。 毕业20年之后，去参加一下同学聚会，不是要你去攀比人生成就，也不是要你去利用同学资源，而是看看人生20年长跑之后，决胜千里的东西究竟是什么，输掉人生长跑的东西又是什么？ 显然，这两段话是对于大学毕业来说的。但让我下定决心去参加聚会的，正是这“看一下决胜千里的东西究竟是什么？”十年间，很多人事业有成，很多人娶妻生子，还有一些像我这样还没有走出校园，如同新华字典中所说，我们都有光明的前途。现在回想起来，小学真是无忧无虑，每天上课听讲，写作业，老师的有些教育方法很野蛮，但也学到了知识。放学后，和同学去附近采桑叶养蚕，去池塘边钓龙虾，周末一起玩游戏王。回忆是美好的，可人终究要去看看世界有多大。 IMG_0233","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"school","slug":"school","permalink":"https://xiang578.com/tags/school/"}]},{"title":"007 不出局活动与我","slug":"007-and-me","date":"2018-02-13T07:32:02.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/007-and-me.html","link":"","permalink":"https://xiang578.com/post/007-and-me.html","excerpt":"","text":"最初看到 007 活动是在申斌龙的公众号中，当时没有什么在意，也没有深入了解。之后，我在知乎上的一个专栏文章中，看到了这个活动的详细介绍。吸引我“冲动”消费加入这个社群的是“每7天写一篇文章，连续写7年。”这种运作方式。简单的计算了一下7年是2555天（简化问题，不计算闰年情况），一共需要写365篇文章才能坚持到最后。而且只需要花369元，相当于你每写一篇就能赚一块多。 除了上面的计算，选择加入007活动的另外一个原因是，战友之间的互相点评。从自己的情况来看，在网上消费了很多文章、视频，很少会去留言以及发弹幕。自己写博客3年多以来，也没有收到多少有价值的留言。所以，除了坚持写作，更需要外部的力量来推动自己进步。 最后一个原因是，我相信了解一个人，最有趣的方式是通过不断地阅读他的文字，见证他的成长历程。 在83班的班级群中，我发了下面的自我介绍，现在贴在这里再介绍一下我自己。 12345678910111213【编号】007-6253【姓名】 Ryen【城市】 杭州【职业】 学生【公众号】 简书：学涯湖畔 博客：xiang578.com【标签】 【爱好】 阅读、研究和分享软件【愿帮助大家】 分享基础的算法知识，推荐能促进生产力的工具，主流的个人博客搭建【愿得大家帮助】 分享各自的生活感悟【从哪里得知的007】知乎【加入007理由】 坚持持续写作，从写作中获得成长的力量【对83班的期望】共同成长【7年后想成为什么样的自己】体会到GTD理念中的心如止水 前几天阅读到一个有意思的文章社群十五问 - 笨方法学写作，今天也在这里分享一下我的回答。 你的名字、地区、职业、长时间持续做的一件事、博客/公众号/简书 是什么？ Ryen，杭州，计算机科学与技术专业大四学生，即将踏入红尘。 大学阶段持续参加程序设计竞赛，博客上有一些相关的流水账：2017年上海 ACM/ICPC ECL-Final - ZenTalk、2017年西安区域赛 - ZenTalk、浙江省第十四届程序设计竞赛总结 - ZenTalk、2016年北京区域赛 - ZenTalk、第七届蓝桥杯决赛杂记 - ZenTalk 博客：ZenTalk 折腾WordPress、Hexo 算法博客：禅境花园 - CSDN博客 之间写的一些算法题解 简书：简书 - 学涯湖畔 如果你要讲一个让人记住你的小故事。你会讲什么？ 写纸质日记4年（高一-大一），目前写电子日记。尝试过晨间日记、时间日志。 在中考前100天，我准备每天写一篇日记，最终只留下的30多篇。在高考前100天，又有了这个念头，最终留下了100篇日记。 如果你要问社群中所有人一个问题，你要问什么？可特别指定三个人回答，被指定的人可自愿回答。 你获取信息的途径，存储的方式，复习的方式，即你是如何管理知识的？ 按照博客中的介绍，还需要补上价值观12问，但目前我还在重建价值观，所以以后有机会再来分享我的价值观12问回答。 IMG_0233","categories":[{"name":"七年之旅","slug":"七年之旅","permalink":"https://xiang578.com/categories/七年之旅/"}],"tags":[{"name":"7","slug":"7","permalink":"https://xiang578.com/tags/7/"},{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"}]},{"title":"重构笔记本系统","slug":"rebuild-notebook-system","date":"2018-01-13T12:22:26.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/rebuild-notebook-system.html","link":"","permalink":"https://xiang578.com/post/rebuild-notebook-system.html","excerpt":"","text":"几天前在手机上使用为知笔记时，发现已经完成创建1000条笔记的成就。记不太清楚这个账号是什么时候开始使用的，但是笔记的数量确实吓自己跳。原来已经这么多了，说句实话，很多内容也只是简单的收藏了一下，之后都再没有碰过，而且自己的笔记本分类很混乱，借这个机会准备重构笔记本系统。 知乎上面关于笔记本系统或者体系的讨论有很多，之前就是参考相关回答的，但都没有坚持搞下去。所以，这次的重构我找的是其他的参考资料，最主要的是陈华伟的知识管理训练营的第五、六两讲。说来也巧，陈华伟几年前在知乎上也分享过他构建笔记本系统的思路，当时我也看过，收藏了，但是到今天都没有按照他说的实践。 这里分享我在他知识管理训练营里面学习到的方法。 搭建笔记本系统分成三个步骤： 第一步，利用思维导图梳理自己的知识体系，明确知识获取的途径。 第二步，在笔记软件中建立相应的文件夹，将获取的知识存储到笔记本软件中。 第三步，在实践中不停的迭代思维导图以及文件夹，这一步是最重要的，因为没有人可以一步到位，完成一个完美的笔记本系统。 然后，我认为这个课程最有价值的部分在于如何给笔记软件中的文件夹命名。陈老师提出了一种综合杜威十进制分类以及Wikipedia分类的方法。将一个文件夹的名字分成三个部分，即 数字编号 - 四大分类 | 具体名称。其中数字编号是从杜威十进制（美国图书馆图书分类方法）中学习得到，通过数字编号，笔记软件可以自动对文件夹进行排序。这是我最佩服的一点，经常在图书馆学习借书，却没有想到学习这种方法来分类笔记本。四大分类是参考Wikipedia，将所以的文件夹分成工作、学习、生活、兴趣四个维度。最后具体名称就是文件夹内放的笔记相关的命名了，我想大部分人之前也才做到这一步吧。 通过这种方法之后，我快速的整理出了新的笔记本系统。 笔记本系统v1.1 如果你也想学习这种方法，建议去找上文提到的课程学习一下。 本文写作过程中，使用幕布搭建框架。幕布是一个简洁、高效的大纲软件。你可以通过我的邀请链接注册，并将免费获赠15天幕布高级版。","categories":[],"tags":[{"name":"wiznote","slug":"wiznote","permalink":"https://xiang578.com/tags/wiznote/"},{"name":"system","slug":"system","permalink":"https://xiang578.com/tags/system/"}]},{"title":"2017 迷茫","slug":"2017","date":"2017-12-31T04:40:45.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/2017.html","link":"","permalink":"https://xiang578.com/post/2017.html","excerpt":"","text":"每年年底，都有很多人在各种地方发表这样的总结，看得我煞是羡慕。去年也想学习2016 无限大中这位大佬这样好好搞一番。最终的结果是，写了一些之后，就放弃了，然后现在草稿都找不到了。今天，终于鼓起勇气，准备在图书馆好好总结一下2017年经历过的事情。那么多年以来，感觉今年经历的事情最多也最复杂，很多事情到现在都没有讲明白。 上半年大三，现在大四，今年在学校上的课实际上是很少的。上半年自己选的只有python入门和软件工程，不知道为什么，对学校的教育是彻底的失望了。MOOC方面，不知道托了多少个月，换了几个班，终于把吴恩达的机器学习给看完了。这样匆忙下来，所学的东西到今天基本也忘得差不多了。前几周，又决定去听一下《机器学习基石》和《机器学习技法》，希望可以坚持下去。另外，还看了一下候老的《深入浅出STL》，收获很大。 应该是寒假的时候，没有多少思考，在陈老师的鼓动之下，又给自己的ACM生涯续了一年左右。队友实在是太给力了，以致于前几天陈老师还说出，我们几个人好好训练，还有机会打进总决赛。抱大腿之下，成绩倒是不错，省赛金、邀请赛金、区域赛金、ecl银，但这和我没有多的的关系。印证了陈老师的一句话，带着功利的目的参加ACM比赛没有好的结果。人家的没有好的结果指的是获不了奖，我的没有好结果是人生的迷茫。 由于没有去考研的原因，也把自己推向了就业，但我都没有准备好。打击在找实习的时候就来了，上一届的学长推荐，很早就面阿里，发挥的很不好，毫无疑问的挂了。自己后知后觉，春招快结束了，都没有找到好一点的公司。幸运地是，在差不多接受去一家杭州本地的游戏公司实习时，突然接到滴滴北京的offer，终于拯救了一下自己。 944_2017-08-29_11-17-42 6月到8月这3个月的时间，自己都在北京实习。这一段小结准备了好久都没有写完，这里简单的介绍一下。那边的部门是路径规划小组，所以我的主要工作就是写最短路……这三个月时间对我来说，最主要的是体验，感受了互联网公司的氛围，认识了很多优秀友善的同事，他们也教了我很多工作上的技能。最重要的是，我也发现自己的不足，工程能力太弱，学历太低等等。三个月的时间很快，还没有在那边做太多的项目就结束了。另外一方面，在北京也去过很多地方体验，文化之都名不虚传，这个也等有机会再写。 回来之后，又是一番秋招。抱着在杭州找一份工作的信念，我面试了很多，贴出9月和10月的日历来留念一下。 最终由于自己的实力问题，结果不是很乐观。这么多面试，唯一想记一下的就是微软面试，有一刻我彻彻底底地感受到了自己的不足，英语水平太差，思维定势。接下来的两个月，就只好老老实实地待在学校，每天不是在寝室就是在图书馆度过。 另外，寒假的时候，突然有兴致想学围棋，又是报live，又是买书、又是买围棋。坚持了几个月之后，发现真的不是时候，没有了太多空闲时间来培养兴趣，也许早个一两年，可以培养出这个兴趣。现在感觉，可惜这些冲动而买下的东西。说回来，这一年空闲的时间大多花在网上，看视屏、看直播、刷论坛，明知很多事情没有意义，却又控制不住自己。 所以，又开始关注个人管理。最大的投入是换了一台Mac，终于不用再忍受原来低配thinkpad的卡顿。这几个月里面，倒是每天都抱着在用，感觉生命都得到加速。不过，现在感觉自己的电子设备有点多，选择什么设备干什么成了新的问题。2017年也是知识付费地兴起的元年（这个是我一直追求的，如果觉得我的哪篇文章把你逗笑了，打开about页面，扫一下二维码支持一下）。在这方面的投入主要是知乎live，由于冲动，也交过一些智商税。比如下面图中知乎最有名的\"智商税\"之一： 其他的还有如何自学计算机专业课程？，道理都懂，不知道自己当时为什么要买这个东西……当然也有不错的比如围棋相关的从入门到入段，这个live最大的优点是配套服务完善。上面也讲了，自己没坚持，才没有学会围棋。前几天买少数派的效率大礼包，还送了一个什么live私家课，又是一个坑……其实我一直很反感知乎live的形式，语音不能检索，聊天信息流的形式不适合深度讨论，最重要的一点，网页端功能太弱，完全想直接喷。有过这些教训之后，应该把知识获取的重心放在课堂以及书本中。在网络上的另一笔花销就是购买软件服务。目前自己周期购买的包括为知笔记会员、滴答清单会员、蓝灯会员、腾讯云服务器以及域名。对于这些还是比较满意。换到Mac之后，自己也购买了一些软件。遗憾的是由种种原因，偶尔还用通过不正常手段获得的软件。 由于这一年感触比较多，也捡起了原来写日记的习惯，毕竟自己忘的实在是太快。之前尝试过很多写日记的方法，比如晨间日记、九宫格日记，或者是纸笔记录，都没有满意的方案。目前搞的一种笔记式的方法再探索。 大致思路是为知笔记上面按每一个月创建一个文件夹，每天创建一篇笔记。在笔记里面可以写东西，贴照片，传附件，保证自由很多样性。然后第二年同一天打开这篇笔记，可以去年今日，再写下新的感悟。更重要的是，通过这种方法可以将之前写的日记整合进去。家中的初中和高中日记本也有机会处理了。 最后，就是关于博客的折腾。年初的时候自己把博客改成了wordpress，但是一年都没有坚持下来，又回到了hexo怀抱，真是呼吸不止，折腾不息。但不应该忘记，博客的本质是分享，现在自己差不多是舍本逐末吧。所以，定一个小目标，新的一年里面更新博客频率不低于每周一篇。 于浙财图书馆。 阅读 上半年基本上没有看完什么书，大部分都是下半年从北京回来之后看的，所幸都有一些简短的记录，特地复制过来记录一下。 编码：顾名思义，这本书讲解的是计算机内部编码的原理。书中主要包括两部分 ，一是编码的理论基础，用多种方式表示字符或者操作指令，二是编码的硬件基础，如何设计硬件处理各种编码以实现相应的功能。 尽在双11：阿里巴巴技术演进与超越：听名字就知道这本书很厉害，结果可想而知。本书讲解的是阿里一些架构演变的历史，里面有太多技术相关的名词，水平太差根本看不懂这些东西，希望有一天能恍然大悟。 程序员修炼之道：看不懂，有缘再读。 腾讯传：之前在知乎查评价时，很多人不是很推荐。这本书主要记录一些腾讯发展过程中重要的里程碑，比如OICQ、QQ秀、QQ空间、微信等等。没有读出这些事件背后深刻的意义。书中另外一个重点是对马化腾的大夸特夸……有种天不生化腾，万古如长夜的感觉。作者号称写这书花了五年时间，实在没有找出什么重磅的料啊。 大学之路：大学生活马上就要结束了，才想起来看这本书。最近深刻体会到高等教育额的价值，对比吴军博士在书中所写的各式各样的美国大学，我们国内的大部分学校除了校名不同，就没有什么其他差别了。最后，吴军博士本人也很励志，从进入清华开始到博士毕业花了18年，真是终生学习的典范。 人类简史 : 从动物到上帝：这本书大概读了五分之一，很多观点比较深刻，但是提不起我的兴趣，所以就选着了放下。另外一方面反映我的人文社科阅读能力实在是太差了。 出梁庄记：这本书和上一本，都是我在三联奋韬书店里看的。有点区别的是，这一本我看完了……作者通过采访生活在各地的梁庄人，反映了生活的不易。当时正是北漂，引起了共鸣 高频交易员：没有仔细看，现在回想起来，自己记住的就是，通过信息传输的时间差来获利，以此来抨击美国金融市场的不公平。 盖洛普优势识别器2.0：这本是最重要的就是配套的测试吧，图书馆借的书没有测试码，所以也很无聊了。 毛泽东选集（第1卷）：这本书里面，其实我只挑了《湖南农民运动考察报告》来看。这篇报告的特点是逻辑清晰，和现在的报告对比，缺少数据分析。 编程之美：为了准备微软面试而读，最后发现没有用到 剑指offer和剑指Offer：名企面试官精讲典型编程题（第2版）：第一版是在软件工程课上读完的，由于之前参加过算法竞赛，所以很多题目都是秒杀的。第二版是在去苏州的火车上看完的，比第一版多了一些题目，质量也不错。确实很多面试都是从这本书里面找的题目。 程序员代码面试指南：IT名企算法与数据结构题目最优解：之前一直黑左神，但他这本书还不错。有一次笔试，我有一道题目没有写出来，听人家一说是这里面的，龙与地下城。去微软面试之前，准备看一下这本书里面的大数据相关题目，但是时间匆忙没有机会了。后来发现面试的时候就问了一道里面的大数据题目，最后gg了。在百度和拼多多面试时，也被问道里面相关的题目。 技术之瞳——阿里巴巴技术笔试心得：为了准备阿里笔试而借的书，说句实话不推荐。书很薄，但是很贵。知识总结也很少，所谓的笔试真题，也只有答案。借用一句话，对于没有解析的书，自己答案印错了都不知道。纯属阿里巴巴骗钱之作。最后吐槽一下，阿里笔试是要招全栈工程师吗？什么问题都有。 淘宝技术这十年：听学长讲座后借的书，由于不是很懂里面的故事，所以我就当成故事书来看了。印象最深的一个多隆的大神，技术伴随着淘宝发展的成长，前几周看到，好像他成了阿里的合伙人。 王道程序员求职宝典：又是一本为了面试而看的书，对于我这种基础不是很扎实的，感觉看起来很不错。王道论坛在计算机考研的口碑应该也是挺好的。 深入理解计算机系统（原书第3版）：五星推荐，串联起了我学过的知识。 C++ Primer 中文版（第 5 版）：这也不错。 围棋入门：围棋入门挺不错的书，上半年想学围棋时而读的。不过目前已经绝版了，我也是买的淘宝复印版。 观影 这一年来看了不少的电影和记录片。不过我用的形式是优酷ViP区的电源以及B站上面的记录片。 Jia Jiang: What I learned from 100 days of rejection：蛮有意思的一个TED演讲，但是我没有勇气去实践。 Professional-职业人的作风 围棋棋士 井山裕太: 这部记录片主要介绍日本围棋史上首位获得国内全部头衔的棋士井山裕太的生活以及一些比赛。 纪实72小时 沉浸在围棋的魔力中: 记录日本的一个围棋茶馆72小时，看完之后感觉也只有在日本才会有这样的茶馆，很多人会在店中待上一个周末。在围棋当中，忘记自己的一切，只有黑与白之间的交锋。对其中的两句话印象深刻，第一句是提问武宫正树儿子有关职业围棋的问题，他回答到，下这盘棋的时候我们的出场费是一样的，如果我输了，我就没有了机会。第二句是提问一位下棋的人为什么这么长时间都待在这里，回答我没有家。原来只是听说日本老年化社会，看完这个视频才对这个问题有感触。接下来，我又看了很多这个系列的记录片，感受到了日本的温度，这是我们所缺失的。 白日梦想家: 有过很多想法,最终却成了空想,每次半夜的激情,醒来之后还有多少? 现代生活的秘密规则：算法 The Secret Rules of Modern Living: Algorithms: 里面的很多算法自己都接触过,但看这部纪录片又有了很多新的感悟. 人民的名义：这是我这几年第一部看完的国产剧吧，喜欢他的原因是讲了很多我原来都没有关注过的反腐问题。 战狼2：今年最火的电源，主题也很深刻，不过现实还是很残酷的。 硅谷四季：一部讲述美国geek创业的故事，很励志，但我却没有一颗创业的心。 华尔街之狼：我觉得看的很爽。 雷神1、2：去年把漫威宇宙中的电源都找出来看了一遍，唯独没有雷神系列，最终补上了。我最喜欢的一个设定是，雷神在远古中出现，但由于时间太久，人类选择认为是神话。这里，我想起了易中天在《中华史》里面提到过的一个观点，也是神话是真的存在过，只是我们后来的记录夸张了那些，比如那时候人很少，而且有足够的水果当成是食物，所以人们不用劳动就能生活。 盗火者：中国教育改革调查：大一的时候听高数老师讲起过，一群不合主流的中国教育者故事。 奇异博士：漫威宇宙中的新作，反正是无脑看的。 西部世界：很烧脑的美剧，反正我是靠知乎剧透才看懂的，还有很多谜题都没有解决，不知道什么时候有新的一季。 权力的游戏：从大一开始就听过这部美剧和书，然后到大四才开始看。到目前为止，看了4季，资源太难获取，就不看了。看他的原因是，讲述了低魔世界的传奇。 如何实现你的儿时梦想：蛮有名的演讲，少年可期。 超级工程：这也属于很早就听过名字，但是没有机会看的。很多东西确实震撼，但是自己没有机会参与其中。 人生七年：已经快60年的社会学实验，看下来很多结论是很残酷的。 我更关心的是怎么样不做事 老树画画：做一个梦：这也有感动。 B站的历史记录有限，很多看过的记录片很遗憾没有记录下来。 暴走大世界：之前很喜欢的一个综艺节目，但是最近却脱粉了。理由很简单，王尼玛居然是一个符号。我确实没有仔细的思考这个问题，当人家爆出来的时候，我也很震惊！ 旗门镖局：阿瓦隆的汉化版，很有意思的逻辑综艺节目。","categories":[],"tags":[{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"},{"name":"book","slug":"book","permalink":"https://xiang578.com/tags/book/"},{"name":"movie","slug":"movie","permalink":"https://xiang578.com/tags/movie/"}]},{"title":"iPhone软件清单","slug":"iphone5s","date":"2017-12-30T13:32:22.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/iphone5s.html","link":"","permalink":"https://xiang578.com/post/iphone5s.html","excerpt":"","text":"本来打算双十二买一部小米6，谁知道当天没有优惠，转而买了一部mix2。当时准备写一篇文章来纪念一下我的第一部智能手机。可谁知道，拿到的mix2品控太差，一周不到前置摄像头就进灰了，果断退货。 目前在用的手机是高中毕业之后买的，当时记得是4500块送1000块话费而买下的。8G的存储空间，对于我这种不拍照的来说也还可以接受。不过最大的问题是电池，实在是太不经用了，而且还有天冷关机的问题。所以，退了手机之后，就在京东上买了一块品胜的电池和线下装机服务。用了大概一周，目前还比较满意。前几天，Apple终于为电量低降频的事情道歉了，而且将换电池的价格从五百多降到了两百多一点。我感觉之后，苹果换电池就很划算了。可惜下一部不想买iPHone了。 QQ、微信、支付宝之类的不写，记录一下，我自己在用，但比较小众的软件。 inoreader：免费里面比较好用的RSS软件，不使用其他服务，墙外的博客都可抓下来，不过图片有很大的问题……几个月之前在国内AppStore被下架了，所以目前绝版，网页版很早就被墙了。 Castro：播客软件，我认为最大的优点是界面优美，从破解版用到它免费，目前还有Castro 2可以选择。 欧路词典 Pro：最大的优势是可以自己装字典，配合网上的盗版资源非常的棒，而且价格也很便宜。 每日英语听力：偶尔听听 诗词之美：有时候掏出来，抄一篇之前背过的诗词。之前这个App还叫西窗烛，后来那个软件加上了很多社交功能，就单独出了一个只能看看诗词的版本。 白描：ocr软件，买少数派效率大礼包时候送的。 熊猫吃短信：通过iOS 11新机制，使用机器学习过滤短信。 Battery Life：查看电池寿命 Scanner Pro：扫描软件，Reedle出品，质量保证。 LastPase：密码软件和chrome上的插件配合使用 Authenticator：两步验证，部分网站支持，在实习时必备。 AppZpp：可以订阅App更新和价格变动 网易有钱：记账，最大的优点是可以同步支付宝的账单。 Forest：集中注意力？可以在手机上种树？ 滴答清单：清单+日历 kindle：阅读 脉脉：查看程序员八卦，行业内人员变动消息传的早，公司内部撕逼等等","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://xiang578.com/tags/iOS/"},{"name":"app","slug":"app","permalink":"https://xiang578.com/tags/app/"}]},{"title":"2017年上海 ACM/ICPC ECL-Final","slug":"icpc-ecl-shanghai-2017","date":"2017-12-18T07:33:38.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/icpc-ecl-shanghai-2017.html","link":"","permalink":"https://xiang578.com/post/icpc-ecl-shanghai-2017.html","excerpt":"","text":"前几周和教练申请，愿意自费出去比赛，所以才有了这一次机会。这也是3年多以来第一次自费出来。 今年的比赛时间安排不是很好，可能是区域赛安排到了12月初的原因。由于周六要参加六级考试，我是那天晚上坐高铁前往上海的，幸亏这些都是轻车熟路。唯一值得记一下的是，我从场中路地铁站出来后，周围比较冷清，只好用滴滴打车去宾馆，这一次司机直接开到推荐上车点来接到我，没有使用电话沟通。前几个月在北京实习时，弦哥经常在内部说这个功能的重要性，可以给滴滴剩下近千万的虚拟号码开销。现在终于享受到了这个好处。 来到宾馆之后，找队友拿了队员证以及衣服。惊喜的是今年谷歌送的小礼品是 Google Cardboard ，平民级别的 VR 眼镜，记得之前内核恐慌里面听 Rio 他们提起过。拿到之后研究了一下，确实蛮有意思的。 由于前几个月在学校内修仙比较严重，睡得比较晚，导致第二天8点10多分，听到外面有人的动静才起来。匆匆的洗漱了一下，就和同学一起前往上海大学。 到达比赛现场之后，惊奇地发现，他们已经把午餐给发下来了，所以直接开始吃零食…… 刚开始都比较正常，我也在那里简单地看看题目。然后，看都L题，题目比较简单，看起来像是一道博弈题，由于没有上机的机会，所以我知道简单的推一下，大概推了10以内的情况，我认为自己找到了规律，所以上去写，不过第一次没有通过。下来之后，我再推了一下16的情况，明天自己刚才错在哪里，所以就重新上去写了一下。在111分钟的时候，我们通过了这一题。最后半小时内，我和老王一起写H题，凭借着他强大的逻辑能力，在289分钟的时候通过了。最后由于人多人会写的J题，我们没有通过，所以只有以7题遗憾结束。 算上打星队伍的排名大概是73名，获得银奖。 最后在等待领证书的时候，一个志愿者突然跑过来，问我们要一包零食，说上午到现在都没有吃东西。我拿出一包豆腐干给了他，祝福每一个努力拼搏的人。ACM生涯在续费一年之后，就结束了。 照片不知道为什么不能调整方向，就放在这里治疗颈椎病吧","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"}]},{"title":"2017年西安区域赛","slug":"icpc-xian-2017","date":"2017-11-05T11:20:49.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/icpc-xian-2017.html","link":"","permalink":"https://xiang578.com/post/icpc-xian-2017.html","excerpt":"","text":"怎么开头呢？ 10月20多号的时候，教练通知我们，一定要在赛前训练一场。仔细一想，上次训练在5个月前……整容倒是没有太大变化，周神（通过微软、腾讯、今日头条、京东和拼多多面试）和God王（曾经在阿里实习过），三个人暑假都在外面实习，回到学校之后也没有心情训练。10月25日（周三），我们拉了一场貌似是印度那边的区域赛，打了一下，感觉手感还行。周四就带着学弟学妹们坐火车前往西安，这也是我校继2015年EC-final之后，再一次派出多支队伍去打区域赛。 周五上午抵达西安站，由于一些情况，果断卖掉学弟学妹，我和队友3个人提前打车去了酒店。到达之后，发现他们离酒店还远，就在旁边找了一家饭店吃法。点了几个菜，到现在都觉得手撕羊肉不错，毕竟60多一斤。然后，就是常规的办理入住手续。上半年过来的时候和其他学校一起住在别墅，还能泡温泉，美滋滋。这次人来太多，经费紧张，就只能让我们住在标间。虽然环境还是可以吐槽的，但至少有wifi可以用了…… 晚上，一群人又出去吃了一顿火锅，西安的物价真便宜，最后剩下好多蔬菜没有吃…… day1 热身赛 早上起来之后，发现喉咙有点痛，和队友表达了明天可能会失声的担忧，并且表示不想去医院看，看好了多尴尬。10点左右集结队伍徒步前往西北工业大学。 这次比赛的地点和上一次不一样，终于搬到了体育馆。不过确实大，都能放下350+队伍，多年之后会不会有学校租个鸟巢打比赛？领完衣服，拍了照片。我们就高兴的去了食堂吃饭。吃完之后，我和周神一起去校园逛了一下，果真是强校，实在是太大了。 下午3点左右，抵达比赛场地，准备热身赛。老王让我上去打一些头文件，我感觉，如果今天一道题目都不会写，不是就白打了吗？队友想了一下，只能无奈地看着我…… 比赛开始之后 我快速地看完A题，说了一下题意，队友就开始写了。在决定猜什么数字的时候，周神强烈要猜2017，我说了一句傻子才会猜2017，然后他们就Yes了，我无法可说…… B题，简单模拟题，周神搞了一下就通过了。 最后就是C题，三个人一起看了好久，没有什么想法。我开始自告奋勇地说要写一个暴力，他们奇迹般的同意了我的方案。于是就获得了上机的机会，写到一半时，后面的队伍就欢呼通过了C题。综合他们一系列的反应来看，绝对是暴力过的，我的心也就放下了。由于好久没有写题目，所以用几分钟调试。通过这题之后发现，排名居然是第三，而且和第一名才差1分钟。毫无疑问，队友对我展开批判，痛失热身赛冠军…… v2-28df8e8bd4ceebc2cc3ac8b461201f54_b 之后的时间就是学习其他人大佬，一顿乱试。时间还有多，队友上机打了一个表，验证一个猜想，就想出了C题的标算。最后，实在没有什么可以干了，决定提前半小时离场吃饭……路上队友怕今天把明天的人品败光，我告诉他，正真的共产主义者是无所畏惧的。这样我们高高兴兴地吃完了晚饭，然后搭车回到了秦龙。 晚上，一个队友在网上认识的河南农业大学大佬请他吃烧烤，我又跟过去混了一顿。和他们聊了一下子，我说了一句，你们一定会成功的。果不其然，青岛赛区他们获得了银奖…… day2 正赛 参加过这么多场比赛，第一次遇到口号是“安全第一”的赛区，周神的祖传行李箱不让带，所以他去寄存。我抱着一大堆书籍和打印的资料走进了体育场，路上还听到有人议论。从赛后来看，这些东西都没有用到…… 比赛开始后，一切都比较顺利。老王单人ACB题，周神一秒钟猜出了F的公式，然后我就鼓励他们不用想其他样例就把猜想给交了。85分钟地时候，老王不动声色的AC了G。看完H题意之后，我和周神说这个贪心地取就好了。他想了一下，决定用线段树。然后在他快要写完的时候，我看有那么多人都过了，感觉不是很像，后来出来一问，暴力也能通过…… 之后做的是L题，有关lol的题目。前一天翔哥还和我吹周神是财经faker，但我发现他居然不知道banpick，于是乎解释了好久。看完第一个样例的后，我决定自己算一下。这时候，突然弹出一个公告，告诉我们怎么计算样例，就这样完全明白了题意。然后队友就给了一个任务，让我去把第二组样例打出来……500个01组成的字符串，我机智的想到去讨论版提问，能不能发送一份样例过来，毫无疑问的no response……所幸老王想出了4个for循环的暴力，复杂度高达10*1e8，出于对西安赛区判题机的信任，他就开始写了。然后，懒得输入样例，膨胀到在写完之后就交。果不其然wa了，多组输入忘记数组清空，改。又是一发wa，看了一下有操作逻辑写反了，再改，AC。接下来就是自我安慰把这40分钟当成输入样例的时间…… 之后我们就陷入长达1个小时的僵局，三个人看着A题感叹世界真是神奇……后来老王不得不放弃这一题，从而转战K题，想了半个多小时，就开始上机码。写完的东西神奇的re了，然后派出换上周神上去改。不知道后来发生了什么，折腾到269分钟AC。之后就是估计排名时间，根据封榜前的排名估计了一下我们大概在32名左右，感觉有点悬……又开始对A题进行一顿乱搞，在最后几分钟里面提交了好多次，但是都没有返回结果…… 离场前气球合影，可惜最后一小时过的题目不发气球。 所以带着最后的悬念来到了颁奖典礼，在等待的过程中，发现学弟遗憾的只是铜首……滚动46名时，发现我们飘到32名。再加上看到下图是，我在学校的群里说了一句最差34名，就这样获得了自己的第一块区域赛金牌，学校acm史上的第二金。 最终排名也是32名，这个世界真是神奇。 IMG_6227 然后又是我上台高高兴兴地领奖。 三人合影留念 搞完这些之后，坐着校车来到了韦曲南站。我带头冲向上半年吃过的烧烤店，后来发现，老王没有一个能吃的，真是尴尬，其他人也没有什么兴致，最后就当我请客了……然后在旁边找了一家宾馆就睡下了…… day3 归来 12点左右，在火车站附近开了一间房，把我们的行李都放下了。然后，在周神的带领下，前往回民街。逛了好久，没有太多意思。19点火车返程杭州。西安区域赛就这样结束了…… 最后还要感叹一下，周神和老王还真厉害，带着我都能获金奖，不过说回来还是喜欢去年自己在北京拿的铜。自己的贡献是一场比一场小……如果没有ec-fianl的话，acm生涯到这里就结束了吧，也留下一些遗憾，如果有来生，我也愿意信来生，再见……","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"}]},{"title":"Mac软件清单","slug":"mac-software","date":"2017-09-18T14:11:31.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/mac-software.html","link":"","permalink":"https://xiang578.com/post/mac-software.html","excerpt":"","text":"最近痛下血本买了一台Mac，这里记录一下我装的软件。 君子生非异也，善假于物也。 更新 2017年9月20日：Anaconda 2017年9月21日：lantern、Dash、Xcode 2017年12月10日：虚拟机相关 Clion C++ IDE Pycharm Python IDE Alfred 3 替换默认的搜索工具 搭配workflow使用效果更佳 MWeb MacOS 下面最喜欢的Markdown编辑器 常用的高级功能有发布WordPress博客和自动将图片上传到图床 Wiznote 用官方的话来讲Mac功能少是小而美 如果不是会员到2020年结束，我一定会转到Evernote上去 iina 视频播放软件 ShadowsocksX-NG 看名字就知道干什么的 突然发现自己买不起服务器，转向蓝灯 Parallels Desktop 虚拟机软件 Sublime Text 3 性感的编辑器？ 处理小文本时使用，写题目还是喜欢vim iTerm2 用来替换默认的Terminal 推荐主题：solarized-dark 推荐字体：Hack Homebrew Terminal内下载软件 使用：brew update;brew install vim 查看软件信息：brew info vim 临时替换：export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles 清华大学镜像地址 中国科学技术大学源地址 tmux 增强终端功能 配置文件：.tmux 更新配置文件：tmux source ~/.tmux.conf 常用快捷键 prefix s 查看/切换sessoin prefix d 离开session prefix $ 重命名当前session prefix c 新建窗口 prefix space 切换到上一个活动的窗口 prefix &amp; 关闭一个窗口 prefix 窗口号 使用窗口号切换 prefix o 切换到下一个窗格 prefix q 查看所有窗格的编号 prefix “ 垂直拆分出一个新窗格 prefix % 水平拆分出一个新窗格 prefix z 暂时把一个窗体放到最大 Oh My ZSH! 用zsh来替换默认的shell 推荐主题：agnoster 最喜欢的是对git的增加以及git相关快捷键的缩写 常用快捷键 gaa: git add . gb: git branch gcm: git checkout master gcmsg: git commit -m gst: git status vim 配置文件：spf3，已经不想折腾这些东西 常用快捷键 dd: 删除当前行,并把删除的行存在剪贴板里面 *#: 匹配当前光标所在的单词移动到下一个或者上一个匹配的单词 %: 匹配括号移动 :set number: 显示绝对行号 :set relativenumber: 显示相对行号 r: 将光标所在的字符替换掉 &lt;&lt;: 将当前行向左移动一个偏移宽度 &gt;&gt;: 将当前行向右移动一个偏移宽度 Powerline fonts vim中的状态栏以及zsh某些主题正确显示的核心 iStat Menus 系统增强工具 在状态栏显示当前网速、CPU占用以及温度等信息 Jietu 腾讯为数不多的良心之作，截图软件 坚果云 良心同步软件，每个月上传流量不多，但是够用 iCloud默认空间实在是太少 Anaconda python 科学计算包？ 最主要使用是jupyter notebook 根据之前在公司安装时得到的经验，完成安装之后还需要更新zsh相关的配置，否者在iTerm中依旧无法使用 在 .zshrc 中添加一条记录：export PATH=\"/Users/xiangrunye/anaconda3/bin:$PATH\" 注意这里需要写绝对地址 更新配置文件：source ~/.zshrc Lantern 捍卫互联网自由？ Dash 快速查阅各种语法的文档 配合Alfred食用更佳，建议将 keyword 改为 ds Xcode 感觉自己可以转ios开发 下这东西才知道，国内连AppStore网络有多差 Parallels Desktop win镜像获取 Chrome 看中的是插件多的特点 MacOS设置为英文的环境下，Chrome里面下载时经常出现文件名乱码，找到了这个网页Mac OS X 下修改 Google Chrome 显示语言的方法 参考链接 MacTalk:Alfred Tmux 速成教程：技巧和调整 vim 学习卡 vim 帮助和配置 从零开始学习Alfred markdown.wordflow","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"mac","slug":"mac","permalink":"https://xiang578.com/tags/mac/"},{"name":"software","slug":"software","permalink":"https://xiang578.com/tags/software/"}]},{"title":"使用 BackWPup 恢复 WordPress","slug":"restore-wordpress-from-backWPup","date":"2017-07-22T14:13:33.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/restore-wordpress-from-backWPup.html","link":"","permalink":"https://xiang578.com/post/restore-wordpress-from-backWPup.html","excerpt":"","text":"昨天晚上手贱点了升级服务器上的 Ubuntu 版本,然后发现 WordPress 不能用了...删了nginx装上 Apache 才解决,之后发现主题有问题,一激动就提交工单把服务器重装了... 接下来就是 WordPress 重装的过程,自己一直使用BackWPup 每周备份 WordPress ,所以本以为重装会很轻松. 早上起来将 WordPress 以及 BackWPup 装好.然后就在 BackWPup 里面翻了好久都没有发现怎么还原的按钮...又用百度谷歌搜了一下,也没有教程... 没有办法,只好硬着头皮去看插件的官网,找到 How to restore a WordPress backup? – BackWPup Docs,按照这篇文章的提示成功恢复 WordPress. 恢复数据库 由于我用的是phpmyadmin,所以登录到服务器的phpmyadmin管理页面.选择数据库WordPress,点击导入,上传BackWPup备份压缩文件中的.sql文件,点击执行,这样数据库中的内容就还原了. -w1277 恢复 WordPress文件 这一步只需要将备份文件里面的文件上传到服务器新的网站目录下覆盖就可以了.由于我用的是iTerm2,所以可以在本地使用scp命令 尾声 zz插件,连个一键还原都没有...","categories":[],"tags":[{"name":"wordpress","slug":"wordpress","permalink":"https://xiang578.com/tags/wordpress/"},{"name":"backwpup","slug":"backwpup","permalink":"https://xiang578.com/tags/backwpup/"}]},{"title":"Alfred 使用记录","slug":"alfred-lists","date":"2017-07-02T12:34:39.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/alfred-lists.html","link":"","permalink":"https://xiang578.com/post/alfred-lists.html","excerpt":"","text":"昨天晚上在看MacTalk时,发现一篇文章神兵利器——Alfred,随后将里面提到的软件下载下来.接触不到一天的时间,就感觉到这个软件的强大.下面就记录一些我用到的功能以及插件. ### 原生功能 #### terminal 最近在公司工作,都要求连接到服务器上去写代码,所以对terminal的需求很大.目前自己用的是iTerm2+zsh的组合,还有类似于powerline的状态栏,工作效率杠杠的.Alfred可以使用一个字符触发terminal,默认的是&gt;以及打开Mac自带的终端. 由于&gt;这个字符比较难打,我将Prefix修改为;,而且也将修改terminal为iTerm2(参考这个链接最下面提到的Alfred Support). #### 网页搜索 给定关键字打开浏览器在特定的网页搜索,例如下图 实现起来也比较简单,在Feature-&gt;Web Search中添加类似于下面的即可. #### 计算器 输入表达式通过Alfred计算 ### WorkFlows 这个是我觉得Alfred最强大功能,毕竟编程改变一切,下面推荐几个插件. #### Markdown img 这个插件的功能是直接将系统的剪贴板里面的图片上传到七牛图床,并把图片链接复制到当前的编辑器和剪贴板.这篇文章的截图就是通过这个插件完成的,终于不用像之前一样苦逼的一张张上传图片.下载地址 #### Workflow Searcher 通过Alfred来搜索workflow并且跳转到下载页面 #### v2ex 快速浏览v2ex的帖子. #### Douban 查看豆瓣的电影,音乐,图书等信息 #### Wunderlist 可以连接奇妙清单,正好符合我这种轻量级用户. #### 有道翻译加强版 满足查字典的需求,虽然mac原生的也很强大,但这个能翻译啊 #### 百度搜索 虽然国内口碑不好,但我还是觉得百度更懂中国人... 最后吐槽几句,垃圾为知笔记,mac版就没有多少功能,发个博客都没有,可惜我剩下三年的vip,这篇文章还是靠 Mweb 发布的.","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://xiang578.com/tags/Mac/"},{"name":"Alfred","slug":"Alfred","permalink":"https://xiang578.com/tags/Alfred/"}]},{"title":"计算机基础：计算机网络","slug":"computer-base-network","date":"2017-04-29T13:57:37.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/computer-base-network.html","link":"","permalink":"https://xiang578.com/post/computer-base-network.html","excerpt":"","text":"电路交换与分组交换的区别？ 优劣对比。 OSI有哪几层，知道主要几层的各自作用。 物理层：定义通信与传输借口硬件的机械、电气、功能和过程特性，实现比特流的透明传输。 数据链路层：无差别传送以帧为单位的数据。 网络层：选择合适的路由和交换结点，分组或者包。 运输层：端到端、或者进程到进程的无差别传送。 会话层：对数据传输的同步进行管理 表示层：信息加密和解密，正文压缩和还原 应用层： TCP/IP有哪几层，会画出来，知道所有层数的作用，会列举各层主要的协议名称。 网络接口层：x.25 网际层：IP 运输层：TCP和UDP 应用层：HTTP、FTP等 硬件(MAC)地址的概念及作用。 媒体访问控制子层：前6位16进制IETF分配。 区分不同的硬件 ARP协议的用途 及算法、在哪一层上会使用 ARP ？ 地址解析协议：IP地址到MAC地址的映射 网络层 知道各个层使用的是哪个数据交换设备。（交换机、路由器、网关） 物理层：中继器、集线器 数据链路层：网桥或者交换机 网络层中继系统：路由器 网络层以上：网关 IP报文的格式，格式的各个字段的含义要理解。 MTU的概念，啥叫路径MTU？ MTU发现机制，TraceRoute(了解)。 RIP协议的概念 及算法。 ICMP协议的主要功能。 网际控制报文协议，提高IP数据报成功交付的机会，报告差错和异常情况。 组播和多播的概念，IGMP的用途。 Ping协议的实现原理，ping 命令格式。 应用层，使用网络层的ICMP协议 子网划分的概念，子网掩码。 A类子网掩码：255.0.0.0，划分：8位+子网X位+主机24-X位 IP地址的分类，如何划分的，及会计算各类地址支持的主机数。 DNS的概念，用途，DNS查询的实现算法。 TCP与UDP的概念，相互的区别及优劣。 TCP和UDP是OSI模型中的运输层中的协议。TCP提供可靠的通信传输，而UDP则常被用于让广播和细节控制交给应用的通信传输。 TCP面向连接，UDP面向非连接即发送数据前不需要建立链接 TCP提供可靠的服务（数据传输），UDP无法保证 TCP面向字节流，UDP面向报文 TCP数据传输慢，UDP数据传输快 UDP报文的格式，字段的意义。 TCP 报文的格式，字段的意义。 TCP通过哪些措施，保证传输可靠？ 三次握手，四次断开过程。 客户端向服务器发送一个SYN j 服务器向客户端响应一个SYN k，并对SYN j进行确认 ACK j+1 客户端向服务器发送一个ACK k+1 一端发送一个FIN 另一端接收到这个FIN分节后，执行被动关闭，对这个FIN进行确认，继续传输数据。 发送数据之后，发送一个FIN 接受到FIN，结束传输。 TIME_WAIT状态的概念及意义。 滑动窗口协议 与 停止等待协议的区别。 TCP的流量控制和拥塞控制实现原理(会画拥塞控制的典型图)。 TCP的快速重传与快速恢复算法。 TFTP 与 FTP的区别。 阻塞方式和非阻塞方式，阻塞connect与非阻塞connect。(比较难，有兴趣可以了解) HTTP基本格式。（java程序员必须掌握）","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"network","slug":"network","permalink":"https://xiang578.com/tags/network/"},{"name":"computer","slug":"computer","permalink":"https://xiang578.com/tags/computer/"}]},{"title":"浙江省第十四届程序设计竞赛总结","slug":"zjp-2017","date":"2017-04-23T14:11:10.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/zjp-2017.html","link":"","permalink":"https://xiang578.com/post/zjp-2017.html","excerpt":"","text":"昨天是我第三次参加浙江省赛，比赛地点依旧设置在浙大。上午和其他人一起坐车过去，然后就是都差不多的流程。 由于一些原因，这一次我的两个队友异乎寻常的厉害（高质量就业：阿里研发工程师+京东算法工程师的组合）。赛前内心毫无波澜，不过周神教导我，作为浙财的五队，我们只要保个铜就可以了。 上午9点25左右，热身赛开始。打开试题册看了一下，我们发现热身赛题目跟去年一样，快速通过A和C题。周神机智的想起去年最后一题（输入一个数对一个非常的大梅森数取模，并判断结果的奇偶性）大数据只有四组，通过一番测试之后，今年也是一样。他写了一个程序直接判小的数据，开始枚举最后四组答案的排列组合。我们大概交了10次左右，就发现枚举的地方出现错误，然后改了一下又重新交，最后枚举到13(1101)的时候过了。本着写完热身赛去吃饭的原则，God王让我们开始写B题。不过仔细想了一下发现，万一写完发现是错的，是去吃饭还是不去吃饭，这是个很大的问题。所以，我们直接前往食堂。 12点15分左右，比赛正式开始。我自告奋勇的从A题开始看，本着看样例猜题意的指导思想，我大胆猜想出写法，趁他们两个不注意直接拿起键盘开始写。由于年纪大了，速度更比不上那些年轻人，所以也写了一会儿，交完之后大概有70只队伍通过。之后God王上来平推了B和D。我看一下C题，发现这是个简单题，把题意告诉老王之后，他也很快做了出来。至此，比赛大概过去45分钟。我们写完全部的签到题，完成保的铜的目标。 三个人继续分头看题目，没过多久他们两个人又开始讨论F题的写，很快也想出一个非常巧妙的方法，不过后来发现这个是超时的，所幸God王又想出了线段树的写法。我和周神开始研究E题的写法，很快也验证了用数位DP的正确性，不过也想到这其中有很多细节要考虑。说到这里，不得不讲一下，我昨天晚上就打印了两页模版，其中一个就是数位DP的模版。God王改完F之后，勇敢提交，直接返回了WA。然后我们三个看了好久代码，都没有找出错误来。God王直接出去逛了一下，我和周神开始写E题。God王发现周神读题目没有讲要输出子树的数量，改之就过了。之后，三个人共同推了E。接下来的时间差不多就是在挣扎了，H题God王写来写去都是超时的，G题周神说要找规律，打印了14页数据开始找没多久，就告诉我想的全部是错的。17点14分59秒，God王最后交了一份H题代码，没有什么奇迹，我们结束了这一次省赛。 晚上，在小剧场举行颁奖，这一个环节最主要的就是听听队名。当本科组银奖没有报到我们的时候，我的内心十分的开心。听到第一批金牌也没有报到我们的时候，周神对我讲了一句，是不是下面没有我们，今天就是冠军。事实证明这是幻想，然后他就高高兴兴的上台领奖了，毕竟昨天晚上特意做了一个发型…… 总的来说，这次省赛比浙大校赛难了很多，所以写完签到题不要错太多的都可以拿铜。中间过度居然拿了一道数位DP和一道线段树，命题人直接高估了浙江省程序设计的教育水平。最后面的几题，我们三个人共享完题意之后，完全没有思路，其他参赛选手也差不多，完全沦为观赏题。后来想一想，这一份题目只是在错误的时间出现，毕竟比赛的覆盖面是final到专科。但是，我感觉这样的比赛，一个人五小时也能做5题，生不逢时，为什么去年没有这么好的命啊。不过还是要感谢两位大佬带我体验了一下省赛获金的感觉。这一次，同场的还有学军和杭二的高中生，颁奖时，有以为领导还感叹大学生不如高中生。殊不知对于这些高中生来说考上浙江大学都是失败。 不知道明年还有没有机会参加。","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"}]},{"title":"C#聊天软件实现","slug":"Csharp-chat-softwave","date":"2017-04-19T12:42:55.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/Csharp-chat-softwave.html","link":"","permalink":"https://xiang578.com/post/Csharp-chat-softwave.html","excerpt":"","text":"前几天面试，被问如何实现一个类似微信的聊天软件。当时说了一个大概的想法，面试官没有怎么评价，只是最后建议我有空多做一些项目。正好最近不怎么忙，就实现一下。写了一个简易的模型，在这个过程中应用了很多原来学过的东西，当然还有很多问题没有解决。 这个软件选择C/S架构，所以写了一个服务器端程序和一个客户端程序，然后通信使用的是TCP协议。 利用socket完成通信，大致的过程如下图所示。 3. 服务器上开两个线程threadListenConnect和threadReceivePacket。服务器上的socket利用bind绑定服务器的ip和端口号。第一个线程死循环监听端口，是否有新的connect请求，并将新的连接socket保存到list中。再利用第二个线程接受客户端发来的数据包，并拆包执行进行相关功能。服务器之后，按照包中的内容判断是否需要发送到其他特定的客户端还是广播消息。 客户端中开一个线程threadReceivePacket。先和服务器连接，然后利用这个线程接受服务器发过来的数据包。这里也实现了发送给某个特定的客户端和广播。发包和拆包过程和服务器上差不多。 数据包主要包括发送方ip、端口和接收方ip、端口，以及操作代码。不同的数据段用‘|’分割，接收方也按这样拆包就可以了。 TCP是面向连接的协议，拥有缓存窗口，所以可能会有粘包现象，可能将程序一次产生的命令，分成多次发送。所以数据包里面还要标记一下，我用了''做新的包开始标记，用'做结束标记，如果聊天内容里面有相同的标记的，可以强行转化一下。 这个程序还可以加上数据库搞出注册以及保存聊天消息，然后自己也没有去写消息的排序以及私聊功能，待做。 最后相关代码放在github上面 服务器端： 客户端1： 客户端2：","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"network","slug":"network","permalink":"https://xiang578.com/tags/network/"}]},{"title":"为博客添加返回顶部按钮","slug":"add-return-button-to-blog","date":"2017-02-11T14:16:49.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/add-return-button-to-blog.html","link":"","permalink":"https://xiang578.com/post/add-return-button-to-blog.html","excerpt":"","text":"适用于WordPress 由于主题没有自带的返回顶部按钮，所以我一直在寻找一种解决方案。之前使用的是wordpress插件提供的返回顶部按钮，后来在网上乱逛，发现有Xnces – 衔铁部落的返回顶部按钮非常的酷炫，正是我要寻找的。于是，我在那个博客下留言询问制作方法。 前几天，看到那个博客上出现了一篇文章-本博客的返回顶部效果。 按照上面的方法，折腾一会儿，我也制作出来了，特地记录一下。 首先，先加载一下环境。在后台functions.php中找到ms_scripts()函数，添加wp_enqueue_script( 'jquery' );调用wordpress默认的JQuery文件。 我将这个效果有关的js代码放在了主题目录下新建的themes.js文件中。所以在上面的函数中添加下面两句话导入这个文件wp_register_script( 'themes_js', THEMEPATH . '/themes.js',array());和wp_enqueue_script( 'themes_js' ); CSS代码我直接加在了主题style.css中。 重新打开网站就能看到效果了。 themes.js文件中代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768var bigfa_scroll = &#123; drawCircle: function(id, percentage, color) &#123; var width = jQuery(id).width(); var height = jQuery(id).height(); var radius = parseInt(width / 2.20); var position = width; var positionBy2 = position / 2; var bg = jQuery(id)[0]; id = id.split(\"#\"); var ctx = bg.getContext(\"2d\"); var imd = null; var circ = Math.PI * 2; var quart = Math.PI / 2; ctx.clearRect(0, 0, width, height); ctx.beginPath(); ctx.strokeStyle = color; ctx.lineCap = \"square\"; ctx.closePath(); ctx.fill(); ctx.lineWidth = 3; imd = ctx.getImageData(0, 0, position, position); var draw = function(current, ctxPass) &#123; ctxPass.putImageData(imd, 0, 0); ctxPass.beginPath(); ctxPass.arc(positionBy2, positionBy2, radius, -(quart), ((circ) * current) - quart, false); ctxPass.stroke(); &#125; draw(percentage / 100, ctx); &#125;, backToTop: function($this) &#123; $this.click(function() &#123; jQuery(\"body,html\").animate(&#123; scrollTop: 0 &#125;, 800); return false; &#125;); &#125;, scrollHook: function($this, color) &#123; color = color ? color: \"#000000\"; $this.scroll(function() &#123; var docHeight = (jQuery(document).height() - jQuery(window).height()), $windowObj = $this, $per = jQuery(\".per\"), percentage = 0; defaultScroll = $windowObj.scrollTop(); percentage = parseInt((defaultScroll / docHeight) * 100); var backToTop = jQuery(\"#backtoTop\"); if (backToTop.length &gt; 0) &#123; if ($windowObj.scrollTop() &gt; 200) &#123; backToTop.addClass(\"button--show\"); &#125; else &#123; backToTop.removeClass(\"button--show\"); &#125; $per.attr(\"data-percent\", percentage); bigfa_scroll.drawCircle(\"#backtoTopCanvas\", percentage, color); &#125; &#125;); &#125;&#125;jQuery(document).ready(function() &#123; jQuery(\"body\").append('&lt;div id=\"backtoTop\" data-action=\"gototop\"&gt;&lt;canvas id=\"backtoTopCanvas\" width=\"48\" height=\"48\"&gt;&lt;/canvas&gt;&lt;div class=\"per\"&gt;&lt;/div&gt;&lt;/div&gt;'); var T = bigfa_scroll; T.backToTop(jQuery(\"#backtoTop\")); T.scrollHook(jQuery(window), \"#555555\");&#125;); CSS文件要添加的代码 1234567891011121314151617181920212223242526272829303132#backtoTop&#123; background-color:#eee; border-radius:100%; bottom:10%;height:48px; position:fixed; right:-100px; width:48px; transition:0.5s; -webkit-transition:0.5s&#125;#backtoTop.button--show&#123; right:10px&#125;.per&#123; font-size:16px; height:48px; line-height:48px; position:absolute; text-align:center; top:0; width:48px; color:#555; cursor:pointer&#125;.per:before&#123; content:attr(data-percent)&#125;.per:hover:before&#123; content:\"↑\";font-size:20px&#125;","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"wordpress","slug":"wordpress","permalink":"https://xiang578.com/tags/wordpress/"},{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"}]},{"title":"2016年北京区域赛","slug":"icpc-beijing-2016","date":"2017-01-29T14:13:28.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/icpc-beijing-2016.html","link":"","permalink":"https://xiang578.com/post/icpc-beijing-2016.html","excerpt":"","text":"今年，一队发挥出色，拿到全部赛区的名额，而且大三也没有多少队伍。所以我们这种弱队还有机会去参加第二场比赛。 作为第三次去北大，流程都是非常熟悉的。周四下午坐Z10出发，第二天上午到达北京。周五下午我一个人去了前两次来北京都没有机会参观的故宫。 故宫之旅并不是很好，主要是游客太多，紫禁城失去了往日的威严。不过，另外加收门票的钟表馆很棒，皇室的藏品居然如此花哨。有些钟表的滴答声一百多年之后依旧飘荡在场馆之内。出了故宫之后，走了半个小时才找到一个地铁站，这个设计非常的愚蠢吧，之后就动身前往北大。 周六下午热身赛，我们出来的比较早，在体育馆的看台上做了一会儿，看着下面排列整齐的两百多台电脑，心中生出一丝感动。之后就是进入场馆搞事情，电脑是五位数的戴尔移动工作站，外接了鼠标、键盘、和显示器。和我自己在寝室里面干的差不多。海巨表示显示器大看着爽，不过也发现键盘上有日文符号，这就比较影响我的发挥了。然后是例行看一下 周围的队伍，前面是北师、后面是北航、右上方有杭电一队，感觉压力又好大，虽然他们根本不会在乎我们。 比赛开始之前，队友想起在大连时，我讲了一句热身赛只要看一下AC是什么颜色就好了，最后被我成功奶死，全部是WA，所以告诫我不要乱说话。拿到题目之后，我们看了一下，发现题目是前几年北大出的区域赛和网络赛的。其中B题还是我们前几周训练过的题目，我拿起键盘表示这次终于有机会拿个一血了，马上写了起来，队友还是表示不相信。当我写完提交后，邪恶战胜了正义，果真发挥了一个WA，此时其他有一个队伍拿到这题的一血，彻底骨折。所幸改了一下还是过了，之后C题也是原来写过的，上去写了一下就过了。之后就讨论了一会儿A题，不过没有得出什么答案，感觉暴力上去肯定会超时，D题也不会写，最后上去让海巨测了一下机器和各种返回就去吃饭了。晚上回去才发现A题比我们想的还要暴力，没有勇气啊。 由于我们知道不会来北大吃太多餐，所以晚饭比较丰盛。一遍吃一遍感叹北大的伙食真好，早知道原来好好读书了。回到宾馆后，海巨在找题目写，翔哥想着阴阳师靠他的符偷渡欧洲，我看着炉石直播学习技术。又是一天。 周日，因为经费紧张，住的宾馆离北大比较远，我们三个7点就出发了，8点半左右到达北大赛场，然后等待比赛的开始。 9点比赛正式开始，开场顺利的写完了两道签到题，然后我们看了一下一道搜索题，我感觉可以写了，另外两个人去看了一下I题。我写差不多后，海巨上来写了I，也很快的写完并提交了，不过返回了WA。换我上去继续写搜索题，很快我写的超时了，只好改的优雅一点，翔哥也指出海巨理解错了题目，他们两想了一下决定帮我一起改搜索题，小改了一些东西之后，又提交了两次，返回两个WA。当时我们很紧张了，不过没有办法，海巨在看代码，我在写最暴力的准备对拍，翔哥在那里构造数据。最后，翔哥构造出了一组强力的数据，成功把我的代码hack了，我发现改掉一点就会对，不过没有想明白怎么回事，就改了一下记录答案的方式，最后自信的提交返回AC。然后我们将目光聚集到了K题，这个破题，题目就看了好久才明白，不过数据范围居然有10的100次方，我们搞了好久才都没有想出来，我感叹了已经上次在大连也是死在这种恶心的题目上，还是要想，我们写了一个10的9次以内的暴力程序打表，神奇的发现最大符合要求的个数只有样例给的83个，然后他们开始推规律，我在那里划水。想着写不出来，只要让海巨上去写一个只要83个答案表的程序，赌一发梦想。虽然感觉北大不会蠢到把最后一个答案告诉你，海巨还是写了，不过交上去还是WA。之后就陷入了江局，封榜后一会儿，我算了一下发现15位数随便填的情况下1就有10的16次左右个了，果断抢来电脑，不知道是什么力量让我点开海巨WA的代码，我看了一下差点吐血，找一个最大不超过n的值被写成立找一个最小超过n的值，我问了一下海巨，他还不知道，我直接改了一下就交上去，再赌一个梦想，然后就AC。我们马上点开排名，不过只能现实封板前的排名103，这题交了应该会前进一些，但是北大只有93个奖牌。海巨表示把I题写了才能稳，我看了一下I题，果断表明我不会数学题，只能靠你们了。然后实力分析一波之后，感觉要ntt操作，找了一下发现没有带这个模版，只好告诉海巨要么你想个其他的方法，要么自己创造一个ntt算法。我也没有找到其他组说的图论题是哪一个，只好提前选择死亡，吃起了发的午餐，果真像赛前说的一样吃东西就等于放弃了，最后还有10分钟结束的时候，我们三个人都愉快的吃起了午餐，把自己的命运交给其他小笨蛋来掌握。边吃边感叹这是第一次在正赛封榜后过题以及终于不是什么题数靠前的队伍了。 比赛结束之后，不知道结果怎么样，我们只好在那里等待，这要是在之前可以直接会师火车站了。北大搞了一堆事情，才开始滚榜，判完我们题目的时候好像跳到了81名去了，他们两个开始统计超过我们的人，又过了一会儿，发现最后名次固定在了85名，然后三个人都好高兴。滚完铜牌区之后，开始颁奖，我也有幸登上北大的领奖台。之后，那些也和我们没有什么关系了，反正两年多的努力有了回报就可以。 从北大出来，直奔麦当劳买了一下晚餐就去火车站了。我特地点了一杯拿铁，稳如拿铁。还是要相信科学。","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"}]},{"title":"为知笔记写博客测试","slug":"use-wiznote-to-write-blog","date":"2017-01-19T14:19:06.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/use-wiznote-to-write-blog.html","link":"","permalink":"https://xiang578.com/post/use-wiznote-to-write-blog.html","excerpt":"","text":"适用于WordPress 如果你可以看到这一篇文件，就表示我测试成功了！ tips: 为知笔记中的标签自动变成博客文章的标签 选择发布时，会自动检测是新发布文章还是修改文章 自带markdown优化，轻松解决wordpress没有什么好用markdown编辑器的问题 参考连接：发布笔记到博客，更好的博客离线撰写工具","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"wordpress","slug":"wordpress","permalink":"https://xiang578.com/tags/wordpress/"},{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"},{"name":"wiznote","slug":"wiznote","permalink":"https://xiang578.com/tags/wiznote/"}]},{"title":"iPad Pro 开箱","slug":"newipad","date":"2016-11-25T12:51:29.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/newipad.html","link":"","permalink":"https://xiang578.com/post/newipad.html","excerpt":"好久没有更新博客，差不多都忘记 hexo 的指令了。 上周六在苹果官网订了 9.7 英寸 iPad Pro WLAN 32GB - 金色 ，前几天刚好到了，所以写一篇庆祝一下。","text":"好久没有更新博客，差不多都忘记 hexo 的指令了。 上周六在苹果官网订了 9.7 英寸 iPad Pro WLAN 32GB - 金色 ，前几天刚好到了，所以写一篇庆祝一下。 自己买一个刻字版的Ipad，发货速度比正常的慢了3天左右。Ipad是从四川那边用顺风次日达发过来的，难道苹果工厂往内陆迁了？ 外包装就是一个普通的纸盒，蛮干净的，所以路上没有遭遇暴力投递。 封口处有一个小箭头提示，从那里轻轻一拉，就打开了包装。 跟一般的包裹不同，里面没有填充其他减震物。 然后就是精心包装的iPad。 迫不及待的打开盒子，新鲜的“苹果味”扑面而来。 取出iPad，里面还附赠充电器和Lightning的数据线，只不过没有耳机。 撕掉iPad的保护膜，翻到背面，就能看见刻的字。本来打算刻两行头文件，却被提示刻字内容不能包含有尖括号或者双引号。最终选择下面这两句符合要求的话。 最后，按照提示一路设置iPad，然后就有一台价值4000+的游戏机。下次有空再来 记录一下自己都装了什么App。","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"ipad","slug":"ipad","permalink":"https://xiang578.com/tags/ipad/"}]},{"title":"杭电OJ AC500题","slug":"hdu500","date":"2016-08-08T02:00:05.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/hdu500.html","link":"","permalink":"https://xiang578.com/post/hdu500.html","excerpt":"距离AC400题，过去了135天。 有很多出去的学长学姐都是不知道把红宝书背了多少遍，又一次和XX聊天，他说我打赌你背的有些单词，你一辈子都不会用上，但是尽管如此，很多人都背下来，这就是毅力，就是韧性。有的人放弃了，有的人却坚持下来，不得不承认，只有耐得住寂寞的人最后才会成功。学习是一项孤独而崇高的事业。","text":"距离AC400题，过去了135天。 有很多出去的学长学姐都是不知道把红宝书背了多少遍，又一次和XX聊天，他说我打赌你背的有些单词，你一辈子都不会用上，但是尽管如此，很多人都背下来，这就是毅力，就是韧性。有的人放弃了，有的人却坚持下来，不得不承认，只有耐得住寂寞的人最后才会成功。学习是一项孤独而崇高的事业。","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"},{"name":"hdu","slug":"hdu","permalink":"https://xiang578.com/tags/hdu/"}]},{"title":"第七届蓝桥杯决赛杂记","slug":"lanqiaobei-2016","date":"2016-06-03T23:46:56.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/lanqiaobei-2016.html","link":"","permalink":"https://xiang578.com/post/lanqiaobei-2016.html","excerpt":"反正最近不想干什么正经事，回忆一下这些东西也好。","text":"反正最近不想干什么正经事，回忆一下这些东西也好。 总的来说，这是我第二次去北京参加蓝桥杯决赛。去年获得了优秀奖，换句话来说就是重在参与。不过，今年成绩比去年好一点。 （订的宾馆一如既往的温馨就不说了）首先，今年我的比赛地点是在北京建筑大学（西城校区）。顾名思义就知道这所学校的建筑系比较有名，不过这和我没有什么关系。这学校位于北京市内，又由于建的比较早，所以进去之后发现植被非常的茂盛，另一方面就是校园建筑看起比较破旧，或者是有历史感。比较有意思的是，他们的计算中心是位于地下的，所以比赛的时候我就是在地下编程了…… 决赛从周六上午9点开始到下午1点结束，一共4个小时。题目有6道，两道填空题，一道代码填空题，和三道编程题。第一题，就是解一个二元一次方程x+y的最小值。第二题，是一个大暴力题，自己想了好久才想出一种方法，不幸的是最后和别人对答案的时候，发现自己想错了。第三题，我模拟了一下正确的转换方式和题目中代码给的转换方式对比想出填入的代码。第四题，枚举了一个每一行第一个字母暴力解出来。第五题是一道计算几何的题目，好像是外国比赛的原题，反正我连凸包都不会。第六题是什么生成树计数，本来想写一个暴力绝杀的，但是在最后还有10分钟的时候发现自己写了40分钟的代码是错的，多么绝望。其实后来发现，这后面两道题目的区分度几乎为0，周神都只写了暴力的上去，不知道理工的final选手有没有完全写出来。 周六下午我就开始干来北京的正事了，和周神坐地铁来到天安门。参观了天安门广场、中国国家博物馆、并且还登上了天安门城楼。最后，还是感觉中国国家博物馆有意思，很多原来在课堂上见过的藏品都亲眼所见了，不过自己上的艺术课程还是太少了，对于很多东西没有鉴赏能力。不过也有遗憾，毛主席纪念馆是每天上午8点到12点开放，没有去成，故宫是从天安门后面的午门进入，不过每天4点停止售票，也没有去成。 周日就是到北大领取小礼品和参加颁奖典礼，虽然我们三个人都是观众。由于去的比较早，我们在未名湖边坐了一会儿。对岸的未名湖石碑人来人往，感觉能看一整天。坐着那里才会明白，为什么那么多人向往着北大。我们在学校中只是生存，他们却是生活，鸟语花香，春意盎然。 颁奖典礼在邱德拔体育馆举行，这也是2008年奥运会乒乓球场馆，里面到现在为止还保留着奥运元素。典礼是一个奖项颁奖加一个或两个节目交叉进行的。也请了一些小明星过来唱歌，其中映象比较深刻的是《明天的烦恼交给明天》的mv。自己觉得拍的很有创意，两名歌手一个来自北大的历史专业，另一个来自考古专业。其他的就是看着理工的三个人拿着四架无人机回去，都可以玩无人机编队了…… 当天晚上和其他三个人一起去了全聚德前门起源店。到达后，发现人比较多，取了号后等了一会儿。 之后就是普通的点菜，一只优秀的鸭子比普通的鸭子才贵30块，不过其他菜还是比较贵的。 厨师切好拼盘后，服务员还不忘提醒我们拍照发朋友圈。我和老王纷纷表示，没有周神，怎么可能吃这么高档的东西。 最后，周一下午做火车回到杭州，结束了北京之旅。","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"}]},{"title":"只是为了好玩——Linux之父林纳斯自传","slug":"just-for-fun","date":"2016-05-03T06:28:48.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/just-for-fun.html","link":"","permalink":"https://xiang578.com/post/just-for-fun.html","excerpt":"","text":"这个是我为了学习Linux操作系统，而从图书馆精心挑选的书。 林纳斯眼中的人生意义：生存、社会秩序和娱乐。 Linux起源于林纳斯对于Minx的不满足，壮大于开源运动。 Linux也是世界上最大的开源协作项目。 内容和题目一样，不过也没有太多有趣的故事。 林纳斯的思想朴实情切，不高深。 至少读完之后，你会了解到没有他那样的基础和机遇去创造新的操作系统。 书中出现了例如微内核、分页管理等计算机组成原理中的名词。 现在已经不能说出这些具体含义了。 希望有一天可以去看Linux源代码。","categories":[{"name":"文渊阁","slug":"文渊阁","permalink":"https://xiang578.com/categories/文渊阁/"}],"tags":[{"name":"book","slug":"book","permalink":"https://xiang578.com/tags/book/"}]},{"title":"浙江省第十三届程序设计竞赛总结","slug":"zjp-2016","date":"2016-04-26T05:59:02.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/zjp-2016.html","link":"","permalink":"https://xiang578.com/post/zjp-2016.html","excerpt":"","text":"周六冒雨前往浙大紫金港校区比赛 最终写了4题 获得铜奖 遗憾","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"}]},{"title":"杭电OJ AC400题","slug":"hdu400","date":"2016-03-26T06:00:05.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/hdu400.html","link":"","permalink":"https://xiang578.com/post/hdu400.html","excerpt":"距离上次AC300题，过去了106天。 有很多出去的学长学姐都是不知道把红宝书背了多少遍，又一次和XX聊天，他说我打赌你背的有些单词，你一辈子都不会用上，但是尽管如此，很多人都背下来，这就是毅力，就是韧性。有的人放弃了，有的人却坚持下来，不得不承认，只有耐得住寂寞的人最后才会成功。学习是一项孤独而崇高的事业。","text":"距离上次AC300题，过去了106天。 有很多出去的学长学姐都是不知道把红宝书背了多少遍，又一次和XX聊天，他说我打赌你背的有些单词，你一辈子都不会用上，但是尽管如此，很多人都背下来，这就是毅力，就是韧性。有的人放弃了，有的人却坚持下来，不得不承认，只有耐得住寂寞的人最后才会成功。学习是一项孤独而崇高的事业。","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"},{"name":"hdu","slug":"hdu","permalink":"https://xiang578.com/tags/hdu/"}]},{"title":"道长写作每日一句01-20","slug":"english-writing-01","date":"2016-03-06T13:45:00.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/english-writing-01.html","link":"","permalink":"https://xiang578.com/post/english-writing-01.html","excerpt":"行百里者半九十。 200906：可惜这件事情也没有坚持下去。","text":"行百里者半九十。 200906：可惜这件事情也没有坚持下去。 1 People respectively choose jogging, playing basketball, swimming, skating, climbing or riding as their regular exercises because exercises never fail to make us stronger and more energetic. 人们分别将慢跑、打篮球、游泳、滑雪、爬山或者骑自行车做为自己的日常运动。这其中的原因在于有规律运动会使我们变得更加强壮和精力充沛。 参考译文：人们分别选择慢跑、打篮球、游泳、滑冰、爬山或骑车作为日常锻炼，这是因为这些运动总是使我们更强壮和精力充沛。 2 City residents suffer a great deal from public hazards, such as noise and air pollution, while in the country, far away from numerous exciting activities, one may feel isolated or bored. 城市居民忍受着例如噪音和空气污染之类的公共问题，然而居住在乡村中，却又有远离大量有趣活动导致的孤独感和无聊感。 参考译文：城市居民饱受噪音和空气污染等公共问题的困扰。而在乡村，人们无法参加众多精彩的活动，可能会感到孤独和无聊。 3 We often hear frustrating parents complain that their children are behaving unreasonably while many a child would so often say that his or her parents are just hopelessly old-fashioned. 我们经常听到沮丧的父母报怨他们子女的表现不可理解，与此同时，孩子们也经常说父母太过于守旧。 参考译文：我们经常听到父母抱怨孩子们的行为难以理解，而很多孩子们总是说父母们不可救药地落伍了。 4 Covering all kinds of topics from daily life to political issues, from individual thoughts to world events, Microblogs offer readers anything they might be interested in. 覆盖从日常生活到政治议题，从个人思想到世界大事的所有的主题 ，微博提供使用者任何他们也许感兴趣的事情。 参考译文：微博涉及各种话题，从日常生活到政治热点，从个人思想到世界大事，为读者提供他们可能感兴趣的任何内容 5 Over the course of thousands of years, the Chinese people have created their own unique customs and traditions, and passed them on from one generation to the next. 在上千年的历史进程中，中国人创造了独一无二的风俗与传统，并将此一代代的传承下去。 参考译文：在几千年的历程中，我国人民创造了自己独特的风俗和传统，并把它们一代代传给后人。 6 A lonely heart tends to regard genuine love from other people as the greatest happiness, and a man confined to a wheelchair will have no other wish than to walk like a normal person. - 孤独的人会将他人真诚的爱作为最大的幸福，坐轮椅的人没有比像正常人一样行走更大的愿望。 - 参考译文：一个孤独的人往往把其他人给予的真爱视为最大的幸福，而一个坐轮椅的人除了希望能像普通人一样走路之外别无他求。 7 Today, whether in Bangkok, Beijing or Berlin, the roads of the world's most populous cities are more crowded with motor vehicles than at any other time in history. - 如今无论在曼谷、北京还是柏林，这些世界上人口众多城市的道路因为机动车问题而堵塞超过历史时期。 - 参考译文：如今，无论在曼谷、北京还是柏林，这些世界上人口最稠密的城市的道路上挤满了机动车，数量比历史上任何时候都要多。 8 In the case of a blotched operation, the patient could end up with an unsightly appearance, or at least one that is worse than his or her pre-op appearance. - 在一个手术之后，病人会以一个难看的外表结束，或者至少比他之前的外貌更糟。 - 参考译文：在手术失败的情况下，患者可能会变得难看，或者至少比手术前丑。 9 An advantage of online shopping is that it allows people to find the cheapest price with a flick of the mouse. - 网上购物的一个优势是人们通过鼠标的点击可以买到最便宜的东西。 - 参考译文：网上购物的一个优势就是让人们只需轻按鼠标就能找到最便宜的价格。 10 Internet technology is becoming increasingly advanced and its development is indeed eye-catching. - 网络技术变得越来越高级，它的发展也十分的吸引眼球。 - 参考译文：因特网技术越来越先进，它的发展确实引人注目。 11 We can get a better understanding of the flora and fauna as well as local customs and practices of the country we are visiting. - 当在一个国家旅行时，我们会更的观察当地动植物和体验风土人情。 - 参考译文：我们可以更好地了解所游览国家的动植物资源以及当地的风土人情。 12 Shoddy products would expose the consumers to health and safety risks. - 假冒伪劣产品会给消费者带来健康和安全的风险。 - 劣质产品会使消费者面临健康和安全风险。 13 China's lower mortality rate, when combined with the one-child policy, has resulted in a rapid aging of China's population, which will lead to a pension problem for the Chinese government. - 中国的低死亡率和独生子女政策加速了人口的老龄化，这些将给政府带来在养老金方面的问题。 - 参考译文：我国较低的死亡率以及独生子女政策造成了我国人口的迅速老龄化，这将给我国政府带来养老金问题。 14 There is a growing recognition that people with a master's degree will have a competitive advantage over those with only a bachelor's degree. - 现在越来越多的人认为那些具有硕士学历的人比只有本科学历的更加有竞争优势。 - 参考译文：越来越多的人认识到，和那些只有学士学位的人相比，拥有硕士学位的人将具有竞争优势。 15 On the one hand, there is no denying that the Internet is currently one of the most efficient media for interpersonal communication. - 在另一个方面，互联网毫无疑问的成为了当前个人之间交流最有效的方式。 - 参考译文：一方面，不可否认因特网目前是人际交流使用的最高效媒介之一。 16 Although physical education is a part of adolescent education, a good many students go through their formative years without developing a lasting interest in sports. - 尽管体育教育是青少年教育中重要的一个组成部分，但是大量的学生在生长发育时期没有养成一个持续的体育爱好。 - 参考译文：虽然体育教育是青少年教育的一部分，但是许多学生在他的的性格形成时期并没有培养起来对体育运动的长期兴趣。 17 Mental illness is often difficult to recognize and even more difficult to treat if a person isn't aware of his or her illness. - 精神疾病通常更难被识别，当本人对自己的病情不关心时就更加难被治愈了。 - 参考译文：心理疾病一般很难发现，而且如果人们不知道自己有心理问题，那么治疗起来更是难上加难。 18 A large number of people consider university education as a springboard for good jobs and high salaries. - 大量的人们认为大学教育是满意工作和高薪的跳板。 - 很多人把大学教育视为找到好工作、拿高薪的起点。 19 Although spending 10 to 15 hours a week in a fast-food restaurant doesn't seem to take up too much time, it still reduces the time that should otherwise be spent studying. - 尽管一周在快餐店花费10到15小时看起来不是很多，但是这仍然减少了本应该用来学习的时间。 - 参考译文：虽然每周在快餐店工作10到15个小时似乎并不占太多时间，但这仍然减少了学生本应用在学习上的时间。 20 While self-reliance is a crucial quality that everyone should strive to develop, it is also vital that we learn how to accept other's help. 参考译文：虽然自立是每个人都应该努力培养的重要品质，学会如何接受他人的帮助也是非常重要的。 21 Unemployment causes young people to suffer from an unstable income as well as the psychological stress of not knowing when they will get their next paycheck. 失业除了导致年轻人没有稳定的收入，还给他们增加了不知道下次什么时候获得薪水的压力。 参考译文：失业使年轻人收入不稳定，并承受不知何时才能再次领到薪水的心理压力。 22 The craze for government posts has become a phenomenon that cannot be ignored, and has stirred a heated debate. government posts 公务员 对公务员狂热的已经成为一种不可以忽视的现象，这引起了公众热烈的讨论。 参考译文：公务员热已经成为一种不能忽视的现象，引发了人们热烈的讨论。 23 The media could play an active role in encouraging energy-efficient measures. 媒体可以在鼓励节能上发挥出重要的作用。 参考译文：媒体在鼓励节能举措方面可以发挥积极的作用。 24 Another harmful human activity is our over-utilization of the limited farmland without allowing land the time it takes to replenish nutrients. 其他有害的人类活动还有对有限的土地的过度使用，导致它们没有时间恢复养分。 参考译文：人类进行的另一种有害活动是我们对有限农田的过度使用，而不给土地所需的补充营养的时间。 25 Urbanization has some negative effects on society as large concentrations of people compete for limited resources. 城市化对社会也有许多消极的影响，比如高度集中的人口争夺有限的资源。 城市化对社会有一些负面的影响，因为大量集中的人口要争夺有限的资源。 26 As a green lifestyle, a low-carbon life advocates low energy consumption, thereby reducing CO2 emissions. - 与绿色生活类似，低碳生活提倡减少能源消耗，以此减少二氧化碳排放 - 作为一种绿色的生活方式，低碳生活提倡低能耗，由此减少了二氧化碳的排放量。","categories":[{"name":"文渊阁","slug":"文渊阁","permalink":"https://xiang578.com/categories/文渊阁/"}],"tags":[{"name":"writing","slug":"writing","permalink":"https://xiang578.com/tags/writing/"},{"name":"english","slug":"english","permalink":"https://xiang578.com/tags/english/"}]},{"title":"2015 总结","slug":"2015","date":"2016-02-09T17:59:51.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/2015.html","link":"","permalink":"https://xiang578.com/post/2015.html","excerpt":"","text":"在成为自己讨厌人的路上加速前进。只写四句吧。这是第三句。写完了。","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[]},{"title":"杭电OJ AC300题","slug":"hdu300","date":"2015-12-11T14:16:22.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/hdu300.html","link":"","permalink":"https://xiang578.com/post/hdu300.html","excerpt":"距离上次AC200题，过去了89天。 有很多出去的学长学姐都是不知道把红宝书背了多少遍，又一次和XX聊天，他说我打赌你背的有些单词，你一辈子都不会用上，但是尽管如此，很多人都背下来，这就是毅力，就是韧性。有的人放弃了，有的人却坚持下来，不得不承认，只有耐得住寂寞的人最后才会成功。学习是一项孤独而崇高的事业。","text":"距离上次AC200题，过去了89天。 有很多出去的学长学姐都是不知道把红宝书背了多少遍，又一次和XX聊天，他说我打赌你背的有些单词，你一辈子都不会用上，但是尽管如此，很多人都背下来，这就是毅力，就是韧性。有的人放弃了，有的人却坚持下来，不得不承认，只有耐得住寂寞的人最后才会成功。学习是一项孤独而崇高的事业。","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"},{"name":"hdu","slug":"hdu","permalink":"https://xiang578.com/tags/hdu/"}]},{"title":"杭电OJ AC200题","slug":"hdu200","date":"2015-09-13T07:47:27.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/hdu200.html","link":"","permalink":"https://xiang578.com/post/hdu200.html","excerpt":"有很多出去的学长学姐都是不知道把红宝书背了多少遍，又一次和XX聊天，他说我打赌你背的有些单词，你一辈子都不会用上，但是尽管如此，很多人都背下来，这就是毅力，就是韧性。有的人放弃了，有的人却坚持下来，不得不承认，只有耐得住寂寞的人最后才会成功。学习是一项孤独而崇高的事业。","text":"有很多出去的学长学姐都是不知道把红宝书背了多少遍，又一次和XX聊天，他说我打赌你背的有些单词，你一辈子都不会用上，但是尽管如此，很多人都背下来，这就是毅力，就是韧性。有的人放弃了，有的人却坚持下来，不得不承认，只有耐得住寂寞的人最后才会成功。学习是一项孤独而崇高的事业。","categories":[{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"},{"name":"hdu","slug":"hdu","permalink":"https://xiang578.com/tags/hdu/"}]},{"title":"从零开始利用 hexo + Github/Coding 搭建个人博客","slug":"how-to-build-a-hexo-blog","date":"2015-08-15T11:43:17.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/how-to-build-a-hexo-blog.html","link":"","permalink":"https://xiang578.com/post/how-to-build-a-hexo-blog.html","excerpt":"更新历史 2015年8月15日：完成初稿 2017年10月05日：添加新主题 Even 相关内容 2017年10月11日：文章阅读次数统计 2018年1月19日： 修改页脚 2018年3月26日：双更新 前几天自己在电脑上装Linux时，不小心把博客的数据文件夹给删了。无奈重新安装，同时写下这篇备忘录以防万一。当然，经历这个失误，看来也要利用网盘对博客的一些文件进行备份。","text":"更新历史 2015年8月15日：完成初稿 2017年10月05日：添加新主题 Even 相关内容 2017年10月11日：文章阅读次数统计 2018年1月19日： 修改页脚 2018年3月26日：双更新 前几天自己在电脑上装Linux时，不小心把博客的数据文件夹给删了。无奈重新安装，同时写下这篇备忘录以防万一。当然，经历这个失误，看来也要利用网盘对博客的一些文件进行备份。 安装git和Node.js 就是正常的在他们官网上下载最新版本，然后再点几下鼠标安装到你的电脑。不过，也许后面安装hexo时要设置node.js的环境变量，具体过程也可以百度到。 安装hexo 打开gitbash（可能需要以管理员身份运行），利用npm命令安装。 1$ npm install -g hexo 安装成功后大概会在shell中出现下面这样的信息 部署hexo 比如我要安装在E盘hexo文件夹内，可以在gitbash中使用下面命令进行。如果你想明白这些命令是什么意思，可以百度cmd指令。（如无特殊说明，下面有的命令都在/E/hexo中使用gitbash完成） 12$ cd /E/hexo$ npm install hexo init 安装成功后大概会在shell中出现下面这样的信息 安装依赖包 基础功能包，采用下面的命令安装 1$ npm install deploy git功能相关插件，网上发布时用的…… 1$ npm install hexo-deployer-git --save 附加功能有sitemap和feed插件，如果你不懂这些也没有必要安装 12$ npm install hexo-generator-sitemap$ npm install hexo-generator-feed 第一次本地查看博客 执行以下命令，然后到浏览器输入localhost:4000查看。 12$ hexo g$ hexo s 默认大概是下面的样子 github相关部署和ssh设置 我的这些功能还可以用，所以没有重新设置。故不能详细论述，你可从下面参考链接中获得方法。 发布功能部署 编辑站点的_config.yml文件。你在部署时，要把下面的xiang578都换成你的账号名。 1234deploy: type: gitrepository: https://github.com/xiang578/xiang578.github.io.gitbranch: master 执行下列指令即可完成部署，中间需要输入github用户名和密码。 1hexo generate hexo deploy 安装主题（以NexT为例） 使用gitbash输入下面指令 12$ cd your-hexo-site$ git clone https://github.com/iissnan/hexo-theme-next themes/next 启用主题 修改Hexo目录下的config.yml配置文件中的theme属性，将其设置为next。运行hexo g和hexo s，并访问 http://localhost:4000，确保站点正确运行。 主题优化 参考相关主题的说明文档进行优化，或者访问使用相同主题的博客，查看博主相关文章。或者速成css和html自己改造。 next主题404页面改造 E:\\hexo\\public中创建404.html文件，复制下面代码，并保存，在配置文件中启用相关功能。 1234567891011121314&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;404 - arao'blog&lt;/title&gt; &lt;meta name=\"description\" content=\"404错误，页面不存在！\"&gt; &lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8;\"/&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" /&gt; &lt;meta name=\"robots\" content=\"all\" /&gt; &lt;meta name=\"robots\" content=\"index,follow\"/&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=\"text/javascript\" src=\"http://qzonestyle.gtimg.cn/qzone_v6/lostchild/search_children.js\" charset=\"utf-8\"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 效果如下： 多说评论改造 参考动动手指，给你的Hexo站点添加最近访客（多说篇） 效果如下： 域名绑定 域名可以在万网上购买，大体上流程和淘宝购物差不多。现在大部分域名第一年价格比较便宜，续费也可以接受。当然如果你非要买.集团这种上万的域名，那么请联系我，土豪做个朋友吧！域名是有兴趣可以选择购买，采用默认的github.io也不错。 首先在E:hexo\\public文件夹下创建名为CNAME文件（不要扩展名）接着输入你的域名。比如： 12xiang578.topwww.xiang578.top 接着在万网的域名控制台增加如下图所示的两个解析 为 next 主题添加分类 参考知乎hexo下新建页面下如何放多个文章？ 去除 Coding Pages 等待跳转页面 根据常识，只需要修改主题下面跟页脚相关的代码即可。比如 Even 主题中相关的代码在 themes/even/layout/_partial/footer.swig。 然后在这个文件中添加如下的代码即可。 123&lt;span class=\"hosted-by-coding-pages\"&gt; Hosted by &lt;a href=\"https://pages.coding.me\" style=\"font-weight: bold\"&gt;Coding Pages&lt;/a&gt; &lt;/span&gt; 最后，在项目的 Pages 服务中勾选已放置 Hosted by Coding Pages，等待审核通过就去除等待跳转页面。 修改Even主题的首页 不知道为什么，我不是很喜欢首页那种标题和文章摘要的形式。所以，决定把首页改造成归档页面的形式。 这步改造的思路是将生成归档页面相关的代码复制到生成首页的模板上去。在 themes/even/layout 文件夹里面修改index.swig，具体如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123;% extends &quot;_layout.swig&quot; %&#125;&#123;% import &apos;_macro/post.swig&apos; as post_template %&#125;&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; &#123;% endblock %&#125;&#123;% block content %&#125; &lt;section id=&quot;posts&quot; class=&quot;posts&quot;&gt; &lt;section id=&quot;archive&quot; class=&quot;archive&quot;&gt; &#123;% if not page.prev %&#125; &lt;div class=&quot;archive-title&quot;&gt; &lt;span class=&quot;archive-post-counter&quot;&gt; &#123;&#123; _p(&quot;counter.archives&quot;, site.posts.length) &#125;&#125; &lt;/span&gt; &lt;/div&gt; &#123;% endif %&#125; &#123;% for post in page.posts %&#125; &#123;% set year %&#125; &#123;% set post.year = date(post.date, &apos;YYYY&apos;) %&#125; &#123;% if post.year !== year %&#125; &#123;% set year = post.year %&#125; &lt;div class=&quot;collection-title&quot;&gt; &lt;h2 class=&quot;archive-year&quot;&gt;&#123;&#123; year &#125;&#125;&lt;/h2&gt; &lt;/div&gt; &#123;% endif %&#125; &lt;div class=&quot;archive-post&quot;&gt; &lt;span class=&quot;archive-post-time&quot;&gt; &#123;&#123; date(post.date, &apos;MM-DD&apos;) &#125;&#125; &lt;/span&gt; &lt;span class=&quot;archive-post-title&quot;&gt; &lt;a href=&quot;&#123;&#123; url_for(post.path) &#125;&#125;&quot; class=&quot;archive-post-link&quot;&gt; &#123;&#123; post.title &#125;&#125; &lt;/a&gt; &lt;/span&gt; &lt;/div&gt; &#123;% endfor %&#125; &lt;/section&gt; &lt;/section&gt; &#123;% include &quot;_partial/pagination.swig&quot; %&#125;&#123;% endblock %&#125; 如果需要在首页显示所有的文章，可以参考 如何设置页面文章的篇数？ 安装需要的插件，将 per_page 设置为0即可解决。 添加文章统计 参考 添加文章访问量统计以及 leanCloud,实现文章阅读量统计 自己使用时发现一个问题：每篇文章只有第一次打开时才显示阅读次数，而且计数都为1 参考Hexo搭建博客系列：（五）Hexo添加不蒜子和LeanCloud统计无标题文章中提到的创建 class 需要 数据条目的默认 ACL 权限 中选择无限制 修改页脚 未修改之前的页脚相关代码(themes/even/layout/_partial/footer.swig) 123456789101112131415161718192021222324&lt;div class=\"copyright\"&gt; &lt;span class=\"hosted-by-coding-pages\"&gt; Hosted by &lt;a href=\"https://pages.coding.me\" style=\"font-weight: bold\"&gt;Coding Pages&lt;/a&gt; &lt;/span&gt; &lt;span class=\"division\"&gt;|&lt;/span&gt; &lt;span class=\"theme-info\"&gt; &#123;&#123; __('footer.theme') &#125;&#125; - &lt;a class=\"theme-link\" href=\"https://github.com/ahonn/hexo-theme-even\"&gt;Even&lt;/a&gt; &lt;/span&gt; &lt;span class=\"copyright-year\"&gt; &#123;% set current = date(Date.now(), \"YYYY\") %&#125; &amp;copy; &#123;% if theme.since and theme.since != current %&#125; &#123;&#123; theme.since &#125;&#125; - &#123;% endif %&#125; &#123;&#123; current &#125;&#125; &lt;span class=\"heart\"&gt; &lt;i class=\"iconfont icon-heart\"&gt;&lt;/i&gt; &lt;/span&gt; &lt;span class=\"author\"&gt;&#123;&#123; config.author &#125;&#125;&lt;/span&gt; &lt;/span&gt;&lt;/div&gt; 未修改之前的效果 双更新 同时在 github 和 coding 上更新，然后根据访问时的 ip 地址跳转到不同的服务上。 下图框中的 a 和 b 为两个库的地址。 github 项目中相关的地址修改 最后，域名解析服务中添加如下的解析方式。 自动备份博客相关的源文件 参考博文自动备份Hexo博客源文件 | Jolson's Blog 大概这样就完成一个博客的安装和部署，接下来你就可以快乐的写博客。 参考文章： 献给写作者的 Markdown 新手指南 hexo系列教程：（二）搭建hexo博客 hexo官方网站 Hexo 使用中遇到的问题总结 如何搭建一个独立博客——简明Github Pages与Hexo教程 动动手指，NexT主题与Hexo更搭哦（基础篇） 从零开始制作 Hexo 主题 Hexo 主题开发指南","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"},{"name":"hexo","slug":"hexo","permalink":"https://xiang578.com/tags/hexo/"}]},{"title":"hello world!","slug":"hello-world","date":"2015-08-06T12:58:27.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/hello-world.html","link":"","permalink":"https://xiang578.com/post/hello-world.html","excerpt":"","text":"由于与 hexo-leancloud-counter-security 软件冲突，将标题从 System.out.println(\"hello world!\"); 修改成 hello world!。具体问题分析见 博客折腾记：hexo-leancloud-counter-security 与标题中的引号冲突。 除去前几篇为了测试hexo博客的各项功能所发的博文，这是为新博客所写的第一篇文章。标题取自刚开始学习编程语言时输出的字符串，\"hello world！\" 。 之前，在QQ空间、博客园、csdn上都开过博客，但是结果都不尽如人意。毕竟免费用人家的东西，也不能奢求什么都朝着自己的意愿改变，情怀本身就敌不过他们的kpi。除此之外，也可以自己购买服务器和域名架设博客。可受限于现阶段经济实力，上面的方法也无法实现。所幸github+hexo架设博客的方式，给了我创造一片自由空间的机会。 另外，学习编程之后，觉得可编程的才有安全感。就好像越来越排斥word格式的难控，转向markdown书写的轻盈。hexo是一种实现静态网页的方式，不过自己目前没有专门学习过网页编程，所以大体上采用人家的现成解决方案，再通过网上的文章对站点进行一些小的改造。 开通这个博客的目的在于发一些自己写的文章，以此记录自己的大学生活、学习过程。 于杭州。","categories":[{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"}],"tags":[{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"},{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"}]},{"title":"钱塘江的风","slug":"before-go-to-univisity","date":"2014-08-28T23:02:06.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/before-go-to-univisity.html","link":"","permalink":"https://xiang578.com/post/before-go-to-univisity.html","excerpt":"","text":"几年前读到过，前方纵然山花烂漫,诺是跃不过眼前窄窄的独木桥，就无法扑向那芬芳绽放的原野，眺望美景只能深深叹息。现在，已经过了。 高考前，路是黑的，只有前方亮着一盏灯，于是你只能坚定的朝那个方向走，而高考后，周围忽然亮了，很多条路在你面前，你反而不知道该往哪儿走了。我想起了罗伯特弗罗斯特的《未选择的路the road not taken》,以前是感叹选择的艰难，现在是体验艰难的选择。也只能和他一样选择了其中的一条路，余下的去回忆，或者遗憾。大概你选什么都会后悔的。就像关于高考完去旅行好还是买 iPhone好的回答，如果你去旅行，上了大学你会后悔没有买iphone。如果你买了iphone，大学毕业后你会后悔没有去旅行。 三个月来，多次翻看一位知友的话，高考最迷人的地方在于其阴差阳错，多一分少一分都有可能让你生活在完全不同的环境，认识完全不同的人，难道想一想这没有让你觉得激动和庆幸吗？无论上哪一所大学，都会让你失望，每一个新生都会骂他的学校。这是因为理想和现实永远存在差距。 听到要去学校的名字，钱塘江女子体育专修学校。大概就可以知道其特点，体育方面要求轻松超过清华（大一男子12分钟跑满分要求3000米，清华3000米满分要求12分钟20秒），清华称一流，我们就是超一流;女子就不用解释……再加上专业，总有一种NIKE还造钢笔的感觉。暑假中多次通过专业的工具考察学校地理位置，更是欣喜地发现这是一个适合读书的地方。当然要体验更丰富的大学生活还是要去综合性大学，一说起本校历史社团是归数学与统计学院，就可以知道大概了吧，或者我见识浅薄，他们是用全新的角度去诠释历史。总体上来讲，背靠下沙区，眺望萧山区，还可以称是杭州。按某位学长所讲，在学校边缘还会收到XX移动欢迎您的提示。旁边还有条江，就是题目中风的来源。更庆幸的是以后我从寝室出来，进入杭城另一所大学比去本校方便。这是不是一份花费，两份体验。 关于大学，现在无法去说它，也无必要。有学校在其新生手册中告诫学生，大部分学校本科教育不是濒临崩溃就是已经崩溃。也有复旦英语老师看到那么多大学开设英语专业时说，复旦这些学校可以凭名声再多挣扎一会儿，大部分学校却没有太大意义。大概就是这样了。某些学校还是不要叫大学为好，毁人不倦，希望我是错的。上大学，多半还是因为这条路通关人生的几率更大，直白点再混几年，什么促进人类文明进步，探索未知领域就太远、太广、太泛。初去时的欣喜可以持续多久，是开了又开？当你身临其中，也没有勇气再去赞同原深圳大学校长章必功关于某些东西的评论。毕竟人家是名校，你的是校名。征途，还会是星辰大海吗？ 生活，没有已知条件。在这之前，又有多少人可以猜对归宿？多少人在仓促中选择，面对结果却只能接受。正是有些东西科学无法解释，世界才更有趣。或者仅仅是失败的安慰，不作就不会死。这个十几天内，我们中间有的去了北京，有的去了南京，或是留在了杭州的某一个遥远的角落，你又会在哪里入眠？有些要一年，有些要四年，有些要余生。某些冒险，唯一的作用就是让你明白自己是可以掌握自己的命运的，哪怕只是一点点。 不知道校园内可不可以吹到钱塘江的风，吹到了又是什么感觉？也不知道这是什么样的选择? 不管愿不愿意，新的生活已经开始了。谁知道又会发生什么？ 谢谢你，看到这一行。 很高兴认识你，或许再也不见了。 2014年8月29日夜，于临海 PS：前几天独立、客观、第三方的在学校贴吧发的贴子都被删……","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"},{"name":"zufe","slug":"zufe","permalink":"https://xiang578.com/tags/zufe/"}]},{"title":"毕业杂感","slug":"something-about-high-school","date":"2014-06-09T23:06:26.000Z","updated":"2021-01-23T13:56:41.119Z","comments":true,"path":"post/something-about-high-school.html","link":"","permalink":"https://xiang578.com/post/something-about-high-school.html","excerpt":"","text":"毕业杂感 三年前，我以为会骄傲的走出一中，下午又是怀着初次走进时的心情，三年仿佛一场梦，现在又回到了起点。 下午，唯一的期待只有排队上去领毕业证书，一中唯数不多的尊重个体的存在，最后的感动。其他的节目也可以无视了。之后，向学长讲的一样，信心满满的准备再来一次七圈半。挣扎完一圈后，获得与原来跑完七圈半一样的感觉，或者我也跑不动了。腰上的肉只是增加了体重。难怪其他人知道我长的这么凶猛，还欺负我。散步才是未来胖子的选择。 晚上吃饭时，压抑许久的东西爆发了。往日平静的人都拼命的喝起酒来，拿着麦克风歇斯底里的大叫。更有勇者，宣布了早已被他人看出的东西，不知后来有没有收到这个季节最合适的回答-------哪凉快哪呆着去,希望是我。我和其他两个人与两个老师退到了窗前，后面是喧闹与灯火通明，前面是寂静。冷风吹久了才明白，凤凰山庄真高档。听着已退休的化学老师感叹退休后真闲。也发现教了三年物理老师一只眼睛已经看不见。原因居然是书看太多…… 与他人约定的雪碧大战啤酒，也在我喝完两杯雪碧后放弃了，幸亏他们又一次放过了我，no zuo，no die。我还是太年轻了，聚会还是喝酒好。在期待中，我们等来了最喜欢的数学老师。带着数学都是谁教的迷惑，我们勇敢的抬起了老师，毕竟五年来最难。每个人都在那里发疯，我也无聊的把啤酒摆成了12两个字。最后数学老师直接拿起了一瓶啤酒喝了起来，为什么没有人叫白酒。 离别时，有人对我说崇拜了我两年，愉快地说。小朋友真诚实。可却不知道，三模后我也开始嫉妒她。或者我是书看的太少，想的太多。想起其他人捡到的一本上届学长高三日记，原本以为他的经历我只能膜拜，到后来我也做到了。见证了他梦想从浙大到南大，最后留下作文跑题的遗憾，而我会有什么？我也没有勇气在自选考完之后，对着学霸讲出：你的时代已经结束了！虽然也只是我认识他，他不认识我。而我，也仅仅是俞敏洪演讲所提到的，有时候不是因为我坚强继续，只是别无选择，所以才全力以赴去做某些事。 此刻不得不提高考，去年6月承受着巨大的压力，有时中午是想起这东西而惊醒。这一年却又是平静。到头来，很少有人可以改变自己。就像六年来，没有发现我认为有巨大改变的人。该干什么，还是干什么。不该干什么，也许也会干。在考场上你才会发现，这就是高考，你已无能为力。打败你的不是试卷，而是你自己。放弃的自己。多数人也再无机会，唯一的选择只是直面惨淡的人生。无论什么结果，生活还要继续。现在才明白，高考不是淘汰，而是磨平一部分人的心。我们有过豪言壮志，非XX不上，再随手吓死看不起自己的人。到今天，更多的是正常水平就好，XX也好。毕竟是等待，为什么不多安排几个人改呢？自此，安逸才是奢求。想看过海韵上的文章所讲一样，8点钟睡觉。不由回想起小学时被逼8点钟睡，而不得已去拨慢时钟，最后在慢了半小时后被发现。真是罪过。现实下，哲学与诗歌终究太过单薄，宗教才是人类心灵之火的栖息地。像语文老师讲过的，信教真幸福。愿意余生在佛与GOD中度过。初中同学的突然生病离开，我才发现在生命面前，高考已无那么重要。有什么事再不做就没有以后了。 高考前觉得干什么都有用，家里人问我为什么看那么多与历史有关的书，我也可以骄傲地回答，可以用来写作文。高考之后换成了看什么都没有用，再看见那些书，只有祈祷，如果有来生，我也愿意信来生，再见面。 又回到自己，究竟是什么？自己认为的失败者。晚上还有人说我很有趣，老师告诉我够了。或者去年运动会时，某高二听了我的解说，认为我像周立波。都是不同，选择的不同。我很难与大多数人竞争，所以经常去干人少的事。就像3000m，实际上到了才发现人更多，所以又一直期待校运会上有5000m。实际上我更认可疯狂，像苹果早年广告所讲的一样，只有疯狂到自己能改变世界的人，才能真正改变世界。毕竟这也是曾经的梦想。 再回来，发现还有许多事情没有干。不提破解教室里的饮水机，踩死操场上所有的小草这些小事。总觉得走的太匆忙，解析几何还是不会，最后一题也仅是看看。达不到数学老师140分免暑假作业的要求。人生就是这样遗憾。 下午听到不是在最美的时光遇见你们，而是因为遇见你们才有了最美的时光。说白了也只是适应环境。去年来12班回访的学长讲到，如果高考考好一点，就可以去浙江财经大学，半年来发现杭电也不错。呆久了就会好的。三年的一切都在脑海里翻转，等待着我去忘记。原以为记录是最好的回忆，现在才明白遗忘才使过去日久弥新。就像教育的本质是那些你几年后还记着的东西。五毒书记张二江出狱后感叹过，关进去前几年，情人还来看过，原来整天称兄道弟官场上朋友，早不见踪影。只有大学同学，不管他是书记也好，囚徒也罢，还过来看看他。这就是北大EMBA招生广告中所宣称的28万让柳传志叫你一声老同学的意义吧。 浮生着甚苦奔忙，盛席华筵终散场。悲喜千般同幻渺，古今一梦尽荒唐。曹雪芹用这样的诗句来表明写红楼梦的内心，我们又何尝不是，三年来，在情感和分数里煎熬，吃的猪狗食，学的数理化，被囚禁的野兽是不会感谢笼子的。欢乐的时光总是短暂的，没有了下期再见。我们也终将忘记高考，终将忘记一中，终将忘记彼此。我也无法否认生命因你们而精彩。 我爱这个世界。 谢谢你看到这一行。 本文仅是午夜梦话，所写一切均可看成虚构，与我无关。 6.10 00：01 于椒江 6.11 00：38修改 6.20 10：05修改","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"}]},{"title":"如果，高一","slug":"if","date":"2012-07-22T23:17:32.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/if.html","link":"","permalink":"https://xiang578.com/post/if.html","excerpt":"","text":"1序 受《如果，宅》影响下编出来的东西，博君一笑。 2某星期三下午 学术报告厅内听校长讲座 校长：“同学们要根据自己未来的职业选择来读文理科。” 主任：“我要当领导，选什么？” 我:“退学！” 过了一会…… 我:“先去泰国做手术，再找个干爸爸……” 主任：…… 3语文课讲《烛之武退秦师》 老师：“烛之武为什么要这样讲，我们先看一下当时的中国地图。” 我纳闷了，没电脑，没插图，怎么看？ 只见老师麻利地檫干净了黑板,化画了三个圈，分别写上秦晋郑。 老师：“当时秦国在这里，晋国夹在秦郑中间。” 4某天听力训练 主任写的不耐烦了，大叫并伴灿烂的笑容：“信B叔，得永生。” 于是写下了B B C 翻开答案一看：B C C 我：“B叔还是太年轻了。” 5 政治课前 我拿出政治书：“翻开政治，一想起国家尚未统一，世界尚未和平，我就没心思读了。” 主任：“建国前，刘少奇说：‘国家尚未统一，选毛主席。’朱德说：‘国家尚未统一，选毛主席。’邓小平说：‘国家尚未统一，选毛主席。’最后就有了毛主席。” 6 中考之后 瓜子：“伯形女儿考了这么好应该请吃饭。” 我：“多吃几张试卷还是有的。” HG：“函数大餐，三角点心，向量甜品。” 7化学课，讲蛋白质，烧了蛋白质物品后 老师：“想要闻一下味道的，自己回去烧一下头发。” 我：“烧一把也没用。” 昏君淡定地笑了。 8数学课上课之后，化学老师还在 伯形：“大家先看一下上面的题目。” 化学老师看着自定义题目说：“这是什么符号。” 众人大笑。 伯形：“化学符号……” 9化学课 教写A2B,AB2型电子式。 老师：“都懂了吗？” 我：“过氧化钠怎么写？” 主任：“HClO怎么写？” …… 10 4月1日数学课 伯形照例抽人上去默写。 伯形：“41号” 我无奈地走了上去。 伯形拿着粉笔笑咪咪地对我说：“今天是你的节日……” 我…… 实际上都是本地老狐狸，装什么聊斋…… 11期中考试后，数学课 伯形：“你们江山一片红，几乎全是三十几分。” 我：“终于全班几十分了。” 12期中考之后，某一下午 伯形在教室叫了几个人默写后。 伯形：“这样查了，如果你们期末还没考好，我就去跳楼。” 在教室的人欢呼雀跃。 伯形：“有什么好激动的，我从一楼跳四楼，从楼梯上跳就好了。” 于是，期末考试数学再创新低。 13 音乐课 老师:“你们站到后面去排练一下。” 主任：“沈XX坏我大事，数学作业写不成了。” 我：“对啊！好几节音乐课没上，我的睡眠规律都被打乱了。” 14语文课，找人上去演讲《我有一个梦想》 王木高票推荐上台。 王木：“今天，我有一个梦想。” 众人：“好！” 某些人：“不是今日吗？” 众人：“哈哈……” 15物理课 老师：“我比较喜欢画V-T图像解题。” HG：“我比较喜欢A-V图像。” …… 16默写完《雨霖铃》之后 老师：“有一位同学，错的很离谱，‘兰舟催发’写成了‘兰州催发’，兰舟就是小船，你写兰州干什么？” 主任：“贴吧玩多了。” 我：“人才！” 某人：“不去贴吧可惜了！” 二默中涌现出更多人才…… 17政治课 讲宗教问题 老师：“清华北大是我国最高学府，大学生研究生出来，用科学思想武装自己，还是宗教思想，你们出来干什么？” 主任：“卖猪肉！” …… 18瓜子生日 主任HG准备好好庆祝一下！！！为新高一奉献一场精彩的表演。 晚读时 主任：“我已经等不到下课了。” 下课之后，我好像没有听到大动静。 主任从外面回来之后：“太爽了，没抬到楼下，在厕所门口就解决了。” 我：“有人帮你们吗？” 主任：“我和HG叫了一声阿鲁巴，一群七班的叫着：‘生活需要激情！’ 帮忙抬起了瓜子，最后瓜子裤子破了，才停。” 回到寝室 我：“晚上咋么样？” HG：“一群七班的大叫：‘生活需要激情！’……” 看来被压抑太久了…… 19 WCG取消PC项目，看完游戏风云专题回顾后 主任：“WCG也欠我一个冠军！” 我：“……没关系，让HG去打鸟赢一个冠军回来。” 20 校园改造，拆除护网后 我：“全拆了。” HG：“不，全都换上了无线网，看不见了，效果依旧。” 我：“一中也要打造无线校园了啊。” 21准备音乐考试 我：“有没有什么歌，整首只有一句歌词，一个调的？” A：“祝你生日快乐~~~~~” 我：“是不是还有英文歌 happy birthday to you？” A：“对的！” 22看着旁边一男的穿了一白背心 我“他穿背心了。” 主任摸了一下他背：“你穿吊带了啊！” 那人：“你神经病啊！” 23继续吊带 我：“今天他有没有穿？” 主任一摸：“穿了。” H：“有什么好笑的，温jianbao也这么穿。” 我：“你怎么知道？” H：“他深入下乡时不是都这样穿得吗？” 24继续 我：“今天领导有没有下乡。” 主任：“有。” H…… 25多日之后 我：“领导好久没下乡了！” 26某星期一早晨第一节课 班主任看着主任位子上没人对我说：“他还没来？” 我：“没有来？” 班主任正准备打电话，主任推开门进来 老师：“怎么这么晚才来？” 主任：“有原因的。” 老师：“起晚了？” 主任：“闹钟没响。” 27主任坐下后 我：“这么晚来都没事的啊？” 主任：“迟到一两分钟门卫会记你的，再晚一点他们不会管你的，所以我在外面逛了好久才进来。” 我：“原来你是为了班级荣誉才来那么晚的啊！” 主任笑了。 28 历史课，讲文学类型 老师：“路漫漫其修远兮，吾将上下而求索。有其他班同学讲漫漫没有三点水。” 众人：“语文书上没有。” 老师：“可能版本不同吧 ？” 我：“语文课上讲，写三点水的没文化。” 老师：…… 29 期中考前一中午 教室后面又要进行阿鲁巴。一群人围住了一女生。 我：“是强奸吗？” 主任：“不，是轮奸。” 抬起时，伯形从外面晃进来。 众人迅速散去。 伯形：“你们只会欺负她一个人。” 伯形：“你们中午不学习，不要吵，去休息。搞的跟国际文武学校一样。” 说完就离开了。 主任：“风气都被七班带坏了。” 我：“又被救一个。” 30浙大学子回访时 在广播台做节目 主持人：“听说浙大美女很多？” 学长：“这么跟你说吧，我们一共有六幢宿舍，一女五男。” 事后一想，不正是一对情侣，两对基。 31期中考前 地理课 由于地理老师没讲清楚，大部分人不知到导引写哪里。 老师看见一个人用手挡住作业本：“不用挡了，我看见你没写了。” 老师继续看其他人。 看见我将作业大开地放在桌子上：“你怎么没写也不挡一下？” 无奈只好用手挡一下…… 32 A:“化学实验手册明天交不交。” 我：“可以期中考后交。” 主任：“也可以不交。” 我：“丢了就好，主任经常丢的。” 主任：“我什么时候干过？” 我：“对，你是忘带了，放在枕头下面。” 33主任经常丢物理作业之后 主任：“我物理书丢了，怎么办？” 我：“再接再厉，帮我们把物理老师丢了。” 主任…… 34未上课前 老师：“把地理试卷拿出来给我看看。” 我：“我先去趟厕所。” 当我在洗手时，又陆续来了一些人…… 我：“大家都来了。” 周哥：“你那边已经检查过了，可以回去了。” …… 35分析2012浙江英语高考作文，读例文时 老周：“Take Abraham Lincoln, for example.” 我：“我可不可以Take my father Li Gang, for example.” 主任：“我爸是李刚，吓坏老师。” 我：“实际上，我爸是李刚，老师你懂的。” 36英语讲单词时 老周指着黑板上的 admire the moon 说：“实际上这moon换成girl更好。” 某人：“beauty！” 老周：“对！” 37 数学课用Word讲题 由于式子太小 伯形：“放大点，给你们看清楚。书没读好，各个都是高度近视。” 众人无语。 伯形：“虽然我书没读好，但眼睛还是好的。没有1.5，至少1.2。” …… 其实我们不关心你视力多少，更想知道书没读好眼睛没坏，到底花时间干什么去了…… 38 去烈士陵园路上 HG：“早知道带副扑克去了。” 主任：“早知道带三国杀去了。” 我：“你们这帮人，战争年代，革命烈士抛头颅洒热血，才换来今天的幸福生活……” 此时，伯形从傍边经过，然后盯着我…… 39 老周不在，14班老师代课 下课之后 昏君：“你们听懂了吗？” 我：“一整节课下来，我只听懂了一样东西，对于这个我也只能回答 I don’t know !” 主任：“都是you know? you know? you know? you know?” 40 继续 教室另一边 A:“有100多遍you know?” B:“130多好吗？我很仔细记了！” 接下来的一周里，课堂上听的都是you know? you know? you know? you know? 41英语课讲二模改错时 讲到一个单词 梳头 老周：“一般女孩子都会带梳子和镜子 ，可以整理一下。” 主任：“HG也有！！！” 42还在讲改错 老周读了一个长句子后，问哪里错了 主任：“这么长的句子，一个标点都没有，肯定有问题。” …… 老周又读完一个句子 主任：“这么长的句子，一个标点都没有，肯定有问题。” 43健力宝在超市门口搞活动 于是我们过去每人买了一瓶准备抽一把雨伞 无奈只抽到一些圆珠笔和餐巾纸 老板：“实际上，你们抽到都赚了。这些东西价格都在一元以上。不是在乡下，餐巾纸都要一元吧。” 我指着大一中超市：“这里面就卖五毛。” 老板…… 44 老板继续：“你们吃完饭后，可以擦嘴。” 他把手放在嘴前做擦嘴动作。 老板：“上完厕所后……” 双手放下…… 然后…… 我们走了…… 45 大合唱排练时 我们在体育馆台阶上练习时 HG指着其他一个班：“那是美女与野兽吗？” 主任：“不，是野兽与野兽。” 我：“不，是禽兽与野兽。” 46大合唱前，男的穿粉红色短袖，女的穿蓝色短袖 我：“等一下，他们以为八班男生都剪短发。” 昏君：“应该是八班男生好矮。” L：“八班男生好多。” 我：“八班女的像男的。” …… 47 午休 英语考试前 老周：“小考小信心，大考大信心。” 主任：“小考小打击，大考大打击……” 48数学课 伯形指着黑板上的a=xb+yc说：“看到这些想起了什么？” 众人沉默 伯形：“苹果？如果想起苹果，那你完蛋了？” 之后 伯形：“想起了什么？” 主任：“苹果。” 我：“蓝莓。” 49数学课 已知P点是三条中线交点。 伯形：“P点是三角形的什么心？” 主任：“内心？” …… 主任外：“外心？” …… 伯形：“没良心……” 50某星期四早读 伯形进来后，语文课代表在讲台上查语文作业 伯形：“实际上，作业不用每天查。一次查到没做乘以五就好了。” 我：“这都什么？” 伯形：“你没学过抽样统计？” 计算着要抄的语文课文数（=1+2+3+4+5） 我：“好残忍！” 伯形：“谁让你犯贱！” 51主任杭州漫展回来后 买了桌游《宿命》，我翻着说明书 主任：“你现在跟我比宿命英雄技能谁知道的多!” 我：“比三国杀，爆你到日本。” 52英语听力时 广播：“衬衫的价格是九磅十五便士。” 老周：“物价上涨，只有衬衫的价格不涨！” 53物理课 老师：“重力做功与路线无关。什么力做功与路线有关？” 我：“摩擦力！” 老师：“对！” 于是在黑板上画了一个大圈。 老师：“从1 2 3走，摩擦力做功不同。” 主任：“老徐生动形像的解释了2b青年 文艺青年 普通青年。” 54 数学课讲练习 我：“又是观察猜想法！” 伯形：“也许你还不会！” 我：“全部填空题我就对这一道容易吗？” 55开学不久后的数学课 伯形指着一个复杂的等式：“会解吗？” 伯形：“不要看见就怕了哦！” 无人理…… 伯形：“跟我在七班说的一样，你们这帮人，只能去造鸡蛋。造原子弹早就把自己炸死了！” 56数学课教求导公式时 伯形：“高斯的老师给学生一道题目1+2+3+…+100 有人从1加到100。 如果是你会怎么算？“ 伯形：“十岁的高斯有个很好的方法。” 我：“首项加末项……” 伯形：“那你还行？” 我：“七岁老师就教过了！” 伯形…… 57期中考 考完政治之后才知道和其他人的差距。 我考完试回来，有人吃完饭回来。 我：“你有没有吃完饭？” 主任：“想出来，出不来。” 我：“为什么？” 主任：“伯形监考。” 我：“脸皮厚一点就好了。” 主任：“有个十九班的要出去，被伯形留了下来。” 我…… 地理考试中，伯形抓住一打小抄作弊者，我们感慨：“真乃电子狗伯形！” 58 某星期一下午 语文课看电影，下课后，伯形进来看。 过来一会儿，语文老师带着电脑走了。 伯形：“其实电影看多了，不好，人会变笨的。” …… 59 期中考完历史后 有一道题目，让你写毛泽东井冈山时期写的文章。 C：“《井冈山的故事》” 其他人：“《井冈山的斗争》” A：“什么故事？” C：“爱…情…故…事…” 60期中考后讲数学试卷 伯形叫了几个人上去写题目，自己在教室里转。 伯形：“你们这些人都是臭鸡蛋！” 众人不解。 伯形：“21题平均分0.9分，不是臭鸡蛋还是什么？” 我：“原来我是两个臭鸡蛋！”（21题 8分） 61 分析数学试卷时 伯形：“你们脑子坏了，用硫酸都融化不了，” 众人：“用王水。” 伯形：“什么王水，硝酸都不行。” 我：“王水酸性比硝酸强！” 62期中考后历史课 老师：“我把选择题答案报一下。” 众人：“写一下吧。” 老师：“写出都不环保。” 我：“讲出来有噪音污染。” 老师：“啊，写出来浪费粉笔。” 我：“高考前禁噪。” 老师…… 63数学课将习题 伯形：“到底记住没有？没记住的都吃一点皮猪肉。” 伯形：“吃皮猪肉会增强记忆力，少吃鸡蛋。” 我：“那你吃了几头？” …… 64 期中考后 梁：“HG，我们政治都一样4X，班长8X，不是人。” 梁：“我们这种人，国家最放心，班长，国家不放心。” 65 英语课分析报纸 我指着试卷上的“go ahead”问主任什么意思。 主任：“去个头。” …… 从此以后再也不问主任…… 66数学课 伯形“现在你们要好好学习，考上大学。以后出去，没大学文凭，人家会看不起你。” 伯形：“为什么现在哪么多老板会不惜几十万买一个文凭，摆出来看。我原来在读硕士，有老板花几万找人上学，再花一二万，找人写论文，一个博士文凭7万！！！” 主任：“才七万。” 我：“不读了吗？” 67化学课 要用酒精灯时 老师：“怎么实验员老是不给我火柴。” 众人大笑 老师：“幸亏我有打火机，呵呵！” 68化学课 老师：“好我们开始上课。” 我：“还有十分钟能讲什么新课？” 主任：“下课还有三十分钟。” 69王木回家喝喜酒回来后 我：“为什么不叫我们过去？” 王木：“……” 某人：“你家朋友多不多？” 王木：“没几个。” 我：“叫我们过去人就多了，等一下，你家亲戚会发现，你的人缘真好，认识的男的多，女的更多。” 70继续 HG：“对啊，天哥过去，会认为是高二升高三，yjn初三的，gx初一的。” …… 71历史课 老师：“这星期作业为导引必修一部分，星期日交。你们每天要写一点。不要以为是星期六晚上的作业。” 主任：“没关系，对于我是星期日早上的作业。” 72物理课——重力势能 老师：“可能这个例子不太好，有一个同学从四楼跳下去，我们来研究这个问题。” 众人：“@#￥%##@￥%……&amp;*￥#%！” 老师：“没什么问题吗？” 我：“思想问题，自杀是犯法的。” 老师：“我没说他往外面跳，他往里面跳。” 我…… 73化学课 老师：“乙炔与乙烯是同系物吗？” 众人:“不是。” 老师：“大学中化学系与物理系不一样。” 我：“魔兽世界与魔兽争霸听着差不多，玩起来会一样吗？” 主任：“这就听懂了。” 74开学不久后 吃完早饭，回寝室中 主任看见有人边走边玩iphone。 主任：“其实不交钱，我也可以买iphone。” 我：“其实我们交了1代到4代外加ipod1 ipod2的钱。” 75数学课上午第一节课 伯形：“前几天，我看了一个电视。说你周六写作业，第一样写了1小时，第二样写了半小时，第三样写了二十分钟。最后你没写完。专家说你睡一觉，觉得时间没了会写完全部作业。” 某人：“那我们先睡半小时在上数学课。” 伯形：…… 我：“难道高考先睡一个半小时，最后半小时，发愤图强将压轴题也秒杀了吗？” 76伯形理完发后，回来上课 站在讲台上扫视一周后 伯形：“后面一排干什么投降，个个低着头。” 我：“被你帅气的发型吓怕了……” 主任笑而不语 77物理课 老师：“第二题写错的同学没有看清两个字：匀加速。” 众人：“不是三个字吗？” 我拿起手指数了一下说：“我该补数学还是语文？” 78期中考前 发下一张物理答题卷 昏君看着选择题 、填空题的横线，找不到题目：“这叫人怎么写？” 主任：“这主要训练考前蒙题能力的，让你结合题号、上下题选项来蒙。” 混君：“填空题怎么办？” 主任：“结合生辰八字、考号、考场位置来蒙。” …… 79地理课其他老师代课 老师：“李老师讲过可持续发展三大原则吗？” 主任：“没有。” 老师：“公平性原则……” 主任：“有。” 回想起他的听课，我：“你懂？” 主任：“……” 80地理课，继续代课老师 老师：“我国黑土地分布在哪里？” 我对主任说：“QQ农场。” 老师：“你们有没有讲过紫色土壤。” 主任：“QQ农场还没出。” 我：“腾讯高级机密……” 81政治课 讲中央机构产生过程 A:“为什么都只有一个候选人？” B：“如果有两个，一个没选上，关系不搞僵了，大家都认识……” 82不知团委书记找老周写什么东西。 老周：“笑话，让我一个英语老师写中文。” 主任对我说:“那我们中国人学什么英文……” 83自修课 看着前面XX与XX在XX 主任对我说：“只允许XX，不允许学习。” 我：“等到老了的时候，有人问你干了什么，你学习了，他们XXX。” 84某天语文课 老师：“现在同志都变味了，要是在以前，一声同志多么亲切，革命友谊。” 众人：“现在什么意思？” 老师：“我不告诉你，自己去查。” 于是，某人掏出手机…… 没有然后了…… 85数学课讲练习 伯形：“大家看看，做选择题就要不择手段，做出来就好了！” 众人大惊 伯形仔细一想，又补充了一句：“当然，作弊除外。你想一下，你把你旁边的人试卷看了，他比你更笨，你把自己正确的答案改了，怎么办？”","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"}]},{"title":"2011","slug":"2011","date":"2012-01-23T03:05:52.000Z","updated":"2021-01-23T13:56:41.115Z","comments":true,"path":"post/2011.html","link":"","permalink":"https://xiang578.com/post/2011.html","excerpt":"","text":"已过 2011年早已过去二十多天。原本打算在元旦完成这篇，可谁知一拖就到了春节。从公历新年到了农历新年。写这些东西，也不完全是一时心血来潮的想法，而是希望留下一些关于2011的记录（毕竟记忆力这东西不牢靠），加上借此机会反思一下自己一年来的行为。 回忆起2011，有什么特别？卡扎菲之死？乔布斯之死？金正日之死？或者三国杀完结？斗破完结？姚明退役？一件件大事跃然纸上，仔细一看却都不是什么好事。更实际些，只剩下中考。此处的中考，不仅仅局限于考试的那几天，更应该包括中考之前的几个月以及中考之后几个月所带来的一系列的变化。 先从中考开始说吧，很可悲，因为中考，才制定了人生中第一个明确的、长远的计划。说白了就是考上一中，仅此而已。2011年初，《网络游戏秘笈》搞了一个活动，写给2012的一封信。看了之后，我也写了一封，不过不是给2012，而是给6个月之后。当然6个月之后，我把它撕了。其中内容也无非是关于高中，当时假设会考上一中。最后，我仅猜对了结果，没有猜对过程。无奈，为国家财政分担教育支出。不过正如一位老师对我说的，再差还是在一中。也就马马虎虎完成了目标。 1月 期末考之后，拥有很大决心，独自跑到新华书店买了十本书（5本天利38套，5本压轴题），准备在下半学期完成。最后，也只是每本写了一点。 接下来的几个月，就是昏天黑地的读书（实际上也没那么严重），只为考上高中。记忆在此处已经是一片空白。 中考之后的暑假，本着白日做梦精神，规划高中学习，前途一片光明。时至今日，我发现我还是继续睡吧。离梦想越来越远。 8月底 可恶的军训，无非是关于痛苦的记忆。印象最深的是一天晚上，全班男生被教官罚蹲（不知是谁发明的，遭天杀的！！！完全无视人身权利），忍受着自身重力所带来的痛苦，一直到那天晚上训练结束。至今不明原因。 9月 高中第一个月，一来就不适应。前几天，一到数学课就想睡，下课铃一响就来精神。还有一读就想睡的政治，不知所云的地理。总之，前几个星期做梦般度过。直到有一星期，一同学打电话问我一道数学题，我才发现几星期来什么也没学，于是开始改变自己。 10月 第一次月考和运动会。成绩也令人失望，也发现自身存在的问题。于是，从那以后，几乎每天晚上到学习室报到。运动会没我什么事，三天都是坐在看台上打酱油，只是被高二高三开幕式创意所震惊。 11月 第二次月考 明白了跟其他人一样的学习方法根本不会有所谓的成功，原来不想做的必须做。也发现每天晚上去学习室写作业只是浪费光阴，造成效率低下。当然也记得，无聊所搞的神棍节大联欢以及那天晚上男生寝室的疯狂 12月 又是一年末。一学期以来习惯越来越差，也有了星期一恐惧症。还有元旦晚会，主要以热闹为主，谁管你好不好看，一上台就鼓掌呐喊，热闹就好。晚上，同学开始在寝室玩阿鲁巴，暴力啊！那么鲁着鲁着就到了2012…… 1月 期末压力加剧，最后以远离计划的结果匆匆地结束了第一学期。 2011年大致就是如此，总结起来或许就是悲剧。然而却又无法否定悲剧所带来的改变。正所谓失败乃成功之母,一年以来各方面一直都在改变，世界观也越来越全面，思考越成熟。 2011年也认识了许多新朋友，虽然代价是与另群人见面更难。失去之后才懂得珍惜。走出去，才知道舞台的宽广。人生正是因为你们而精彩。 那么2011就结束吧！ 对于2012，最先想到的是世界末日，主要归功于美国灾难大片《2012》的影响（只是听多了，却为曾看过）。世界将在2012年12月毁灭。当然，我也不想信1.听说玛雅人的预言水平与我的水平差不了都少。2.科学点，2012年只是玛雅文明新纪元的开始，就像我们从腊月廿九到龙年正月初一，只不过人家5000多年才新纪元，以及关于这之后没有预言（预言本质上看都是编的）3.世界上最公平的事，方舟不可能造出来，官二代富二代都与我们一样。 实际点，会考以及文理分班更值得担心。 当然对于2012，我充满信心。或者，我要活着看到2013年太阳的升起。 就是这一些吧。终于写完了！！！！！！ 一万年太久，只争朝夕。 2012年1月23日 壬辰龙年正月初一 各位新年快乐一点吧！！！！！！去二十多天。原本打算在元旦完成这篇，可谁知一拖就到了春节。从公历新年到了农历新年。写这些东西，也不完全是一时心血来潮的想法，而是希望留下一些关于2011的记录（毕竟记忆力这东西不牢靠），加上借此机会反思一下自己一年来的行为。 回忆起2011，有什么特别？卡扎菲之死？乔布斯之死？金正日之死？或者三国杀完结？斗破完结？姚明退役？一件件大事跃然纸上，仔细一看却都不是什么好事。更实际些，只剩下中考。此处的中考，不仅仅局限于考试的那几天，更应该包括中考之前的几个月以及中考之后几个月所带来的一系列的变化。 先从中考开始说吧，很可悲，因为中考，才制定了人生中第一个明确的、长远的计划。说白了就是考上一中，仅此而已。2011年初，《网络游戏秘笈》搞了一个活动，写给2012的一封信。看了之后，我也写了一封，不过不是给2012，而是给6个月之后。当然6个月之后，我把它撕了。其中内容也无非是关于高中，当时假设会考上一中。最后，我仅猜对了结果，没有猜对过程。无奈，为国家财政分担教育支出。不过正如一位老师对我说的，再差还是在一中。也就马马虎虎完成了目标。 1月 期末考之后，拥有很大决心，独自跑到新华书店买了十本书（5本天利38套，5本压轴题），准备在下半学期完成。最后，也只是每本写了一点。 接下来的几个月，就是昏天黑地的读书（实际上也没那么严重），只为考上高中。记忆在此处已经是一片空白。 中考之后的暑假，本着白日做梦精神，规划高中学习，前途一片光明。时至今日，我发现我还是继续睡吧。离梦想越来越远。 8月底 可恶的军训，无非是关于痛苦的记忆。印象最深的是一天晚上，全班男生被教官罚蹲（不知是谁发明的，遭天杀的！！！完全无视人身权利），忍受着自身重力所带来的痛苦，一直到那天晚上训练结束。至今不明原因。 9月 高中第一个月，一来就不适应。前几天，一到数学课就想睡，下课铃一响就来精神。还有一读就想睡的政治，不知所云的地理。总之，前几个星期做梦般度过。直到有一星期，一同学打电话问我一道数学题，我才发现几星期来什么也没学，于是开始改变自己。 10月 第一次月考和运动会。成绩也令人失望，也发现自身存在的问题。于是，从那以后，几乎每天晚上到学习室报到。运动会没我什么事，三天都是坐在看台上打酱油，只是被高二高三开幕式创意所震惊。 11月 第二次月考 明白了跟其他人一样的学习方法根本不会有所谓的成功，原来不想做的必须做。也发现每天晚上去学习室写作业只是浪费光阴，造成效率低下。当然也记得，无聊所搞的神棍节大联欢以及那天晚上男生寝室的疯狂 12月 又是一年末。一学期以来习惯越来越差，也有了星期一恐惧症。还有元旦晚会，主要以热闹为主，谁管你好不好看，一上台就鼓掌呐喊，热闹就好。晚上，同学开始在寝室玩阿鲁巴，暴力啊！那么鲁着鲁着就到了2012…… 1月 期末压力加剧，最后以远离计划的结果匆匆地结束了第一学期。 2011年大致就是如此，总结起来或许就是悲剧。然而却又无法否定悲剧所带来的改变。正所谓失败乃成功之母,一年以来各方面一直都在改变，世界观也越来越全面，思考越成熟。 2011年也认识了许多新朋友，虽然代价是与另群人见面更难。失去之后才懂得珍惜。走出去，才知道舞台的宽广。人生正是因为你们而精彩。 那么2011就结束吧！ 对于2012，最先想到的是世界末日，主要归功于美国灾难大片《2012》的影响（只是听多了，却为曾看过）。世界将在2012年12月毁灭。当然，我也不想信1.听说玛雅人的预言水平与我的水平差不了都少。2.科学点，2012年只是玛雅文明新纪元的开始，就像我们从腊月廿九到龙年正月初一，只不过人家5000多年才新纪元，以及关于这之后没有预言（预言本质上看都是编的）3.世界上最公平的事，方舟不可能造出来，官二代富二代都与我们一样。 实际点，会考以及文理分班更值得担心。 当然对于2012，我充满信心。或者，我要活着看到2013年太阳的升起。 就是这一些吧。终于写完了！！！！！！ 一万年太久，只争朝夕。 2012年1月23日 壬辰龙年正月初一 各位新年快乐一点吧！！！！！！","categories":[{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"}],"tags":[]}],"categories":[{"name":"文渊阁","slug":"文渊阁","permalink":"https://xiang578.com/categories/文渊阁/"},{"name":"机器学习","slug":"机器学习","permalink":"https://xiang578.com/categories/机器学习/"},{"name":"Never-Reading","slug":"Never-Reading","permalink":"https://xiang578.com/categories/Never-Reading/"},{"name":"生活志","slug":"生活志","permalink":"https://xiang578.com/categories/生活志/"},{"name":"站务","slug":"站务","permalink":"https://xiang578.com/categories/站务/"},{"name":"程序园","slug":"程序园","permalink":"https://xiang578.com/categories/程序园/"},{"name":"七年之旅","slug":"七年之旅","permalink":"https://xiang578.com/categories/七年之旅/"},{"name":"军机处","slug":"军机处","permalink":"https://xiang578.com/categories/军机处/"}],"tags":[{"name":"book","slug":"book","permalink":"https://xiang578.com/tags/book/"},{"name":"nlp","slug":"nlp","permalink":"https://xiang578.com/tags/nlp/"},{"name":"google","slug":"google","permalink":"https://xiang578.com/tags/google/"},{"name":"bert","slug":"bert","permalink":"https://xiang578.com/tags/bert/"},{"name":"code","slug":"code","permalink":"https://xiang578.com/tags/code/"},{"name":"transformer","slug":"transformer","permalink":"https://xiang578.com/tags/transformer/"},{"name":"algorithm","slug":"algorithm","permalink":"https://xiang578.com/tags/algorithm/"},{"name":"reinforcement-learning","slug":"reinforcement-learning","permalink":"https://xiang578.com/tags/reinforcement-learning/"},{"name":"life","slug":"life","permalink":"https://xiang578.com/tags/life/"},{"name":"movie","slug":"movie","permalink":"https://xiang578.com/tags/movie/"},{"name":"game","slug":"game","permalink":"https://xiang578.com/tags/game/"},{"name":"ml","slug":"ml","permalink":"https://xiang578.com/tags/ml/"},{"name":"gdbt","slug":"gdbt","permalink":"https://xiang578.com/tags/gdbt/"},{"name":"fm","slug":"fm","permalink":"https://xiang578.com/tags/fm/"},{"name":"ftrl","slug":"ftrl","permalink":"https://xiang578.com/tags/ftrl/"},{"name":"reinforcementlearning","slug":"reinforcementlearning","permalink":"https://xiang578.com/tags/reinforcementlearning/"},{"name":"monthly-issue","slug":"monthly-issue","permalink":"https://xiang578.com/tags/monthly-issue/"},{"name":"course","slug":"course","permalink":"https://xiang578.com/tags/course/"},{"name":"blog","slug":"blog","permalink":"https://xiang578.com/tags/blog/"},{"name":"writing","slug":"writing","permalink":"https://xiang578.com/tags/writing/"},{"name":"lstm","slug":"lstm","permalink":"https://xiang578.com/tags/lstm/"},{"name":"widedeep","slug":"widedeep","permalink":"https://xiang578.com/tags/widedeep/"},{"name":"didi","slug":"didi","permalink":"https://xiang578.com/tags/didi/"},{"name":"lr","slug":"lr","permalink":"https://xiang578.com/tags/lr/"},{"name":"dnn","slug":"dnn","permalink":"https://xiang578.com/tags/dnn/"},{"name":"ctr","slug":"ctr","permalink":"https://xiang578.com/tags/ctr/"},{"name":"iOS","slug":"iOS","permalink":"https://xiang578.com/tags/iOS/"},{"name":"app","slug":"app","permalink":"https://xiang578.com/tags/app/"},{"name":"best of","slug":"best-of","permalink":"https://xiang578.com/tags/best-of/"},{"name":"machine learing","slug":"machine-learing","permalink":"https://xiang578.com/tags/machine-learing/"},{"name":"gbdt","slug":"gbdt","permalink":"https://xiang578.com/tags/gbdt/"},{"name":"rime","slug":"rime","permalink":"https://xiang578.com/tags/rime/"},{"name":"hexo","slug":"hexo","permalink":"https://xiang578.com/tags/hexo/"},{"name":"vps","slug":"vps","permalink":"https://xiang578.com/tags/vps/"},{"name":"hack","slug":"hack","permalink":"https://xiang578.com/tags/hack/"},{"name":"travis","slug":"travis","permalink":"https://xiang578.com/tags/travis/"},{"name":"cos","slug":"cos","permalink":"https://xiang578.com/tags/cos/"},{"name":"cnn","slug":"cnn","permalink":"https://xiang578.com/tags/cnn/"},{"name":"alexnet","slug":"alexnet","permalink":"https://xiang578.com/tags/alexnet/"},{"name":"font","slug":"font","permalink":"https://xiang578.com/tags/font/"},{"name":"github","slug":"github","permalink":"https://xiang578.com/tags/github/"},{"name":"travisci","slug":"travisci","permalink":"https://xiang578.com/tags/travisci/"},{"name":"Sleep","slug":"Sleep","permalink":"https://xiang578.com/tags/Sleep/"},{"name":"workflow","slug":"workflow","permalink":"https://xiang578.com/tags/workflow/"},{"name":"alfred","slug":"alfred","permalink":"https://xiang578.com/tags/alfred/"},{"name":"ifttt","slug":"ifttt","permalink":"https://xiang578.com/tags/ifttt/"},{"name":"7","slug":"7","permalink":"https://xiang578.com/tags/7/"},{"name":"wrtiting","slug":"wrtiting","permalink":"https://xiang578.com/tags/wrtiting/"},{"name":"zufe","slug":"zufe","permalink":"https://xiang578.com/tags/zufe/"},{"name":"reading","slug":"reading","permalink":"https://xiang578.com/tags/reading/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://xiang578.com/tags/MachineLearning/"},{"name":"podcast","slug":"podcast","permalink":"https://xiang578.com/tags/podcast/"},{"name":"gtd","slug":"gtd","permalink":"https://xiang578.com/tags/gtd/"},{"name":"shcool","slug":"shcool","permalink":"https://xiang578.com/tags/shcool/"},{"name":"taskpaper","slug":"taskpaper","permalink":"https://xiang578.com/tags/taskpaper/"},{"name":"timetrack","slug":"timetrack","permalink":"https://xiang578.com/tags/timetrack/"},{"name":"applescript","slug":"applescript","permalink":"https://xiang578.com/tags/applescript/"},{"name":"omnifocus","slug":"omnifocus","permalink":"https://xiang578.com/tags/omnifocus/"},{"name":"dash","slug":"dash","permalink":"https://xiang578.com/tags/dash/"},{"name":"evernote","slug":"evernote","permalink":"https://xiang578.com/tags/evernote/"},{"name":"mubu","slug":"mubu","permalink":"https://xiang578.com/tags/mubu/"},{"name":"school","slug":"school","permalink":"https://xiang578.com/tags/school/"},{"name":"wiznote","slug":"wiznote","permalink":"https://xiang578.com/tags/wiznote/"},{"name":"system","slug":"system","permalink":"https://xiang578.com/tags/system/"},{"name":"acm","slug":"acm","permalink":"https://xiang578.com/tags/acm/"},{"name":"mac","slug":"mac","permalink":"https://xiang578.com/tags/mac/"},{"name":"software","slug":"software","permalink":"https://xiang578.com/tags/software/"},{"name":"wordpress","slug":"wordpress","permalink":"https://xiang578.com/tags/wordpress/"},{"name":"backwpup","slug":"backwpup","permalink":"https://xiang578.com/tags/backwpup/"},{"name":"Mac","slug":"Mac","permalink":"https://xiang578.com/tags/Mac/"},{"name":"Alfred","slug":"Alfred","permalink":"https://xiang578.com/tags/Alfred/"},{"name":"network","slug":"network","permalink":"https://xiang578.com/tags/network/"},{"name":"computer","slug":"computer","permalink":"https://xiang578.com/tags/computer/"},{"name":"ipad","slug":"ipad","permalink":"https://xiang578.com/tags/ipad/"},{"name":"hdu","slug":"hdu","permalink":"https://xiang578.com/tags/hdu/"},{"name":"english","slug":"english","permalink":"https://xiang578.com/tags/english/"}]}