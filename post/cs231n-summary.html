<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="rgba(0,0,0,0)"><meta name="generator" content="Hexo 6.3.0">
<link rel="preconnect" href="//fonts.loli.net" crossorigin>
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="rgba(0,0,0,0)">
  <link rel="manifest" href="/manifest.json">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-center-atom.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"xiang578.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"buttons","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="去年学习这门做的部分笔记，现在分享出来。笔记格式有些问题，持续整理中。   大量内容参考 mbadry1&#x2F;CS231n-2017-Summary  Table of contents Table of contents Course Info 01. Introduction to CNN for visual recognition 02. Image classification">
<meta property="og:type" content="blog">
<meta property="og:title" content="Standford CS231n 2017 课程部分总结">
<meta property="og:url" content="https://xiang578.com/post/cs231n-summary.html">
<meta property="og:site_name" content="算法花园">
<meta property="og:description" content="去年学习这门做的部分笔记，现在分享出来。笔记格式有些问题，持续整理中。   大量内容参考 mbadry1&#x2F;CS231n-2017-Summary  Table of contents Table of contents Course Info 01. Introduction to CNN for visual recognition 02. Image classification">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15387098703701.jpg">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15387110626779.jpg">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15388343449162.jpg">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15388374738605.jpg">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15390088806657.jpg">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15390092983570.jpg">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15395012747510.jpg">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15499574039274.jpg">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15705415499052.jpg">
<meta property="og:image" content="https://media.xiang578.com/2019-10-08-15705420412305.jpg">
<meta property="article:published_time" content="2019-12-04T09:58:39.000Z">
<meta property="article:modified_time" content="2023-01-09T11:02:42.704Z">
<meta property="article:author" content="Ryen Xiang">
<meta property="article:tag" content="ml">
<meta property="article:tag" content="course">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://media.xiang578.com/2019-10-08-15387098703701.jpg">


<link rel="canonical" href="https://xiang578.com/post/cs231n-summary.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://xiang578.com/post/cs231n-summary.html","path":"post/cs231n-summary.html","title":"Standford CS231n 2017 课程部分总结"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Standford CS231n 2017 课程部分总结 | 算法花园</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="算法花园" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">算法花园</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-desktop fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-week"><a href="/tags/weekly/" rel="section"><i class="fa fa-book fa-fw"></i>Week</a></li><li class="menu-item menu-item-link"><a href="/link/" rel="section"><i class="fa fa-link fa-fw"></i>Link</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Table-of-contents"><span class="nav-number">1.</span> <span class="nav-text">Table of contents</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Course-Info"><span class="nav-number">2.</span> <span class="nav-text">Course Info</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#01-Introduction-to-CNN-for-visual-recognition"><span class="nav-number">3.</span> <span class="nav-text">01. Introduction to CNN for visual recognition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#02-Image-classification"><span class="nav-number">4.</span> <span class="nav-text">02. Image classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#03-Loss-function-and-optimization"><span class="nav-number">5.</span> <span class="nav-text">03. Loss function and optimization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#04-Introduction-to-Neural-network"><span class="nav-number">6.</span> <span class="nav-text">04. Introduction to Neural network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#05-Convolutional-neural-networks-CNNs"><span class="nav-number">7.</span> <span class="nav-text">05. Convolutional neural networks (CNNs)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#06-Training-neural-networks-I"><span class="nav-number">8.</span> <span class="nav-text">06. Training neural networks I</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#07-Training-neural-networks-II"><span class="nav-number">9.</span> <span class="nav-text">07. Training neural networks II</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#09-CNN-architectures"><span class="nav-number">10.</span> <span class="nav-text">09. CNN architectures</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">11.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ryen Xiang"
      src="https://media.xiang578.com/avatar-ryenxx.jpeg">
  <p class="site-author-name" itemprop="name">Ryen Xiang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">105</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">80</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xiang578.com/post/cs231n-summary.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://media.xiang578.com/avatar-ryenxx.jpeg">
      <meta itemprop="name" content="Ryen Xiang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="算法花园">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Standford CS231n 2017 课程部分总结 | 算法花园">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Standford CS231n 2017 课程部分总结
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-12-04 17:58:39" itemprop="dateCreated datePublished" datetime="2019-12-04T17:58:39+08:00">2019-12-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-01-09 19:02:42" itemprop="dateModified" datetime="2023-01-09T19:02:42+08:00">2023-01-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>10k</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <blockquote>
<p>去年学习这门做的部分笔记，现在分享出来。<br>笔记格式有些问题，持续整理中。</p>
</blockquote>
<ul>
<li>大量内容参考 <a target="_blank" rel="noopener" href="https://github.com/mbadry1/CS231n-2017-Summary/blob/master/README.md">mbadry1&#x2F;CS231n-2017-Summary</a></li>
</ul>
<h2 id="Table-of-contents"><a href="#Table-of-contents" class="headerlink" title="Table of contents"></a>Table of contents</h2><ul>
<li><a href="#table-of-contents">Table of contents</a></li>
<li><a href="#course-info">Course Info</a></li>
<li><a href="#01-introduction-to-cnn-for-visual-recognition">01. Introduction to CNN for visual recognition</a></li>
<li><a href="#02-image-classification">02. Image classification</a></li>
<li><a href="#03-loss-function-and-optimization">03. Loss function and optimization</a></li>
<li><a href="#04-introduction-to-neural-network">04. Introduction to Neural network</a></li>
<li><a href="#05-convolutional-neural-networks-cnns">05. Convolutional neural networks (CNNs)</a></li>
<li><a href="#06-training-neural-networks-i">06. Training neural networks I</a></li>
<li><a href="#07-training-neural-networks-ii">07. Training neural networks II</a></li>
<li><a href="#09-cnn-architectures">09. CNN architectures</a></li>
<li><a href="#reference">Reference</a></li>
</ul>
<h2 id="Course-Info"><a href="#Course-Info" class="headerlink" title="Course Info"></a>Course Info</h2><ul>
<li>主页: <a target="_blank" rel="noopener" href="http://cs231n.stanford.edu/">http://cs231n.stanford.edu/</a></li>
<li>视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av17204303">斯坦福深度学习课程CS231N 2017中文字幕版+全部作业参考_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></li>
<li>大纲：<a target="_blank" rel="noopener" href="http://cs231n.stanford.edu/2017/syllabus">Syllabus | CS 231N</a></li>
<li>课件：<a target="_blank" rel="noopener" href="http://cs231n.stanford.edu/slides/2017/">Index of &#x2F;slides&#x2F;2017</a></li>
<li>笔记：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21930884">贺完结！CS231n官方笔记授权翻译总集篇发布</a></li>
<li>作业仓库：<a target="_blank" rel="noopener" href="https://github.com/xiang578/MachineLearning/tree/master/CS231n">MachineLearning&#x2F;CS231n at master · xiang578&#x2F;MachineLearning</a></li>
<li>总课时: <strong>16</strong></li>
</ul>
<h2 id="01-Introduction-to-CNN-for-visual-recognition"><a href="#01-Introduction-to-CNN-for-visual-recognition" class="headerlink" title="01. Introduction to CNN for visual recognition"></a>01. Introduction to CNN for visual recognition</h2><ul>
<li>视觉地出现促进了物种竞争。</li>
<li>ImageNet 是由李飞飞维护的一个大型图像数据集。</li>
<li>自从 2012 年 CNN 出现之后，图像分类的错误率大幅度下降。 神经网络的深度也从 7 层增加到 2015 年的 152 层。截止到目前，机器分类准确率已经超过人类，所以 ImageNet 也不再举办相关比赛。</li>
<li>CNN 在 1998 年就被提出，但是这几年才流行开来。主要原因有：1) 硬件发展，并行计算速度提到 2）大规模带标签的数据集。</li>
<li>Gola: Understand how to write from scratch, debug and train convolutional neural networks.</li>
</ul>
<h2 id="02-Image-classification"><a href="#02-Image-classification" class="headerlink" title="02. Image classification"></a>02. Image classification</h2><ul>
<li>图像由一大堆没有规律的数字组成，无法直观的进行分类，所以存在语义鸿沟。分类的挑战有：视角变化、大小变化、形变、遮挡、光照条件、背景干扰、类内差异。<ul>
<li><img data-src="https://media.xiang578.com/2019-10-08-15387098703701.jpg"></li>
</ul>
</li>
<li>Data-Driven Approach<ul>
<li>Collect a dataset of images and labels</li>
<li>Use Machine Learning to train a classifier</li>
<li>Evaluate the classifier on new images</li>
</ul>
</li>
<li>图像分类流程：输入、学习、评估</li>
<li>图像分类数据集：<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>，这个数据集包含了60000张32X32的小图像。每张图像都有10种分类标签中的一种。这60000张图像被分为包含50000张图像的训练集和包含10000张图像的测试集。</li>
<li>一种直观的图像分类算法：K-nearest neighbor(knn)<ul>
<li>为每一张需要预测的图片找到距离最近的 k 张训练集中的图片，然后选着在这 k 张图片中出现次数最多的标签做为预测图片的标签（多数表决）。</li>
<li>训练过程：记录所有的数据和标签 ${O(1)}$</li>
<li>预测过程：预测给定图片的标签 ${O(n)}$</li>
<li>Hyperparameters：k and the distance Metric</li>
<li>Distance Metric<ul>
<li>L1 distance(Manhattan Distance)</li>
<li>L2 distance(Euclidean Distance)</li>
</ul>
</li>
<li>knn 缺点<ul>
<li>Very slow at test time</li>
<li>Distance metrics on pixels are not informative</li>
</ul>
</li>
<li>反例：下面四张图片的 L2 距离相同<ul>
<li><img data-src="https://media.xiang578.com/2019-10-08-15387110626779.jpg" alt="-w622"></li>
</ul>
</li>
</ul>
</li>
<li>Hyperparameters: choices about the algorithm that we set ranther than learn</li>
<li>留一法 Setting Hyperparameters by Cross-validation:<ul>
<li>将数据划分为 f 个集合以及一个 test 集合，数据划分中药保证数据集的分布一致。</li>
<li>给定超参数，利用 f-1 个集合对算法进行训练，在剩下的一个集合中测试训练效果，重复这一个过程，直到所有的集合都当过测试集。</li>
<li>选择在训练集中平均表现最好的超参数。</li>
</ul>
</li>
<li>Linear classification: <code>Y = wX + b</code><ul>
<li>b 为 bias，调节模型对结果的偏好</li>
<li>通过最小化损失函数来，来确定 w 和 b 的值。</li>
</ul>
</li>
<li><strong>Linear SVM</strong>:  classifier is an option for solving the image classification problem, but the curse of dimensions makes it stop improving at some point. @todo</li>
<li><strong>Logistics Regression</strong>: 无法解决非线性的图像数据</li>
</ul>
<h2 id="03-Loss-function-and-optimization"><a href="#03-Loss-function-and-optimization" class="headerlink" title="03. Loss function and optimization"></a>03. Loss function and optimization</h2><ul>
<li>通过 Loss function 评估参数质量<ul>
<li>比如 $$L&#x3D;\frac{1}{N}\sum_iL_i\left(f\left(x_i,W\right),y_i\right)$$</li>
</ul>
</li>
<li>Multiclass SVM loss 多分类支持向量机损失函数<ul>
<li>$$L_i&#x3D;\sum_{j \neq y_j}\max\left(0,s_j-s_{y_i}+1\right)$$</li>
<li>这种损失函数被称为合页损失 Hinge loss</li>
<li>SVM 的损失函数要求正确类别的分类分数要比其他类别的高出一个边界值。</li>
<li>L2-SVM 中使用平方折叶损失函数$$\max(0,-)^2$$能更强烈地惩罚过界的边界值。但是选择使用哪一个损失函数需要通过实验结果来判断。</li>
<li>举例<ul>
<li><img data-src="https://media.xiang578.com/2019-10-08-15388343449162.jpg"></li>
<li>根据上面的公式计算：$$L &#x3D; \max(0,437.9-(-96.8)) + \max(0,61.95-(-96.8))&#x3D;695.45$$</li>
<li>猫的分类得分在三个类别中不是最高得，所以我们需要继续优化。</li>
</ul>
</li>
</ul>
</li>
<li>Suppose that we found a W such that L &#x3D; 0. Is this W unique?<ul>
<li>No! 2W is also has L &#x3D; 0!</li>
</ul>
</li>
<li>Regularization: 正则化，向某一些特定的权值 W 添加惩罚，防止权值过大，减轻模型的复杂度，提高泛化能力，也避免在数据集中过拟合现象。<ul>
<li>$$L&#x3D;\frac{1}{N}\sum_iL_i\left(f\left(x_i,W\right),y_i\right) + \lambda R(W)$$</li>
<li><code>R</code> 正则项 $$\lambda$$ 正则化参数</li>
</ul>
</li>
<li>常用正则化方法<ul>
<li>L2$$\begin{matrix} R(W)&#x3D;\sum_{k}\sum_l W^2_{k,l} \end{matrix}$$</li>
<li>L1$$\begin{matrix} R(W)&#x3D;\sum_{k}\sum_l \left\vert W_{k,l} \right\vert \end{matrix}$$</li>
<li>Elastic net(L1 + L2): $$\begin{matrix} R(W)&#x3D;\sum_{k}\sum_l \beta W^2_{k,l} + \left\vert W_{k,l} \right\vert \end{matrix}$$</li>
<li>Dropout</li>
<li>Batch normalization</li>
<li>etc</li>
</ul>
</li>
<li>L2 惩罚倾向于更小更分散的权重向量，L1 倾向于稀疏项。</li>
<li>Softmax function：<ul>
<li>$$f_j(z)&#x3D;\frac{e^{s_i}}{\sum e^{s_j}}$$</li>
<li>该分类器将输出向量 f 中的评分值解释为没有归一化的对数概率，通过归一化之后，所有概率之和为1。</li>
<li>Loss 也称交叉熵损失 cross-entropy loss $$L_i &#x3D; - \log\left(\frac{e^{s_i}}{\sum e^{s_j}}\right)$$</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.<span class="built_in">sum</span>(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.<span class="built_in">max</span>(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.<span class="built_in">sum</span>(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure>
<ul>
<li>SVM 和 Softmax 比较<ol>
<li>评分，SVM 的损失函数鼓励正确的分类的分值比其他分类的分值高出一个边界值。</li>
<li>对数概率，Softmax 鼓励正确的分类归一化后的对数概率提高。</li>
<li>Softmax 永远不会满意，SVM 超过边界值就满意了。</li>
</ol>
</li>
<li>Optimization：最优化过程<ul>
<li>Follow the slope<ul>
<li><img data-src="https://media.xiang578.com/2019-10-08-15388374738605.jpg"></li>
</ul>
</li>
</ul>
</li>
<li>梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量，它是各个维度的斜率组成的向量。<ul>
<li>Numerical gradient: Approximate, slow, easy to write. (But its useful in debugging.)</li>
<li>Analytic gradient: Exact, Fast, Error-prone. (Always used in practice)</li>
<li>实际应用中使用分析梯度法，但可以用数值梯度法去检查分析梯度法的正确性。</li>
</ul>
</li>
<li>利用梯度优化参数的过程：<code>W = W - learning_rate * W_grad</code></li>
<li>learning_rate 被称为是学习率，是一个比较重要的超参数</li>
<li>Stochastic Gradient Descent SGD 随机梯度下降法<ul>
<li>每次使用一小部分的数据进行梯度计算，这样可以加快计算的速度。</li>
<li>每个批量中只有1个数据样本，则被称为随机梯度下降（在线梯度下降）</li>
</ul>
</li>
<li>图像分类任务中三大关键部分：<ol>
<li>评分函数</li>
<li>损失函数：量化某个具体参数 ${W}$ 的质量</li>
<li>最优化：寻找能使得损失函数值最小化的参数 ${W}$ 的过程</li>
</ol>
</li>
</ul>
<h2 id="04-Introduction-to-Neural-network"><a href="#04-Introduction-to-Neural-network" class="headerlink" title="04. Introduction to Neural network"></a>04. Introduction to Neural network</h2><ul>
<li>反向传播：在已知损失函数 ${L}$ 的基础上，如何计算导数${\nabla _WL}$？</li>
<li>计算图<ul>
<li>由于计算神经网络中某些函数的梯度很困难，所以引入计算图的概念简化运算。</li>
<li>在计算图中，对应函数所有的变量转换成为计算图的输入，运算符号变成图中的一个节点（门单元）。</li>
</ul>
</li>
<li>反向传播：从尾部开始，根据链式法则递归地向前计算梯度，一直到网络的输入端。<ul>
<li><img data-src="https://media.xiang578.com/2019-10-08-15390088806657.jpg" alt="-w1107"></li>
<li>绿色是正向传播，红色是反向传播。</li>
</ul>
</li>
<li>对于计算图中的每一个节点，我们需要计算这个节点上的局部梯度，之后根据链式法则反向传递梯度。</li>
<li>Sigmoid 函数：${f(w,x)&#x3D;\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}}$<ul>
<li><img data-src="https://media.xiang578.com/2019-10-08-15390092983570.jpg"></li>
<li>对于门单元 ${\frac{1}{x}}$，求导的结果是 ${-\frac{1}{x^2}}$，输入为 1.37，梯度返回值为 1.00，所以这一步中的梯度是 ${(\frac{-1}{1.37^2})*1.00&#x3D;-0.53}$。</li>
<li>模块化思想：对 ${\sigma(x)&#x3D;\frac{1}{1+e^{-x}}}$ 求导的结果是 ${(1-\sigma(x))\sigma(x)}$。如果 sigmoid 表达式输入值为 1.0 时，则前向传播中的结果是 0.73。根据求导结果计算可得局部梯度是 ${(1-0.73)*0.73&#x3D;0.2}$。</li>
</ul>
</li>
<li>Modularized implementation: forward&#x2F;backwar API</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultuplyGate</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  x,y are scalars</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x,y</span>):</span><br><span class="line">    z = x*y</span><br><span class="line">    self.x = x  <span class="comment"># Cache</span></span><br><span class="line">    self.y = y	<span class="comment"># Cache</span></span><br><span class="line">    <span class="comment"># We cache x and y because we know that the derivatives contains them.</span></span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">dz</span>):</span><br><span class="line">    dx = self.y * dz         <span class="comment">#self.y is dx</span></span><br><span class="line">    dy = self.x * dz</span><br><span class="line">    <span class="keyword">return</span> [dx, dy]</span><br></pre></td></tr></table></figure>
<ul>
<li>深度学习框架中会实现的门单元：Multiplication、Max、Plus、Minus、Sigmoid、Convolution</li>
<li>常用计算单元<ul>
<li><strong>加法门单元：</strong>把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。</li>
<li><strong>取最大值门单元：</strong>将梯度转给前向传播中值最大的那个输入，其余输入的值为0。</li>
<li><strong>乘法门单元：</strong>等值缩放。局部梯度就是输入值，但是需要相互交换，然后根据链式法则乘以输出值得梯度。</li>
</ul>
</li>
<li>Neural NetWorks<ul>
<li>(Before) Linear score function $$f &#x3D; Wx$$</li>
<li>(Now) 2-layer Neural NetWork $$f&#x3D;W_2\max(0,W_1x)$$</li>
<li>ReLU $$\max(0,x)$$ 是激活函数，如果不使用激活函数，神经网络只是线性模型的组合，无法拟合非线性情况。</li>
<li>神经网络是更复杂的模型的基础组件</li>
</ul>
</li>
</ul>
<h2 id="05-Convolutional-neural-networks-CNNs"><a href="#05-Convolutional-neural-networks-CNNs" class="headerlink" title="05. Convolutional neural networks (CNNs)"></a>05. Convolutional neural networks (CNNs)</h2><ul>
<li>这一轮浪潮的开端：<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlxNet</a></li>
<li>卷积神经网络<ul>
<li>Fully Connected Layer 全连接层：这一层中所有的神经元链接在一起。</li>
<li>Convolution Layer：<ul>
<li>通过参数共享来控制参数的数量。Parameter sharing</li>
<li>Sparsity of connections</li>
</ul>
</li>
<li>卷积神经网络能学习到不同层次的输入信息</li>
<li>常见的神经网络结构：<code>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC</code></li>
<li>使用小的卷积核大小的优点：多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好地特征。而且使用的参数也会更少</li>
</ul>
</li>
<li>计算卷积层输出<ul>
<li>stride 是卷积核在移动时的步长</li>
<li>通用公式 (N-F)&#x2F;stride + 1<ul>
<li>stride 1 &#x3D;&gt; (7-3)&#x2F;1 + 1 &#x3D; 5</li>
<li>stride 2 &#x3D;&gt; (7-3)&#x2F;2 + 1 &#x3D; 3</li>
<li>stride 3 &#x3D;&gt; (7-3)&#x2F;3 + 1 &#x3D; 2.33</li>
</ul>
</li>
<li>Zero pad the border: 用零填充所有的边界，保证输入输出图像大小相同，保留图像边缘信息，提高算法性能<ul>
<li>步长为 1 时，需要填充的边界计算公式：(F-1)&#x2F;2<ul>
<li>F &#x3D; 3 &#x3D;&gt; zero pad with 1</li>
<li>F &#x3D; 5 &#x3D;&gt; zero pad with 2</li>
<li>F &#x3D; 7 &#x3D;&gt; zero pad with 3</li>
</ul>
</li>
</ul>
</li>
<li>计算例子<ul>
<li>输入大小 <code>32*32*3</code> 卷积大小 10 5*5 stride 1 pad 2</li>
<li>output <code>32*32*10</code></li>
<li>每个 filter 的参数数量：<code>5*5*3+1 =76</code> bias</li>
<li>全部参数数量 76*10&#x3D;760</li>
</ul>
</li>
</ul>
</li>
<li>卷积常用超参数设置<ul>
<li>卷积使用小尺寸滤波器</li>
<li>卷积核数量 K 一般为 2 的次方倍</li>
<li>卷积核的空间尺寸 F</li>
<li>步长 S</li>
<li>零填充数量 P</li>
</ul>
</li>
<li>Pooling layer<ul>
<li>降维，减少参数数量。在卷积层中不对数据做降采样</li>
<li>卷积特征往往对应某个局部的特征，通过池化聚合这些局部特征为全局特征</li>
</ul>
</li>
<li>Max pooling<ul>
<li>2*2 stride 2</li>
<li>避免区域重叠</li>
</ul>
</li>
<li>Average pooling</li>
</ul>
<h2 id="06-Training-neural-networks-I"><a href="#06-Training-neural-networks-I" class="headerlink" title="06. Training neural networks I"></a>06. Training neural networks I</h2><ul>
<li><p>Activation functions 激活函数</p>
<ul>
<li>不使用激活函数，最后的输出会是输入的线性组合。利用激活函数对数据进行修正。</li>
<li><img data-src="https://media.xiang578.com/2019-10-08-15395012747510.jpg"></li>
<li>Sigmoid<ul>
<li>限制输出在 [0,1]区间内</li>
<li>firing rate</li>
<li>二分类输出层激活函数</li>
<li>Problem<ul>
<li>梯度消失：x很大或者很小时，梯度很小，接近于0（考虑图像中的斜率。无法得到梯度反馈。</li>
<li>输出不是 0 均值的数据，梯度更新效率低</li>
<li>exp is a bit compute expensive</li>
</ul>
</li>
</ul>
</li>
<li>tanh<ul>
<li>输出范围 [-1, 1]</li>
<li>0 均值</li>
<li>x 很大时，依然没有梯度</li>
<li>${f(x)&#x3D;\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}}$</li>
<li>${1-(tanh(x))^2}$</li>
</ul>
</li>
<li>RELU rectified linear unit 线性修正单元<ul>
<li>一半空间梯度不会饱和，计算速度快，对结果又有精确的计算</li>
<li>不是 0 均值</li>
</ul>
</li>
<li>Leaky RELU<ul>
<li><code>leaky_RELU(x) = max(0.01x, x)</code></li>
<li>梯度不会消失</li>
<li>需要学习参数</li>
</ul>
</li>
<li>ELU<ul>
<li>比 ReLU 好用</li>
<li>反激活机制</li>
</ul>
</li>
<li>Maxout <ul>
<li>maxout(x) &#x3D; max(w1.T<em>x + b1, w2.T</em>x + b2)</li>
<li>梯度不会消失</li>
<li>增大参数数量</li>
</ul>
</li>
<li>激活函数选取经验<ul>
<li>使用 ReLU ，但要仔细选取学习率</li>
<li>尝试使用 Leaky ReLU Maxout ELU</li>
<li>使用 tanh 时，不要抱有太大的期望</li>
<li>不要使用 sigmoid</li>
</ul>
</li>
</ul>
</li>
<li><p>数据预处理 Data Preprocessing</p>
<ul>
<li>均值减法：对数据中每个独立特征减去平均值，从几何上来看是将数据云的中心都迁移到原点。</li>
<li>归一化：将数据中的所有维度都归一化，使数值范围近似相等。但是在图像处理中，像素的数值范围几乎一致，所以不需要额外处理。</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line">X /= np.std(X, axis =<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>图像归一化<ul>
<li>Subtract the mean image AlexNet<ul>
<li>mean image 32,32,3</li>
</ul>
</li>
<li>Subtract per-channel mean VGGNet<ul>
<li>mean along each channel &#x3D; 3 numbers</li>
</ul>
</li>
<li>如果需要进行均值减法时，均值应该是从训练集中的图片平均值，然后训练集、验证集、测试集中的图像再减去这个平均值。</li>
</ul>
</li>
<li>Weight Initialization<ul>
<li>全零初始化<ul>
<li>网络中的每个神经元都计算出相同的输出，然后它们就会在反向传播中计算出相同的梯度。神经元之间会从源头上对称。</li>
</ul>
</li>
<li>Small random numbers<ul>
<li>初始化权值要非常接近 0 又不能等于 0。将权重初始化为很小的数值，以此来打破对称性</li>
<li>randn 函数是基于零均值和标准差的高斯分布的随机函数</li>
<li>W &#x3D; 0.01 * np.random.rand(D,H)</li>
<li>问题：一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度。会减小反向传播中的“梯度信号”，在深度网络中就会出现问题。</li>
</ul>
</li>
<li>Xavier initialization<ul>
<li>W &#x3D; np.random.rand(in, out) &#x2F; np.sqrt(in)</li>
<li>校准方差，解决输入数据量增长，随机初始化的神经元输出数据的分布中的方差也增大问题。</li>
</ul>
</li>
<li>He initialization<ul>
<li>W &#x3D; np.random.rand(in, out) &#x2F; np.sqrt(in&#x2F;2)</li>
</ul>
</li>
</ul>
</li>
<li>Batch normalization<ul>
<li>保证输入到神经网络中的数据服从标准的高斯分布</li>
<li>通过批量归一化可以加快训练的速度</li>
<li>步骤<ul>
<li>首先计算每个特征的平均值和平方差</li>
<li>通过减去平局值和除以方差对数据进行归一化</li>
<li><code>Result = gamma * normalizedX + beta</code><ul>
<li>对数据进行线性变换，相当于对数据分布进行一次移动，可以恢复数据之前的分布特征</li>
</ul>
</li>
</ul>
</li>
<li>BN 的好处<ul>
<li>加快训练速度</li>
<li>可以使用更快的而学习率</li>
<li>减少数据对初始化权值的敏感程度</li>
<li>相当于进行一次正则化</li>
</ul>
</li>
<li>BN 适用于卷积神经网络和常规的 DNN，在 RNN 和增强学习中表现不是很好</li>
</ul>
</li>
<li>Babysitting the Learning Provess</li>
<li>Hyperparameter Optimization<ul>
<li>Cross-validation 策略训练</li>
<li>小范围内随机搜索</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="07-Training-neural-networks-II"><a href="#07-Training-neural-networks-II" class="headerlink" title="07. Training neural networks II"></a>07. Training neural networks II</h2><ul>
<li>Optimization Algorithms:<ul>
<li><p>SGD 的问题</p>
<ul>
<li><code>x += - learning_rate * dx</code></li>
<li>梯度在某一个方向下降速度快，在其他方向下降缓慢</li>
<li>遇到局部最小值点，鞍点</li>
</ul>
</li>
<li><p>mini-batches GD</p>
<ul>
<li>Shuffling and Partitioning are the two steps required to build mini-batches</li>
<li>Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.</li>
</ul>
</li>
<li><p>SGD + Momentun</p>
<ul>
<li>动量更新：从物理学角度启发最优化问题</li>
<li><code>V[t+1] = rho * v[t] + dx; x[t+1] = x[t] - learningRate * V[t+1]</code></li>
<li>rho 被看做是动量，其物理意义与摩擦系数想类似，常取 0.9 或0.99</li>
<li>和 momentun 项更新方向相同的可以快速更新。</li>
<li>在 dx 中改变梯度方向后， rho 可以减少更新。momentun 能在相关方向加速 SGD，抑制震荡，加快收敛。</li>
</ul>
</li>
<li><p>Nestrov momentum</p>
<ul>
<li><img data-src="https://media.xiang578.com/2019-10-08-15499574039274.jpg"></li>
<li><code>v_prev = v; v = mu * v - learning_rate * dx; x += -mu * v_prev + (1 + mu) * v</code></li>
</ul>
</li>
<li><p>AdaGrad</p>
<ul>
<li>$n_t&#x3D;n_{t-1}+g^2_t$</li>
<li>$\Delta \theta _t &#x3D; -\frac{\eta}{\sqrt{n_t+\epsilon}}$</li>
<li>下面根号中会递推形成一个约束项。前期这一项比较大，能够放大梯度。后期这一项比较小，能约束梯度。</li>
<li>gt 的平方累积会使梯度趋向于 0</li>
</ul>
</li>
<li><p>RMSProp</p>
<ul>
<li>RMS 均方根</li>
<li>自适应学习率方法</li>
<li>求梯度的平方和平均数：<code>cache =  decay_rate * cache + (1 - decay_rate) * dx**2</code></li>
<li><code>x += - learning_rate * dx / (sqrt(cache) + eps)</code></li>
<li>依赖全局学习率</li>
</ul>
</li>
<li><p>Adam</p>
<ul>
<li>RMSProp + Momentum</li>
<li>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). </li>
<li>It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). </li>
<li>一阶到导数累积，二阶导数累积</li>
<li>It updates parameters in a direction based on combining information from “1” and “2”.</li>
<li>The update rule is, for $l &#x3D; 1, …, L$:<br>  $$\begin{cases}<br>  v_{dW^{[l]}} &#x3D; \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \<br>  v^{corrected}<em>{dW^{[l]}} &#x3D; \frac{v</em>{dW^{[l]}}}{1 - (\beta_1)^t} \<br>  s_{dW^{[l]}} &#x3D; \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \<br>  s^{corrected}<em>{dW^{[l]}} &#x3D; \frac{s</em>{dW^{[l]}}}{1 - (\beta_1)^t} \<br>  W^{[l]} &#x3D; W^{[l]} - \alpha \frac{v^{corrected}<em>{dW^{[l]}}}{\sqrt{s^{corrected}</em>{dW^{[l]}}} + \varepsilon}<br>  \end{cases}$$</li>
</ul>
<p>  where:</p>
<ul>
<li>t counts the number of steps taken of Adam </li>
<li>L is the number of layers</li>
<li>$\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages. </li>
<li>$\alpha$ is the learning rate</li>
<li>$\varepsilon$ is a very small number to avoid dividing by zero</li>
</ul>
<p>  特点：</p>
<ul>
<li>适用于大数据集和高维空间。</li>
<li>对不同的参数计算不同的自适应学习率。</li>
</ul>
</li>
<li><p>Learning decay</p>
<ul>
<li>学习率随着训练变化，比如每一轮在前一轮的基础上减少一半。</li>
<li>防止学习停止</li>
</ul>
</li>
<li><p>Second order optimization</p>
</li>
</ul>
</li>
<li>Regularization<ul>
<li>Dropout<ul>
<li>每一轮中随机使部分神经元失活，减少模型对神经元的依赖，增强模型的鲁棒性。</li>
</ul>
</li>
</ul>
</li>
<li>Transfer learning<ul>
<li>CNN 中的人脸识别，可以在大型的模型基础上利用少量的相关图像进行继续训练。</li>
</ul>
</li>
</ul>
<h2 id="09-CNN-architectures"><a href="#09-CNN-architectures" class="headerlink" title="09. CNN architectures"></a>09. CNN architectures</h2><ul>
<li>研究模型的方法：搞清楚每一层的输入和输出的大小关系。</li>
<li>LeNet - 5 [1998]<ul>
<li>60k 参数</li>
<li>深度加深，图片大小减少，通道数量增加</li>
<li>ac: Sigmod&#x2F;tanh</li>
</ul>
</li>
<li>AlexNet [2012]<ul>
<li>(227,227,3) （原文错误）</li>
<li>60M 参数</li>
<li>LRN：局部响应归一化，之后很少使用</li>
</ul>
</li>
<li>VGG - 16 [2015]<ul>
<li>138 M</li>
<li>结构不复杂，相对一致，图像缩小比例和通道增加数量有规律</li>
</ul>
</li>
<li>ZFNet [2013]<ul>
<li>在 AlexNet 的基础上修改<ul>
<li><code>CONV1</code>: change from (11 x 11 stride 4) to (7 x 7 stride 2)</li>
<li><code>CONV3,4,5</code>: instead of 384, 384, 256 filters use 512, 1024, 512</li>
</ul>
</li>
</ul>
</li>
<li>VGG [2014]<ul>
<li>模型中只使用 3*3 conv：与 77 卷积有相同的感受野，而且可以将网络做得更深。比如每一层可以获取到原始图像的范围：第一层 33，第二层 55，第三层 77。</li>
<li>前面的卷积层参数量很少，模型中大部分参数属于底部的全连接层。</li>
</ul>
</li>
</ul>
<p><img data-src="https://media.xiang578.com/2019-10-08-15705415499052.jpg"></p>
<ul>
<li>GoogLeNet<ul>
<li>引入 <code>Inception module</code><ul>
<li>design a good local network topology (network within a network) and then stack these modules on top of each other</li>
<li>该模块可以并行计算</li>
<li>conv 和 pool 层进行 padding，最后将结果 concat 在一起</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img data-src="https://media.xiang578.com/2019-10-08-15705420412305.jpg" alt="Reset"></p>
<ul>
<li>ResNet<ul>
<li>目标：深层模型表现不应该差于浅层模型，解决随着网络加深，准确率下降的问题。</li>
<li><code>Y = (W2* RELU(W1x+b1) + b2) + X</code></li>
<li>如果网络已经达到最优，继续加深网络，residual mapping会被设置为 0，一直保存网络最优的情况。</li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/22252270">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam） - 知乎</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ml/" rel="tag"># ml</a>
              <a href="/tags/course/" rel="tag"># course</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/post/blog-writing-checklist.html" rel="prev" title="算法花园写作风格清单">
                  <i class="fa fa-chevron-left"></i> 算法花园写作风格清单
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/post/2019-consumer-report.html" rel="next" title="2019 年软硬件指北">
                  2019 年软硬件指北 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备17004638号-1 </a>
  </div>

<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ryen Xiang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">202k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">3:04</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/lozad@1.16.0/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.1/dist/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>

  





</body>
</html>
